#!/usr/bin/env python3
"""
çµ±ä¸€çš„ç§‘ç ”è«–æ–‡è™•ç†ç®¡é“
æ•´åˆæ•¸æ“šçˆ¬å–ã€é¡åˆ¥éæ¿¾ã€æœŸåˆŠä¿¡æ¯è™•ç†ç­‰åŠŸèƒ½

åŠŸèƒ½ï¼š
1. å¾ arXiv çˆ¬å– overview/review/survey è«–æ–‡
2. éæ¿¾æŒ‡å®šé¡åˆ¥çš„è«–æ–‡
3. ç²å–æœŸåˆŠ/æœƒè­°ä¿¡æ¯
4. ç”Ÿæˆå¤šç¨®è¼¸å‡ºæ ¼å¼
5. æä¾›è©³ç´°çµ±è¨ˆå ±å‘Š
"""

import requests
import xml.etree.ElementTree as ET
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
from urllib.parse import quote
import argparse

@dataclass
class PipelineConfig:
    """ç®¡é“é…ç½®"""
    keywords: List[str] = None
    target_categories: List[str] = None
    max_papers_per_keyword: int = 5000
    delay_between_requests: float = 1.0
    enable_journal_lookup: bool = True
    output_formats: List[str] = None

    def __post_init__(self):
        if self.keywords is None:
            self.keywords = ["overview", "review", "survey"]
        if self.target_categories is None:
            self.target_categories = ["cs.CL", "cs.LG", "cs.DL", "cs.AI", "cs.IR", "cs.SE"]
        if self.output_formats is None:
            self.output_formats = ["json", "jsonl", "summary"]

class ArxivPipeline:
    """çµ±ä¸€çš„ arXiv è™•ç†ç®¡é“"""

    def __init__(self, config: PipelineConfig):
        self.config = config
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            'total_papers_found': 0,
            'papers_after_filtering': 0,
            'papers_with_doi': 0,
            'papers_with_journal_info': 0,
            'start_time': datetime.now(),
            'end_time': None,
            'categories_count': {},
            'keywords_count': {}
        }

    def search_arxiv_papers(self, keyword: str, max_results: int = 5000) -> List[Dict]:
        """æœç´¢ arXiv è«–æ–‡"""

        print(f"ğŸ” æœç´¢é—œéµè©: {keyword}")

        papers = []
        page = 0
        base_url = "http://export.arxiv.org/api/query"

        while len(papers) < max_results:
            start = page * 100

            params = {
                'search_query': f'ti:"{keyword}"',
                'start': start,
                'max_results': min(100, max_results - len(papers)),
                'sortBy': 'relevance',
                'sortOrder': 'descending'
            }

            try:
                response = self.session.get(base_url, params=params, timeout=30)
                response.raise_for_status()

                root = ET.fromstring(response.content)
                ns = {'atom': 'http://www.w3.org/2005/Atom', 'arxiv': 'http://arxiv.org/schemas/atom'}

                entries = root.findall('atom:entry', ns)

                if not entries:
                    break  # æ²’æœ‰æ›´å¤šçµæœ

                for entry in entries:
                    paper_data = self.extract_paper_data(entry, ns)
                    if paper_data:
                        papers.append(paper_data)

                page += 1
                time.sleep(self.config.delay_between_requests)

            except Exception as e:
                print(f"   âŒ æœç´¢å¤±æ•—: {e}")
                break

        print(f"   âœ… æ‰¾åˆ° {len(papers)} ç¯‡è«–æ–‡")
        return papers

    def extract_paper_data(self, entry, ns) -> Optional[Dict]:
        """å¾ XML æ¢ç›®æå–è«–æ–‡æ•¸æ“š"""

        try:
            # åŸºæœ¬ä¿¡æ¯
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text.strip() if title_elem is not None else ""

            id_elem = entry.find('atom:id', ns)
            arxiv_id = id_elem.text.split('/')[-1] if id_elem is not None else ""

            # æ‘˜è¦
            summary_elem = entry.find('atom:summary', ns)
            abstract = summary_elem.text.strip() if summary_elem is not None else ""

            # ä½œè€…
            authors = []
            for author in entry.findall('atom:author', ns):
                name_elem = author.find('atom:name', ns)
                if name_elem is not None:
                    authors.append(name_elem.text)
            authors_str = '; '.join(authors)

            # ç™¼ä½ˆæ—¥æœŸå’Œå¹´ä»½
            published_elem = entry.find('atom:published', ns)
            published_date = ""
            year = ""
            if published_elem is not None and published_elem.text:
                published_date = published_elem.text[:10]
                year = published_date[:4]

            # åˆ†é¡
            categories = []
            for category in entry.findall('atom:category', ns):
                if category.get('term'):
                    categories.append(category.get('term'))
            categories_str = '; '.join(categories)

            # éˆæ¥
            url_pdf = ""
            url_landing = ""
            doi = ""

            for link in entry.findall('atom:link', ns):
                if link.get('title') == 'pdf':
                    url_pdf = link.get('href', '')
                elif link.get('rel') == 'alternate':
                    url_landing = link.get('href', '')
                elif link.get('title') == 'doi':
                    doi = link.get('href', '')

            # æœŸåˆŠå¼•ç”¨ä¿¡æ¯
            journal_ref = ""
            if self.config.enable_journal_lookup:
                journal_ref_elem = entry.find('arxiv:journal_ref', ns)
                if journal_ref_elem is not None and journal_ref_elem.text:
                    journal_ref = journal_ref_elem.text.strip()

            return {
                'id': f"arxiv:{arxiv_id}",
                'source': 'arXiv',
                'title': title,
                'abstract': abstract,
                'authors': authors_str,
                'venue': 'arXiv',
                'year': year,
                'published_date': published_date,
                'doi': doi,
                'arxiv_id': arxiv_id,
                'url_pdf': url_pdf,
                'url_landing': url_landing,
                'categories': categories_str,
                'journal_ref': journal_ref,
                'search_keyword': '',  # ç¨å¾Œå¡«å……
                'retrieved_at': datetime.now().isoformat()
            }

        except Exception as e:
            print(f"   âŒ æå–è«–æ–‡æ•¸æ“šå¤±æ•—: {e}")
            return None

    def filter_by_categories(self, papers: List[Dict]) -> List[Dict]:
        """æ ¹æ“šé¡åˆ¥éæ¿¾è«–æ–‡"""

        print(f"ğŸ¯ éæ¿¾é¡åˆ¥: {self.config.target_categories}")

        filtered_papers = []
        target_cats = set(self.config.target_categories)

        for paper in papers:
            paper_cats = set(paper['categories'].split('; ')) if paper['categories'] else set()

            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›®æ¨™é¡åˆ¥
            if paper_cats & target_cats:
                filtered_papers.append(paper)

                # çµ±è¨ˆé¡åˆ¥
                for cat in paper_cats:
                    if cat in target_cats:
                        self.stats['categories_count'][cat] = self.stats['categories_count'].get(cat, 0) + 1

        print(f"   âœ… éæ¿¾å¾Œå‰©é¤˜ {len(filtered_papers)} ç¯‡è«–æ–‡")
        return filtered_papers

    def enhance_with_journal_info(self, papers: List[Dict]) -> List[Dict]:
        """ç‚ºè«–æ–‡æ·»åŠ æœŸåˆŠä¿¡æ¯"""

        if not self.config.enable_journal_lookup:
            return papers

        print("ğŸ“š å¢å¼·æœŸåˆŠä¿¡æ¯")

        enhanced_papers = []

        for paper in papers:
            if paper.get('doi'):
                journal_info = self.lookup_journal_quick(paper['doi'])
                if journal_info:
                    paper.update(journal_info)
                    self.stats['papers_with_journal_info'] += 1

            enhanced_papers.append(paper)

        return enhanced_papers

    def lookup_journal_quick(self, doi: str) -> Optional[Dict]:
        """å¿«é€ŸæŸ¥è©¢æœŸåˆŠä¿¡æ¯"""

        if not doi:
            return None

        try:
            # CrossRef API
            url = f"https://api.crossref.org/works/{doi}"
            response = self.session.get(url, timeout=10)

            if response.status_code == 200:
                data = response.json()
                work = data.get('message', {})

                return {
                    'journal': work.get('container-title', [''])[0] if work.get('container-title') else '',
                    'publisher': work.get('publisher', ''),
                    'volume': work.get('volume', ''),
                    'issue': work.get('issue', ''),
                    'pages': work.get('page', '')
                }

        except Exception as e:
            print(f"   âŒ æœŸåˆŠæŸ¥è©¢å¤±æ•—: {e}")

        return None

    def save_results(self, papers: List[Dict], output_dir: str = "."):
        """ä¿å­˜çµæœ"""

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON æ ¼å¼
        if 'json' in self.config.output_formats:
            json_file = f"{output_dir}/filtered_papers_{timestamp}.json"
            # è™•ç†çµ±è¨ˆä¸­çš„ datetime å°è±¡
            processed_stats = self.stats.copy()
            if 'start_time' in processed_stats:
                processed_stats['start_time'] = processed_stats['start_time'].isoformat()
            if 'end_time' in processed_stats and processed_stats['end_time']:
                processed_stats['end_time'] = processed_stats['end_time'].isoformat()

            data = {
                'metadata': {
                    'description': 'éæ¿¾å¾Œçš„è«–æ–‡æ•¸æ“šé›†',
                    'total_papers': len(papers),
                    'target_categories': self.config.target_categories,
                    'search_keywords': self.config.keywords,
                    'generated_at': datetime.now().isoformat(),
                    'stats': processed_stats
                },
                'papers': papers
            }

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜ JSON: {json_file}")

        # JSONL æ ¼å¼
        if 'jsonl' in self.config.output_formats:
            jsonl_file = f"{output_dir}/filtered_papers_{timestamp}.jsonl"
            with open(jsonl_file, 'w', encoding='utf-8') as f:
                # å¯«å…¥ metadataï¼ˆè™•ç† datetime å°è±¡ï¼‰
                processed_stats = self.stats.copy()
                if 'start_time' in processed_stats:
                    processed_stats['start_time'] = processed_stats['start_time'].isoformat()
                if 'end_time' in processed_stats and processed_stats['end_time']:
                    processed_stats['end_time'] = processed_stats['end_time'].isoformat()

                metadata = {
                    'type': 'metadata',
                    'description': 'éæ¿¾å¾Œçš„è«–æ–‡æ•¸æ“šé›†',
                    'total_papers': len(papers),
                    'target_categories': self.config.target_categories,
                    'search_keywords': self.config.keywords,
                    'generated_at': datetime.now().isoformat(),
                    'stats': processed_stats
                }
                f.write(json.dumps(metadata, ensure_ascii=False) + '\n')

                # å¯«å…¥è«–æ–‡æ•¸æ“š
                for paper in papers:
                    f.write(json.dumps(paper, ensure_ascii=False) + '\n')
            print(f"ğŸ’¾ ä¿å­˜ JSONL: {jsonl_file}")

        # æ‘˜è¦å ±å‘Š
        if 'summary' in self.config.output_formats:
            summary_file = f"{output_dir}/filtered_papers_{timestamp}_summary.txt"
            self.generate_summary_report(papers, summary_file)
            print(f"ğŸ’¾ ä¿å­˜æ‘˜è¦: {summary_file}")

    def generate_summary_report(self, papers: List[Dict], filename: str):
        """ç”Ÿæˆæ‘˜è¦å ±å‘Š"""

        with open(filename, 'w', encoding='utf-8') as f:
            f.write("ç§‘ç ”è«–æ–‡æ•¸æ“šé›†æ‘˜è¦å ±å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # åŸºæœ¬çµ±è¨ˆ
            f.write("ğŸ“Š åŸºæœ¬çµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"ç¸½è«–æ–‡æ•¸: {len(papers)}\n")
            f.write(f"ç›®æ¨™é¡åˆ¥: {', '.join(self.config.target_categories)}\n")
            f.write(f"æœç´¢é—œéµè©: {', '.join(self.config.keywords)}\n")
            f.write(f"è™•ç†æ™‚é–“: {self.stats['start_time'].strftime('%Y-%m-%d %H:%M:%S')} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # é¡åˆ¥åˆ†ä½ˆ
            f.write("ğŸ“ˆ é¡åˆ¥åˆ†ä½ˆ\n")
            f.write("-" * 30 + "\n")
            for cat, count in sorted(self.stats['categories_count'].items()):
                f.write(f"{cat}: {count} ç¯‡\n")
            f.write("\n")

            # é—œéµè©çµ±è¨ˆ
            f.write("ğŸ” é—œéµè©çµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            for keyword, count in sorted(self.stats['keywords_count'].items()):
                f.write(f"{keyword}: {count} ç¯‡\n")
            f.write("\n")

            # æœŸåˆŠä¿¡æ¯çµ±è¨ˆ
            if self.config.enable_journal_lookup:
                f.write("ğŸ“š æœŸåˆŠä¿¡æ¯çµ±è¨ˆ\n")
                f.write("-" * 30 + "\n")
                f.write(f"æœ‰ DOI çš„è«–æ–‡: {self.stats['papers_with_doi']}\n")
                f.write(f"æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡: {self.stats['papers_with_journal_info']}\n")
                f.write("\n")

            # æ¨£æœ¬è«–æ–‡
            f.write("ğŸ“ æ¨£æœ¬è«–æ–‡\n")
            f.write("-" * 30 + "\n")
            for i, paper in enumerate(papers[:5], 1):
                f.write(f"{i}. {paper['title'][:80]}...\n")
                f.write(f"   ä½œè€…: {paper['authors'][:50]}...\n")
                f.write(f"   é¡åˆ¥: {paper['categories']}\n")
                if paper.get('journal'):
                    f.write(f"   æœŸåˆŠ: {paper['journal']}\n")
                f.write("\n")

    def save_journal_papers(self, journal_papers: List[Dict], output_dir: str = "."):
        """ä¿å­˜æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡"""

        print("ğŸ“š ä¿å­˜æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡")
        print("=" * 60)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # JSON æ ¼å¼
        json_file = f"{output_dir}/journal_papers_{timestamp}.json"
        data = {
            'metadata': {
                'description': 'æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡æ•¸æ“šé›†',
                'total_papers': len(journal_papers),
                'target_categories': self.config.target_categories,
                'search_keywords': self.config.keywords,
                'generated_at': datetime.now().isoformat(),
                'stats': {
                    'total_journal_papers': len(journal_papers),
                    'journal_categories': {},
                    'journal_years': {}
                }
            },
            'papers': journal_papers
        }

        # çµ±è¨ˆæœŸåˆŠè«–æ–‡çš„é¡åˆ¥
        for paper in journal_papers:
            categories = paper.get('categories', '').split('; ')
            for cat in categories:
                if cat in self.config.target_categories:
                    data['metadata']['stats']['journal_categories'][cat] = \
                        data['metadata']['stats']['journal_categories'].get(cat, 0) + 1

        # çµ±è¨ˆæœŸåˆŠè«–æ–‡çš„å¹´ä»½
        for paper in journal_papers:
            year = paper.get('year', '')
            if year:
                data['metadata']['stats']['journal_years'][year] = \
                    data['metadata']['stats']['journal_years'].get(year, 0) + 1

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜æœŸåˆŠè«–æ–‡ JSON: {json_file}")

        # JSONL æ ¼å¼
        jsonl_file = f"{output_dir}/journal_papers_{timestamp}.jsonl"
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            # å¯«å…¥ metadata
            metadata = {
                'type': 'metadata',
                'description': 'æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡æ•¸æ“šé›†',
                'total_papers': len(journal_papers),
                'target_categories': self.config.target_categories,
                'search_keywords': self.config.keywords,
                'generated_at': datetime.now().isoformat(),
                'stats': data['metadata']['stats']
            }
            f.write(json.dumps(metadata, ensure_ascii=False) + '\n')

            # å¯«å…¥è«–æ–‡æ•¸æ“š
            for paper in journal_papers:
                f.write(json.dumps(paper, ensure_ascii=False) + '\n')
        print(f"ğŸ’¾ ä¿å­˜æœŸåˆŠè«–æ–‡ JSONL: {jsonl_file}")

        # ç”ŸæˆæœŸåˆŠè«–æ–‡å°ˆç”¨æ‘˜è¦
        summary_file = f"{output_dir}/journal_papers_{timestamp}_summary.txt"
        self.generate_journal_summary_report(journal_papers, summary_file)
        print(f"ğŸ’¾ ä¿å­˜æœŸåˆŠè«–æ–‡æ‘˜è¦: {summary_file}")

    def generate_journal_summary_report(self, journal_papers: List[Dict], filename: str):
        """ç”ŸæˆæœŸåˆŠè«–æ–‡å°ˆç”¨æ‘˜è¦å ±å‘Š"""

        with open(filename, 'w', encoding='utf-8') as f:
            f.write("æœŸåˆŠè«–æ–‡æ•¸æ“šé›†æ‘˜è¦å ±å‘Š\n")
            f.write("=" * 50 + "\n\n")

            # åŸºæœ¬çµ±è¨ˆ
            f.write("ğŸ“Š æœŸåˆŠè«–æ–‡çµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            f.write(f"æœŸåˆŠè«–æ–‡ç¸½æ•¸: {len(journal_papers)}\n")
            f.write(f"ä½”ç¸½æ•¸æ“šé›†æ¯”ä¾‹: {len(journal_papers)}/{self.stats['papers_after_filtering']} ({len(journal_papers)/self.stats['papers_after_filtering']*100:.1f}%)\n")
            f.write(f"ç›®æ¨™é¡åˆ¥: {', '.join(self.config.target_categories)}\n\n")

            # æœŸåˆŠè«–æ–‡é¡åˆ¥çµ±è¨ˆ
            journal_categories = {}
            for paper in journal_papers:
                categories = paper.get('categories', '').split('; ')
                for cat in categories:
                    if cat in self.config.target_categories:
                        journal_categories[cat] = journal_categories.get(cat, 0) + 1

            f.write("ğŸ“š æœŸåˆŠè«–æ–‡é¡åˆ¥åˆ†ä½ˆ\n")
            f.write("-" * 30 + "\n")
            for cat, count in sorted(journal_categories.items()):
                percentage = count / len(journal_papers) * 100
                f.write(f"{cat}: {count} ç¯‡ ({percentage:.1f}%)\n")
            f.write("\n")

            # æœŸåˆŠè«–æ–‡å¹´ä»½çµ±è¨ˆ
            journal_years = {}
            for paper in journal_papers:
                year = paper.get('year', '')
                if year:
                    journal_years[year] = journal_years.get(year, 0) + 1

            f.write("ğŸ“… æœŸåˆŠè«–æ–‡å¹´ä»½åˆ†ä½ˆ\n")
            f.write("-" * 30 + "\n")
            for year, count in sorted(journal_years.items(), reverse=True):
                f.write(f"{year}: {count} ç¯‡\n")
            f.write("\n")

            # æœŸåˆŠåç¨±çµ±è¨ˆ
            journal_names = {}
            for paper in journal_papers:
                journal = paper.get('journal_ref', '')
                if journal:
                    # ç°¡åŒ–æœŸåˆŠåç¨±ï¼ˆå–å‰50å€‹å­—ç¬¦ï¼‰
                    short_name = journal[:50] + "..." if len(journal) > 50 else journal
                    journal_names[short_name] = journal_names.get(short_name, 0) + 1

            f.write("ğŸ·ï¸ æœŸåˆŠä¾†æºçµ±è¨ˆ\n")
            f.write("-" * 30 + "\n")
            for journal, count in sorted(journal_names.items(), key=lambda x: x[1], reverse=True):
                f.write(f"{journal}: {count} ç¯‡\n")
            f.write("\n")

            # æ¨£æœ¬æœŸåˆŠè«–æ–‡
            f.write("ğŸ“ æ¨£æœ¬æœŸåˆŠè«–æ–‡\n")
            f.write("-" * 30 + "\n")
            for i, paper in enumerate(journal_papers[:5], 1):
                f.write(f"{i}. {paper['title'][:70]}...\n")
                f.write(f"   ä½œè€…: {paper['authors'][:50]}...\n")
                f.write(f"   æœŸåˆŠ: {paper['journal_ref'][:60]}...\n")
                f.write(f"   å¹´ä»½: {paper.get('year', 'N/A')}\n")
                f.write(f"   é¡åˆ¥: {paper['categories']}\n")
                f.write("\n")

    def run_pipeline(self) -> List[Dict]:
        """é‹è¡Œå®Œæ•´ç®¡é“"""

        print("ğŸš€ å•Ÿå‹•çµ±ä¸€ç§‘ç ”è«–æ–‡è™•ç†ç®¡é“")
        print("=" * 60)

        all_papers = []

        # 1. æœç´¢è«–æ–‡
        for keyword in self.config.keywords:
            papers = self.search_arxiv_papers(keyword, self.config.max_papers_per_keyword)

            # ç‚ºè«–æ–‡æ·»åŠ é—œéµè©æ¨™è¨˜
            for paper in papers:
                paper['search_keyword'] = keyword
                self.stats['keywords_count'][keyword] = self.stats['keywords_count'].get(keyword, 0) + 1

            all_papers.extend(papers)
            self.stats['total_papers_found'] += len(papers)

        # 2. é¡åˆ¥éæ¿¾
        filtered_papers = self.filter_by_categories(all_papers)
        self.stats['papers_after_filtering'] = len(filtered_papers)

        # 3. çµ±è¨ˆ DOI
        for paper in filtered_papers:
            if paper.get('doi'):
                self.stats['papers_with_doi'] += 1

        # 4. å¢å¼·æœŸåˆŠä¿¡æ¯
        if self.config.enable_journal_lookup:
            filtered_papers = self.enhance_with_journal_info(filtered_papers)

        # 5. ä¿å­˜çµæœ
        self.save_results(filtered_papers)

        # 6. é¡å¤–è¼¸å‡ºæœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡
        if self.config.enable_journal_lookup:
            journal_papers = [paper for paper in filtered_papers if paper.get('journal_ref')]
            if journal_papers:
                self.save_journal_papers(journal_papers)

        self.stats['end_time'] = datetime.now()

        # è¨ˆç®—æœŸåˆŠè«–æ–‡æ•¸é‡
        journal_papers_count = len([paper for paper in filtered_papers if paper.get('journal_ref')])

        # æœ€çµ‚çµ±è¨ˆ
        print("\nğŸ¯ è™•ç†å®Œæˆçµ±è¨ˆ")
        print("=" * 60)
        print(f"åŸå§‹è«–æ–‡æ•¸: {self.stats['total_papers_found']}")
        print(f"éæ¿¾å¾Œè«–æ–‡æ•¸: {self.stats['papers_after_filtering']}")
        print(f"æœ‰ DOI çš„è«–æ–‡: {self.stats['papers_with_doi']}")
        print(f"æœ‰æœŸåˆŠä¿¡æ¯çš„è«–æ–‡: {self.stats['papers_with_journal_info']}")
        if journal_papers_count > 0:
            print(f"ğŸ“š æœŸåˆŠè«–æ–‡æ•¸æ“šé›†: {journal_papers_count} ç¯‡è«–æ–‡å·²å–®ç¨ä¿å­˜")

        return filtered_papers

def main():
    """ä¸»å‡½æ•¸"""

    parser = argparse.ArgumentParser(description='çµ±ä¸€ç§‘ç ”è«–æ–‡è™•ç†ç®¡é“')
    parser.add_argument('--keywords', nargs='+',
                       default=['systematic review', 'systematic literature review', 'SLR',
                               'scoping review', 'PRISMA-ScR', 'systematic mapping',
                               'mapping study', 'SMS', 'tertiary study', 'bibliometric analysis',
                               'science mapping', 'co-citation', 'VOSviewer', 'CiteSpace',
                               'meta-analysis', 'umbrella review', 'review of reviews',
                               'taxonomy', 'classification', 'typology', 'framework',
                               'tutorial', 'primer', 'hands-on', 'how-to', 'state of the art',
                               'landscape', 'overview', 'survey', 'comparative study',
                               'benchmark', 'evaluation'],
                       help='æœç´¢é—œéµè©')
    parser.add_argument('--categories', nargs='+',
                       default=['cs.CL', 'cs.LG', 'cs.DL', 'cs.AI', 'cs.IR', 'cs.SE'],
                       help='ç›®æ¨™é¡åˆ¥')
    parser.add_argument('--max-papers', type=int, default=2000,
                       help='æ¯å€‹é—œéµè©æœ€å¤§è«–æ–‡æ•¸')
    parser.add_argument('--no-journal-lookup', action='store_true',
                       help='ç¦ç”¨æœŸåˆŠä¿¡æ¯æŸ¥è©¢')
    parser.add_argument('--output-formats', nargs='+', default=['json', 'jsonl', 'summary'],
                       choices=['json', 'jsonl', 'summary'],
                       help='è¼¸å‡ºæ ¼å¼')

    args = parser.parse_args()

    # å‰µå»ºé…ç½®
    config = PipelineConfig(
        keywords=args.keywords,
        target_categories=args.categories,
        max_papers_per_keyword=args.max_papers,
        enable_journal_lookup=not args.no_journal_lookup,
        output_formats=args.output_formats
    )

    # é‹è¡Œç®¡é“
    pipeline = ArxivPipeline(config)
    results = pipeline.run_pipeline()

    print(f"\nâœ… è™•ç†å®Œæˆï¼å…±ç²å– {len(results)} ç¯‡è«–æ–‡")

if __name__ == "__main__":
    main()
