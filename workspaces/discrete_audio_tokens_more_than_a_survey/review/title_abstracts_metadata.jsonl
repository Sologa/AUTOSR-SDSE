{"key": "itakura1968analysis", "query_title": "{Analysis synthesis telephony based on the maximum likelihood method}", "normalized_title": "analysis synthesis telephony based on the maximum likelihood method", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "atal1970speech", "query_title": "{Speech analysis and synthesis by linear prediction of the speech wave}", "normalized_title": "speech analysis and synthesis by linear prediction of the speech wave", "title": "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave", "abstract": "We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.", "source": "semantic_scholar", "source_id": "d738cfd0354ec9e70849057d5c2f20524a81e55a", "match_status": "exact_title", "missing_reason": null}
{"key": "schroeder1985code", "query_title": "{Code-excited linear prediction (CELP): High-quality speech at very low bit rates}", "normalized_title": "code excited linear prediction celp high quality speech at very low bit rates", "title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates", "abstract": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.", "source": "semantic_scholar", "source_id": "02876a3eb2ffe88cadfbc9ac779f272ccdc6f91a", "match_status": "exact_title", "missing_reason": null}
{"key": "tian2025espnet", "query_title": "{{ESP}net-{S}peech{LM}: An open speech language model toolkit}", "normalized_title": "espnet speechlm an open speech language model toolkit", "title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "abstract": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.", "source": "semantic_scholar", "source_id": "6245784c904ca849d4ac5b92de4878ee633997dd", "match_status": "exact_title", "missing_reason": null}
{"key": "hayashi2020discretalk", "query_title": "{Discretalk: Text-to-speech as a machine translation problem}", "normalized_title": "discretalk text to speech as a machine translation problem", "title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "abstract": "This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.", "source": "arxiv", "source_id": "2005.05525v1", "match_status": "exact_title", "missing_reason": null}
{"key": "vqvae2017", "query_title": "{Neural discrete representation learning}", "normalized_title": "neural discrete representation learning", "title": "Neural Discrete Representation Learning", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "source": "arxiv", "source_id": "1711.00937v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zeghidour2021soundstream", "query_title": "{{SoundStream}: An End-to-End Neural Audio Codec}", "normalized_title": "soundstream an end to end neural audio codec", "title": "SoundStream: An End-to-End Neural Audio Codec", "abstract": "We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.", "source": "arxiv", "source_id": "2107.03312v1", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2024versa", "query_title": "{{VERSA}: A Versatile Evaluation Toolkit for Speech, Audio, and Music}", "normalized_title": "versa a versatile evaluation toolkit for speech audio and music", "title": "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music", "abstract": "In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at https://github.com/wavlab-speech/versa.", "source": "arxiv", "source_id": "2412.17667v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024towards", "query_title": "{Towards audio language modeling-an overview}", "normalized_title": "towards audio language modeling an overview", "title": "Towards audio language modeling -- an overview", "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.", "source": "arxiv", "source_id": "2402.13236v1", "match_status": "exact_title", "missing_reason": null}
{"key": "jiang2023disentangled", "query_title": "{Disentangled feature learning for real-time neural speech coding}", "normalized_title": "disentangled feature learning for real time neural speech coding", "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding", "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.", "source": "arxiv", "source_id": "2211.11960v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lakhotia2021generative", "query_title": "{On generative spoken language modeling from raw audio}", "normalized_title": "on generative spoken language modeling from raw audio", "title": "On Generative Spoken Language Modeling from Raw Audio", "abstract": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1", "source": "semantic_scholar", "source_id": "7c39adb2049e79951dd6b92c970abaa4d81819b1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2023speechgen", "query_title": "{Speechgen: Unlocking the generative power of speech language models with prompts}", "normalized_title": "speechgen unlocking the generative power of speech language models with prompts", "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts", "abstract": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}", "source": "arxiv", "source_id": "2306.02207v3", "match_status": "exact_title", "missing_reason": null}
{"key": "zhen2020psychoacoustic", "query_title": "{Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding}", "normalized_title": "psychoacoustic calibration of loss functions for efficient end to end neural audio coding", "title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding", "abstract": "Conventional audio coding technologies commonly leverage human perception of sound, or psychoacoustics, to reduce the bitrate while preserving the perceptual quality of the decoded audio signals. For neural audio codecs, however, the objective nature of the loss function usually leads to suboptimal sound quality as well as high run-time complexity due to the large model size. In this work, we present a psychoacoustic calibration scheme to re-define the loss functions of neural audio coding systems so that it can decode signals more perceptually similar to the reference, yet with a much lower model complexity. The proposed loss function incorporates the global masking threshold, allowing the reconstruction error that corresponds to inaudible artifacts. Experimental results show that the proposed model outperforms the baseline neural codec twice as large and consuming 23.4% more bits per second. With the proposed method, a lightweight neural codec, with only 0.9 million parameters, performs near-transparent audio coding comparable with the commercial MPEG-1 Audio Layer III codec at 112 kbps.", "source": "arxiv", "source_id": "2101.00054v1", "match_status": "exact_title", "missing_reason": null}
{"key": "garbacea2019low", "query_title": "{Low bit-rate speech coding with VQ-VAE and a WaveNet decoder}", "normalized_title": "low bit rate speech coding with vq vae and a wavenet decoder", "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.", "source": "arxiv", "source_id": "1910.06464v1", "match_status": "exact_title", "missing_reason": null}
{"key": "jang2024personalized", "query_title": "{Personalized neural speech codec}", "normalized_title": "personalized neural speech codec", "title": "Personalized Neural Speech Codec", "abstract": "In this paper, we propose a personalized neural speech codec, envisioning that personalization can reduce the model complexity or improve perceptual speech quality. Despite the common usage of speech codecs where only a single talker is involved on each side of the communication, personalizing a codec for the specific user has rarely been explored in the literature. First, we assume speakers can be grouped into smaller subsets based on their perceptual similarity. Then, we also postulate that a group-specific codec can focus on the group's speech characteristics to improve its perceptual quality and computational efficiency. To this end, we first develop a Siamese network that learns the speaker embeddings from the LibriSpeech dataset, which are then grouped into underlying speaker clusters. Finally, we retrain the LPCNet-based speech codec baselines on each of the speaker clusters. Subjective listening tests show that the proposed personalization scheme introduces model compression while maintaining speech quality. In other words, with the same model complexity, personalized codecs produce better speech quality.", "source": "arxiv", "source_id": "2404.00791v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Kleijn2018wavenet", "query_title": "{Wavenet Based Low Rate Speech Coding}", "normalized_title": "wavenet based low rate speech coding", "title": "Wavenet based low rate speech coding", "abstract": "Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a WaveNet generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments confirm the high performance of the WaveNet based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not significantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model.", "source": "arxiv", "source_id": "1712.01120v1", "match_status": "exact_title", "missing_reason": null}
{"key": "valin2019real", "query_title": "{A real-time wideband neural vocoder at 1.6 kb/s using LPCNet}", "normalized_title": "a real time wideband neural vocoder at 1 6 kb s using lpcnet", "title": "A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet", "abstract": "Neural speech synthesis algorithms are a promising new approach for coding speech at very low bitrate. They have so far demonstrated quality that far exceeds traditional vocoders, at the cost of very high complexity. In this work, we present a low-bitrate neural vocoder based on the LPCNet model. The use of linear prediction and sparse recurrent networks makes it possible to achieve real-time operation on general-purpose hardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP and that uncompressed LPCNet can exceed the quality of a waveform codec operating at low bitrate. This opens the way for new codec designs based on neural synthesis models.", "source": "arxiv", "source_id": "1903.12087v2", "match_status": "exact_title", "missing_reason": null}
{"key": "valin2019lpcnet", "query_title": "{{LPCN}et: Improving neural speech synthesis through linear prediction}", "normalized_title": "lpcnet improving neural speech synthesis through linear prediction", "title": "LPCNET: Improving Neural Speech Synthesis through Linear Prediction", "abstract": "Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.", "source": "semantic_scholar", "source_id": "da7329db3e14cb7301e9ce95a131136bd85e24ba", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023neural", "query_title": "{Neural feature predictor and discriminative residual coding for low-bitrate speech coding}", "normalized_title": "neural feature predictor and discriminative residual coding for low bitrate speech coding", "title": "Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding", "abstract": "Low and ultra-low-bitrate neural speech coding achieves unprecedented coding gain by generating speech signals from compact speech features. This paper introduces additional coding efficiency in neural speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a recurrent neural predictor. The prediction can achieve a low-entropy residual representation, which we discriminatively code based on their contribution to the signal reconstruction. The harmonization of feature prediction and discriminative coding results in a dynamic bit allocation algorithm that spends more bits on unpredictable but rare events. As a result, we develop a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder. While the proposed method guarantees causality in its prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to LPCNet and Lyra V2 in the very low bitrates.", "source": "arxiv", "source_id": "2211.02506v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Jang2016Gumbel", "query_title": "{Categorical reparameterization with gumbel-softmax}", "normalized_title": "categorical reparameterization with gumbel softmax", "title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "source": "arxiv", "source_id": "1611.01144v5", "match_status": "exact_title", "missing_reason": null}
{"key": "maddison2016concrete", "query_title": "{The concrete distribution: A continuous relaxation of discrete random variables}", "normalized_title": "the concrete distribution a continuous relaxation of discrete random variables", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.", "source": "arxiv", "source_id": "1611.00712v3", "match_status": "exact_title", "missing_reason": null}
{"key": "della2025focalcodec", "query_title": "{FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks}", "normalized_title": "focalcodec low bitrate speech coding via focal modulation networks", "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.", "source": "arxiv", "source_id": "2502.04465v2", "match_status": "exact_title", "missing_reason": null}
{"key": "petermann2021harp", "query_title": "{Harp-net: Hyper-autoencoded reconstruction propagation for scalable neural audio coding}", "normalized_title": "harp net hyper autoencoded reconstruction propagation for scalable neural audio coding", "title": "HARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding", "abstract": "An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.", "source": "arxiv", "source_id": "2107.10843v2", "match_status": "exact_title", "missing_reason": null}
{"key": "Jiang2022EndtoEndNS", "query_title": "{End-to-End Neural Speech Coding for Real-Time Communications}", "normalized_title": "end to end neural speech coding for real time communications", "title": "End-to-End Neural Speech Coding for Real-Time Communications", "abstract": "Deep-learning based methods have shown their advantages in audio coding over traditional ones but limited attention has been paid on real-time communications (RTC). This paper proposes the TFNet, an end-to-end neural speech codec with low latency for RTC. It takes an encoder-temporal filtering-decoder paradigm that has seldom been investigated in audio coding. An interleaved structure is proposed for temporal filtering to capture both short-term and long-term temporal dependencies. Furthermore, with end-to-end optimization, the TFNet is jointly optimized with speech enhancement and packet loss concealment, yielding a one-for-all network for three tasks. Both subjective and objective results demonstrate the efficiency of the proposed TFNet.", "source": "arxiv", "source_id": "2201.09429v3", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024codecslt", "query_title": "{Codec-{SUPERB}@ {SLT} 2024: A lightweight benchmark for neural audio codec models}", "normalized_title": "codec superb at slt 2024 a lightweight benchmark for neural audio codec models", "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models", "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.", "source": "arxiv", "source_id": "2409.14085v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2021source", "query_title": "{Source-aware neural speech coding for noisy speech compression}", "normalized_title": "source aware neural speech coding for noisy speech compression", "title": "Source-Aware Neural Speech Coding for Noisy Speech Compression", "abstract": "This paper introduces a novel neural network-based speech coding system that can process noisy speech effectively. The proposed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources so that the more important source sounds better in the decoded signal. We target a new use case where the user on the receiver side cares about the quality of the non-speech components in speech communication, while the speech source still carries the most crucial information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech better than the baseline neural audio coding system, which is with no source-aware coding mechanism, and two conventional codecs.", "source": "arxiv", "source_id": "2008.12889v2", "match_status": "exact_title", "missing_reason": null}
{"key": "omran2023disentangling", "query_title": "{Disentangling speech from surroundings with neural embeddings}", "normalized_title": "disentangling speech from surroundings with neural embeddings", "title": "Disentangling speech from surroundings with neural embeddings", "abstract": "We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.", "source": "arxiv", "source_id": "2203.15578v2", "match_status": "exact_title", "missing_reason": null}
{"key": "Ji2024LanguageCodecRT", "query_title": "{Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models}", "normalized_title": "language codec reducing the gaps between discrete codec representation and speech language models", "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models", "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .", "source": "arxiv", "source_id": "2402.12208", "match_status": "exact_id", "missing_reason": null}
{"key": "Pan2024PSCodecAS", "query_title": "{PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders}", "normalized_title": "pscodec a series of high fidelity low bitrate neural speech codecs leveraging prompt encoders", "title": "PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders", "abstract": "Neural speech codecs have recently emerged as a focal point in the fields of speech compression and generation. Despite this progress, achieving high-quality speech reconstruction under low-bitrate scenarios remains a significant challenge. In this paper, we propose PSCodec, a series of neural speech codecs based on prompt encoders, comprising PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN, which are capable of delivering high-performance speech reconstruction with low bandwidths. Specifically, we first introduce PSCodec-Base, which leverages a pretrained speaker verification model-based prompt encoder (VPP-Enc) and a learnable Mel-spectrogram-based prompt encoder (MelP-Enc) to effectively disentangle and integrate voiceprint and Mel-related features in utterances. To further enhance feature utilization efficiency, we propose PSCodec-DRL-ICT, incorporating a structural similarity (SSIM) based disentangled representation loss (DRL) and an incremental continuous training (ICT) strategy. While PSCodec-DRL-ICT demonstrates impressive performance, its reliance on extensive hyperparameter tuning and multi-stage training makes it somewhat labor-intensive. To circumvent these limitations, we propose PSCodec-CasAN, utilizing an advanced cascaded attention network (CasAN) to enhance representational capacity of the entire system. Extensive experiments show that our proposed PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN all significantly outperform several state-of-the-art neural codecs, exhibiting substantial improvements in both speech reconstruction quality and speaker similarity under low-bitrate conditions.", "source": "arxiv", "source_id": "2404.02702v3", "match_status": "exact_title", "missing_reason": null}
{"key": "yang24h_interspeech", "query_title": "{Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens}", "normalized_title": "genhancer high fidelity speech enhancement via generative modeling on discrete codec tokens", "title": "Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens", "abstract": "We present a high-fidelity generative speech enhancement model, Genhancer, which generates clean speech as discrete codec tokens while conditioning on the input speech features. Discrete codec tokens provide an efficient latent domain in place of the conventional time or time-frequency domain of signals, so as to enable complex modeling of speech and allow generative modeling to enforce speaker consistency and content continuity. We provide insights into the best-fit generation scheme for enhancement among parallel prediction, auto-regression, and masking to demonstrate the benefits of conditioning on both pre-trained and jointly learned speech features. Subjective and objective tests show that Genhancer significantly improves audio quality and speaker-identity retention over the SOTA baselines, including conventional and generative ones while preserving content accuracy. Audio samples and supplement materials are available at https://minjekim.com/", "source": "semantic_scholar", "source_id": "ed6b910c45a0ab819c98b5fd4f9b8e4ca2ac7482", "match_status": "exact_title", "missing_reason": null}
{"key": "xue24lowlatency", "query_title": "{Low-Latency Speech Enhancement via Speech Token Generation}", "normalized_title": "low latency speech enhancement via speech token generation", "title": "Low-latency Speech Enhancement via Speech Token Generation", "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.", "source": "arxiv", "source_id": "2310.08981v3", "match_status": "exact_title", "missing_reason": null}
{"key": "Ahn2024HILCodecHA", "query_title": "{HILCodec: High-Fidelity and Lightweight Neural Audio Codec}", "normalized_title": "hilcodec high fidelity and lightweight neural audio codec", "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec", "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.", "source": "arxiv", "source_id": "2405.04752v2", "match_status": "exact_title", "missing_reason": null}
{"key": "Ye2024CodecDM", "query_title": "{Codec does matter: Exploring the semantic shortcoming of codec for audio language model}", "normalized_title": "codec does matter exploring the semantic shortcoming of codec for audio language model", "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model", "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)", "source": "arxiv", "source_id": "2408.17175v3", "match_status": "exact_title", "missing_reason": null}
{"key": "Guo2024AddressingIC", "query_title": "{Addressing Index Collapse of Large-Codebook Speech Tokenizer With Dual-Decoding Product-Quantized Variational Auto-Encoder}", "normalized_title": "addressing index collapse of large codebook speech tokenizer with dual decoding product quantized variational auto encoder", "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder", "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.", "source": "arxiv", "source_id": "2406.02940v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Zhou2024WMCodecEN", "query_title": "{WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification}", "normalized_title": "wmcodec end to end neural speech codec with deep watermarking for authenticity verification", "title": "WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification", "abstract": "Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness.", "source": "arxiv", "source_id": "2409.12121v3", "match_status": "exact_title", "missing_reason": null}
{"key": "Wu2024TS3CodecTS", "query_title": "{TS3-Codec: Transformer-Based Simple Streaming Single Codec}", "normalized_title": "ts3 codec transformer based simple streaming single codec", "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "abstract": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.", "source": "arxiv", "source_id": "2411.18803v2", "match_status": "exact_title", "missing_reason": null}
{"key": "Jiang2024MDCTCodecAL", "query_title": "{MDCTCodec: A Lightweight MDCT-Based Neural Audio Codec Towards High Sampling Rate and Low Bitrate Scenarios}", "normalized_title": "mdctcodec a lightweight mdct based neural audio codec towards high sampling rate and low bitrate scenarios", "title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios", "abstract": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.", "source": "arxiv", "source_id": "2411.00464v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Guo2024LSCodecLA", "query_title": "{LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec}", "normalized_title": "lscodec low bitrate and speaker decoupled discrete speech codec", "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.", "source": "arxiv", "source_id": "2410.15764v3", "match_status": "exact_title", "missing_reason": null}
{"key": "zaiem2023icassp", "query_title": "{Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study}", "normalized_title": "fine tuning strategies for faster inference using speech self supervised models a comparative study", "title": "Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study", "abstract": "Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings. In this context, it has been demonstrated that larger self-supervised feature extractors are crucial for achieving lower downstream ASR error rates. Thus, better performance might be sanctioned with longer inferences. This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times. Interestingly, we found that given enough downstream data, a simple downsampling of the input sequences outperforms the other methods with both low performance drops and high computational savings, reducing computations by 61.3% with an WER increase of only 0.81. Finally, we analyze the robustness of the comparison to changes in dataset conditions, revealing sensitivity to dataset size.", "source": "arxiv", "source_id": "2303.06740v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2021superb", "query_title": "{{SUPERB: Speech Processing Universal PERformance Benchmark}}", "normalized_title": "superb speech processing universal performance benchmark", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.", "source": "arxiv", "source_id": "2105.01051v4", "match_status": "exact_title", "missing_reason": null}
{"key": "Rabiner:1993dq", "query_title": "{Fundamentals of Speech Recognition}", "normalized_title": "fundamentals of speech recognition", "title": "Fundamentals of speech recognition", "abstract": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition.", "source": "semantic_scholar", "source_id": "2392e94df520e707e8b1422311bfdc552954dea9", "match_status": "exact_title", "missing_reason": null}
{"key": "busso2008iemocap", "query_title": "{{IEMOCAP}: Interactive emotional dyadic motion capture database}", "normalized_title": "iemocap interactive emotional dyadic motion capture database", "title": "IEMOCAP: interactive emotional dyadic motion capture database", "abstract": "Speech is the most significant mode of communication among human beings and a potential method for human-computer interaction (HCI) by using a microphone sensor. Quantifiable emotion recognition using these sensors from speech signals is an emerging area of research in HCI, which applies to multiple applications such as human-reboot interaction, virtual reality, behavior assessment, healthcare, and emergency call centers to determine the speaker’s emotional state from an individual’s speech. In this paper, we present major contributions for; (i) increasing the accuracy of speech emotion recognition (SER) compared to state of the art and (ii) reducing the computational complexity of the presented SER model. We propose an artificial intelligence-assisted deep stride convolutional neural network (DSCNN) architecture using the plain nets strategy to learn salient and discriminative features from spectrogram of speech signals that are enhanced in prior steps to perform better. Local hidden patterns are learned in convolutional layers with special strides to down-sample the feature maps rather than pooling layer and global discriminative features are learned in fully connected layers. A SoftMax classifier is used for the classification of emotions in speech. The proposed technique is evaluated on Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) datasets to improve accuracy by 7.85% and 4.5%, respectively, with the model size reduced by 34.5 MB. It proves the effectiveness and significance of the proposed SER technique and reveals its applicability in real-world applications.", "source": "semantic_scholar", "source_id": "5cf0d213f3253cd46673d955209f8463db73cc51", "match_status": "exact_title", "missing_reason": null}
{"key": "lecun2015deep", "query_title": "{Deep learning}", "normalized_title": "deep learning", "title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "source": "arxiv", "source_id": "1807.07987v2", "match_status": "exact_title", "missing_reason": null}
{"key": "GoodBengCour16", "query_title": "{Deep Learning}", "normalized_title": "deep learning", "title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "source": "arxiv", "source_id": "1807.07987v2", "match_status": "exact_title", "missing_reason": null}
{"key": "geminiteam2023gemini", "query_title": "{Gemini: A Family of Highly Capable Multimodal Models}", "normalized_title": "gemini a family of highly capable multimodal models", "title": "Gemini: A Family of Highly Capable Multimodal Models", "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.", "source": "arxiv", "source_id": "2312.11805v5", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2025esi", "query_title": "{Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model}", "normalized_title": "towards efficient speech text jointly decoding within one speech language model", "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model", "abstract": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.", "source": "arxiv", "source_id": "2506.04518v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kumar2023high", "query_title": "{High-Fidelity Audio Compression with Improved {RVQGAN}}", "normalized_title": "high fidelity audio compression with improved rvqgan", "title": "High-Fidelity Audio Compression with Improved RVQGAN", "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.", "source": "arxiv", "source_id": "2306.06546v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2023speechtokenizer", "query_title": "{{SpeechTokenizer}: Unified speech tokenizer for speech large language models}", "normalized_title": "speechtokenizer unified speech tokenizer for speech large language models", "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models", "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.", "source": "arxiv", "source_id": "2308.16692v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2023audiodec", "query_title": "{Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec}", "normalized_title": "audiodec an open source streaming high fidelity neural audio codec", "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec", "abstract": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.", "source": "arxiv", "source_id": "2305.16608v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023hifi", "query_title": "{{HiFi-Codec}: Group-residual Vector quantization for High Fidelity Audio Codec}", "normalized_title": "hifi codec group residual vector quantization for high fidelity audio codec", "title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec", "abstract": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}", "source": "arxiv", "source_id": "2305.02765v2", "match_status": "exact_title", "missing_reason": null}
{"key": "du2023funcodec", "query_title": "{{FunCodec}: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec}", "normalized_title": "funcodec a fundamental reproducible and integrable open source toolkit for neural speech codec", "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec", "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.", "source": "arxiv", "source_id": "2309.07405v2", "match_status": "exact_title", "missing_reason": null}
{"key": "borsos2023audiolm", "query_title": "{{AudioLM}: A Language Modeling Approach to Audio Generation}", "normalized_title": "audiolm a language modeling approach to audio generation", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.", "source": "arxiv", "source_id": "2209.03143v2", "match_status": "exact_title", "missing_reason": null}
{"key": "rubenstein2023audiopalm", "query_title": "{{AudioPaLM}: A Large Language Model That Can Speak and Listen}", "normalized_title": "audiopalm a large language model that can speak and listen", "title": "AudioPaLM: A Large Language Model That Can Speak and Listen", "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples", "source": "arxiv", "source_id": "2306.12925v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024viola", "query_title": "{Viola: Conditional language models for speech recognition, synthesis, and translation}", "normalized_title": "viola conditional language models for speech recognition synthesis and translation", "title": "VioLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional language model task via multi-task learning framework. To accomplish this, we first convert the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence prediction problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID), language IDs (LID), and LSTM-based acoustic embedding into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "source": "semantic_scholar", "source_id": "66fd85f42596fcd018d517a4bf6acc75100e59da", "match_status": "exact_title", "missing_reason": null}
{"key": "sicherman2023analysing", "query_title": "{Analysing discrete self supervised speech representation for spoken language modeling}", "normalized_title": "analysing discrete self supervised speech representation for spoken language modeling", "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling", "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations", "source": "arxiv", "source_id": "2301.00591v3", "match_status": "exact_title", "missing_reason": null}
{"key": "arora2025landscape", "query_title": "{On The Landscape of Spoken Language Models: A Comprehensive Survey}", "normalized_title": "on the landscape of spoken language models a comprehensive survey", "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "abstract": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.", "source": "arxiv", "source_id": "2504.08528v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2023lauragpt", "query_title": "{{LauraGPT}: Listen, attend, understand, and regenerate audio with {GPT}}", "normalized_title": "lauragpt listen attend understand and regenerate audio with gpt", "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT", "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.", "source": "arxiv", "source_id": "2310.04673v4", "match_status": "exact_title", "missing_reason": null}
{"key": "borsos2023soundstorm", "query_title": "{{SoundStorm}: Efficient Parallel Audio Generation}", "normalized_title": "soundstorm efficient parallel audio generation", "title": "SoundStorm: Efficient Parallel Audio Generation", "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.", "source": "arxiv", "source_id": "2305.09636v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023viola", "query_title": "{{VioLA}: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation}", "normalized_title": "viola unified codec language models for speech recognition synthesis and translation", "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "source": "arxiv", "source_id": "2305.16107v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023speechx", "query_title": "{SpeechX: Neural Codec Language Model as a Versatile Speech Transformer}", "normalized_title": "speechx neural codec language model as a versatile speech transformer", "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer", "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.", "source": "arxiv", "source_id": "2308.06873v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kreuk2022audiogen", "query_title": "{{AudioGen}: Textually Guided Audio Generation}", "normalized_title": "audiogen textually guided audio generation", "title": "AudioGen: Textually Guided Audio Generation", "abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen", "source": "arxiv", "source_id": "2209.15352v2", "match_status": "exact_title", "missing_reason": null}
{"key": "desplanques2020ecapa", "query_title": "{{ECAPA-TDNN}: Emphasized channel attention, propagation and aggregation in {TDNN} based speaker verification}", "normalized_title": "ecapa tdnn emphasized channel attention propagation and aggregation in tdnn based speaker verification", "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification", "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.", "source": "arxiv", "source_id": "2005.07143v3", "match_status": "exact_title", "missing_reason": null}
{"key": "ardila2019common", "query_title": "\"{Common Voice}: A Massively-Multilingual Speech Corpus\"", "normalized_title": "common voice a massively multilingual speech corpus", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.", "source": "arxiv", "source_id": "1912.06670v2", "match_status": "exact_title", "missing_reason": null}
{"key": "agostinelli2023musiclm", "query_title": "{{MusicLM}: Generating music from text}", "normalized_title": "musiclm generating music from text", "title": "MusicLM: Generating Music From Text", "abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.", "source": "arxiv", "source_id": "2301.11325v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023uniaudio", "query_title": "{{UniAudio}: Towards universal audio generation with large language models}", "normalized_title": "uniaudio towards universal audio generation with large language models", "title": "UniAudio: Towards Universal Audio Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.", "source": "semantic_scholar", "source_id": "61632c78b26ca366b5a1c8cbf3d0f50981126e8e", "match_status": "exact_title", "missing_reason": null}
{"key": "wells2022phonetic", "query_title": "{Phonetic analysis of self-supervised representations of {E}nglish speech}", "normalized_title": "phonetic analysis of self supervised representations of english speech", "title": "Phonetic Analysis of Self-supervised Representations of English Speech", "abstract": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when consid-ering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.", "source": "semantic_scholar", "source_id": "fc5c506887b53d0e132cb6a4fb239c92cd5c9830", "match_status": "exact_title", "missing_reason": null}
{"key": "polyak2021speech", "query_title": "{Speech Resynthesis from Discrete Disentangled Self-Supervised Representations}", "normalized_title": "speech resynthesis from discrete disentangled self supervised representations", "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.", "source": "arxiv", "source_id": "2104.00355v3", "match_status": "exact_title", "missing_reason": null}
{"key": "du2025codecfake", "query_title": "{CodecFake-Omni: A Large-Scale Codec-based Deepfake Speech Dataset}", "normalized_title": "codecfake omni a large scale codec based deepfake speech dataset", "title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset", "abstract": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.", "source": "arxiv", "source_id": "2501.08238", "match_status": "exact_id", "missing_reason": null}
{"key": "touvron2023llama", "query_title": "{Llama 2: Open foundation and fine-tuned chat models}", "normalized_title": "llama 2 open foundation and fine tuned chat models", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "source": "arxiv", "source_id": "2307.09288v2", "match_status": "exact_title", "missing_reason": null}
{"key": "defossez2024moshi", "query_title": "{Moshi: a speech-text foundation model for real-time dialogue}", "normalized_title": "moshi a speech text foundation model for real time dialogue", "title": "Moshi: a speech-text foundation model for real-time dialogue", "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.", "source": "arxiv", "source_id": "2410.00037v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kharitonov2021text", "query_title": "\"Text-Free Prosody-Aware Generative Spoken Language Modeling\"", "normalized_title": "text free prosody aware generative spoken language modeling", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "source": "arxiv", "source_id": "2109.03264v2", "match_status": "exact_title", "missing_reason": null}
{"key": "nguyen2023generative", "query_title": "{Generative spoken dialogue language modeling}", "normalized_title": "generative spoken dialogue language modeling", "title": "Generative Spoken Dialogue Language Modeling", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn-taking compared to a text-based cascaded model.", "source": "arxiv", "source_id": "2203.16502v2", "match_status": "exact_title", "missing_reason": null}
{"key": "popuri2022enhanced", "query_title": "{Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation}", "normalized_title": "enhanced direct speech to speech translation using self supervised pre training and data augmentation", "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation", "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "source": "arxiv", "source_id": "2204.02967v3", "match_status": "exact_title", "missing_reason": null}
{"key": "inaguma2022unity", "query_title": "{UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units}", "normalized_title": "unity two pass direct speech to speech translation with discrete units", "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units", "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.", "source": "arxiv", "source_id": "2212.08055v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2022speechprompt", "query_title": "{SpeechPrompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks}", "normalized_title": "speechprompt an exploration of prompt tuning on generative spoken language model for speech processing tasks", "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks", "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.", "source": "arxiv", "source_id": "2203.16773v3", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2023speechprompt", "query_title": "{Speechprompt v2: Prompt tuning for speech classification tasks}", "normalized_title": "speechprompt v2 prompt tuning for speech classification tasks", "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks", "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.", "source": "arxiv", "source_id": "2303.00733v1", "match_status": "exact_title", "missing_reason": null}
{"key": "hsu2023exploration", "query_title": "{An exploration of in-context learning for speech language model}", "normalized_title": "an exploration of in context learning for speech language model", "title": "An Exploration of In-Context Learning for Speech Language Model", "abstract": "Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner.", "source": "semantic_scholar", "source_id": "d628f3c65e0c8c3024d9ff0678d44383fc9fe27c", "match_status": "exact_title", "missing_reason": null}
{"key": "kuan2023towards", "query_title": "{Towards General-Purpose Text-Instruction-Guided Voice Conversion}", "normalized_title": "towards general purpose text instruction guided voice conversion", "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion", "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.", "source": "arxiv", "source_id": "2309.14324v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zeng2024glm", "query_title": "{G{LM}-4-voice: Towards intelligent and human-like end-to-end spoken chatbot}", "normalized_title": "glm 4 voice towards intelligent and human like end to end spoken chatbot", "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.", "source": "arxiv", "source_id": "2412.02612", "match_status": "exact_id", "missing_reason": null}
{"key": "huang2023dynamic", "query_title": "{Dynamic-{SUPERB}: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech}", "normalized_title": "dynamic superb towards a dynamic collaborative and comprehensive instruction tuning benchmark for speech", "title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech", "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.", "source": "arxiv", "source_id": "2309.09510v2", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2024dynamic", "query_title": "{Dynamic-superb phase-2: A collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks}", "normalized_title": "dynamic superb phase 2 a collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks", "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.", "source": "arxiv", "source_id": "2411.05361v2", "match_status": "exact_title", "missing_reason": null}
{"key": "xu2025qwen2", "query_title": "{Qwen2. 5-omni technical report}", "normalized_title": "qwen2 5 omni technical report", "title": "Qwen2.5-Omni Technical Report", "abstract": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", "source": "arxiv", "source_id": "2503.20215", "match_status": "exact_id", "missing_reason": null}
{"key": "liu2023pre", "query_title": "{Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing}", "normalized_title": "pre train prompt and predict a systematic survey of prompting methods in natural language processing", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "abstract": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.", "source": "arxiv", "source_id": "2107.13586v1", "match_status": "exact_title", "missing_reason": null}
{"key": "lester2021power", "query_title": "{The power of scale for parameter-efficient prompt tuning}", "normalized_title": "the power of scale for parameter efficient prompt tuning", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.", "source": "arxiv", "source_id": "2104.08691v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2023exploring", "query_title": "{Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study}", "normalized_title": "exploring speech recognition translation and understanding with discrete speech units a comparative study", "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study", "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.", "source": "arxiv", "source_id": "2309.15800v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023selm", "query_title": "{{SELM}: Speech Enhancement Using Discrete Tokens and Language Models}", "normalized_title": "selm speech enhancement using discrete tokens and language models", "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models", "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.", "source": "arxiv", "source_id": "2312.09747v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024codec", "query_title": "{{Codec-SUPERB}: An In-Depth Analysis of Sound Codec Models}", "normalized_title": "codec superb an in depth analysis of sound codec models", "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.", "source": "arxiv", "source_id": "2402.13071v3", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2023dub", "query_title": "\"{DUB}: Discrete Unit Back-translation for Speech Translation\"", "normalized_title": "dub discrete unit back translation for speech translation", "title": "DUB: Discrete Unit Back-translation for Speech Translation", "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST. Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation (DUB) to answer two questions: (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://github.com/0nutation/DUB.", "source": "arxiv", "source_id": "2305.11411v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024toksing", "query_title": "{TokSing: Singing Voice Synthesis based on Discrete Tokens}", "normalized_title": "toksing singing voice synthesis based on discrete tokens", "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens", "abstract": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.", "source": "arxiv", "source_id": "2406.08416v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yip2024towards", "query_title": "{Towards audio codec-based speech separation}", "normalized_title": "towards audio codec based speech separation", "title": "Towards Audio Codec-based Speech Separation", "abstract": "Recent improvements in neural audio codec (NAC) models have generated interest in adopting pre-trained codecs for a variety of speech processing applications to take advantage of the efficiencies gained from high compression, but these have yet been applied to the speech separation (SS) task. SS can benefit from high compression because the compute required for traditional SS models makes them impractical for many edge computing use cases. However, SS is a waveform-masking task where compression tends to introduce distortions that severely impact performance. Here we propose a novel task of Audio Codec-based SS, where SS is performed within the embedding space of a NAC, and propose a new model, Codecformer, to address this task. At inference, Codecformer achieves a 52x reduction in MAC while producing separation performance comparable to a cloud deployment of Sepformer. This method charts a new direction for performing efficient SS in practical scenarios.", "source": "arxiv", "source_id": "2406.12434v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chung2021w2v", "query_title": "{{w2v-BERT}: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training}", "normalized_title": "w2v bert combining contrastive learning and masked language modeling for self supervised speech pre training", "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training", "abstract": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.", "source": "arxiv", "source_id": "2108.06209v2", "match_status": "exact_title", "missing_reason": null}
{"key": "hsu2021hubert", "query_title": "{{HuBERT}: Self-supervised speech representation learning by masked prediction of hidden units}", "normalized_title": "hubert self supervised speech representation learning by masked prediction of hidden units", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.", "source": "arxiv", "source_id": "2106.07447v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2022wavlm", "query_title": "{{WavLM}: Large-scale self-supervised pre-training for full stack speech processing}", "normalized_title": "wavlm large scale self supervised pre training for full stack speech processing", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing", "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.", "source": "arxiv", "source_id": "2110.13900v5", "match_status": "exact_title", "missing_reason": null}
{"key": "baevski2020wav2vec", "query_title": "{wav2vec 2.0: A framework for self-supervised learning of speech representations}", "normalized_title": "wav2vec 2 0 a framework for self supervised learning of speech representations", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.", "source": "arxiv", "source_id": "2006.11477v3", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2023exploration", "query_title": "{Exploration of Efficient End-to-End {ASR} using Discretized Input from Self-Supervised Learning}", "normalized_title": "exploration of efficient end to end asr using discretized input from self supervised learning", "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.", "source": "arxiv", "source_id": "2305.18108v1", "match_status": "exact_title", "missing_reason": null}
{"key": "pasad2023comparative", "query_title": "{Comparative layer-wise analysis of self-supervised speech models}", "normalized_title": "comparative layer wise analysis of self supervised speech models", "title": "Comparative layer-wise analysis of self-supervised speech models", "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models.", "source": "arxiv", "source_id": "2211.03929v3", "match_status": "exact_title", "missing_reason": null}
{"key": "goodfellow2020generative", "query_title": "{Generative adversarial networks}", "normalized_title": "generative adversarial networks", "title": "Generative Adversarial Networks", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "source": "arxiv", "source_id": "1406.2661v1", "match_status": "exact_title", "missing_reason": null}
{"key": "kong2020hifigan", "query_title": "{{HiFi-GAN}: generative adversarial networks for efficient and high fidelity speech synthesis}", "normalized_title": "hifi gan generative adversarial networks for efficient and high fidelity speech synthesis", "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis", "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.", "source": "arxiv", "source_id": "2010.05646v2", "match_status": "exact_title", "missing_reason": null}
{"key": "oord2016wavenet", "query_title": "{WaveNet: A Generative Model for Raw Audio}", "normalized_title": "wavenet a generative model for raw audio", "title": "WaveNet: A Generative Model for Raw Audio", "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "source": "arxiv", "source_id": "1609.03499v2", "match_status": "exact_title", "missing_reason": null}
{"key": "prenger2018waveglow", "query_title": "{Waveglow: A Flow-based Generative Network for Speech Synthesis}", "normalized_title": "waveglow a flow based generative network for speech synthesis", "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "abstract": "In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.", "source": "arxiv", "source_id": "1811.00002v1", "match_status": "exact_title", "missing_reason": null}
{"key": "transformer", "query_title": "{Attention is All You Need}", "normalized_title": "attention is all you need", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "source": "arxiv", "source_id": "1706.03762v7", "match_status": "exact_title", "missing_reason": null}
{"key": "somos", "query_title": "{{SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis}}", "normalized_title": "somos the samsung open mos dataset for the evaluation of neural text to speech synthesis", "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis", "abstract": "In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the samples' variation depends only on the acoustic models. The synthesized utterances provide balanced and adequate domain and length coverage. We collect MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and share practices leading to reliable crowdsourced annotations for this task. We provide baseline results of state-of-the-art MOS prediction models on the SOMOS dataset and show the limitations that such models face when assigned to evaluate TTS utterances.", "source": "arxiv", "source_id": "2204.03040v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023towards", "query_title": "{Towards Universal Speech Discrete Tokens: A Case Study for {ASR} and {TTS}}", "normalized_title": "towards universal speech discrete tokens a case study for asr and tts", "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS", "abstract": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.", "source": "arxiv", "source_id": "2309.07377v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2021dwer", "query_title": "{Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement}", "normalized_title": "sequential multi frame neural beamforming for speech separation and enhancement", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "source": "arxiv", "source_id": "1911.07953v3", "match_status": "exact_title", "missing_reason": null}
{"key": "wichern2019wham", "query_title": "{{WHAM!}: Extending Speech Separation to Noisy Environments}", "normalized_title": "wham extending speech separation to noisy environments", "title": "WHAM!: Extending Speech Separation to Noisy Environments", "abstract": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches.", "source": "arxiv", "source_id": "1907.01160v1", "match_status": "exact_title", "missing_reason": null}
{"key": "le2019sdr", "query_title": "{{SDR}--half-baked or well done?}", "normalized_title": "sdr half baked or well done", "title": "SDR - half-baked or well done?", "abstract": "In speech enhancement and source separation, signal-to-noise ratio is a ubiquitous objective measure of denoising/separation quality. A decade ago, the BSS_eval toolkit was developed to give researchers worldwide a way to evaluate the quality of their algorithms in a simple, fair, and hopefully insightful way: it attempted to account for channel variations, and to not only evaluate the total distortion in the estimated signal but also split it in terms of various factors such as remaining interference, newly added artifacts, and channel errors. In recent years, hundreds of papers have been relying on this toolkit to evaluate their proposed methods and compare them to previous works, often arguing that differences on the order of 0.1 dB proved the effectiveness of a method over others. We argue here that the signal-to-distortion ratio (SDR) implemented in the BSS_eval toolkit has generally been improperly used and abused, especially in the case of single-channel separation, resulting in misleading results. We propose to use a slightly modified definition, resulting in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We present various examples of critical failure of the original SDR that SI-SDR overcomes.", "source": "arxiv", "source_id": "1811.02508v1", "match_status": "exact_title", "missing_reason": null}
{"key": "ljspeech17", "query_title": "{The {LJ} Speech Dataset}", "normalized_title": "the lj speech dataset", "title": "{The {LJ} Speech Dataset}", "abstract": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.", "source": "keithito", "source_id": "https://keithito.com/LJ-Speech-Dataset/", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "srivastava2014dropout", "query_title": "{Dropout: a simple way to prevent neural networks from overfitting}", "normalized_title": "dropout a simple way to prevent neural networks from overfitting", "title": "Dropout: a simple way to prevent neural networks from overfitting", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "source": "semantic_scholar", "source_id": "34f25a8704614163c4095b3ee2fc969b60de4698", "match_status": "exact_title", "missing_reason": null}
{"key": "nagrani2017voxceleb", "query_title": "{{VoxCeleb}: A Large-Scale Speaker Identification Dataset}", "normalized_title": "voxceleb a large scale speaker identification dataset", "title": "VoxCeleb: a large-scale speaker identification dataset", "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.", "source": "arxiv", "source_id": "1706.08612v2", "match_status": "exact_title", "missing_reason": null}
{"key": "1360861705599880960", "query_title": "\"{{UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022}}\"", "normalized_title": "utmos utokyo sarulab system for voicemos challenge 2022", "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022", "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests. Our system is based on ensemble learning of strong and weak learners. Strong learners incorporate several improvements to the previous fine-tuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features. In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks. In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.", "source": "arxiv", "source_id": "2204.02152v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zaiem2025speech", "query_title": "{Speech self-supervised representations benchmarking: a case for larger probing heads}", "normalized_title": "speech self supervised representations benchmarking a case for larger probing heads", "title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads", "abstract": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference costs, generalization and multi-level feature exploitation.", "source": "arxiv", "source_id": "2308.14456v2", "match_status": "exact_title", "missing_reason": null}
{"key": "defossez2022high", "query_title": "{High Fidelity Neural Audio Compression}", "normalized_title": "high fidelity neural audio compression", "title": "High Fidelity Neural Audio Compression", "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.", "source": "arxiv", "source_id": "2210.13438v1", "match_status": "exact_title", "missing_reason": null}
{"key": "DevlinCLT19", "query_title": "\"{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding\"", "normalized_title": "bert pre training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "source": "arxiv", "source_id": "1810.04805v2", "match_status": "exact_title", "missing_reason": null}
{"key": "newell2020useful", "query_title": "{How useful is self-supervised pretraining for visual tasks?}", "normalized_title": "how useful is self supervised pretraining for visual tasks", "title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "abstract": "Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}.", "source": "arxiv", "source_id": "2003.14323v1", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2021self", "query_title": "{Self-supervised learning: Generative or contrastive}", "normalized_title": "self supervised learning generative or contrastive", "title": "Self-supervised Learning: Generative or Contrastive", "abstract": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.", "source": "arxiv", "source_id": "2006.08218v5", "match_status": "exact_title", "missing_reason": null}
{"key": "JMLR:v24:22-1144", "query_title": "{{PaLM}: scaling language modeling with pathways}", "normalized_title": "palm scaling language modeling with pathways", "title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "source": "arxiv", "source_id": "2204.02311v5", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2023gpt", "query_title": "{{GPT} understands, too}", "normalized_title": "gpt understands too", "title": "GPT Understands, Too", "abstract": "Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.", "source": "arxiv", "source_id": "2103.10385v2", "match_status": "exact_title", "missing_reason": null}
{"key": "speechbrain_ravanelli", "query_title": "{Open-source conversational ai with speechbrain 1.0}", "normalized_title": "open source conversational ai with speechbrain 1 0", "title": "Open-Source Conversational AI with SpeechBrain 1.0", "abstract": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete \"recipes\" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.", "source": "arxiv", "source_id": "2407.00463v5", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024evaluating", "query_title": "{Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model}", "normalized_title": "evaluating text to speech synthesis from a large discrete token based speech language model", "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model", "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.", "source": "arxiv", "source_id": "2405.09768v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023neural", "query_title": "{Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers}", "normalized_title": "neural codec language models are zero shot text to speech synthesizers", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.", "source": "arxiv", "source_id": "2301.02111v1", "match_status": "exact_title", "missing_reason": null}
{"key": "kharitonov2023speak", "query_title": "{Speak, read and prompt: High-fidelity text-to-speech with minimal supervision}", "normalized_title": "speak read and prompt high fidelity text to speech with minimal supervision", "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision", "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.", "source": "arxiv", "source_id": "2302.03540v1", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2021discretization", "query_title": "{Discretization and re-synthesis: an alternative method to solve the cocktail party problem}", "normalized_title": "discretization and re synthesis an alternative method to solve the cocktail party problem", "title": "Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem", "abstract": "Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.", "source": "arxiv", "source_id": "2112.09382v2", "match_status": "exact_title", "missing_reason": null}
{"key": "erdogan2023tokensplit", "query_title": "{{TokenSplit}: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition}", "normalized_title": "tokensplit using discrete speech representations for direct refined and transcript conditioned speech separation and recognition", "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition", "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.", "source": "arxiv", "source_id": "2308.10415v1", "match_status": "exact_title", "missing_reason": null}
{"key": "puvvada2023discrete", "query_title": "{Discrete Audio Representation as an Alternative to {M}el-Spectrograms for Speaker and Speech Recognition}", "normalized_title": "discrete audio representation as an alternative to mel spectrograms for speaker and speech recognition", "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition", "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.", "source": "semantic_scholar", "source_id": "c78d2a875f6cfd3a7b70c8d4c03a828894146892", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2024codecfake", "query_title": "{Codecfake: Enhancing anti-spoofing models against deepfake audios from codec-based speech synthesis systems}", "normalized_title": "codecfake enhancing anti spoofing models against deepfake audios from codec based speech synthesis systems", "title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems", "abstract": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.", "source": "arxiv", "source_id": "2406.07237v1", "match_status": "exact_title", "missing_reason": null}
{"key": "ren2024emo", "query_title": "{EMO-Codec: An In-Depth Look at Emotion Preservation Capacity of Legacy and Neural Codec Models with Subjective and Objective Evaluations}", "normalized_title": "emo codec an in depth look at emotion preservation capacity of legacy and neural codec models with subjective and objective evaluations", "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations", "abstract": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.", "source": "arxiv", "source_id": "2407.15458v4", "match_status": "exact_title", "missing_reason": null}
{"key": "mousavi2024", "query_title": "{Semantic Token Tuning: How Should We Extract Discrete Audio Tokens from Self-Supervised Models?}", "normalized_title": "semantic token tuning how should we extract discrete audio tokens from self supervised models", "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.", "source": "semantic_scholar", "source_id": "40875b1e950fe6636ef277df4ed6d501f4933307", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "korvas_2014", "query_title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "normalized_title": "free english and czech telephone speech corpus shared under the cc by sa 3 0 license", "title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "abstract": "We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish also the trained models described in this paper. The data are released under the CC-BY-SA~3.0 license, the scripts are licensed under Apache~2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.", "source": "lrec_conf", "source_id": "lrec2014:535", "match_status": "exact_title", "missing_reason": null}
{"key": "han2020contextnet", "query_title": "{{ContextNet}: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context}", "normalized_title": "contextnet improving convolutional neural networks for automatic speech recognition with global context", "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context", "abstract": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.", "source": "arxiv", "source_id": "2005.03191v3", "match_status": "exact_title", "missing_reason": null}
{"key": "snyder2018x", "query_title": "{X-vectors: Robust {DNN} embeddings for speaker recognition}", "normalized_title": "x vectors robust dnn embeddings for speaker recognition", "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition", "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.", "source": "semantic_scholar", "source_id": "389cd9824428be98a710f5f4de67121a70c15fd3", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2018additive", "query_title": "{Additive margin softmax for face verification}", "normalized_title": "additive margin softmax for face verification", "title": "Additive Margin Softmax for Face Verification", "abstract": "In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax", "source": "arxiv", "source_id": "1801.05599v4", "match_status": "exact_title", "missing_reason": null}
{"key": "bastianelli2020slurp", "query_title": "\"{SLURP}: A Spoken Language Understanding Resource Package\"", "normalized_title": "slurp a spoken language understanding resource package", "title": "SLURP: A Spoken Language Understanding Resource Package", "abstract": "Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp.", "source": "arxiv", "source_id": "2011.13205v1", "match_status": "exact_title", "missing_reason": null}
{"key": "warden2017speech", "query_title": "{{Speech Commands}: A Dataset for Limited-Vocabulary Speech Recognition}", "normalized_title": "speech commands a dataset for limited vocabulary speech recognition", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "abstract": "Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.", "source": "arxiv", "source_id": "1804.03209v1", "match_status": "exact_title", "missing_reason": null}
{"key": "tacotron2", "query_title": "{Natural {TTS} Synthesis by Conditioning {WaveNet} on {Mel} Spectrogram Predictions}", "normalized_title": "natural tts synthesis by conditioning wavenet on mel spectrogram predictions", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "source": "arxiv", "source_id": "1712.05884v2", "match_status": "exact_title", "missing_reason": null}
{"key": "guided-attention", "query_title": "{Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention}", "normalized_title": "efficiently trainable text to speech system based on deep convolutional networks with guided attention", "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention", "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks. Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.", "source": "arxiv", "source_id": "1710.08969v2", "match_status": "exact_title", "missing_reason": null}
{"key": "levenshtein", "query_title": "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}", "normalized_title": "binary codes capable of correcting deletions insertions and reversals", "title": "Binary codes capable of correcting deletions, insertions, and reversals", "abstract": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.", "source": "semantic_scholar", "source_id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd", "match_status": "exact_title", "missing_reason": null}
{"key": "dwer", "query_title": "{Alternating Between Spectral and Spatial Estimation for Speech Separation and Enhancement}", "normalized_title": "alternating between spectral and spatial estimation for speech separation and enhancement", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "source": "arxiv", "source_id": "1911.07953", "match_status": "exact_id", "missing_reason": null}
{"key": "abouelenin2025phi", "query_title": "{Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras}", "normalized_title": "phi 4 mini technical report compact yet powerful multimodal language models via mixture of loras", "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "source": "arxiv", "source_id": "2503.01743v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chang2024speechprompt", "query_title": "{Speechprompt: Prompting speech language models for speech processing tasks}", "normalized_title": "speechprompt prompting speech language models for speech processing tasks", "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "source": "arxiv", "source_id": "2408.13040v1", "match_status": "exact_title", "missing_reason": null}
{"key": "cp-decomposition", "query_title": "{The Expression of a Tensor or a Polyadic as a Sum of Products}", "normalized_title": "the expression of a tensor or a polyadic as a sum of products", "title": "The Expression of a Tensor or a Polyadic as a Sum of Products", "abstract": "In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.", "source": "semantic_scholar", "source_id": "5265853dd172f6afe0ff1e820a0cc7bd0e58c765", "match_status": "exact_title", "missing_reason": null}
{"key": "dubey2024icassp", "query_title": "{{ICASSP} 2023 deep noise suppression challenge}", "normalized_title": "icassp 2023 deep noise suppression challenge", "title": "ICASSP 2023 Deep Noise Suppression Challenge", "abstract": "Deep Speech Enhancement Challenge is the 5th edition of deep noise suppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand Challenges. DNS challenges were organized during 2019-2023 to stimulate research in deep speech enhancement (DSE). Previous DNS challenges were organized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022. From prior editions, we learnt that improving signal quality (SIG) is challenging particularly in presence of simultaneously active interfering talkers and noise. This challenge aims to develop models for joint denosing, dereverberation and suppression of interfering talkers. When primary talker wears a headphone, certain acoustic properties of their speech such as direct-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it possible to suppress neighboring talkers even without enrollment data for primary talker. This motivated us to create two tracks for this challenge: (i) Track-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz) training data and testset, and each testclips has a corresponding enrollment data (10-30s duration) for primary talker. Each track invited submissions of personalized and non-personalized models all of which are evaluated through same subjective evaluation. Most models submitted to challenge were personalized models, same team is winner in both tracks where the best models has improvement of 0.145 and 0.141 in challenge's Score as compared to noisy blind testset.", "source": "arxiv", "source_id": "2303.11510v2", "match_status": "exact_title", "missing_reason": null}
{"key": "gemmeke2017audio", "query_title": "{{Audio Set}: An ontology and human-labeled dataset for audio events}", "normalized_title": "audio set an ontology and human labeled dataset for audio events", "title": "Audio Set: An ontology and human-labeled dataset for audio events", "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.", "source": "semantic_scholar", "source_id": "5ba2218b708ca64ab556e39d5997202e012717d5", "match_status": "exact_title", "missing_reason": null}
{"key": "fonseca2021fsd50k", "query_title": "{{FSD50K}: an open dataset of human-labeled sound events}", "normalized_title": "fsd50k an open dataset of human labeled sound events", "title": "FSD50K: An Open Dataset of Human-Labeled Sound Events", "abstract": "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.", "source": "arxiv", "source_id": "2010.00475v2", "match_status": "exact_title", "missing_reason": null}
{"key": "bogdanov2019mtg", "query_title": "{The {MTG-Jamendo} dataset for automatic music tagging}", "normalized_title": "the mtg jamendo dataset for automatic music tagging", "title": "The MTG-Jamendo Dataset for Automatic Music Tagging", "abstract": "Comunicació presentada a: ML4MD Machine Learning for Music Discovery Workshop del congrés ICML2019 celebrat el 15 de juny de 2019 a Long Beach, California.", "source": "semantic_scholar", "source_id": "23037085b0815455e6d47333089b925c8c0e21d5", "match_status": "exact_title", "missing_reason": null}
{"key": "mysore2014can", "query_title": "{Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?—{A} dataset, insights, and challenges}", "normalized_title": "can we automatically transform speech recorded on common consumer devices in real world environments into professional production quality speech a dataset insights and challenges", "title": "Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges", "abstract": "The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.", "source": "semantic_scholar", "source_id": "5aa5d627f4741eaeec41d59efe6589e71a3080c6", "match_status": "exact_title", "missing_reason": null}
{"key": "vctk2017", "query_title": "{{CSTR VCTK Corpus}: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)}", "normalized_title": "cstr vctk corpus english multi speaker corpus for cstr voice cloning toolkit version 0 92", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "match_status": "exact_title", "missing_reason": null}
{"key": "reddy2022dnsmos", "query_title": "{{DNSMOS P.835}: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors}", "normalized_title": "dnsmos p 835 a non intrusive perceptual objective speech quality metric to evaluate noise suppressors", "title": "DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors", "abstract": "Human subjective evaluation is the gold standard to evaluate speech quality optimized for human perception. Perceptual objective metrics serve as a proxy for subjective scores. We have recently developed a non-intrusive speech quality metric called Deep Noise Suppression Mean Opinion Score (DNSMOS) using the scores from ITU-T Rec. P.808 subjective evaluation. The P.808 scores reflect the overall quality of the audio clip. ITU-T Rec. P.835 subjective evaluation framework gives the standalone quality scores of speech and background noise in addition to the overall quality. In this work, we train an objective metric based on P.835 human ratings that outputs 3 scores: i) speech quality (SIG), ii) background noise quality (BAK), and iii) the overall quality (OVRL) of the audio. The developed metric is highly correlated with human ratings, with a Pearson's Correlation Coefficient (PCC)=0.94 for SIG and PCC=0.98 for BAK and OVRL. This is the first non-intrusive P.835 predictor we are aware of. DNSMOS P.835 is made publicly available as an Azure service.", "source": "arxiv", "source_id": "2110.01763v4", "match_status": "exact_title", "missing_reason": null}
{"key": "valentinibotinhao2016voicebank", "query_title": "{Investigating {RNN}-based speech enhancement methods for noise-robust Text-to-Speech}", "normalized_title": "investigating rnn based speech enhancement methods for noise robust text to speech", "title": "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech", "abstract": "The quality of text-to-speech (TTS) voices built from noisy speech is compromised. Enhancing the speech data before training has been shown to improve quality but voices built with clean speech are still preferred. In this paper we investigate two different approaches for speech enhancement to train TTS systems. In both approaches we train a recursive neural network (RNN) to map acoustic features extracted from noisy speech to features describing clean speech. The enhanced data is then used to train the TTS acoustic model. In one approach we use the features conventionally employed to train TTS acoustic models, i.e Mel cepstral (MCEP) coefficients, aperiodicity values and fundamental frequency (F0). In the other approach, following conventional speech enhancement methods, we train an RNN using only the MCEP coefficients extracted from the magnitude spectrum. The enhanced MCEP features and the phase extracted from noisy speech are combined to reconstruct the waveform which is then used to extract acoustic features to train the TTS system. We show that the second approach results in larger MCEP distortion but smaller F0 errors. Subjective evaluation shows that synthetic voices trained with data enhanced with this method were rated higher and with similar to scores to voices trained with clean speech.", "source": "semantic_scholar", "source_id": "0764300b1ba8c29e25e748c1df2851feba8ea1b6", "match_status": "exact_title", "missing_reason": null}
{"key": "cosentino2020librimix", "query_title": "{{LibriMix}: An Open-Source Dataset for Generalizable Speech Separation}", "normalized_title": "librimix an open source dataset for generalizable speech separation", "title": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation", "abstract": "In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.", "source": "arxiv", "source_id": "2005.11262v1", "match_status": "exact_title", "missing_reason": null}
{"key": "gulati_conformer", "query_title": "{Conformer: Convolution-augmented Transformer for Speech Recognition}", "normalized_title": "conformer convolution augmented transformer for speech recognition", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "source": "arxiv", "source_id": "2005.08100v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zhan2024anygpt", "query_title": "{AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling}", "normalized_title": "anygpt unified multimodal llm with discrete sequence modeling", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/", "source": "arxiv", "source_id": "2402.12226v5", "match_status": "exact_title", "missing_reason": null}
{"key": "xin2024bigcodec", "query_title": "{Bigcodec: Pushing the limits of low-bitrate neural speech codec}", "normalized_title": "bigcodec pushing the limits of low bitrate neural speech codec", "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.", "source": "arxiv", "source_id": "2409.05377v1", "match_status": "exact_title", "missing_reason": null}
{"key": "niu2024ndvq", "query_title": "{{NDVQ}: Robust neural audio codec with normal distribution-based vector quantization}", "normalized_title": "ndvq robust neural audio codec with normal distribution based vector quantization", "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization", "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.", "source": "arxiv", "source_id": "2409.12717v1", "match_status": "exact_title", "missing_reason": null}
{"key": "siuzdak2024snac", "query_title": "{{SNAC}: Multi-scale neural audio codec}", "normalized_title": "snac multi scale neural audio codec", "title": "SNAC: Multi-Scale Neural Audio Codec", "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.", "source": "arxiv", "source_id": "2410.14411v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chiu2022bestrq", "query_title": "{Self-supervised learning with random-projection quantizer for speech recognition}", "normalized_title": "self supervised learning with random projection quantizer for speech recognition", "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition", "abstract": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.", "source": "arxiv", "source_id": "2202.01855v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2023googleusm", "query_title": "{Google {USM}: Scaling automatic speech recognition beyond 100 languages}", "normalized_title": "google usm scaling automatic speech recognition beyond 100 languages", "title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.", "source": "arxiv", "source_id": "2303.01037v3", "match_status": "exact_title", "missing_reason": null}
{"key": "langman2024spectral", "query_title": "{Spectral codecs: Spectrogram-based audio codecs for high quality speech synthesis}", "normalized_title": "spectral codecs spectrogram based audio codecs for high quality speech synthesis", "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs", "abstract": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.", "source": "arxiv", "source_id": "2406.05298", "match_status": "exact_id", "missing_reason": null}
{"key": "gu2024esc", "query_title": "{Esc: Efficient speech coding with cross-scale residual vector quantized transformers}", "normalized_title": "esc efficient speech coding with cross scale residual vector quantized transformers", "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.", "source": "arxiv", "source_id": "2404.19441v3", "match_status": "exact_title", "missing_reason": null}
{"key": "tang2024singomd", "query_title": "{Sing{OMD}: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models}", "normalized_title": "singomd singing oriented multi resolution discrete representation construction from speech models", "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models", "abstract": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.", "source": "semantic_scholar", "source_id": "b7359944a3a8d9a8a6d4acd83e7a507a4da1094a", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2024mmm", "query_title": "{{MMM}: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model}", "normalized_title": "mmm multi layer multi residual multi stream discrete speech representation from self supervised learning model", "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model", "abstract": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.", "source": "arxiv", "source_id": "2406.09869v1", "match_status": "exact_title", "missing_reason": null}
{"key": "messica2024nast", "query_title": "{Nast: Noise aware speech tokenization for speech language models}", "normalized_title": "nast noise aware speech tokenization for speech language models", "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.", "source": "arxiv", "source_id": "2406.11037v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen-etal-2024-towards-robust", "query_title": "\"Towards Robust Speech Representation Learning for Thousands of Languages\"", "normalized_title": "towards robust speech representation learning for thousands of languages", "title": "Towards Robust Speech Representation Learning for Thousands of Languages", "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.", "source": "arxiv", "source_id": "2407.00837v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2023speechgpt", "query_title": "{Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities}", "normalized_title": "speechgpt empowering large language models with intrinsic cross modal conversational abilities", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "source": "arxiv", "source_id": "2305.11000v2", "match_status": "exact_title", "missing_reason": null}
{"key": "valin2012definition", "query_title": "{RFC 6716: Definition of the Opus audio codec}", "normalized_title": "rfc 6716 definition of the opus audio codec", "title": "Definition of the Opus Audio Codec.", "abstract": "This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances.  It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s.  Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music.", "source": "rfc_editor", "source_id": "RFC6716", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "dietz2015overview", "query_title": "{Overview of the {EVS} codec architecture}", "normalized_title": "overview of the evs codec architecture", "title": "Overview of the EVS codec architecture", "abstract": "The recently standardized 3GPP codec for Enhanced Voice Services (EVS) offers new features and improvements for low-delay real-time communication systems. Based on a novel, switched low-delay speech/audio codec, the EVS codec contains various tools for better compression efficiency and higher quality for clean/noisy speech, mixed content and music, including support for wideband, super-wideband and full-band content. The EVS codec operates in a broad range of bitrates, is highly robust against packet loss and provides an AMR-WB interoperable mode for compatibility with existing systems. This paper gives an overview of the underlying architecture as well as the novel technologies in the EVS codec and presents listening test results showing the performance of the new codec in terms of compression and speech/audio quality.", "source": "semantic_scholar", "source_id": "d63c953aa3a7327202d7af9baeda6fd0a646d5a5", "match_status": "exact_title", "missing_reason": null}
{"key": "fma_dataset", "query_title": "{{FMA}: A Dataset for Music Analysis}", "normalized_title": "fma a dataset for music analysis", "title": "FMA: A Dataset For Music Analysis", "abstract": "We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma", "source": "arxiv", "source_id": "1612.01840v3", "match_status": "exact_title", "missing_reason": null}
{"key": "evans2025stable", "query_title": "{Stable audio open}", "normalized_title": "stable audio open", "title": "Stable Audio Open", "abstract": "Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.", "source": "arxiv", "source_id": "2407.14358v2", "match_status": "exact_title", "missing_reason": null}
{"key": "van2022comparison", "query_title": "{A comparison of discrete and soft speech units for improved voice conversion}", "normalized_title": "a comparison of discrete and soft speech units for improved voice conversion", "title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion", "abstract": "The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech. Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code available at https://github.com/bshall/soft-vc/.", "source": "arxiv", "source_id": "2111.02392v2", "match_status": "exact_title", "missing_reason": null}
{"key": "guo2024socodec", "query_title": "{SoCodec: A Semantic-Ordered Multi-Stream Speech Codec For Efficient Language Model Based Text-to-Speech Synthesis}", "normalized_title": "socodec a semantic ordered multi stream speech codec for efficient language model based text to speech synthesis", "title": "SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis", "abstract": "The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.", "source": "arxiv", "source_id": "2409.00933v1", "match_status": "exact_title", "missing_reason": null}
{"key": "casanova2024lfsc", "query_title": "{Low frame-rate speech codec: a codec designed for fast high-quality speech {LLM} training and inference}", "normalized_title": "low frame rate speech codec a codec designed for fast high quality speech llm training and inference", "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.", "source": "arxiv", "source_id": "2409.12117v1", "match_status": "exact_title", "missing_reason": null}
{"key": "ren2024ticodec", "query_title": "{Fewer-token neural speech codec with time-invariant codes}", "normalized_title": "fewer token neural speech codec with time invariant codes", "title": "Fewer-token Neural Speech Codec with Time-invariant Codes", "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance and force it to capture more global information, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model.", "source": "arxiv", "source_id": "2310.00014v2", "match_status": "exact_title", "missing_reason": null}
{"key": "parker2024scaling", "query_title": "{Scaling Transformers for Low-Bitrate High-Quality Speech Coding}", "normalized_title": "scaling transformers for low bitrate high quality speech coding", "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "abstract": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "source": "arxiv", "source_id": "2411.19842v1", "match_status": "exact_title", "missing_reason": null}
{"key": "bai2024dmel", "query_title": "{{D}mel: Speech tokenization made simple}", "normalized_title": "dmel speech tokenization made simple", "title": "dMel: Speech Tokenization made Simple", "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.", "source": "arxiv", "source_id": "2407.15835", "match_status": "exact_id", "missing_reason": null}
{"key": "ai2024apcodec", "query_title": "{APCodec: A neural audio codec with parallel amplitude and phase spectrum encoding and decoding}", "normalized_title": "apcodec a neural audio codec with parallel amplitude and phase spectrum encoding and decoding", "title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding", "abstract": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.", "source": "arxiv", "source_id": "2402.10533v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kolbaek2017multitalker", "query_title": "{{M}ultitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks}", "normalized_title": "multitalker speech separation with utterance level permutation invariant training of deep recurrent neural networks", "title": "Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks", "abstract": "In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "source": "semantic_scholar", "source_id": "256ad591c6fd5269fc6f88b9715bf379f210f53d", "match_status": "exact_title", "missing_reason": null}
{"key": "piczak2015esc", "query_title": "{ESC: Dataset for environmental sound classification}", "normalized_title": "esc dataset for environmental sound classification", "title": "ESC: Dataset for Environmental Sound Classification", "abstract": "One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.", "source": "semantic_scholar", "source_id": "99e6f700d374e34c8376f1f43af994b278924f28", "match_status": "exact_title", "missing_reason": null}
{"key": "tzanetakis2002musical", "query_title": "{Musical genre classification of audio signals}", "normalized_title": "musical genre classification of audio signals", "title": "Musical genre classification of audio signals", "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.", "source": "semantic_scholar", "source_id": "7ab881283270e427b05c6e9469562ff39dd6282a", "match_status": "exact_title", "missing_reason": null}
{"key": "li2023mert", "query_title": "{{MERT}: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}", "normalized_title": "mert acoustic music understanding model with large scale self supervised training", "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training", "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.", "source": "arxiv", "source_id": "2306.00107v5", "match_status": "exact_title", "missing_reason": null}
{"key": "kong2020panns", "query_title": "{Panns: Large-scale pretrained audio neural networks for audio pattern recognition}", "normalized_title": "panns large scale pretrained audio neural networks for audio pattern recognition", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "abstract": "Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.", "source": "arxiv", "source_id": "1912.10211v5", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2022usb", "query_title": "{Usb: A unified semi-supervised learning benchmark for classification}", "normalized_title": "usb a unified semi supervised learning benchmark for classification", "title": "USB: A Unified Semi-supervised Learning Benchmark for Classification", "abstract": "Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.", "source": "arxiv", "source_id": "2208.07204v2", "match_status": "exact_title", "missing_reason": null}
{"key": "mousavi2024dasb", "query_title": "{DASB--Discrete Audio and Speech Benchmark}", "normalized_title": "dasb discrete audio and speech benchmark", "title": "DASB - Discrete Audio and Speech Benchmark", "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.", "source": "arxiv", "source_id": "2406.14294v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2023lmvc", "query_title": "{Lm-vc: Zero-shot voice conversion via speech generation based on language models}", "normalized_title": "lm vc zero shot voice conversion via speech generation based on language models", "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models", "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.", "source": "arxiv", "source_id": "2306.10521v2", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2023speaking", "query_title": "{Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units}", "normalized_title": "speaking style conversion in the waveform domain using discrete self supervised units", "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units", "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "source": "arxiv", "source_id": "2212.09730v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024streamvoice", "query_title": "{StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion}", "normalized_title": "streamvoice streamable context aware language modeling for real time zero shot voice conversion", "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion", "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model's forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice's streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.", "source": "arxiv", "source_id": "2401.11053v5", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2024espnet", "query_title": "{{ESPnet-Codec}: Comprehensive training and evaluation of neural codecs for audio, music, and speech}", "normalized_title": "espnet codec comprehensive training and evaluation of neural codecs for audio music and speech", "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech", "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.", "source": "arxiv", "source_id": "2409.15897v2", "match_status": "exact_title", "missing_reason": null}
{"key": "vashishth2024stab", "query_title": "{{STAB}: Speech Tokenizer Assessment Benchmark}", "normalized_title": "stab speech tokenizer assessment benchmark", "title": "STAB: Speech Tokenizer Assessment Benchmark", "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.", "source": "arxiv", "source_id": "2409.02384v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zen2019libritts", "query_title": "{Libri{TTS}: A Corpus Derived from LibriSpeech for Text-to-Speech}", "normalized_title": "libritts a corpus derived from librispeech for text to speech", "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech", "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL.", "source": "semantic_scholar", "source_id": "2789b6c84ba1422746246685001accba5563e7c1", "match_status": "exact_title", "missing_reason": null}
{"key": "7178964", "query_title": "{Librispeech: An ASR corpus based on public domain audio books}", "normalized_title": "librispeech an asr corpus based on public domain audio books", "title": "Librispeech: An ASR corpus based on public domain audio books", "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.", "source": "semantic_scholar", "source_id": "34038d9424ce602d7ac917a4e582d977725d4393", "match_status": "exact_title", "missing_reason": null}
{"key": "lam2023efficient", "query_title": "{Efficient neural music generation}", "normalized_title": "efficient neural music generation", "title": "Efficient Neural Music Generation", "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.", "source": "arxiv", "source_id": "2305.15719v1", "match_status": "exact_title", "missing_reason": null}
{"key": "rouard2024audio", "query_title": "{Audio conditioning for music generation via discrete bottleneck features}", "normalized_title": "audio conditioning for music generation via discrete bottleneck features", "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features", "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on https://musicgenstyle.github.io in order to show the quality of our model.", "source": "arxiv", "source_id": "2407.12563v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024musicldm", "query_title": "{Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies}", "normalized_title": "musicldm enhancing novelty in text to music generation using beat synchronous mixup strategies", "title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies", "abstract": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.", "source": "arxiv", "source_id": "2308.01546v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2024generative", "query_title": "{Generative de-quantization for neural speech codec via latent diffusion}", "normalized_title": "generative de quantization for neural speech codec via latent diffusion", "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion", "abstract": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.", "source": "arxiv", "source_id": "2311.08330v2", "match_status": "exact_title", "missing_reason": null}
{"key": "li2024single", "query_title": "{Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation}", "normalized_title": "single codec single codebook speech codec towards high performance speech generation", "title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation", "abstract": "The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.", "source": "arxiv", "source_id": "2406.07422v1", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2023repcodec", "query_title": "\"{R}ep{C}odec: A Speech Representation Codec for Speech Tokenization\"", "normalized_title": "repcodec a speech representation codec for speech tokenization", "title": "RepCodec: A Speech Representation Codec for Speech Tokenization", "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.", "source": "semantic_scholar", "source_id": "8b6c00246a0ae34f097aa64af7d9cb35b2b43a30", "match_status": "exact_title", "missing_reason": null}
{"key": "siuzdak2023vocos", "query_title": "{Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}", "normalized_title": "vocos closing the gap between time domain and fourier based neural vocoders for high quality audio synthesis", "title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis", "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced at https://github.com/gemelo-ai/vocos.", "source": "arxiv", "source_id": "2306.00814v3", "match_status": "exact_title", "missing_reason": null}
{"key": "kang2024libriheavy", "query_title": "{Libriheavy: A 50,000 hours ASR corpus with punctuation casing and context}", "normalized_title": "libriheavy a 50 000 hours asr corpus with punctuation casing and context", "title": "Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context", "abstract": "In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting of 50,000 hours of read English speech derived from LibriVox. To the best of our knowledge, Libriheavy is the largest freely-available corpus of speech with supervisions. Different from other open-sourced datasets that only provide normalized transcriptions, Libriheavy contains richer information such as punctuation, casing and text context, which brings more flexibility for system building. Specifically, we propose a general and efficient pipeline to locate, align and segment the audios in previously published Librilight to its corresponding texts. The same as Librilight, Libriheavy also has three training subsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We also extract the dev and test evaluation sets from the aligned audios and guarantee there is no overlapping speakers and books in training sets. Baseline systems are built on the popular CTC-Attention and transducer models. Additionally, we open-source our dataset creatation pipeline which can also be used to other audio alignment tasks.", "source": "arxiv", "source_id": "2309.08105v2", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2024suite", "query_title": "{Salmon: A Suite for Acoustic Language Model Evaluation}", "normalized_title": "salmon a suite for acoustic language model evaluation", "title": "Salmon: A Suite for Acoustic Language Model Evaluation", "abstract": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. We make the code and data publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "source": "arxiv", "source_id": "2409.07437v3", "match_status": "exact_title", "missing_reason": null}
{"key": "LMCodec", "query_title": "\"Towards Codec-{LM} Co-design for Neural Codec Language Models\"", "normalized_title": "towards codec lm co design for neural codec language models", "title": "Towards Codec-LM Co-design for Neural Codec Language Models", "abstract": "Neural codec language models (or codec LMs ) are emerging as a powerful framework for audio generation tasks like text-to-speech (TTS). These models leverage advancements in language modeling and residual vector quantization (RVQ)-based audio codecs, which compress audios into discrete codes for LMs to process. Despite the close interdependence of codecs and LMs in these systems, research on codecs and LMs has largely remained siloed. In this work, we propose three techniques for better codec-LM co-design: (i) a frame-wise codec encoder that improves both LM log-likelihood and end-to-end TTS metrics, (ii) LM codebook level dropout , a method to efficiently navigate a portion of the codec-LM design space by training a single LM, and (iii) increased codec frame duration , which we show can accelerate inference while maintaining end-to-end performance. Our experiments demonstrate that combining all three co-design techniques results in doubled inference speed, and improvements in intelligibility, audio quality, and speaker con-trol in TTS relative to a siloed baseline.", "source": "semantic_scholar", "source_id": "9ae0d46500c36c58533ad0b456f517d0b4da17b4", "match_status": "exact_title", "missing_reason": null}
{"key": "bie2024learning", "query_title": "{Learning source disentanglement in neural audio codec}", "normalized_title": "learning source disentanglement in neural audio codec", "title": "Learning Source Disentanglement in Neural Audio Codec", "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.", "source": "arxiv", "source_id": "2409.11228v2", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2024semanticodec", "query_title": "{Semanticodec: An ultra low bitrate semantic audio codec for general sound}", "normalized_title": "semanticodec an ultra low bitrate semantic audio codec for general sound", "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.", "source": "arxiv", "source_id": "2405.00233v2", "match_status": "exact_title", "missing_reason": null}
{"key": "jiang22_interspeech", "query_title": "{Cross-Scale Vector Quantization for Scalable Neural Speech Coding}", "normalized_title": "cross scale vector quantization for scalable neural speech coding", "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding", "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.", "source": "arxiv", "source_id": "2207.03067v1", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2024qwen2", "query_title": "{Qwen2 technical report}", "normalized_title": "qwen2 technical report", "title": "Qwen2 Technical Report", "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.", "source": "arxiv", "source_id": "2407.10671v4", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2022mulan", "query_title": "{MuLan: A Joint Embedding of Music Audio and Natural Language}", "normalized_title": "mulan a joint embedding of music audio and natural language", "title": "MuLan: A Joint Embedding of Music Audio and Natural Language", "abstract": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.", "source": "arxiv", "source_id": "2208.12415v1", "match_status": "exact_title", "missing_reason": null}
{"key": "turetzky_last_2024", "query_title": "{{LAST}: {Language} {Model} {Aware} {Speech} {Tokenization}}", "normalized_title": "last language model aware speech tokenization", "title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "source": "arxiv", "source_id": "2409.03701v2", "match_status": "exact_title", "missing_reason": null}
{"key": "nguyen2024spiritlminterleavedspokenwritten", "query_title": "\"{S}pi{R}it-{LM}: Interleaved Spoken and Written Language Model\"", "normalized_title": "spirit lm interleaved spoken and written language model", "title": "SpiRit-LM: Interleaved Spoken and Written Language Model", "abstract": "Abstract We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2", "source": "semantic_scholar", "source_id": "7547e30ba98a0217f07a6bb9fc393902bbc89269", "match_status": "exact_title", "missing_reason": null}
{"key": "nguyen2020zeroresourcespeechbenchmark", "query_title": "{The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling}", "normalized_title": "the zero resource speech benchmark 2021 metrics and baselines for unsupervised spoken language modeling", "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling", "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.", "source": "arxiv", "source_id": "2011.11588v2", "match_status": "exact_title", "missing_reason": null}
{"key": "manilow2019cutting", "query_title": "{Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity}", "normalized_title": "cutting music source separation some slakh a dataset to study the impact of training data quality and quantity", "title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity", "abstract": "Music source separation performance has greatly improved in recent years with the advent of approaches based on deep learning. Such methods typically require large amounts of labelled training data, which in the case of music consist of mixtures and corresponding instrument stems. However, stems are unavailable for most commercial music, and only limited datasets have so far been released to the public. It can thus be difficult to draw conclusions when comparing various source separation methods, as the difference in performance may stem as much from better data augmentation techniques or training tricks to alleviate the limited availability of training data, as from intrinsically better model architectures and objective functions. In this paper, we present the synthesized Lakh dataset (Slakh) as a new tool for music source separation research. Slakh consists of high-quality renderings of instrumental mixtures and corresponding stems generated from the Lakh MIDI dataset (LMD) using professional-grade sample-based virtual instruments. A first version, Slakh2100, focuses on 2100 songs, resulting in 145 hours of mixtures. While not fully comparable because it is purely instrumental, this dataset contains an order of magnitude more data than MUSDB18, the {\\it de facto} standard dataset in the field. We show that Slakh can be used to effectively augment existing datasets for musical instrument separation, while opening the door to a wide array of data-intensive music signal analysis tasks.", "source": "arxiv", "source_id": "1909.08494v1", "match_status": "exact_title", "missing_reason": null}
{"key": "arnault2020urban", "query_title": "{Urban Sound Classification: striving towards a fair comparison}", "normalized_title": "urban sound classification striving towards a fair comparison", "title": "Urban Sound Classification : striving towards a fair comparison", "abstract": "Urban sound classification has been achieving remarkable progress and is still an active research area in audio pattern recognition. In particular, it allows to monitor the noise pollution, which becomes a growing concern for large cities. The contribution of this paper is two-fold. First, we present our DCASE 2020 task 5 winning solution which aims at helping the monitoring of urban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse / fine classification on validation set. Moreover, it reaches accuracies of 89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not easy to find a fair comparison and to reproduce the performance of existing models. Sometimes authors copy-pasting the results of the original papers which is not helping reproducibility. As a result, we provide a fair comparison by using the same input representation, metrics and optimizer to assess performances. We preserve data augmentation used by the original papers. We hope this framework could help evaluate new architectures in this field. For better reproducibility, the code is available on our GitHub repository.", "source": "arxiv", "source_id": "2010.11805v1", "match_status": "exact_title", "missing_reason": null}
{"key": "wisdom2021s", "query_title": "{What’s all the fuss about free universal sound separation data?}", "normalized_title": "whats all the fuss about free universal sound separation data", "title": "What’s all the Fuss about Free Universal Sound Separation Data?", "abstract": "We introduce the Free Universal Sound Separation (FUSS) dataset, a new corpus for experiments in separating mixtures of an unknown number of sounds from an open domain of sound types. The dataset consists of 23 hours of single-source audio data drawn from 357 classes, which are used to create mixtures of one to four sources. To simulate reverberation, an acoustic room simulator is used to generate impulse responses of box-shaped rooms with frequency-dependent reflective walls. Additional open-source data augmentation tools are also provided to produce new mixtures with different combinations of sources and room simulations. Finally, we introduce an open-source baseline separation model, based on an improved time-domain convolutional network (TDCN++), that can separate a variable number of sources in a mixture. This model achieves 9.8 dB of scale-invariant signal-to-noise ratio improvement (SI-SNRi) on mixtures with two to four sources, while reconstructing single-source inputs with 35.8 dB absolute SI-SNR. We hope this dataset will lower the barrier to new research and allow for fast iteration and application of novel techniques from other machine learning domains to the sound separation challenge.", "source": "semantic_scholar", "source_id": "1932eb0e2ede027909ef42bfa065d9983824e0a9", "match_status": "exact_title", "missing_reason": null}
{"key": "hershey2016deep", "query_title": "{Deep clustering: Discriminative embeddings for segmentation and separation}", "normalized_title": "deep clustering discriminative embeddings for segmentation and separation", "title": "Deep clustering: Discriminative embeddings for segmentation and separation", "abstract": "We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be \"decoded\" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.", "source": "arxiv", "source_id": "1508.04306v1", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2024revisiting", "query_title": "{Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective}", "normalized_title": "revisiting self supervised learning of speech representation from a mutual information perspective", "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective", "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.", "source": "arxiv", "source_id": "2401.08833v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chung2025kad", "query_title": "{KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation}", "normalized_title": "kad no more fad an effective and efficient evaluation metric for audio generation", "title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation", "abstract": "Although being widely adopted for evaluating generated audio signals, the Fréchet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD captures nuanced differences between real and generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.", "source": "arxiv", "source_id": "2502.15602v2", "match_status": "exact_title", "missing_reason": null}
{"key": "fadtk", "query_title": "{Adapting frechet audio distance for generative music evaluation}", "normalized_title": "adapting frechet audio distance for generative music evaluation", "title": "Adapting Frechet Audio Distance for Generative Music Evaluation", "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.", "source": "arxiv", "source_id": "2311.01616v2", "match_status": "exact_title", "missing_reason": null}
{"key": "koutini2021passt", "query_title": "{Efficient Training of Audio Transformers with Patchout}", "normalized_title": "efficient training of audio transformers with patchout", "title": "Efficient Training of Audio Transformers with Patchout", "abstract": "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST", "source": "arxiv", "source_id": "2110.05069v3", "match_status": "exact_title", "missing_reason": null}
{"key": "san2023discrete", "query_title": "{From discrete tokens to high-fidelity audio using multi-band diffusion}", "normalized_title": "from discrete tokens to high fidelity audio using multi band diffusion", "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion", "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.", "source": "arxiv", "source_id": "2308.02560v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2023diffsound", "query_title": "{Diffsound: Discrete diffusion model for text-to-sound generation}", "normalized_title": "diffsound discrete diffusion model for text to sound generation", "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation", "abstract": "Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.", "source": "arxiv", "source_id": "2207.09983v2", "match_status": "exact_title", "missing_reason": null}
{"key": "wu2023large", "query_title": "{Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation}", "normalized_title": "large scale contrastive language audio pretraining with feature fusion and keyword to caption augmentation", "title": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation", "abstract": "Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.", "source": "arxiv", "source_id": "2211.06687v4", "match_status": "exact_title", "missing_reason": null}
{"key": "huang2023make", "query_title": "{Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models}", "normalized_title": "make an audio text to audio generation with prompt enhanced diffusion models", "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models", "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io", "source": "arxiv", "source_id": "2301.12661v1", "match_status": "exact_title", "missing_reason": null}
{"key": "audiocaps", "query_title": "{AudioCaps: Generating Captions for Audios in The Wild}", "normalized_title": "audiocaps generating captions for audios in the wild", "title": "AudioCaps: Generating Captions for Audios in The Wild", "abstract": "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.", "source": "semantic_scholar", "source_id": "c4798919e74411d87f7745840e45b8bcf61128ff", "match_status": "exact_title", "missing_reason": null}
{"key": "macs", "query_title": "{Diversity and bias in audio captioning datasets}", "normalized_title": "diversity and bias in audio captioning datasets", "title": "Diversity and Bias in Audio Captioning Datasets", "abstract": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.", "source": "semantic_scholar", "source_id": "2ee853d36dced58204983c1bb108c14d3310cd2c", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2023audioldm", "query_title": "{{A}udio{LDM}: Text-to-Audio Generation with Latent Diffusion Models}", "normalized_title": "audioldm text to audio generation with latent diffusion models", "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models", "abstract": "Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.", "source": "semantic_scholar", "source_id": "fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b", "match_status": "exact_title", "missing_reason": null}
{"key": "liu2024audioldm2", "query_title": "{Audio{LDM} 2: Learning holistic audio generation with self-supervised pretraining}", "normalized_title": "audioldm 2 learning holistic audio generation with self supervised pretraining", "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining", "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches.", "source": "semantic_scholar", "source_id": "33de773be1733347a01cb07a5bb1b6cdfa956a47", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2024v2a", "query_title": "{V2a-mapper: A lightweight solution for vision-to-audio generation by connecting foundation models}", "normalized_title": "v2a mapper a lightweight solution for vision to audio generation by connecting foundation models", "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models", "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.", "source": "arxiv", "source_id": "2308.09300v4", "match_status": "exact_title", "missing_reason": null}
{"key": "pascual2024masked", "query_title": "{Masked generative video-to-audio transformers with enhanced synchronicity}", "normalized_title": "masked generative video to audio transformers with enhanced synchronicity", "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .", "source": "arxiv", "source_id": "2407.10387v1", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2024foleycrafter", "query_title": "{Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds}", "normalized_title": "foleycrafter bring silent videos to life with lifelike and synchronized sounds", "title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds", "abstract": "We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. Models and codes are available at https://github.com/open-mmlab/FoleyCrafter.", "source": "arxiv", "source_id": "2407.01494v1", "match_status": "exact_title", "missing_reason": null}
{"key": "dong2023clipsonic", "query_title": "{Clipsonic: Text-to-audio synthesis with unlabeled videos and pretrained language-vision models}", "normalized_title": "clipsonic text to audio synthesis with unlabeled videos and pretrained language vision models", "title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models", "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.", "source": "arxiv", "source_id": "2306.09635v2", "match_status": "exact_title", "missing_reason": null}
{"key": "jeong2024read", "query_title": "{Read, watch and scream! sound generation from text and video}", "normalized_title": "read watch and scream sound generation from text and video", "title": "Read, Watch and Scream! Sound Generation from Text and Video", "abstract": "Despite the impressive progress of multimodal generative models, video-to-audio generation still suffers from limited performance and limits the flexibility to prioritize sound synthesis for specific objects within the scene. Conversely, text-to-audio generation methods generate high-quality audio but pose challenges in ensuring comprehensive scene depiction and time-varying control. To tackle these challenges, we propose a novel video-and-text-to-audio generation method, called \\ours, where video serves as a conditional control for a text-to-audio generation model. Especially, our method estimates the structural information of sound (namely, energy) from the video while receiving key content cues from a user prompt. We employ a well-performing text-to-audio model to consolidate the video control, which is much more efficient for training multimodal diffusion models with massive triplet-paired (audio-video-text) data. In addition, by separating the generative components of audio, it becomes a more flexible system that allows users to freely adjust the energy, surrounding environment, and primary sound source according to their preferences. Experimental results demonstrate that our method shows superiority in terms of quality, controllability, and training efficiency. Code and demo are available at https://naver-ai.github.io/rewas.", "source": "arxiv", "source_id": "2407.05551v2", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2024video", "query_title": "{Video-guided foley sound generation with multimodal controls}", "normalized_title": "video guided foley sound generation with multimodal controls", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "source": "arxiv", "source_id": "2411.17698v4", "match_status": "exact_title", "missing_reason": null}
{"key": "saito2024soundctm", "query_title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "normalized_title": "soundctm uniting score based and consistency models for text to sound generation", "title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "abstract": "This repository is the official implementation of \"SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation\"", "source": "github", "source_id": "sony/soundctm", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "wang2025frieren", "query_title": "{Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching}", "normalized_title": "frieren efficient video to audio generation network with rectified flow matching", "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "abstract": "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io.", "source": "arxiv", "source_id": "2406.00320v4", "match_status": "exact_title", "missing_reason": null}
{"key": "luo2023diff", "query_title": "{Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models}", "normalized_title": "diff foley synchronized video to audio synthesis with latent diffusion models", "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models", "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/", "source": "arxiv", "source_id": "2306.17203v1", "match_status": "exact_title", "missing_reason": null}
{"key": "sheffer2023hear", "query_title": "{I hear your true colors: Image guided audio generation}", "normalized_title": "i hear your true colors image guided audio generation", "title": "I Hear Your True Colors: Image Guided Audio Generation", "abstract": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.", "source": "arxiv", "source_id": "2211.03089v2", "match_status": "exact_title", "missing_reason": null}
{"key": "raffel2020exploring", "query_title": "{Exploring the limits of transfer learning with a unified text-to-text transformer}", "normalized_title": "exploring the limits of transfer learning with a unified text to text transformer", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.", "source": "arxiv", "source_id": "1910.10683v4", "match_status": "exact_title", "missing_reason": null}
{"key": "peng2024survey", "query_title": "{A survey on speech large language models}", "normalized_title": "a survey on speech large language models", "title": "A Survey on Speech Large Language Models for Understanding", "abstract": "Speech understanding is essential for interpreting the diverse forms of information embedded in spoken language, including linguistic, paralinguistic, and non-linguistic cues that are vital for effective human-computer interaction. The rapid advancement of large language models (LLMs) has catalyzed the emergence of Speech Large Language Models (Speech LLMs), which marks a transformative shift toward general-purpose speech understanding systems. To further clarify and systematically delineate task objectives, in this paper, we formally define the concept of speech understanding and introduce a structured taxonomy encompassing its informational, functional, and format dimensions. Within this scope of definition, we present a comprehensive review of current Speech LLMs, analyzing their architectures through a three-stage abstraction: Modality Feature Extraction, Modality Information Fusion, and LLM Inference. In addition, we examine training strategies, discuss representative datasets, and review evaluation methodologies adopted in the field. Based on empirical analyses and experimental evidence, we identify two key challenges currently facing Speech LLMs: instruction sensitivity and degradation in semantic reasoning and propose concrete directions for addressing these issues. Through this systematic and detailed survey, we aim to offer a foundational reference for researchers and practitioners working toward more robust, generalizable, and human-aligned Speech LLMs.", "source": "arxiv", "source_id": "2410.18908", "match_status": "exact_id", "missing_reason": null}
{"key": "cui2024recent", "query_title": "{Recent advances in speech language models: A survey}", "normalized_title": "recent advances in speech language models a survey", "title": "Recent Advances in Speech Language Models: A Survey", "abstract": "Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "source": "arxiv", "source_id": "2410.03751v4", "match_status": "exact_title", "missing_reason": null}
{"key": "ji2024wavchat", "query_title": "{Wavchat: A survey of spoken dialogue models}", "normalized_title": "wavchat a survey of spoken dialogue models", "title": "WavChat: A Survey of Spoken Dialogue Models", "abstract": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.", "source": "arxiv", "source_id": "2411.13577v2", "match_status": "exact_title", "missing_reason": null}
{"key": "latif2023sparks", "query_title": "{Sparks of large audio models: A survey and outlook}", "normalized_title": "sparks of large audio models a survey and outlook", "title": "Sparks of Large Audio Models: A Survey and Outlook", "abstract": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.", "source": "arxiv", "source_id": "2308.12792v3", "match_status": "exact_title", "missing_reason": null}
{"key": "dunbar2021zero", "query_title": "{The Zero Resource Speech Challenge 2021: Spoken Language Modelling}", "normalized_title": "the zero resource speech challenge 2021 spoken language modelling", "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.", "source": "arxiv", "source_id": "2104.14700v2", "match_status": "exact_title", "missing_reason": null}
{"key": "lin2024alignslm", "query_title": "{Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback}", "normalized_title": "align slm textless spoken language models with reinforcement learning from ai feedback", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.", "source": "arxiv", "source_id": "2411.01834v2", "match_status": "exact_title", "missing_reason": null}
{"key": "radford2023robust", "query_title": "{Robust speech recognition via large-scale weak supervision}", "normalized_title": "robust speech recognition via large scale weak supervision", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "source": "arxiv", "source_id": "2212.04356v1", "match_status": "exact_title", "missing_reason": null}
{"key": "mostafazadeh2016corpus", "query_title": "\"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories\"", "normalized_title": "a corpus and cloze evaluation for deeper understanding of commonsense stories", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "source": "semantic_scholar", "source_id": "85b68477a6e031d88b963833e15a4b4fc6855264", "match_status": "exact_title", "missing_reason": null}
{"key": "yang2024uniaudio", "query_title": "{Uni{A}udio 1.5: Large Language Model-Driven Audio Codec is A Few-Shot Audio Task Learner}", "normalized_title": "uniaudio 1 5 large language model driven audio codec is a few shot audio task learner", "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner", "abstract": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.", "source": "semantic_scholar", "source_id": "33672a1ef7bb4722902a53c77e30ec92b338a94c", "match_status": "exact_title", "missing_reason": null}
{"key": "park2024long", "query_title": "{Long-Form Speech Generation with Spoken Language Models}", "normalized_title": "long form speech generation with spoken language models", "title": "Long-Form Speech Generation with Spoken Language Models", "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.", "source": "arxiv", "source_id": "2412.18603v2", "match_status": "exact_title", "missing_reason": null}
{"key": "dhariwal2020jukebox", "query_title": "{Jukebox: A generative model for music}", "normalized_title": "jukebox a generative model for music", "title": "Jukebox: A Generative Model for Music", "abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox", "source": "arxiv", "source_id": "2005.00341v1", "match_status": "exact_title", "missing_reason": null}
{"key": "ziv2024masked", "query_title": "{Masked Audio Generation using a Single Non-Autoregressive Transformer}", "normalized_title": "masked audio generation using a single non autoregressive transformer", "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer", "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.", "source": "arxiv", "source_id": "2401.04577v2", "match_status": "exact_title", "missing_reason": null}
{"key": "garcia2023vampnet", "query_title": "{VampNet: Music Generation via Masked Acoustic Token Modeling}", "normalized_title": "vampnet music generation via masked acoustic token modeling", "title": "VampNet: Music Generation via Masked Acoustic Token Modeling", "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.", "source": "arxiv", "source_id": "2307.04686v2", "match_status": "exact_title", "missing_reason": null}
{"key": "gu2021efficiently", "query_title": "{Efficiently Modeling Long Sequences with Structured State Spaces}", "normalized_title": "efficiently modeling long sequences with structured state spaces", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.", "source": "arxiv", "source_id": "2111.00396v3", "match_status": "exact_title", "missing_reason": null}
{"key": "elmakies2025unsupervised", "query_title": "{Unsupervised Speech Segmentation: A General Approach Using Speech Language Models}", "normalized_title": "unsupervised speech segmentation a general approach using speech language models", "title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models", "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation, which builds on previously researched approaches, e.g., Speaker Diarization, while being applicable to an inclusive set of acoustic-semantic distinctions, paving a path towards a general Unsupervised Speech Segmentation approach. Unlike traditional speech and audio segmentation, which mainly focuses on spectral changes in the input signal, e.g., phone segmentation, our approach tries to segment the spoken utterance into chunks with differing acoustic-semantic styles, focusing on acoustic-semantic information that does not translate well into text, e.g., emotion or speaker. While most Speech Segmentation tasks only handle one style change, e.g., emotion diarization, our approach tries to handle multiple acoustic-semantic style changes. Leveraging recent advances in Speech Language Models (SLMs), we propose a simple unsupervised method to segment a given speech utterance. We empirically demonstrate the effectiveness of the proposed approach by considering several setups. Results suggest that the proposed method is superior to the evaluated baselines on boundary detection, segment purity, and over-segmentation. Code is available at https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.", "source": "arxiv", "source_id": "2501.03711v1", "match_status": "exact_title", "missing_reason": null}
{"key": "salmonn", "query_title": "{{SALMONN}: Towards Generic Hearing Abilities for Large Language Models}", "normalized_title": "salmonn towards generic hearing abilities for large language models", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "source": "arxiv", "source_id": "2310.13289v2", "match_status": "exact_title", "missing_reason": null}
{"key": "qwen_audio", "query_title": "{Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models}", "normalized_title": "qwen audio advancing universal audio understanding via unified large scale audio language models", "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models", "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.", "source": "arxiv", "source_id": "2311.07919v2", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2025slamming", "query_title": "{Slamming: Training a Speech Language Model on One GPU in a Day}", "normalized_title": "slamming training a speech language model on one gpu in a day", "title": "Slamming: Training a Speech Language Model on One GPU in a Day", "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "source": "arxiv", "source_id": "2502.15814v2", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2021aishell", "query_title": "{{AISHELL-3}: A Multi-Speaker Mandarin {TTS} Corpus}", "normalized_title": "aishell 3 a multi speaker mandarin tts corpus", "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus", "abstract": "In this paper, we present AISHELL-3 , a large-scale multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-To-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spanning across 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Moreover, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We also present some data processing strategies and techniques which match with the characteristics of the presented corpus and conduct experiments on multiple speech-synthesis systems to assess the quality of the generated speech samples, showing promising results. The corpus is available online at openslr.org/93/ under Apache v2.0 license.", "source": "semantic_scholar", "source_id": "a6387132862d1270b1a0b7ec8352b1773ec2e990", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2021highland", "query_title": "{Highland {P}uebla {N}ahuatl speech translation corpus for endangered language documentation}", "normalized_title": "highland puebla nahuatl speech translation corpus for endangered language documentation", "title": "Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation", "abstract": "Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR > MT) pipeline when translating endangered language documentation materials.", "source": "semantic_scholar", "source_id": "59f3e3cad309eb4965d67773d68bc2f91b2e376f", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2021leveraging", "query_title": "{Leveraging End-to-End {ASR} for Endangered Language Documentation: An Empirical Study on {Y}ol{\\'o}xochitl {M}ixtec}", "normalized_title": "leveraging end to end asr for endangered language documentation an empirical study on yol oxochitl mixtec", "title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolóxochitl Mixtec", "abstract": "“Transcription bottlenecks”, created by a shortage of effective human transcribers (i.e., transcriber shortage), are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.", "source": "semantic_scholar", "source_id": "8c4d1e81c277f71cd9e3c9a0af356203c7948dca", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "huang2021multi", "query_title": "{Multi-singer: Fast multi-singer singing voice vocoder with a large-scale corpus}", "normalized_title": "multi singer fast multi singer singing voice vocoder with a large scale corpus", "title": "Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus", "abstract": "High-fidelity multi-singer singing voice synthesis is challenging for neural vocoder due to the singing voice data shortage, limited singer generalization, and large computational cost. Existing open corpora could not meet requirements for high-fidelity singing voice synthesis because of the scale and quality weaknesses. Previous vocoders have difficulty in multi-singer modeling, and a distinct degradation emerges when conducting unseen singer singing voice generation. To accelerate singing voice researches in the community, we release a large-scale, multi-singer Chinese singing voice dataset OpenSinger. To tackle the difficulty in unseen singer modeling, we propose Multi-Singer, a fast multi-singer vocoder with generative adversarial networks. Specifically, 1) Multi-Singer uses a multi-band generator to speed up both training and inference procedure. 2) to capture and rebuild singer identity from the acoustic feature (i.e., mel-spectrogram), Multi-Singer adopts a singer conditional discriminator and conditional adversarial training objective. 3) to supervise the reconstruction of singer identity in the spectrum envelopes in frequency domain, we propose an auxiliary singer perceptual loss. The joint training approach effectively works in GANs for multi-singer voices modeling. Experimental results verify the effectiveness of OpenSinger and show that Multi-Singer improves unseen singer singing voices modeling in both speed and quality over previous methods. The further experiment proves that combined with FastSpeech 2 as the acoustic model, Multi-Singer achieves strong robustness in the multi-singer singing voice synthesis pipeline. Samples are available at https://Multi-Singer.github.io/", "source": "arxiv", "source_id": "2112.10358v1", "match_status": "exact_title", "missing_reason": null}
{"key": "dai2023singstyle111", "query_title": "{Singstyle111: A multilingual singing dataset with style transfer}", "normalized_title": "singstyle111 a multilingual singing dataset with style transfer", "title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer", "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.", "source": "semantic_scholar", "source_id": "0cab8f24dec845c4b8e64e97dd4a92a1b6338ea1", "match_status": "exact_title", "missing_reason": null}
{"key": "zhang2022m4singer", "query_title": "{M4singer: A multi-style, multi-singer and musical score provided mandarin singing corpus}", "normalized_title": "m4singer a multi style multi singer and musical score provided mandarin singing corpus", "title": "M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus", "abstract": "Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.", "source": "semantic_scholar", "source_id": "9441b35773ec51c19eb5c028a46705a3f3f732d5", "match_status": "exact_title", "missing_reason": null}
{"key": "ogawa2021tohoku", "query_title": "{Tohoku {K}iritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs}", "normalized_title": "tohoku kiritan singing database a singing database for statistical parametric singing synthesis using japanese pop songs", "title": "Tohoku Kiritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs", "abstract": ": We have built a singing database that can be used for research purposes. Since recent songs are protected by copyright law, researchers typically use songs that can be used without copyright. With changes to the copyright law in Japan in 2019, we can now release a singing database consisting of songs protected by the law under several restrictions. Our database mainly consists of Japanese pop songs by a professional singer. We collected a total of 50 songs with around 57 minutes of vocals recorded in a studio. After recording, we labeled the phoneme boundaries and converted the songs into the MusicXML format required for the study of statistical parametric singing synthesis. Statistical analysis of the database was then carried out. First, we counted the number of phonemes to clarify their distribution. Second, we performed acoustical analysis on the distribution of pitch, the interval between notes, and duration. Results showed that although the information is biased, the amount of singing is suﬃcient in light of the ﬁndings of a prior study on singing synthesis. The corpus is freely available at our website, https://zunko.jp/kiridev/login.php [1].", "source": "semantic_scholar", "source_id": "f6781f89263f0e30070bfad392de76681653379c", "match_status": "exact_title", "missing_reason": null}
{"key": "wang2022opencpop", "query_title": "{Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis}", "normalized_title": "opencpop a high quality open source chinese popular song corpus for singing voice synthesis", "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis", "abstract": "This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.", "source": "arxiv", "source_id": "2201.07429v2", "match_status": "exact_title", "missing_reason": null}
{"key": "shi2024singing", "query_title": "{Singing Voice Data Scaling-up: An Introduction to {ACE-Opencpop and ACE-KiSing}}", "normalized_title": "singing voice data scaling up an introduction to ace opencpop and ace kising", "title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing", "abstract": "In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (\\url{https://github.com/espnet/espnet})", "source": "arxiv", "source_id": "2401.17619v3", "match_status": "exact_title", "missing_reason": null}
{"key": "koguchi2020pjs", "query_title": "{{PJS}: Phoneme-balanced Japanese singing-voice corpus}", "normalized_title": "pjs phoneme balanced japanese singing voice corpus", "title": "PJS: phoneme-balanced Japanese singing voice corpus", "abstract": "This paper presents a free Japanese singing voice corpus that can be used for highly applicable and reproducible singing voice synthesis research. A singing voice corpus helps develop singing voice synthesis, but existing corpora have two critical problems: data imbalance (singing voice corpora do not guarantee phoneme balance, unlike speaking-voice corpora) and copyright issues (cannot legally share data). As a way to avoid these problems, we constructed a PJS (phoneme-balanced Japanese singing voice) corpus that guarantees phoneme balance and is licensed with CC BY-SA 4.0, and we composed melodies using a phoneme-balanced speaking-voice corpus. This paper describes how we built the corpus.", "source": "arxiv", "source_id": "2006.02959v1", "match_status": "exact_title", "missing_reason": null}
{"key": "takamichi2020jsut", "query_title": "{{JSUT and JVS}: Free Japanese voice corpora for accelerating speech synthesis research}", "normalized_title": "jsut and jvs free japanese voice corpora for accelerating speech synthesis research", "title": "JSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research", "abstract": ": In this paper, we develop two corpora for speech synthesis research. Thanks to improvements in machine learning techniques, including deep learning, speech synthesis is becoming a machine learning task. To accelerate speech synthesis research, we aim at developing Japanese voice corpora reasonably accessible from not only academic institutions but also commercial companies. In this paper, we construct the JSUT and JVS corpora. They are designed mainly for text-to-speech synthesis and voice conversion, respectively. The JSUT corpus contains 10 hours of reading-style speech uttered by a single speaker, and the JVS corpus contains 30 hours containing three styles of speech uttered by 100 speakers. This paper describes how we designed the corpora and summarizes the speciﬁcations. The corpora are available at our project pages.", "source": "semantic_scholar", "source_id": "aa9b455bb5b16f49a3cfdb3239303e250c41f10f", "match_status": "exact_title", "missing_reason": null}
{"key": "amith_yoloxochitl_mixtec", "query_title": "{Audio corpus of Yoloxóchitl Mixtec with accompanying time-coded transcriptions in ELAN}", "normalized_title": "audio corpus of yoloxochitl mixtec with accompanying time coded transcriptions in elan", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "amith_audio_corpus_sierra", "query_title": "{Audio corpus of Sierra Nororiental and Sierra Norte de Puebla Nahuat(l) with accompanying time-code transcriptions in ELAN}", "normalized_title": "audio corpus of sierra nororiental and sierra norte de puebla nahuat l with accompanying time code transcriptions in elan", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "amith_totonac", "query_title": "{Audio corpus of Totonac recordings from northern Puebla and adjacent areas of Veracruz}", "normalized_title": "audio corpus of totonac recordings from northern puebla and adjacent areas of veracruz", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found"}
{"key": "kuhn2014daps", "query_title": "{{DAPS}: Intelligent delay-aware packet scheduling for multipath transport}", "normalized_title": "daps intelligent delay aware packet scheduling for multipath transport", "title": "DAPS: Intelligent delay-aware packet scheduling for multipath transport", "abstract": "International audience", "source": "semantic_scholar", "source_id": "b9f0e3ffa0d102ffd9aca268b1a7e024b89fc66a", "match_status": "exact_title", "missing_reason": null}
{"key": "yamagishi2019cstr", "query_title": "{{CSTR VCTK} Corpus: English multi-speaker corpus for {CSTR} voice cloning toolkit (version 0.92)}", "normalized_title": "cstr vctk corpus english multi speaker corpus for cstr voice cloning toolkit version 0 92", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "match_status": "exact_title", "missing_reason": null}
{"key": "yan2023espnet", "query_title": "{{ESPnet-ST-v2}: Multipurpose Spoken Language Translation Toolkit}", "normalized_title": "espnet st v2 multipurpose spoken language translation toolkit", "title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.", "source": "arxiv", "source_id": "2304.04596v3", "match_status": "exact_title", "missing_reason": null}
{"key": "vincent2006bss", "query_title": "{Performance measurement in blind audio source separation}", "normalized_title": "performance measurement in blind audio source separation", "title": "Performance measurement in blind audio source separation", "abstract": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels", "source": "semantic_scholar", "source_id": "29de8281b8cbc764d605a20d00b818eba6d47da1", "match_status": "exact_title", "missing_reason": null}
{"key": "allal2024SmolLM", "query_title": "{SmolLM - blazingly fast and remarkably powerful}", "normalized_title": "smollm blazingly fast and remarkably powerful", "title": "{SmolLM - blazingly fast and remarkably powerful}", "abstract": "Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance.", "source": "github", "source_id": "huggingface/smollm", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "rouard2022hybrid", "query_title": "{Hybrid transformers for music source separation}", "normalized_title": "hybrid transformers for music source separation", "title": "Hybrid Transformers for Music Source Separation", "abstract": "A natural question arising in Music Source Separation (MSS) is whether long range contextual information is useful, or whether local acoustic features are sufficient. In other fields, attention based Transformers have shown their ability to integrate information over long sequences. In this work, we introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral bi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a cross-domain Transformer Encoder, using self-attention within one domain, and cross-attention across domains. While it performs poorly when trained only on MUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by 0.45 dB of SDR when using 800 extra training songs. Using sparse attention kernels to extend its receptive field, and per source fine-tuning, we achieve state-of-the-art results on MUSDB with extra training data, with 9.20 dB of SDR.", "source": "arxiv", "source_id": "2211.08553v1", "match_status": "exact_title", "missing_reason": null}
{"key": "LuoY2019conv-tasnet", "query_title": "{Conv-tasnet: Surpassing ideal time--frequency magnitude masking for speech separation}", "normalized_title": "conv tasnet surpassing ideal time frequency magnitude masking for speech separation", "title": "Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation", "abstract": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.", "source": "arxiv", "source_id": "1809.07454v3", "match_status": "exact_title", "missing_reason": null}
{"key": "Saijo2024_TFLoco", "query_title": "{TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement}", "normalized_title": "tf locoformer transformer with local modeling by convolution for speech separation and enhancement", "title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement", "abstract": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.", "source": "arxiv", "source_id": "2408.03440v1", "match_status": "exact_title", "missing_reason": null}
{"key": "Kavalerov2019UniversalSS", "query_title": "{Universal Sound Separation}", "normalized_title": "universal sound separation", "title": "Universal Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.", "source": "arxiv", "source_id": "1905.03330v2", "match_status": "exact_title", "missing_reason": null}
{"key": "espnet", "query_title": "{{ESPnet}: End-to-End Speech Processing Toolkit}", "normalized_title": "espnet end to end speech processing toolkit", "title": "ESPnet: End-to-End Speech Processing Toolkit", "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.", "source": "arxiv", "source_id": "1804.00015v1", "match_status": "exact_title", "missing_reason": null}
{"key": "mars6", "query_title": "{MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model}", "normalized_title": "mars6 a small and robust hierarchical codec text to speech model", "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model", "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/", "source": "arxiv", "source_id": "2501.05787v1", "match_status": "exact_title", "missing_reason": null}
{"key": "simplespeech", "query_title": "{SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models}", "normalized_title": "simplespeech towards simple and efficient text to speech with scalar latent transformer diffusion models", "title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models", "abstract": "In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space. Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability. Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement. Demos are released.", "source": "arxiv", "source_id": "2406.02328v3", "match_status": "exact_title", "missing_reason": null}
{"key": "cuervo2024scalingpropertiesspeechlanguag", "query_title": "{{Scaling Properties of Speech Language Models}}", "normalized_title": "scaling properties of speech language models", "title": "Scaling Properties of Speech Language Models", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.", "source": "arxiv", "source_id": "2404.00685v2", "match_status": "exact_title", "missing_reason": null}
{"key": "hassid2023textually", "query_title": "{Textually pretrained speech language models}", "normalized_title": "textually pretrained speech language models", "title": "Textually Pretrained Speech Language Models", "abstract": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .", "source": "arxiv", "source_id": "2305.13009v3", "match_status": "exact_title", "missing_reason": null}
{"key": "copet2024musicgen", "query_title": "{Simple and controllable music generation}", "normalized_title": "simple and controllable music generation", "title": "Simple and Controllable Music Generation", "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft", "source": "arxiv", "source_id": "2306.05284v3", "match_status": "exact_title", "missing_reason": null}
{"key": "wolf2020huggingfacestransformersstateoftheartnatural", "query_title": "{HuggingFace's Transformers: State-of-the-art Natural Language Processing}", "normalized_title": "huggingface s transformers state of the art natural language processing", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{https://github.com/huggingface/transformers}.", "source": "arxiv", "source_id": "1910.03771v5", "match_status": "exact_title", "missing_reason": null}
{"key": "ji2024wavtokenizer", "query_title": "{WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling}", "normalized_title": "wavtokenizer an efficient acoustic discrete codec tokenizer for audio language modeling", "title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling", "abstract": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1)extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The related code, demos, and pre-trained models are available at https://github.com/jishengpeng/WavTokenizer.", "source": "arxiv", "source_id": "2408.16532v3", "match_status": "exact_title", "missing_reason": null}
{"key": "gpt", "query_title": "{Language Models are Few-Shot Learners}", "normalized_title": "language models are few shot learners", "title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "source": "arxiv", "source_id": "2005.14165v4", "match_status": "exact_title", "missing_reason": null}
{"key": "bark2023", "query_title": "{Bark: Text-Prompted Generative Audio Model}", "normalized_title": "bark text prompted generative audio model", "title": "{Bark: Text-Prompted Generative Audio Model}", "abstract": "> Notice: Bark is Suno's open-source text-to-speech+ model. If you are looking for our text-to-music models, please visit us on our [web page](https://suno.ai) and join our community on [Discord](https://suno.ai/discord).", "source": "github", "source_id": "suno-ai/bark", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "guo2025recent", "query_title": "{Recent Advances in Discrete Speech Tokens: A Review}", "normalized_title": "recent advances in discrete speech tokens a review", "title": "Recent Advances in Discrete Speech Tokens: A Review", "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "source": "arxiv", "source_id": "2502.06490v4", "match_status": "exact_title", "missing_reason": null}
{"key": "zaiem2023speech", "query_title": "{Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?}", "normalized_title": "speech self supervised representation benchmarking are we doing it right", "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?", "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.", "source": "arxiv", "source_id": "2306.00452v1", "match_status": "exact_title", "missing_reason": null}
{"key": "watanabe2023tree", "query_title": "{Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance}", "normalized_title": "tree structured parzen estimator understanding its algorithm components and their roles for better empirical performance", "title": "Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance", "abstract": "Recent scientific advances require complex experiment design, necessitating the meticulous tuning of many experiment parameters. Tree-structured Parzen estimator (TPE) is a widely used Bayesian optimization method in recent parameter tuning frameworks such as Hyperopt and Optuna. Despite its popularity, the roles of each control parameter in TPE and the algorithm intuition have not been discussed so far. The goal of this paper is to identify the roles of each control parameter and their impacts on parameter tuning based on the ablation studies using diverse benchmark datasets. The recommended setting concluded from the ablation studies is demonstrated to improve the performance of TPE. Our TPE implementation used in this paper is available at https://github.com/nabenabe0928/tpe/tree/single-opt.", "source": "arxiv", "source_id": "2304.11127v4", "match_status": "exact_title", "missing_reason": null}
{"key": "xavier_bouthillier_2022_0_2_6", "query_title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "normalized_title": "epistimio orion asynchronous distributed hyperparameter optimization", "title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "abstract": "***** Oríon *****", "source": "github", "source_id": "Epistimio/orion", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "mentzer2023finite", "query_title": "{Finite Scalar Quantization: VQ-VAE Made Simple}", "normalized_title": "finite scalar quantization vq vae made simple", "title": "Finite Scalar Quantization: VQ-VAE Made Simple", "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.", "source": "arxiv", "source_id": "2309.15505v2", "match_status": "exact_title", "missing_reason": null}
{"key": "du2024cosyvoiceAS", "query_title": "{CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens}", "normalized_title": "cosyvoice a scalable multilingual zero shot text to speech synthesizer based on supervised semantic tokens", "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "source": "arxiv", "source_id": "2407.05407v2", "match_status": "exact_title", "missing_reason": null}
{"key": "du2024cosyvoice", "query_title": "{Cosyvoice 2: Scalable streaming speech synthesis with large language models}", "normalized_title": "cosyvoice 2 scalable streaming speech synthesis with large language models", "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.", "source": "arxiv", "source_id": "2412.10117v3", "match_status": "exact_title", "missing_reason": null}
{"key": "tong2023improving", "query_title": "{Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport}", "normalized_title": "improving and generalizing flow based generative models with minibatch optimal transport", "title": "Improving and generalizing flow-based generative models with minibatch optimal transport", "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference.", "source": "arxiv", "source_id": "2302.00482v4", "match_status": "exact_title", "missing_reason": null}
{"key": "ho2020denoising", "query_title": "{Denoising diffusion probabilistic models}", "normalized_title": "denoising diffusion probabilistic models", "title": "Denoising Diffusion Probabilistic Models", "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion", "source": "arxiv", "source_id": "2006.11239v2", "match_status": "exact_title", "missing_reason": null}
{"key": "rombach2022high", "query_title": "{High-resolution image synthesis with latent diffusion models}", "normalized_title": "high resolution image synthesis with latent diffusion models", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .", "source": "arxiv", "source_id": "2112.10752v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kim2024neural", "query_title": "{Neural speech and audio coding}", "normalized_title": "neural speech and audio coding", "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs", "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "source": "arxiv", "source_id": "2408.06954", "match_status": "exact_id", "missing_reason": null}
{"key": "anees2024speech", "query_title": "{Speech coding techniques and challenges: A comprehensive literature survey}", "normalized_title": "speech coding techniques and challenges a comprehensive literature survey", "title": "Speech coding techniques and challenges: a comprehensive literature survey", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "source": "semantic_scholar", "source_id": "ba02cf95af70c5e29e26ad4db7f59360e85a3786", "match_status": "exact_title", "missing_reason": null}
{"key": "shannon1948mathematical", "query_title": "{A mathematical theory of communication}", "normalized_title": "a mathematical theory of communication", "title": "A Mathematical Theory of Communication", "abstract": "The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> and Hartley <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.", "source": "semantic_scholar", "source_id": "6d12a1d23b21a9b170118a56386552bc5d4727de", "match_status": "exact_title", "missing_reason": null}
{"key": "nyquist1928certain", "query_title": "{Certain topics in telegraph transmission theory}", "normalized_title": "certain topics in telegraph transmission theory", "title": "Certain Topics in Telegraph Transmission Theory", "abstract": "The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.", "source": "semantic_scholar", "source_id": "db0172576316dc748aea82e8f13fb4719ac933d5", "match_status": "exact_title", "missing_reason": null}
{"key": "o1988linear", "query_title": "{Linear predictive coding}", "normalized_title": "linear predictive coding", "title": "{Linear predictive coding}", "abstract": "The basic principles of linear predictive coding (LPC) are presented. Least-squares methods for obtaining the LPC coefficients characterizing the all-pole filter are described. Computational factors, instantaneous updating, and spectral estimation are discussed.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>", "source": "openalex", "source_id": null, "match_status": "exact_title", "missing_reason": null}
{"key": "wang2003modified", "query_title": "{Modified discrete cosine transform: Its implications for audio coding and error concealment}", "normalized_title": "modified discrete cosine transform its implications for audio coding and error concealment", "title": "Modified Discrete Cosine Transform: Its Implications for Audio Coding and Error Concealment", "abstract": "A study of the modified discrete cosine transform (MDCT) and its implications for audio coding and error concealment is presented from the perspective of Fourier frequency analysis. A relationship between MDCT and DFT via shifted discrete fourier transform (SDFT) is established, which provides a possible fast implementation of MDCT employing a fast Fourier transform (FFT) routine. The concept of time-domain alias cancellation (TDAC), the symmetric and nonorthogonal properties of MDCT, is analyzed and illustrated with intuitive examples. New insights are given for innovative solutions in audio codec design and MDCT-domain audio processing such as error concealment.", "source": "semantic_scholar", "source_id": "042aa1735de37d4b17972b795c412d572e0b32c9", "match_status": "exact_title", "missing_reason": null}
{"key": "noll1997mpeg", "query_title": "{MPEG digital audio coding}", "normalized_title": "mpeg digital audio coding", "title": "MPEG digital audio coding", "abstract": "The Moving Pictures Expert Group (MPEG) within the International Organization of Standardization (ISO) has developed a series of audio-visual standards known as MFEG-1 and MPEG-2. These audio-coding standards are the first international standards in the field of high-quality digital audio compression. MPEG-1 covers coding of stereophonic audio signals at high sampling rates aiming at transparent quality, whereas MPEG-2 also offers stereophonic audio coding at lower sampling rates. In addition, MPEG-2 introduces multichannel coding with and without backwards compatibility to MPEG-1 to provide an improved acoustical image for audio-only applications and for enhanced television and video-conferencing systems. MPEG-2 audio coding without backwards compatibility, called IMPEG-2 Advanced Audio Coding (AAC), offers the highest compression rates. Typical application areas for MPEG-based digital audio are in the fields of audio production, program distribution and exchange, digital sound broadcasting, digital storage, and various multimedia applications. We describe in some detail the key technologies and main features of MPEG-1 and MPEG-2 audio coders. We also present the MPEG-4 standard and discuss some of the typical applications for MPEG audio compression.", "source": "semantic_scholar", "source_id": "64dd904dc1a8eeb84dae812335a4d46b202f7f4b", "match_status": "exact_title", "missing_reason": null}
{"key": "painter2000perceptual", "query_title": "{Perceptual coding of digital audio}", "normalized_title": "perceptual coding of digital audio", "title": "Perceptual coding of digital audio", "abstract": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (-1, -2, -4), the Lucent Technologies PAC/EPAC/MPAC, the Dolby AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.", "source": "semantic_scholar", "source_id": "ecf98b58e733badb55d9b6a449b943dbf58342fb", "match_status": "exact_title", "missing_reason": null}
{"key": "jage2016celp", "query_title": "{CELP and MELP speech coding techniques}", "normalized_title": "celp and melp speech coding techniques", "title": "CELP and MELP speech coding techniques", "abstract": "Speech is one of the natural ways of communication amongst humans. Nowadays there is insatiable demand for speech communication as it carries more information like speaker identity, emotional state, prosodic nuance which adds naturalness in communication. With rapid growth and increased number of applications there exists a need for devising an approach for data compression techniques which reduces communication cost by using available bandwidth and storage space effectively. The speech coding techniques helps to achieve bit rate reduction by simultaneously maintaining original speech quality. In this paper, Hybrid speech coding technique i.e. Code Excited Linear Prediction (CELP) and Parametric coding technique i.e. Mixed Excitation Linear Prediction (MELP) are discussed and CELP technique is implemented using MATLAB. The parameters like mean square error (MSE), Mean Opinion Score (MOS), and Signal to Noise Ratio are calculated for CELP technique which shows that CELP technique is an improvement to a coder called Linear Predictive Coder (LPC). It is an efficient coding technique for the bit rate of 16-9.6 kbps. The MELP coder discussed here helps to remove the voicing error in two state excitation model of LPC. It is a low bit rate coder having a bit rate of 2.4 kbps and mainly used by military and federal standards.", "source": "semantic_scholar", "source_id": "3f58e96c1b528458a5a9a629920f1bf6cda84793", "match_status": "exact_title", "missing_reason": null}
{"key": "maimon2025scaling", "query_title": "{Scaling Analysis of Interleaved Speech-Text Language Models}", "normalized_title": "scaling analysis of interleaved speech text language models", "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .", "source": "arxiv", "source_id": "2504.02398v2", "match_status": "exact_title", "missing_reason": null}
{"key": "qwen2025qwen25technicalreport", "query_title": "{Qwen2.5 Technical Report}", "normalized_title": "qwen2 5 technical report", "title": "Qwen2.5 Technical Report", "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.", "source": "arxiv", "source_id": "2412.15115v2", "match_status": "exact_title", "missing_reason": null}
{"key": "cuervo2025textspeechlanguagemodelsimproved", "query_title": "{Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels}", "normalized_title": "text speech language models with improved cross modal transfer by aligning abstraction levels", "title": "Late Fusion and Multi-Level Fission Amplify Cross-Modal Transfer in Text-Speech LMs", "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly process and generate text and speech -- are commonly trained through an early modality fusion/fission approach, in which both modalities are fed and predicted from a shared backbone via linear layers. We hypothesize that this approach limits cross-modal transfer by neglecting feature compositionality -- specifically, the finer-grained nature of speech representations compared to text -- preventing the emergence of a shared feature hierarchy within model layers. In this paper, we argue that this limitation can be addressed through late fusion and fission, with a fission process that accesses both high- and low-level features for speech generation. Our models implementing these principles, SmolTolk, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute, and achieve significantly improved cross-modal performance relative to early fusion/fission baselines. Representation analyses further suggest that our method enhances the model's ability to abstract higher-level, more semantic features from speech, and leads to increasingly shared representation spaces across layers.", "source": "arxiv", "source_id": "2503.06211", "match_status": "exact_id", "missing_reason": null}
{"key": "kharitonov2022textfreeprosodyawaregenerativespoken", "query_title": "{Text-Free Prosody-Aware Generative Spoken Language Modeling}", "normalized_title": "text free prosody aware generative spoken language modeling", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "source": "arxiv", "source_id": "2109.03264v2", "match_status": "exact_title", "missing_reason": null}
{"key": "agustsson2017soft", "query_title": "{Soft-to-hard vector quantization for end-to-end learning compressible representations}", "normalized_title": "soft to hard vector quantization for end to end learning compressible representations", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.", "source": "arxiv", "source_id": "1704.00648v2", "match_status": "exact_title", "missing_reason": null}
{"key": "kankanahalli2018end", "query_title": "{End-to-end optimized speech coding with deep neural networks}", "normalized_title": "end to end optimized speech coding with deep neural networks", "title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "abstract": "Modern compression algorithms are often the result of laborious domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took years to develop and were largely hand-designed. We present a deep neural network model which optimizes all the steps of a wideband speech coding pipeline (compression, quantization, entropy coding, and decompression) end-to-end directly from raw speech data -- no manual feature engineering necessary, and it trains in hours. In testing, our DNN-based coder performs on par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps). It also runs in realtime on a 3.8GhZ Intel CPU.", "source": "arxiv", "source_id": "1710.09064v3", "match_status": "exact_title", "missing_reason": null}
{"key": "choi2024selfsupervisedspeechrepresentationsphonetic", "query_title": "{Self-Supervised Speech Representations are More Phonetic than Semantic}", "normalized_title": "self supervised speech representations are more phonetic than semantic", "title": "Self-Supervised Speech Representations are More Phonetic than Semantic", "abstract": "Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.", "source": "arxiv", "source_id": "2406.08619v1", "match_status": "exact_title", "missing_reason": null}
{"key": "grattafiori2024llama3herdmodels", "query_title": "{The llama 3 herd of models}", "normalized_title": "the llama 3 herd of models", "title": "The Llama 3 Herd of Models", "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.", "source": "arxiv", "source_id": "2407.21783v3", "match_status": "exact_title", "missing_reason": null}
{"key": "radhakrishnan2023whispering", "query_title": "{Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition}", "normalized_title": "whispering llama a cross modal generative error correction framework for speech recognition", "title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition", "abstract": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.", "source": "arxiv", "source_id": "2310.06434v2", "match_status": "exact_title", "missing_reason": null}
{"key": "libritts-alignments", "query_title": "{LibriTTS-Phones-and-Mel}", "normalized_title": "libritts phones and mel", "title": "{LibriTTS-Phones-and-Mel}", "abstract": "Dataset containing Mel Spectrograms, Prosody and Phone Alignments for the LibriTTS dataset.", "source": "huggingface", "source_id": "cdminix/libritts-phones-and-mel", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "ren2019fastspeech", "query_title": "{Fastspeech: Fast, robust and controllable text to speech}", "normalized_title": "fastspeech fast robust and controllable text to speech", "title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "abstract": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.", "source": "arxiv", "source_id": "1905.09263v5", "match_status": "exact_title", "missing_reason": null}
{"key": "morise2016world", "query_title": "{WORLD: a vocoder-based high-quality speech synthesis system for real-time applications}", "normalized_title": "world a vocoder based high quality speech synthesis system for real time applications", "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications", "abstract": "SUMMARY A vocoder-based speech synthesis system, named WORLD, was developed in an e ﬀ ort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been di ﬃ cult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The e ﬀ ectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.", "source": "semantic_scholar", "source_id": "ba91dabec842d507a647aab97ad224b4abdc1635", "match_status": "exact_title", "missing_reason": null}
{"key": "ren2020fastspeech", "query_title": "{FastSpeech 2: Fast and High-Quality End-to-End Text to Speech}", "normalized_title": "fastspeech 2 fast and high quality end to end text to speech", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.", "source": "arxiv", "source_id": "2006.04558v8", "match_status": "exact_title", "missing_reason": null}
{"key": "kim2021conditional", "query_title": "{Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech}", "normalized_title": "conditional variational autoencoder with adversarial learning for end to end text to speech", "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech", "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.", "source": "arxiv", "source_id": "2106.06103v1", "match_status": "exact_title", "missing_reason": null}
{"key": "ju2024naturalspeech", "query_title": "{NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models}", "normalized_title": "naturalspeech 3 zero shot speech synthesis with factorized codec and diffusion models", "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models", "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.", "source": "arxiv", "source_id": "2403.03100v3", "match_status": "exact_title", "missing_reason": null}
{"key": "mcauliffe2017montreal", "query_title": "{Montreal forced aligner: Trainable text-speech alignment using {k}aldi.}", "normalized_title": "montreal forced aligner trainable text speech alignment using kaldi", "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi", "abstract": "We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.", "source": "semantic_scholar", "source_id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7", "match_status": "exact_title", "missing_reason": null}
{"key": "har2025past", "query_title": "{PAST: Phonetic-Acoustic Speech Tokenizer}", "normalized_title": "past phonetic acoustic speech tokenizer", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST", "source": "arxiv", "source_id": "2505.14470v2", "match_status": "exact_title", "missing_reason": null}
{"key": "10889092", "query_title": "{What Are They Doing? Joint Audio-Speech Co-Reasoning}", "normalized_title": "what are they doing joint audio speech co reasoning", "title": "What Are They Doing? Joint Audio-Speech Co-Reasoning", "abstract": "In audio and speech processing, tasks usually focus on either the audio or speech modality, even when both sounds and human speech are present in the same audio clip. Recent Auditory Large Language Models (ALLMs) have made it possible to process audio and speech simultaneously within a single model, leading to further considerations of joint audio-speech tasks. In this paper, we establish a novel benchmark to investigate how well ALLMs can perform joint audio-speech processing. Specifically, we introduce Joint Audio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech processing, strictly requiring co-reasoning across both modalities. We also release a scene-reasoning dataset called \"What Are They Doing\". Additionally, we provide deeper insights into the models' behaviors by analyzing their dependence on each modality.", "source": "arxiv", "source_id": "2409.14526v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zengscaling", "query_title": "{Scaling Speech-Text Pre-training with Synthetic Interleaved Data}", "normalized_title": "scaling speech text pre training with synthetic interleaved data", "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.", "source": "arxiv", "source_id": "2411.17607v2", "match_status": "exact_title", "missing_reason": null}
{"key": "yosha2025stresstest", "query_title": "{{StressTest}: Can YOUR Speech {LM} Handle the Stress?}", "normalized_title": "stresstest can your speech lm handle the stress", "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "abstract": "Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.", "source": "arxiv", "source_id": "2505.22765v2", "match_status": "exact_title", "missing_reason": null}
{"key": "mousavi2025listen", "query_title": "{LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs}", "normalized_title": "listen learning soft token embeddings for neural audio llms", "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "abstract": "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.", "source": "arxiv", "source_id": "2505.18517v1", "match_status": "exact_title", "missing_reason": null}
{"key": "kumar2024sila", "query_title": "{SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation}", "normalized_title": "sila signal to language augmentation for enhanced control in text to audio generation", "title": "SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation", "abstract": "The field of text-to-audio generation has seen significant advancements, and yet the ability to finely control the acoustic characteristics of generated audio remains under-explored. In this paper, we introduce a novel yet simple approach to generate sound effects with control over key acoustic parameters such as loudness, pitch, reverb, fade, brightness, noise and duration, enabling creative applications in sound design and content creation. These parameters extend beyond traditional Digital Signal Processing (DSP) techniques, incorporating learned representations that capture the subtleties of how sound characteristics can be shaped in context, enabling a richer and more nuanced control over the generated audio. Our approach is model-agnostic and is based on learning the disentanglement between audio semantics and its acoustic features. Our approach not only enhances the versatility and expressiveness of text-to-audio generation but also opens new avenues for creative audio production and sound design. Our objective and subjective evaluation results demonstrate the effectiveness of our approach in producing high-quality, customizable audio outputs that align closely with user specifications.", "source": "arxiv", "source_id": "2412.09789v1", "match_status": "exact_title", "missing_reason": null}
{"key": "chen2025video", "query_title": "{Video-guided foley sound generation with multimodal controls}", "normalized_title": "video guided foley sound generation with multimodal controls", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "source": "arxiv", "source_id": "2411.17698v4", "match_status": "exact_title", "missing_reason": null}
{"key": "international1993coding", "query_title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "normalized_title": "coding of moving pictures and associated audio for digital storage media at up to about 1 5 mbit s", "title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "abstract": "BS EN ISO/IEC 11172-4:1997: The Standard for Information technology. Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s - Compliance testing", "source": "bsi", "source_id": "https://knowledge.bsigroup.com/products/information-technology-coding-of-moving-pictures-and-associated-audio-for-digital-storage-media-at-up-to-about-1-5-mbit-s-compliance-testing", "match_status": "fuzzy_title", "missing_reason": null}
{"key": "6530580", "query_title": "{MPEG Unified Speech and Audio Coding}", "normalized_title": "mpeg unified speech and audio coding", "title": "MPEG Unified Speech and Audio Coding", "abstract": "The MPEG Audio Subgroup has a rich history of accomplishments in creating music coding technology. At higher bit rates, MPEG technology can represent arbitrary sounds, including the human voice, with excellent quality. MPEG-1 and MPEG-2 Audio coders use perceptually shaped quantization noise as the primary tool for achieving compression. The MPEG-4 High-Efficiency Advanced Audio Coding (AAC) standard is a single technology capable of compressing speech, speech mixed with music, or music signals with quality that is always at least as good as the best of two state-of-the-art reference codecs, one optimized for speech and mixed content (AMR-WB B;) and the other optimized for music and general audio (HE-AACv2). This article provides an overview of the USAC architecture and summarizes the performance relative to the best state-of-the-art speech and audio codecs.", "source": "semantic_scholar", "source_id": "9d0ea1deb125e736478c6a07c89519c40eb7f154", "match_status": "exact_title", "missing_reason": null}
{"key": "devlin2019bert", "query_title": "{Bert: Pre-training of deep bidirectional transformers for language understanding}", "normalized_title": "bert pre training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "source": "arxiv", "source_id": "1810.04805v2", "match_status": "exact_title", "missing_reason": null}
{"key": "turetzky2024last", "query_title": "{Last: Language model aware speech tokenization}", "normalized_title": "last language model aware speech tokenization", "title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "source": "arxiv", "source_id": "2409.03701v2", "match_status": "exact_title", "missing_reason": null}
{"key": "zheng2025ervq", "query_title": "{ERVQ: Enhanced residual vector quantization with intra-and-inter-codebook optimization for neural audio codecs}", "normalized_title": "ervq enhanced residual vector quantization with intra and inter codebook optimization for neural audio codecs", "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.", "source": "arxiv", "source_id": "2410.12359v2", "match_status": "exact_title", "missing_reason": null}
{"key": "esser2021taming", "query_title": "{Taming transformers for high-resolution image synthesis}", "normalized_title": "taming transformers for high resolution image synthesis", "title": "Taming Transformers for High-Resolution Image Synthesis", "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .", "source": "arxiv", "source_id": "2012.09841v3", "match_status": "exact_title", "missing_reason": null}
