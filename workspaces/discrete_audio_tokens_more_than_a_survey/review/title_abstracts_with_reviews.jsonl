{"key": "itakura1968analysis", "query_title": "{Analysis synthesis telephony based on the maximum likelihood method}", "normalized_title": "analysis synthesis telephony based on the maximum likelihood method", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found", "review_count": 0, "reviews": []}
{"key": "atal1970speech", "query_title": "{Speech analysis and synthesis by linear prediction of the speech wave}", "normalized_title": "speech analysis and synthesis by linear prediction of the speech wave", "title": "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave", "abstract": "We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.", "source": "semantic_scholar", "source_id": "d738cfd0354ec9e70849057d5c2f20524a81e55a", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave", "abstract": "We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.", "metadata": {"arxiv_id": "semantic_scholar:d738cfd0354ec9e70849057d5c2f20524a81e55a", "title": "Speech Analysis and Synthesis by Linear Prediction of the Speech Wave", "summary": "We describe a procedure for efficient encoding of the speech wave by representing it in terms of time-varying parameters related to the transfer function of the vocal tract and the characteristics of the excitation. The speech wave, sampled at 10 kHz, is analyzed by predicting the present speech sample as a linear combination of the 12 previous samples. The 12 predictor coefficients are determined by minimizing the mean-squared error between the actual and the predicted values of the speech samples. Fifteen parameters—namely, the 12 predictor coefficients, the pitch period, a binary parameter indicating whether the speech is voiced or unvoiced, and the rms value of the speech samples—are derived by analysis of the speech wave, encoded and transmitted to the synthesizer. The speech wave is synthesized as the output of a linear recursive filter excited by either a sequence of quasiperiodic pulses or a white-noise source. Application of this method for efficient transmission and storage of speech signals as well as procedures for determining other speech characteristics, such as formant frequencies and bandwidths, the spectral envelope, and the autocorrelation function, are discussed.", "source": "semantic_scholar", "source_id": "d738cfd0354ec9e70849057d5c2f20524a81e55a", "year": 2000}, "round-A_JuniorNano_output": {"reasoning": "The abstract presents LPC-based continuous speech analysis/synthesis with predictor coefficients, not any discrete tokenization/codec/vocabulary for tokens, so it fails the discrete audio tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract presents LPC-based continuous speech analysis/synthesis with predictor coefficients, not any discrete tokenization/codec/vocabulary for tokens, so it fails the discrete audio tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on linear prediction for speech wave encoding and synthesis, describing parameters like predictor coefficients and pitch, but it does not mention any discrete tokenization process, use of neural audio codecs, vector quantization, or discrete representation mapping consistent with the \"Discrete Audio Tokens\" theme. The study concerns continuous speech signal modeling and traditional signal processing methods rather than discrete token sequences or neural discrete representations, thus failing the inclusion criteria related to discrete token generation and use.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on linear prediction for speech wave encoding and synthesis, describing parameters like predictor coefficients and pitch, but it does not mention any discrete tokenization process, use of neural audio codecs, vector quantization, or discrete representation mapping consistent with the \"Discrete Audio Tokens\" theme. The study concerns continuous speech signal modeling and traditional signal processing methods rather than discrete token sequences or neural discrete representations, thus failing the inclusion criteria related to discrete token generation and use.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "schroeder1985code", "query_title": "{Code-excited linear prediction (CELP): High-quality speech at very low bit rates}", "normalized_title": "code excited linear prediction celp high quality speech at very low bit rates", "title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates", "abstract": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.", "source": "semantic_scholar", "source_id": "02876a3eb2ffe88cadfbc9ac779f272ccdc6f91a", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates", "abstract": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.", "metadata": {"arxiv_id": "semantic_scholar:02876a3eb2ffe88cadfbc9ac779f272ccdc6f91a", "title": "Code-excited linear prediction(CELP): High-quality speech at very low bit rates", "summary": "We describe in this paper a code-excited linear predictive coder in which the optimum innovation sequence is selected from a code book of stored sequences to optimize a given fidelity criterion. Each sample of the innovation sequence is filtered sequentially through two time-varying linear recursive filters, one with a long-delay (related to pitch period) predictor in the feedback loop and the other with a short-delay predictor (related to spectral envelope) in the feedback loop. We code speech, sampled at 8 kHz, in blocks of 5-msec duration. Each block consisting of 40 samples is produced from one of 1024 possible innovation sequences. The bit rate for the innovation sequence is thus 1/4 bit per sample. We compare in this paper several different random and deterministic code books for their effectiveness in providing the optimum innovation sequence in each block. Our results indicate that a random code book has a slight speech quality advantage at low bit rates. Examples of speech produced by the above method will be played at the conference.", "source": "semantic_scholar", "source_id": "02876a3eb2ffe88cadfbc9ac779f272ccdc6f91a", "publication_date": "1985-04-26"}, "round-A_JuniorNano_output": {"reasoning": "This paper is a traditional CELP speech coder focused on optimizing an innovation sequence with a codebook for compression, not on generating or analyzing discrete audio tokens for modeling tasks, so it should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper is a traditional CELP speech coder focused on optimizing an innovation sequence with a codebook for compression, not on generating or analyzing discrete audio tokens for modeling tasks, so it should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper describes a traditional speech coding method (CELP) focusing on low bit rate speech coding using codebooks for innovation sequences, but does not discuss discrete audio tokens produced by neural audio codecs or self-supervised models, nor does it focus on token design impacting downstream modeling or cross-modal tasks; thus, it does not meet the inclusion criteria for discrete audio token research nor addresses tokenization methods relevant to the topic.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper describes a traditional speech coding method (CELP) focusing on low bit rate speech coding using codebooks for innovation sequences, but does not discuss discrete audio tokens produced by neural audio codecs or self-supervised models, nor does it focus on token design impacting downstream modeling or cross-modal tasks; thus, it does not meet the inclusion criteria for discrete audio token research nor addresses tokenization methods relevant to the topic.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "tian2025espnet", "query_title": "{{ESP}net-{S}peech{LM}: An open speech language model toolkit}", "normalized_title": "espnet speechlm an open speech language model toolkit", "title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "abstract": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.", "source": "semantic_scholar", "source_id": "6245784c904ca849d4ac5b92de4878ee633997dd", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "abstract": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.", "metadata": {"arxiv_id": "semantic_scholar:6245784c904ca849d4ac5b92de4878ee633997dd", "title": "ESPnet-SpeechLM: An Open Speech Language Model Toolkit", "summary": "We present ESPnet-SpeechLM, an open toolkit designed to democratize the development of speech language models (SpeechLMs) and voice-driven agentic applications. The toolkit standardizes speech processing tasks by framing them as universal sequential modeling problems, encompassing a cohesive workflow of data preprocessing, pre-training, inference, and task evaluation. With ESPnet-SpeechLM, users can easily define task templates and configure key settings, enabling seamless and streamlined SpeechLM development. The toolkit ensures flexibility, efficiency, and scalability by offering highly configurable modules for every stage of the workflow. To illustrate its capabilities, we provide multiple use cases demonstrating how competitive SpeechLMs can be constructed with ESPnet-SpeechLM, including a 1.7B-parameter model pre-trained on both text and speech tasks, across diverse benchmarks. The toolkit and its recipes are fully transparent and reproducible at: https://github.com/espnet/espnet/tree/speechlm.", "source": "semantic_scholar", "source_id": "6245784c904ca849d4ac5b92de4878ee633997dd", "year": 2025}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes an open toolkit for SpeechLM development and does not discuss discrete audio tokens, tokenizer/codec design, token vocabularies, or token-level analysis, so it does not satisfy the core inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes an open toolkit for SpeechLM development and does not discuss discrete audio tokens, tokenizer/codec design, token vocabularies, or token-level analysis, so it does not satisfy the core inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes ESPnet-SpeechLM as a toolkit for developing speech language models encompassing text and speech tasks, but it does not mention the use or generation of discrete audio tokens, neural audio codecs, or vector quantization methods that map continuous audio into discrete token sequences, which is central to the inclusion criteria. Without explicit information about discretization processes or use of discrete tokens as core representations, it cannot be deemed to meet the topic's core definition. Therefore, this study appears to focus more on end-to-end speech language modeling rather than discrete audio token modeling.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes ESPnet-SpeechLM as a toolkit for developing speech language models encompassing text and speech tasks, but it does not mention the use or generation of discrete audio tokens, neural audio codecs, or vector quantization methods that map continuous audio into discrete token sequences, which is central to the inclusion criteria. Without explicit information about discretization processes or use of discrete tokens as core representations, it cannot be deemed to meet the topic's core definition. Therefore, this study appears to focus more on end-to-end speech language modeling rather than discrete audio token modeling.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "hayashi2020discretalk", "query_title": "{Discretalk: Text-to-speech as a machine translation problem}", "normalized_title": "discretalk text to speech as a machine translation problem", "title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "abstract": "This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.", "source": "arxiv", "source_id": "2005.05525v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "abstract": "This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.", "metadata": {"arxiv_id": "2005.05525v1", "title": "DiscreTalk: Text-to-Speech as a Machine Translation Problem", "summary": "This paper proposes a new end-to-end text-to-speech (E2E-TTS) model based on neural machine translation (NMT). The proposed model consists of two components; a non-autoregressive vector quantized variational autoencoder (VQ-VAE) model and an autoregressive Transformer-NMT model. The VQ-VAE model learns a mapping function from a speech waveform into a sequence of discrete symbols, and then the Transformer-NMT model is trained to estimate this discrete symbol sequence from a given input text. Since the VQ-VAE model can learn such a mapping in a fully-data-driven manner, we do not need to consider hyperparameters of the feature extraction required in the conventional E2E-TTS models. Thanks to the use of discrete symbols, we can use various techniques developed in NMT and automatic speech recognition (ASR) such as beam search, subword units, and fusions with a language model. Furthermore, we can avoid an over smoothing problem of predicted features, which is one of the common issues in TTS. The experimental evaluation with the JSUT corpus shows that the proposed method outperforms the conventional Transformer-TTS model with a non-autoregressive neural vocoder in naturalness, achieving the performance comparable to the reconstruction of the VQ-VAE model.", "authors": ["Tomoki Hayashi", "Shinji Watanabe"], "published": "2020-05-12T02:45:09Z", "updated": "2020-05-12T02:45:09Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2005.05525v1", "landing_url": "https://arxiv.org/abs/2005.05525v1", "doi": "https://doi.org/10.48550/arXiv.2005.05525"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on discrete audio tokens produced by a neural audio codec (VQ-VAE) that maps waveforms to a sequence of discrete symbols and uses these tokens in an NMT-based TTS framework, fulfilling the discrete-token generation and downstream modeling criteria without triggering exclusion conditions.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper centers on discrete audio tokens produced by a neural audio codec (VQ-VAE) that maps waveforms to a sequence of discrete symbols and uses these tokens in an NMT-based TTS framework, fulfilling the discrete-token generation and downstream modeling criteria without triggering exclusion conditions.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a method that uses a VQ-VAE model to discretize continuous speech waveforms into sequences of discrete symbols, which aligns with the definition of \"Discrete Audio Tokens.\" It also discusses the usage of these discrete tokens in a Transformer-NMT model for text-to-speech synthesis and provides experimental evaluation, fulfilling the inclusion criteria for token generation details and application to audio generation tasks. The abstract is in English and clearly explains the token generation and usage, supporting reproducibility and evaluation. There are no exclusion criteria met, as the study focuses on discrete token representation for audio generation rather than continuous features or conventional compression methods.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a method that uses a VQ-VAE model to discretize continuous speech waveforms into sequences of discrete symbols, which aligns with the definition of \"Discrete Audio Tokens.\" It also discusses the usage of these discrete tokens in a Transformer-NMT model for text-to-speech synthesis and provides experimental evaluation, fulfilling the inclusion criteria for token generation details and application to audio generation tasks. The abstract is in English and clearly explains the token generation and usage, supporting reproducibility and evaluation. There are no exclusion criteria met, as the study focuses on discrete token representation for audio generation rather than continuous features or conventional compression methods.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "vqvae2017", "query_title": "{Neural discrete representation learning}", "normalized_title": "neural discrete representation learning", "title": "Neural Discrete Representation Learning", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "source": "arxiv", "source_id": "1711.00937v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Neural Discrete Representation Learning", "abstract": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "metadata": {"arxiv_id": "1711.00937v2", "title": "Neural Discrete Representation Learning", "summary": "Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of \"posterior collapse\" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.", "authors": ["Aaron van den Oord", "Oriol Vinyals", "Koray Kavukcuoglu"], "published": "2017-11-02T21:14:44Z", "updated": "2018-05-30T14:58:27Z", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/1711.00937v2", "landing_url": "https://arxiv.org/abs/1711.00937v2", "doi": "https://doi.org/10.48550/arXiv.1711.00937"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents a discrete latent representation approach (VQ-VAE) using vector quantization, with demonstrated relevance to speech/phonemes, aligning with discrete audio token concepts and potential downstream audio modeling tasks, thus it should be included.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper presents a discrete latent representation approach (VQ-VAE) using vector quantization, with demonstrated relevance to speech/phonemes, aligning with discrete audio token concepts and potential downstream audio modeling tasks, thus it should be included.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a Vector Quantised-Variational AutoEncoder (VQ-VAE) that learns discrete latent representations through vector quantization, applied to audio domains such as speech and phoneme learning, suggesting it covers the production of discrete audio tokens and their downstream usage in generative and understanding tasks, aligning well with the inclusion criteria for discrete audio token research using neural audio encoding and vector quantization; moreover, the abstract is in English with relevant technical details, making it evaluable for reproducibility and token design analysis.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a Vector Quantised-Variational AutoEncoder (VQ-VAE) that learns discrete latent representations through vector quantization, applied to audio domains such as speech and phoneme learning, suggesting it covers the production of discrete audio tokens and their downstream usage in generative and understanding tasks, aligning well with the inclusion criteria for discrete audio token research using neural audio encoding and vector quantization; moreover, the abstract is in English with relevant technical details, making it evaluable for reproducibility and token design analysis.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "zeghidour2021soundstream", "query_title": "{{SoundStream}: An End-to-End Neural Audio Codec}", "normalized_title": "soundstream an end to end neural audio codec", "title": "SoundStream: An End-to-End Neural Audio Codec", "abstract": "We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.", "source": "arxiv", "source_id": "2107.03312v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SoundStream: An End-to-End Neural Audio Codec", "abstract": "We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.", "metadata": {"arxiv_id": "2107.03312v1", "title": "SoundStream: An End-to-End Neural Audio Codec", "summary": "We present SoundStream, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. SoundStream relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3kbps to 18kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24kHz sampling rate, SoundStream at 3kbps outperforms Opus at 12kbps and approaches EVS at 9.6kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.", "authors": ["Neil Zeghidour", "Alejandro Luebs", "Ahmed Omran", "Jan Skoglund", "Marco Tagliasacchi"], "published": "2021-07-07T15:45:42Z", "updated": "2021-07-07T15:45:42Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2107.03312v1", "landing_url": "https://arxiv.org/abs/2107.03312v1", "doi": "https://doi.org/10.48550/arXiv.2107.03312"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes a neural audio codec (SoundStream) that uses a residual vector quantizer to produce discrete quantized embeddings, i.e., token-like discrete audio representations derived from a neural codec, satisfying the discrete audio tokens criterion; it provides English abstract and details on the quantization process/codebook, enabling assessment and reproducibility, and the tokenization mechanism is central to the method.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract describes a neural audio codec (SoundStream) that uses a residual vector quantizer to produce discrete quantized embeddings, i.e., token-like discrete audio representations derived from a neural codec, satisfying the discrete audio tokens criterion; it provides English abstract and details on the quantization process/codebook, enabling assessment and reproducibility, and the tokenization mechanism is central to the method.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents SoundStream, a neural audio codec using a fully convolutional encoder/decoder with residual vector quantization trained end-to-end to compress audio into quantized embeddings, which aligns with the inclusion criteria's emphasis on discrete audio tokens produced by neural codecs with vector quantization; however, the abstract lacks explicit details about the token vocabulary, multi-codebook architectures, or discrete token sequences used for downstream modeling or generation tasks. The work focuses mainly on compression and enhancement rather than discrete token design for generative or multimodal modeling. Therefore, while it partially aligns with the topic of neural audio codec based discretization, it does not sufficiently articulate discrete token sequence usage for modeling or cross-modal inference as required by the criteria.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The paper presents SoundStream, a neural audio codec using a fully convolutional encoder/decoder with residual vector quantization trained end-to-end to compress audio into quantized embeddings, which aligns with the inclusion criteria's emphasis on discrete audio tokens produced by neural codecs with vector quantization; however, the abstract lacks explicit details about the token vocabulary, multi-codebook architectures, or discrete token sequences used for downstream modeling or generation tasks. The work focuses mainly on compression and enhancement rather than discrete token design for generative or multimodal modeling. Therefore, while it partially aligns with the topic of neural audio codec based discretization, it does not sufficiently articulate discrete token sequence usage for modeling or cross-modal inference as required by the criteria.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "SoundStream is a neural audio codec that explicitly uses a residual vector quantizer and quantized embeddings (discrete representations) and reports codec design choices (quantizer layers, structured dropout for variable bitrates, bitrates), so it meets the topic of neural-audio-codec–based discretization; JuniorNano recommended inclusion noting the quantization/token mechanism is central, while JuniorMini correctly flagged the abstract lacks exact vocab/codebook sizes or downstream token-sequence usage, but the paper nonetheless proposes a discrete-token–producing codec and contributes measurable design improvements (low bitrate, multi-bitrate, low latency) relevant to the “Discrete Audio Tokens” theme. Therefore include.", "evaluation": 5}, "round-B_SeniorLead_reasoning": "SoundStream is a neural audio codec that explicitly uses a residual vector quantizer and quantized embeddings (discrete representations) and reports codec design choices (quantizer layers, structured dropout for variable bitrates, bitrates), so it meets the topic of neural-audio-codec–based discretization; JuniorNano recommended inclusion noting the quantization/token mechanism is central, while JuniorMini correctly flagged the abstract lacks exact vocab/codebook sizes or downstream token-sequence usage, but the paper nonetheless proposes a discrete-token–producing codec and contributes measurable design improvements (low bitrate, multi-bitrate, low latency) relevant to the “Discrete Audio Tokens” theme. Therefore include.", "round-B_SeniorLead_evaluation": 5, "final_verdict": "include (senior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "shi2024versa", "query_title": "{{VERSA}: A Versatile Evaluation Toolkit for Speech, Audio, and Music}", "normalized_title": "versa a versatile evaluation toolkit for speech audio and music", "title": "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music", "abstract": "In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at https://github.com/wavlab-speech/versa.", "source": "arxiv", "source_id": "2412.17667v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music", "abstract": "In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at https://github.com/wavlab-speech/versa.", "metadata": {"arxiv_id": "2412.17667v2", "title": "VERSA: A Versatile Evaluation Toolkit for Speech, Audio, and Music", "summary": "In this work, we introduce VERSA, a unified and standardized evaluation toolkit designed for various speech, audio, and music signals. The toolkit features a Pythonic interface with flexible configuration and dependency control, making it user-friendly and efficient. With full installation, VERSA offers 65 metrics with 729 metric variations based on different configurations. These metrics encompass evaluations utilizing diverse external resources, including matching and non-matching reference audio, text transcriptions, and text captions. As a lightweight yet comprehensive toolkit, VERSA is versatile to support the evaluation of a wide range of downstream scenarios. To demonstrate its capabilities, this work highlights example use cases for VERSA, including audio coding, speech synthesis, speech enhancement, singing synthesis, and music generation. The toolkit is available at https://github.com/wavlab-speech/versa.", "authors": ["Jiatong Shi", "Hye-jin Shim", "Jinchuan Tian", "Siddhant Arora", "Haibin Wu", "Darius Petermann", "Jia Qi Yip", "You Zhang", "Yuxun Tang", "Wangyou Zhang", "Dareen Safar Alharthi", "Yichen Huang", "Koichi Saito", "Jionghao Han", "Yiwen Zhao", "Chris Donahue", "Shinji Watanabe"], "published": "2024-12-23T15:53:21Z", "updated": "2025-03-27T03:50:10Z", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2412.17667v2", "landing_url": "https://arxiv.org/abs/2412.17667v2", "doi": "https://doi.org/10.48550/arXiv.2412.17667"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a general evaluation toolkit (VERSA) for speech, audio, and music, with many metrics, but it does not address discrete audio tokens, tokenizers, or token-based modeling as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a general evaluation toolkit (VERSA) for speech, audio, and music, with many metrics, but it does not address discrete audio tokens, tokenizers, or token-based modeling as required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study introduces VERSA, a comprehensive evaluation toolkit for speech, audio, and music with numerous metrics spanning various modalities and downstream applications. However, the abstract does not mention discrete audio tokens, their generation processes, or any tokenization method involving neural audio codecs or self-supervised models, which are central to the inclusion criteria. It focuses on evaluation metrics and toolkits rather than proposing or analyzing discrete token representations or their use in modeling, generation, or cross-modal tasks. Thus, it does not meet the inclusion criteria centered on discrete audio token generation and usage, nor does it provide the necessary technical details on tokenization, warranting exclusion.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study introduces VERSA, a comprehensive evaluation toolkit for speech, audio, and music with numerous metrics spanning various modalities and downstream applications. However, the abstract does not mention discrete audio tokens, their generation processes, or any tokenization method involving neural audio codecs or self-supervised models, which are central to the inclusion criteria. It focuses on evaluation metrics and toolkits rather than proposing or analyzing discrete token representations or their use in modeling, generation, or cross-modal tasks. Thus, it does not meet the inclusion criteria centered on discrete audio token generation and usage, nor does it provide the necessary technical details on tokenization, warranting exclusion.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wu2024towards", "query_title": "{Towards audio language modeling-an overview}", "normalized_title": "towards audio language modeling an overview", "title": "Towards audio language modeling -- an overview", "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.", "source": "arxiv", "source_id": "2402.13236v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards audio language modeling -- an overview", "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.", "metadata": {"arxiv_id": "2402.13236v1", "title": "Towards audio language modeling -- an overview", "summary": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.", "authors": ["Haibin Wu", "Xuanjun Chen", "Yi-Cheng Lin", "Kai-wei Chang", "Ho-Lam Chung", "Alexander H. Liu", "Hung-yi Lee"], "published": "2024-02-20T18:50:25Z", "updated": "2024-02-20T18:50:25Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2402.13236v1", "landing_url": "https://arxiv.org/abs/2402.13236v1", "doi": "https://doi.org/10.48550/arXiv.2402.13236"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly focuses on discrete audio tokens produced by neural codecs and their use in audio language modeling, addressing token design, evaluation, and downstream applications; it aligns with the 'Discrete Audio Tokens' inclusion criteria and does not fall under exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly focuses on discrete audio tokens produced by neural codecs and their use in audio language modeling, addressing token design, evaluation, and downstream applications; it aligns with the 'Discrete Audio Tokens' inclusion criteria and does not fall under exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper discusses neural audio codecs that convert continuous audio into discrete codes usable for audio language models, aligning with the inclusion criteria of studies focusing on discrete audio tokens derived via neural audio codecs and their use in modeling and generation tasks, and it provides an overview which may touch on token design and evaluation aspects; also, it has English title and abstract, making it eligible for evaluation.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper discusses neural audio codecs that convert continuous audio into discrete codes usable for audio language models, aligning with the inclusion criteria of studies focusing on discrete audio tokens derived via neural audio codecs and their use in modeling and generation tasks, and it provides an overview which may touch on token design and evaluation aspects; also, it has English title and abstract, making it eligible for evaluation.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "jiang2023disentangled", "query_title": "{Disentangled feature learning for real-time neural speech coding}", "normalized_title": "disentangled feature learning for real time neural speech coding", "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding", "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.", "source": "arxiv", "source_id": "2211.11960v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Disentangled Feature Learning for Real-Time Neural Speech Coding", "abstract": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.", "metadata": {"arxiv_id": "2211.11960v2", "title": "Disentangled Feature Learning for Real-Time Neural Speech Coding", "summary": "Recently end-to-end neural audio/speech coding has shown its great potential to outperform traditional signal analysis based audio codecs. This is mostly achieved by following the VQ-VAE paradigm where blind features are learned, vector-quantized and coded. In this paper, instead of blind end-to-end learning, we propose to learn disentangled features for real-time neural speech coding. Specifically, more global-like speaker identity and local content features are learned with disentanglement to represent speech. Such a compact feature decomposition not only achieves better coding efficiency by exploiting bit allocation among different features but also provides the flexibility to do audio editing in embedding space, such as voice conversion in real-time communications. Both subjective and objective results demonstrate its coding efficiency and we find that the learned disentangled features show comparable performance on any-to-any voice conversion with modern self-supervised speech representation learning models with far less parameters and low latency, showing the potential of our neural coding framework.", "authors": ["Xue Jiang", "Xiulian Peng", "Yuan Zhang", "Yan Lu"], "published": "2022-11-22T02:50:12Z", "updated": "2023-02-25T02:30:23Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2211.11960v2", "landing_url": "https://arxiv.org/abs/2211.11960v2", "doi": "https://doi.org/10.48550/arXiv.2211.11960"}, "round-A_JuniorNano_output": {"reasoning": "The paper references vector-quantized (discrete) representations in neural speech coding and real-time voice conversion, aligning with discrete token generation/use; however, its primary focus is on disentangled features rather than discrete token design, so it partially meets the criteria.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper references vector-quantized (discrete) representations in neural speech coding and real-time voice conversion, aligning with discrete token generation/use; however, its primary focus is on disentangled features rather than discrete token design, so it partially meets the criteria.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study proposes learning disentangled features for real-time neural speech coding following a VQ-VAE paradigm involving vector quantization, which indicates discrete token generation from neural audio coding; however, the abstract does not clearly detail the tokenizer architecture, codebook settings, or explicit discrete token sequence usage for generative or cross-modal tasks, making it partially aligned but with missing explicit token design and evaluation details required by criteria.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study proposes learning disentangled features for real-time neural speech coding following a VQ-VAE paradigm involving vector quantization, which indicates discrete token generation from neural audio coding; however, the abstract does not clearly detail the tokenizer architecture, codebook settings, or explicit discrete token sequence usage for generative or cross-modal tasks, making it partially aligned but with missing explicit token design and evaluation details required by criteria.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the paper references vector-quantized (VQ-VAE) representations and uses them as core features for neural speech coding and downstream any-to-any voice conversion/editing, which aligns with the “Discrete Audio Tokens” topic (neural codec producing discrete units used for generation/control). The abstract explicitly mentions vector quantization and that disentangled, compact features are learned and used for coding and real-time voice conversion, indicating discrete token usage as a central method; however, the abstract does not report low-level tokenizer details (codebook sizes, time downsampling, bitrate) required for full reproducibility per the strictest inclusion wording. Given the explicit use of VQ-based discrete representations and application to generative/editing tasks, I recommend inclusion pending full-paper verification of tokenizer/codebook details. One-sentence explanation: The study explicitly uses VQ-VAE/vector quantization to produce discrete, disentangled features used for neural speech coding and downstream voice conversion, so it aligns with the Discrete Audio Tokens topic, though the abstract omits detailed tokenizer/codebook specs.", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper references vector-quantized (VQ-VAE) representations and uses them as core features for neural speech coding and downstream any-to-any voice conversion/editing, which aligns with the “Discrete Audio Tokens” topic (neural codec producing discrete units used for generation/control). The abstract explicitly mentions vector quantization and that disentangled, compact features are learned and used for coding and real-time voice conversion, indicating discrete token usage as a central method; however, the abstract does not report low-level tokenizer details (codebook sizes, time downsampling, bitrate) required for full reproducibility per the strictest inclusion wording. Given the explicit use of VQ-based discrete representations and application to generative/editing tasks, I recommend inclusion pending full-paper verification of tokenizer/codebook details. One-sentence explanation: The study explicitly uses VQ-VAE/vector quantization to produce discrete, disentangled features used for neural speech coding and downstream voice conversion, so it aligns with the Discrete Audio Tokens topic, though the abstract omits detailed tokenizer/codebook specs.", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "lakhotia2021generative", "query_title": "{On generative spoken language modeling from raw audio}", "normalized_title": "on generative spoken language modeling from raw audio", "title": "On Generative Spoken Language Modeling from Raw Audio", "abstract": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1", "source": "semantic_scholar", "source_id": "7c39adb2049e79951dd6b92c970abaa4d81819b1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "On Generative Spoken Language Modeling from Raw Audio", "abstract": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1", "metadata": {"arxiv_id": "semantic_scholar:7c39adb2049e79951dd6b92c970abaa4d81819b1", "title": "On Generative Spoken Language Modeling from Raw Audio", "summary": "Abstract We introduce Generative Spoken Language Modeling, the task of learning the acoustic and linguistic characteristics of a language from raw audio (no text, no labels), and a set of metrics to automatically evaluate the learned representations at acoustic and linguistic levels for both encoding and generation. We set up baseline systems consisting of a discrete speech encoder (returning pseudo-text units), a generative language model (trained on pseudo- text), and a speech decoder (generating a waveform from pseudo-text) all trained without supervision and validate the proposed metrics with human evaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder- dependent way, and that some combinations approach text-based systems.1", "source": "semantic_scholar", "source_id": "7c39adb2049e79951dd6b92c970abaa4d81819b1", "publication_date": "2021-02-01"}, "round-A_JuniorNano_output": {"reasoning": "The work explicitly uses discrete token units produced by neural audio encoders (pseudo-text units) and analyzes how token design affects acoustic/linguistic generation and evaluation, aligning with the discrete tokens criteria and not falling under any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work explicitly uses discrete token units produced by neural audio encoders (pseudo-text units) and analyzes how token design affects acoustic/linguistic generation and evaluation, aligning with the discrete tokens criteria and not falling under any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a generative spoken language modeling approach from raw audio without text or labels, using discrete pseudo-text units obtained from different speech encoders and evaluates their impact on modeling; this matches the inclusion criteria about using discrete audio tokens from self-supervised models for downstream generation tasks, includes detailed description of discrete tokens from neural encoders, and focuses on token design affecting downstream tasks, while providing English title and abstract. It does not fall under any exclusion criteria since it centers on discrete token sequences and their evaluation for audio generation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a generative spoken language modeling approach from raw audio without text or labels, using discrete pseudo-text units obtained from different speech encoders and evaluates their impact on modeling; this matches the inclusion criteria about using discrete audio tokens from self-supervised models for downstream generation tasks, includes detailed description of discrete tokens from neural encoders, and focuses on token design affecting downstream tasks, while providing English title and abstract. It does not fall under any exclusion criteria since it centers on discrete token sequences and their evaluation for audio generation.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2023speechgen", "query_title": "{Speechgen: Unlocking the generative power of speech language models with prompts}", "normalized_title": "speechgen unlocking the generative power of speech language models with prompts", "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts", "abstract": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}", "source": "arxiv", "source_id": "2306.02207v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts", "abstract": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}", "metadata": {"arxiv_id": "2306.02207v3", "title": "SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts", "summary": "Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unified framework holds great promise for efficiency and effectiveness, particularly with the imminent arrival of advanced speech LMs, which will significantly enhance the capabilities of the framework. The code and demos of SpeechGen will be available on the project website: \\url{https://ga642381.github.io/SpeechPrompt/speechgen}", "authors": ["Haibin Wu", "Kai-Wei Chang", "Yuan-Kuei Wu", "Hung-yi Lee"], "published": "2023-06-03T22:35:27Z", "updated": "2023-08-25T16:10:18Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2306.02207v3", "landing_url": "https://arxiv.org/abs/2306.02207v3", "doi": "https://doi.org/10.48550/arXiv.2306.02207"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on prompting and generation with speech-language models rather than proposing or evaluating discrete audio token representations, so it fails the 'Discrete Audio Tokens' inclusion and fits the exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work centers on prompting and generation with speech-language models rather than proposing or evaluating discrete audio token representations, so it fails the 'Discrete Audio Tokens' inclusion and fits the exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces SpeechGen, a framework that applies prompt tuning to speech language models for generation tasks, emphasizing the use of continuous speech signals and LLMs processing discrete tokens. However, the abstract lacks explicit information about the generation or use of discrete audio tokens via discretization processes such as neural audio codecs or vector quantization, and does not clarify token design choices, tokenizers, or codebook details necessary to confirm alignment with the 'Discrete Audio Tokens' topic. Therefore, it is unclear whether the work fits the inclusion criteria centered on discrete token representations derived from continuous audio waveforms.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The paper introduces SpeechGen, a framework that applies prompt tuning to speech language models for generation tasks, emphasizing the use of continuous speech signals and LLMs processing discrete tokens. However, the abstract lacks explicit information about the generation or use of discrete audio tokens via discretization processes such as neural audio codecs or vector quantization, and does not clarify token design choices, tokenizers, or codebook details necessary to confirm alignment with the 'Discrete Audio Tokens' topic. Therefore, it is unclear whether the work fits the inclusion criteria centered on discrete token representations derived from continuous audio waveforms.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the same key gap: the abstract describes prompt tuning for speech LMs and generation (SpeechGen) but provides no information about discretizing continuous audio into tokens (no mention of neural audio codecs, vector quantization, tokenizers, codebooks, bitrate, or token design). The inclusion criteria require explicit, reproducible discrete audio-token methods or use of discrete units as core representations; this paper appears focused on prompting and continuous-speech adaptation without describing or evaluating discrete token schemes. Therefore it does not meet the “Discrete Audio Tokens” topic and should be excluded.", "evaluation": 1}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the same key gap: the abstract describes prompt tuning for speech LMs and generation (SpeechGen) but provides no information about discretizing continuous audio into tokens (no mention of neural audio codecs, vector quantization, tokenizers, codebooks, bitrate, or token design). The inclusion criteria require explicit, reproducible discrete audio-token methods or use of discrete units as core representations; this paper appears focused on prompting and continuous-speech adaptation without describing or evaluating discrete token schemes. Therefore it does not meet the “Discrete Audio Tokens” topic and should be excluded.", "round-B_SeniorLead_evaluation": 1, "final_verdict": "exclude (senior:1)", "review_skipped": false, "discard_reason": "exclude (senior:1)"}]}
{"key": "zhen2020psychoacoustic", "query_title": "{Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding}", "normalized_title": "psychoacoustic calibration of loss functions for efficient end to end neural audio coding", "title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding", "abstract": "Conventional audio coding technologies commonly leverage human perception of sound, or psychoacoustics, to reduce the bitrate while preserving the perceptual quality of the decoded audio signals. For neural audio codecs, however, the objective nature of the loss function usually leads to suboptimal sound quality as well as high run-time complexity due to the large model size. In this work, we present a psychoacoustic calibration scheme to re-define the loss functions of neural audio coding systems so that it can decode signals more perceptually similar to the reference, yet with a much lower model complexity. The proposed loss function incorporates the global masking threshold, allowing the reconstruction error that corresponds to inaudible artifacts. Experimental results show that the proposed model outperforms the baseline neural codec twice as large and consuming 23.4% more bits per second. With the proposed method, a lightweight neural codec, with only 0.9 million parameters, performs near-transparent audio coding comparable with the commercial MPEG-1 Audio Layer III codec at 112 kbps.", "source": "arxiv", "source_id": "2101.00054v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding", "abstract": "Conventional audio coding technologies commonly leverage human perception of sound, or psychoacoustics, to reduce the bitrate while preserving the perceptual quality of the decoded audio signals. For neural audio codecs, however, the objective nature of the loss function usually leads to suboptimal sound quality as well as high run-time complexity due to the large model size. In this work, we present a psychoacoustic calibration scheme to re-define the loss functions of neural audio coding systems so that it can decode signals more perceptually similar to the reference, yet with a much lower model complexity. The proposed loss function incorporates the global masking threshold, allowing the reconstruction error that corresponds to inaudible artifacts. Experimental results show that the proposed model outperforms the baseline neural codec twice as large and consuming 23.4% more bits per second. With the proposed method, a lightweight neural codec, with only 0.9 million parameters, performs near-transparent audio coding comparable with the commercial MPEG-1 Audio Layer III codec at 112 kbps.", "metadata": {"arxiv_id": "2101.00054v1", "title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End Neural Audio Coding", "summary": "Conventional audio coding technologies commonly leverage human perception of sound, or psychoacoustics, to reduce the bitrate while preserving the perceptual quality of the decoded audio signals. For neural audio codecs, however, the objective nature of the loss function usually leads to suboptimal sound quality as well as high run-time complexity due to the large model size. In this work, we present a psychoacoustic calibration scheme to re-define the loss functions of neural audio coding systems so that it can decode signals more perceptually similar to the reference, yet with a much lower model complexity. The proposed loss function incorporates the global masking threshold, allowing the reconstruction error that corresponds to inaudible artifacts. Experimental results show that the proposed model outperforms the baseline neural codec twice as large and consuming 23.4% more bits per second. With the proposed method, a lightweight neural codec, with only 0.9 million parameters, performs near-transparent audio coding comparable with the commercial MPEG-1 Audio Layer III codec at 112 kbps.", "authors": ["Kai Zhen", "Mi Suk Lee", "Jongmo Sung", "Seungkwon Beack", "Minje Kim"], "published": "2020-12-31T19:46:46Z", "updated": "2020-12-31T19:46:46Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2101.00054v1", "landing_url": "https://arxiv.org/abs/2101.00054v1", "doi": "https://doi.org/10.1109/LSP.2020.3039765"}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on psychoacoustic calibration of loss functions for end-to-end neural audio coding and does not involve discrete audio tokens, token generation, or token-based modeling, so it fails the token-centric inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work focuses on psychoacoustic calibration of loss functions for end-to-end neural audio coding and does not involve discrete audio tokens, token generation, or token-based modeling, so it fails the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on psychoacoustic calibration of loss functions for neural audio coding to improve perceptual audio quality and reduce model size. However, it does not explicitly mention discrete audio tokens, vector quantization, or tokenization processes that map continuous audio waveforms into discrete token sequences used in downstream tasks such as generation or cross-modal alignment. The emphasis is on loss function design for neural audio codecs rather than the design or evaluation of discrete audio token representations themselves. Hence, it does not meet the inclusion criteria focused on discrete audio tokens as core representations.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on psychoacoustic calibration of loss functions for neural audio coding to improve perceptual audio quality and reduce model size. However, it does not explicitly mention discrete audio tokens, vector quantization, or tokenization processes that map continuous audio waveforms into discrete token sequences used in downstream tasks such as generation or cross-modal alignment. The emphasis is on loss function design for neural audio codecs rather than the design or evaluation of discrete audio token representations themselves. Hence, it does not meet the inclusion criteria focused on discrete audio tokens as core representations.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "garbacea2019low", "query_title": "{Low bit-rate speech coding with VQ-VAE and a WaveNet decoder}", "normalized_title": "low bit rate speech coding with vq vae and a wavenet decoder", "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.", "source": "arxiv", "source_id": "1910.06464v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "abstract": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.", "metadata": {"arxiv_id": "1910.06464v1", "title": "Low Bit-Rate Speech Coding with VQ-VAE and a WaveNet Decoder", "summary": "In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.", "authors": ["Cristina Gârbacea", "Aäron van den Oord", "Yazhe Li", "Felicia S C Lim", "Alejandro Luebs", "Oriol Vinyals", "Thomas C Walters"], "published": "2019-10-14T23:54:08Z", "updated": "2019-10-14T23:54:08Z", "categories": ["cs.LG", "cs.SD", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1910.06464v1", "landing_url": "https://arxiv.org/abs/1910.06464v1", "doi": "https://doi.org/10.1109/ICASSP.2019.8683277"}, "round-A_JuniorNano_output": {"reasoning": "本研究使用 VQ-VAE（离散潜变量来自码本）对语音进行编码，产生离散 token 作为主要表征并支撑重构，符合离散音訊 token 的定义，故应包含。", "evaluation": 5}, "round-A_JuniorNano_reasoning": "本研究使用 VQ-VAE（离散潜变量来自码本）对语音进行编码，产生离散 token 作为主要表征并支撑重构，符合离散音訊 token 的定义，故应包含。", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study describes a neural network architecture using VQ-VAE, which is a vector quantization-based codec, to achieve low bit-rate speech coding. The use of vector quantized variational autoencoders aligns with the generation of discrete tokens from continuous audio signals, fulfilling the inclusion criterion of producing discrete audio tokens via neural audio codec methods. The abstract mentions the bit-rate and quality details, indicating information about discretization and codec design, addressing reproducibility and tokenizer settings. Although the abstract focuses primarily on compression and reconstruction quality, the architecture inherently produces discrete token sequences representing speech, making it relevant to the topic of discrete audio tokens. There is no indication this is purely continuous representation or traditional codec without neural discrete token modeling, nor is it focused on discriminative tasks only. Thus, it meets all inclusion criteria and none of the exclusion ones.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study describes a neural network architecture using VQ-VAE, which is a vector quantization-based codec, to achieve low bit-rate speech coding. The use of vector quantized variational autoencoders aligns with the generation of discrete tokens from continuous audio signals, fulfilling the inclusion criterion of producing discrete audio tokens via neural audio codec methods. The abstract mentions the bit-rate and quality details, indicating information about discretization and codec design, addressing reproducibility and tokenizer settings. Although the abstract focuses primarily on compression and reconstruction quality, the architecture inherently produces discrete token sequences representing speech, making it relevant to the topic of discrete audio tokens. There is no indication this is purely continuous representation or traditional codec without neural discrete token modeling, nor is it focused on discriminative tasks only. Thus, it meets all inclusion criteria and none of the exclusion ones.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "jang2024personalized", "query_title": "{Personalized neural speech codec}", "normalized_title": "personalized neural speech codec", "title": "Personalized Neural Speech Codec", "abstract": "In this paper, we propose a personalized neural speech codec, envisioning that personalization can reduce the model complexity or improve perceptual speech quality. Despite the common usage of speech codecs where only a single talker is involved on each side of the communication, personalizing a codec for the specific user has rarely been explored in the literature. First, we assume speakers can be grouped into smaller subsets based on their perceptual similarity. Then, we also postulate that a group-specific codec can focus on the group's speech characteristics to improve its perceptual quality and computational efficiency. To this end, we first develop a Siamese network that learns the speaker embeddings from the LibriSpeech dataset, which are then grouped into underlying speaker clusters. Finally, we retrain the LPCNet-based speech codec baselines on each of the speaker clusters. Subjective listening tests show that the proposed personalization scheme introduces model compression while maintaining speech quality. In other words, with the same model complexity, personalized codecs produce better speech quality.", "source": "arxiv", "source_id": "2404.00791v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Personalized Neural Speech Codec", "abstract": "In this paper, we propose a personalized neural speech codec, envisioning that personalization can reduce the model complexity or improve perceptual speech quality. Despite the common usage of speech codecs where only a single talker is involved on each side of the communication, personalizing a codec for the specific user has rarely been explored in the literature. First, we assume speakers can be grouped into smaller subsets based on their perceptual similarity. Then, we also postulate that a group-specific codec can focus on the group's speech characteristics to improve its perceptual quality and computational efficiency. To this end, we first develop a Siamese network that learns the speaker embeddings from the LibriSpeech dataset, which are then grouped into underlying speaker clusters. Finally, we retrain the LPCNet-based speech codec baselines on each of the speaker clusters. Subjective listening tests show that the proposed personalization scheme introduces model compression while maintaining speech quality. In other words, with the same model complexity, personalized codecs produce better speech quality.", "metadata": {"arxiv_id": "2404.00791v1", "title": "Personalized Neural Speech Codec", "summary": "In this paper, we propose a personalized neural speech codec, envisioning that personalization can reduce the model complexity or improve perceptual speech quality. Despite the common usage of speech codecs where only a single talker is involved on each side of the communication, personalizing a codec for the specific user has rarely been explored in the literature. First, we assume speakers can be grouped into smaller subsets based on their perceptual similarity. Then, we also postulate that a group-specific codec can focus on the group's speech characteristics to improve its perceptual quality and computational efficiency. To this end, we first develop a Siamese network that learns the speaker embeddings from the LibriSpeech dataset, which are then grouped into underlying speaker clusters. Finally, we retrain the LPCNet-based speech codec baselines on each of the speaker clusters. Subjective listening tests show that the proposed personalization scheme introduces model compression while maintaining speech quality. In other words, with the same model complexity, personalized codecs produce better speech quality.", "authors": ["Inseon Jang", "Haici Yang", "Wootaek Lim", "Seungkwon Beack", "Minje Kim"], "published": "2024-03-31T20:32:40Z", "updated": "2024-03-31T20:32:40Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2404.00791v1", "landing_url": "https://arxiv.org/abs/2404.00791v1", "doi": "https://doi.org/10.1109/ICASSP48485.2024.10446067"}, "round-A_JuniorNano_output": {"reasoning": "The abstract discusses a personalized neural speech codec and model compression across speaker clusters, with no mention of discrete audio tokens, tokenizers, or token-based modeling required by the criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract discusses a personalized neural speech codec and model compression across speaker clusters, with no mention of discrete audio tokens, tokenizers, or token-based modeling required by the criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on personalizing a neural speech codec using speaker embeddings and retraining codecs per speaker cluster, aiming to improve speech quality and reduce model complexity. However, it does not explicitly mention the discretization of audio waveforms into discrete tokens or the use of vector quantization or similar mechanisms for producing discrete tokens, nor does it provide sufficient detail on tokenizer architecture or codebook settings. Thus, it does not clearly meet the inclusion criteria centered on discrete audio token sequences as core representations for downstream modeling or generation.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on personalizing a neural speech codec using speaker embeddings and retraining codecs per speaker cluster, aiming to improve speech quality and reduce model complexity. However, it does not explicitly mention the discretization of audio waveforms into discrete tokens or the use of vector quantization or similar mechanisms for producing discrete tokens, nor does it provide sufficient detail on tokenizer architecture or codebook settings. Thus, it does not clearly meet the inclusion criteria centered on discrete audio token sequences as core representations for downstream modeling or generation.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "Kleijn2018wavenet", "query_title": "{Wavenet Based Low Rate Speech Coding}", "normalized_title": "wavenet based low rate speech coding", "title": "Wavenet based low rate speech coding", "abstract": "Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a WaveNet generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments confirm the high performance of the WaveNet based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not significantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model.", "source": "arxiv", "source_id": "1712.01120v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Wavenet based low rate speech coding", "abstract": "Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a WaveNet generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments confirm the high performance of the WaveNet based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not significantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model.", "metadata": {"arxiv_id": "1712.01120v1", "title": "Wavenet based low rate speech coding", "summary": "Traditional parametric coding of speech facilitates low rate but provides poor reconstruction quality because of the inadequacy of the model used. We describe how a WaveNet generative speech model can be used to generate high quality speech from the bit stream of a standard parametric coder operating at 2.4 kb/s. We compare this parametric coder with a waveform coder based on the same generative model and show that approximating the signal waveform incurs a large rate penalty. Our experiments confirm the high performance of the WaveNet based coder and show that the speech produced by the system is able to additionally perform implicit bandwidth extension and does not significantly impair recognition of the original speaker for the human listener, even when that speaker has not been used during the training of the generative model.", "authors": ["W. Bastiaan Kleijn", "Felicia S. C. Lim", "Alejandro Luebs", "Jan Skoglund", "Florian Stimberg", "Quan Wang", "Thomas C. Walters"], "published": "2017-12-01T09:45:30Z", "updated": "2017-12-01T09:45:30Z", "categories": ["eess.AS", "cs.SD", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/1712.01120v1", "landing_url": "https://arxiv.org/abs/1712.01120v1", "doi": "https://doi.org/10.48550/arXiv.1712.01120"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on WaveNet-based low-rate speech coding using a parametric coder and a waveform coder, with no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not satisfy the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on WaveNet-based low-rate speech coding using a parametric coder and a waveform coder, with no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not satisfy the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a WaveNet generative speech model used for low-rate parametric coding, focusing on speech quality and reconstruction from bitstreams of standard parametric coders. However, it does not mention any discrete tokenization or the creation of discrete audio token sequences as defined in the inclusion criteria; there is no discussion of quantization, codebooks, or token vocabularies used as core representations for downstream modeling. Therefore, the study primarily deals with continuous speech waveform modeling and coding rather than discrete token-based audio representations, which falls under exclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes a WaveNet generative speech model used for low-rate parametric coding, focusing on speech quality and reconstruction from bitstreams of standard parametric coders. However, it does not mention any discrete tokenization or the creation of discrete audio token sequences as defined in the inclusion criteria; there is no discussion of quantization, codebooks, or token vocabularies used as core representations for downstream modeling. Therefore, the study primarily deals with continuous speech waveform modeling and coding rather than discrete token-based audio representations, which falls under exclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "valin2019real", "query_title": "{A real-time wideband neural vocoder at 1.6 kb/s using LPCNet}", "normalized_title": "a real time wideband neural vocoder at 1 6 kb s using lpcnet", "title": "A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet", "abstract": "Neural speech synthesis algorithms are a promising new approach for coding speech at very low bitrate. They have so far demonstrated quality that far exceeds traditional vocoders, at the cost of very high complexity. In this work, we present a low-bitrate neural vocoder based on the LPCNet model. The use of linear prediction and sparse recurrent networks makes it possible to achieve real-time operation on general-purpose hardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP and that uncompressed LPCNet can exceed the quality of a waveform codec operating at low bitrate. This opens the way for new codec designs based on neural synthesis models.", "source": "arxiv", "source_id": "1903.12087v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet", "abstract": "Neural speech synthesis algorithms are a promising new approach for coding speech at very low bitrate. They have so far demonstrated quality that far exceeds traditional vocoders, at the cost of very high complexity. In this work, we present a low-bitrate neural vocoder based on the LPCNet model. The use of linear prediction and sparse recurrent networks makes it possible to achieve real-time operation on general-purpose hardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP and that uncompressed LPCNet can exceed the quality of a waveform codec operating at low bitrate. This opens the way for new codec designs based on neural synthesis models.", "metadata": {"arxiv_id": "1903.12087v2", "title": "A Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet", "summary": "Neural speech synthesis algorithms are a promising new approach for coding speech at very low bitrate. They have so far demonstrated quality that far exceeds traditional vocoders, at the cost of very high complexity. In this work, we present a low-bitrate neural vocoder based on the LPCNet model. The use of linear prediction and sparse recurrent networks makes it possible to achieve real-time operation on general-purpose hardware. We demonstrate that LPCNet operating at 1.6 kb/s achieves significantly higher quality than MELP and that uncompressed LPCNet can exceed the quality of a waveform codec operating at low bitrate. This opens the way for new codec designs based on neural synthesis models.", "authors": ["Jean-Marc Valin", "Jan Skoglund"], "published": "2019-03-28T16:09:58Z", "updated": "2019-06-28T02:14:53Z", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/1903.12087v2", "landing_url": "https://arxiv.org/abs/1903.12087v2", "doi": "https://doi.org/10.48550/arXiv.1903.12087"}, "round-A_JuniorNano_output": {"reasoning": "This study focuses on a real-time neural vocoder using LPCNet at very low bitrate and does not discuss discrete audio tokens, tokenization, codebooks, or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criterion for discrete token research.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This study focuses on a real-time neural vocoder using LPCNet at very low bitrate and does not discuss discrete audio tokens, tokenization, codebooks, or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criterion for discrete token research.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a neural vocoder for low-bitrate speech coding using LPCNet, focusing on speech synthesis quality and real-time operation. However, it does not explicitly mention discretizing continuous audio into discrete token sequences, nor describe producing discrete tokens, codebooks, or vocabularies as defined in the inclusion criteria. The work centers on waveform codecs and LPCNet neural vocoder operation rather than discrete audio tokens used for sequence modeling, generation, or alignment. Thus, it fails to meet the core inclusion criteria about discrete audio tokens as the main representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a neural vocoder for low-bitrate speech coding using LPCNet, focusing on speech synthesis quality and real-time operation. However, it does not explicitly mention discretizing continuous audio into discrete token sequences, nor describe producing discrete tokens, codebooks, or vocabularies as defined in the inclusion criteria. The work centers on waveform codecs and LPCNet neural vocoder operation rather than discrete audio tokens used for sequence modeling, generation, or alignment. Thus, it fails to meet the core inclusion criteria about discrete audio tokens as the main representation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "valin2019lpcnet", "query_title": "{{LPCN}et: Improving neural speech synthesis through linear prediction}", "normalized_title": "lpcnet improving neural speech synthesis through linear prediction", "title": "LPCNET: Improving Neural Speech Synthesis through Linear Prediction", "abstract": "Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.", "source": "semantic_scholar", "source_id": "da7329db3e14cb7301e9ce95a131136bd85e24ba", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LPCNET: Improving Neural Speech Synthesis through Linear Prediction", "abstract": "Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.", "metadata": {"arxiv_id": "semantic_scholar:da7329db3e14cb7301e9ce95a131136bd85e24ba", "title": "LPCNET: Improving Neural Speech Synthesis through Linear Prediction", "summary": "Neural speech synthesis models have recently demonstrated the ability to synthesize high quality speech for text-to-speech and compression applications. These new models often require powerful GPUs to achieve real-time operation, so being able to reduce their complexity would open the way for many new applications. We propose LPCNet, a WaveRNN variant that combines linear prediction with recurrent neural networks to significantly improve the efficiency of speech synthesis. We demonstrate that LPCNet can achieve significantly higher quality than WaveRNN for the same network size and that high quality LPCNet speech synthesis is achievable with a complexity under 3 GFLOPS. This makes it easier to deploy neural synthesis applications on lower-power devices, such as embedded systems and mobile phones.", "source": "semantic_scholar", "source_id": "da7329db3e14cb7301e9ce95a131136bd85e24ba", "publication_date": "2018-10-28"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses reducing computational complexity in neural speech synthesis using LPCNet; it does not involve discrete audio tokens, tokenizers, or token-based modeling as required.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses reducing computational complexity in neural speech synthesis using LPCNet; it does not involve discrete audio tokens, tokenizers, or token-based modeling as required.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on LPCNet, a model improving efficiency in neural speech synthesis by combining linear prediction with RNNs. However, it does not mention discretizing continuous audio waveforms into discrete token sequences, nor describe generating or using discrete audio tokens or codebooks, which are central to the inclusion criteria. The focus is on optimizing speech synthesis rather than on tokenization or discrete audio token modeling/use. Therefore, it fails to meet the core inclusion criteria regarding discrete audio tokens and their detailed generation and use.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on LPCNet, a model improving efficiency in neural speech synthesis by combining linear prediction with RNNs. However, it does not mention discretizing continuous audio waveforms into discrete token sequences, nor describe generating or using discrete audio tokens or codebooks, which are central to the inclusion criteria. The focus is on optimizing speech synthesis rather than on tokenization or discrete audio token modeling/use. Therefore, it fails to meet the core inclusion criteria regarding discrete audio tokens and their detailed generation and use.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yang2023neural", "query_title": "{Neural feature predictor and discriminative residual coding for low-bitrate speech coding}", "normalized_title": "neural feature predictor and discriminative residual coding for low bitrate speech coding", "title": "Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding", "abstract": "Low and ultra-low-bitrate neural speech coding achieves unprecedented coding gain by generating speech signals from compact speech features. This paper introduces additional coding efficiency in neural speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a recurrent neural predictor. The prediction can achieve a low-entropy residual representation, which we discriminatively code based on their contribution to the signal reconstruction. The harmonization of feature prediction and discriminative coding results in a dynamic bit allocation algorithm that spends more bits on unpredictable but rare events. As a result, we develop a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder. While the proposed method guarantees causality in its prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to LPCNet and Lyra V2 in the very low bitrates.", "source": "arxiv", "source_id": "2211.02506v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding", "abstract": "Low and ultra-low-bitrate neural speech coding achieves unprecedented coding gain by generating speech signals from compact speech features. This paper introduces additional coding efficiency in neural speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a recurrent neural predictor. The prediction can achieve a low-entropy residual representation, which we discriminatively code based on their contribution to the signal reconstruction. The harmonization of feature prediction and discriminative coding results in a dynamic bit allocation algorithm that spends more bits on unpredictable but rare events. As a result, we develop a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder. While the proposed method guarantees causality in its prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to LPCNet and Lyra V2 in the very low bitrates.", "metadata": {"arxiv_id": "2211.02506v1", "title": "Neural Feature Predictor and Discriminative Residual Coding for Low-Bitrate Speech Coding", "summary": "Low and ultra-low-bitrate neural speech coding achieves unprecedented coding gain by generating speech signals from compact speech features. This paper introduces additional coding efficiency in neural speech coding by reducing the temporal redundancy existing in the frame-level feature sequence via a recurrent neural predictor. The prediction can achieve a low-entropy residual representation, which we discriminatively code based on their contribution to the signal reconstruction. The harmonization of feature prediction and discriminative coding results in a dynamic bit allocation algorithm that spends more bits on unpredictable but rare events. As a result, we develop a scalable, lightweight, low-latency, and low-bitrate neural speech coding system. We demonstrate the advantage of the proposed methods using the LPCNet as a neural vocoder. While the proposed method guarantees causality in its prediction, the subjective tests and feature space analysis show that our model achieves superior coding efficiency compared to LPCNet and Lyra V2 in the very low bitrates.", "authors": ["Haici Yang", "Wootaek Lim", "Minje Kim"], "published": "2022-11-04T15:07:42Z", "updated": "2022-11-04T15:07:42Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2211.02506v1", "landing_url": "https://arxiv.org/abs/2211.02506v1", "doi": "https://doi.org/10.48550/arXiv.2211.02506"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on neural speech coding with predictive residuals and dynamic bit allocation, but there is no discussion of discrete audio tokens, tokenization methods, codebooks, or token-based modeling, so it does not meet the discrete-token inclusion criteria and violates the exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on neural speech coding with predictive residuals and dynamic bit allocation, but there is no discussion of discrete audio tokens, tokenization methods, codebooks, or token-based modeling, so it does not meet the discrete-token inclusion criteria and violates the exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on neural speech coding using feature prediction and discriminative residual coding to improve coding efficiency and bit allocation, but it does not explicitly describe any discrete tokenization process (such as vector quantization or token generation) that maps continuous audio to discrete tokens for downstream modeling or analysis; thus, it lacks clear evidence of using or analyzing discrete audio tokens as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on neural speech coding using feature prediction and discriminative residual coding to improve coding efficiency and bit allocation, but it does not explicitly describe any discrete tokenization process (such as vector quantization or token generation) that maps continuous audio to discrete tokens for downstream modeling or analysis; thus, it lacks clear evidence of using or analyzing discrete audio tokens as required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "Jang2016Gumbel", "query_title": "{Categorical reparameterization with gumbel-softmax}", "normalized_title": "categorical reparameterization with gumbel softmax", "title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "source": "arxiv", "source_id": "1611.01144v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Categorical Reparameterization with Gumbel-Softmax", "abstract": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "metadata": {"arxiv_id": "1611.01144v5", "title": "Categorical Reparameterization with Gumbel-Softmax", "summary": "Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.", "authors": ["Eric Jang", "Shixiang Gu", "Ben Poole"], "published": "2016-11-03T19:48:08Z", "updated": "2017-08-05T22:45:19Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1611.01144v5", "landing_url": "https://arxiv.org/abs/1611.01144v5", "doi": "https://doi.org/10.48550/arXiv.1611.01144"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on a gradient estimator for discrete latent variables (Gumbel-Softmax) in neural networks, not on discrete audio tokens, tokenization, or audio-centric discrete representations, so it does not meet the inclusion criteria and should be excluded per the stated criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on a gradient estimator for discrete latent variables (Gumbel-Softmax) in neural networks, not on discrete audio tokens, tokenization, or audio-centric discrete representations, so it does not meet the inclusion criteria and should be excluded per the stated criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on a novel gradient estimator for categorical variables in neural networks, which is a general machine learning method and does not specifically address discretizing continuous audio waveforms into discrete audio tokens, nor does it discuss neural audio codecs or tokenizers for audio representation as required by the inclusion criteria. There is no mention of discrete audio token generation processes, token design details, or audio tasks such as speech or music generation, understanding, or cross-modal reasoning that are central to the topic. Therefore, it does not meet the inclusion criteria and falls under exclusion categories related to lack of discrete audio token focus.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on a novel gradient estimator for categorical variables in neural networks, which is a general machine learning method and does not specifically address discretizing continuous audio waveforms into discrete audio tokens, nor does it discuss neural audio codecs or tokenizers for audio representation as required by the inclusion criteria. There is no mention of discrete audio token generation processes, token design details, or audio tasks such as speech or music generation, understanding, or cross-modal reasoning that are central to the topic. Therefore, it does not meet the inclusion criteria and falls under exclusion categories related to lack of discrete audio token focus.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "maddison2016concrete", "query_title": "{The concrete distribution: A continuous relaxation of discrete random variables}", "normalized_title": "the concrete distribution a continuous relaxation of discrete random variables", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.", "source": "arxiv", "source_id": "1611.00712v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "abstract": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.", "metadata": {"arxiv_id": "1611.00712v3", "title": "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables", "summary": "The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.", "authors": ["Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh"], "published": "2016-11-02T18:25:40Z", "updated": "2017-03-05T16:59:44Z", "categories": ["cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1611.00712v3", "landing_url": "https://arxiv.org/abs/1611.00712v3", "doi": "https://doi.org/10.48550/arXiv.1611.00712"}, "round-A_JuniorNano_output": {"reasoning": "The paper introduces Concrete distributions as continuous relaxations of discrete random variables, focusing on gradient estimators and ML optimization rather than discrete audio tokenization or token design; it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper introduces Concrete distributions as continuous relaxations of discrete random variables, focusing on gradient estimators and ML optimization rather than discrete audio tokenization or token design; it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on continuous relaxations of discrete random variables for gradient-based optimization, with applications in density estimation and structured prediction using neural networks. However, it does not discuss discrete audio tokens derived from audio waveforms, nor the discrete tokenization processes specifically for audio data such as neural audio codecs, vector quantization, or self-supervised audio tokenizers. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage for audio generative or multimodal modeling tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on continuous relaxations of discrete random variables for gradient-based optimization, with applications in density estimation and structured prediction using neural networks. However, it does not discuss discrete audio tokens derived from audio waveforms, nor the discrete tokenization processes specifically for audio data such as neural audio codecs, vector quantization, or self-supervised audio tokenizers. Therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage for audio generative or multimodal modeling tasks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "della2025focalcodec", "query_title": "{FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks}", "normalized_title": "focalcodec low bitrate speech coding via focal modulation networks", "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.", "source": "arxiv", "source_id": "2502.04465v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.", "metadata": {"arxiv_id": "2502.04465v2", "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks", "summary": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.", "authors": ["Luca Della Libera", "Francesco Paissan", "Cem Subakan", "Mirco Ravanelli"], "published": "2025-02-06T19:24:50Z", "updated": "2025-10-24T19:00:25Z", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2502.04465v2", "landing_url": "https://arxiv.org/abs/2502.04465v2", "doi": "https://doi.org/10.48550/arXiv.2502.04465"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes a neural audio codec using a discrete binary codebook to compress speech and evaluates downstream tasks (speech resynthesis, voice conversion, generative modeling), aligning with the Discrete Audio Tokens criteria, and provides English content suitable for evaluation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper describes a neural audio codec using a discrete binary codebook to compress speech and evaluates downstream tasks (speech resynthesis, voice conversion, generative modeling), aligning with the Discrete Audio Tokens criteria, and provides English content suitable for evaluation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study introduces FocalCodec, a low-bitrate neural audio codec that discretizes continuous speech into tokens via a single binary codebook, explicitly addressing discrete token design choices, bitrate, and downstream task evaluations, which aligns well with the inclusion criteria focusing on discrete audio tokens generated by neural audio codecs and their impact on downstream tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study introduces FocalCodec, a low-bitrate neural audio codec that discretizes continuous speech into tokens via a single binary codebook, explicitly addressing discrete token design choices, bitrate, and downstream task evaluations, which aligns well with the inclusion criteria focusing on discrete audio tokens generated by neural audio codecs and their impact on downstream tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "petermann2021harp", "query_title": "{Harp-net: Hyper-autoencoded reconstruction propagation for scalable neural audio coding}", "normalized_title": "harp net hyper autoencoded reconstruction propagation for scalable neural audio coding", "title": "HARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding", "abstract": "An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.", "source": "arxiv", "source_id": "2107.10843v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding", "abstract": "An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.", "metadata": {"arxiv_id": "2107.10843v2", "title": "HARP-Net: Hyper-Autoencoded Reconstruction Propagation for Scalable Neural Audio Coding", "summary": "An autoencoder-based codec employs quantization to turn its bottleneck layer activation into bitstrings, a process that hinders information flow between the encoder and decoder parts. To circumvent this issue, we employ additional skip connections between the corresponding pair of encoder-decoder layers. The assumption is that, in a mirrored autoencoder topology, a decoder layer reconstructs the intermediate feature representation of its corresponding encoder layer. Hence, any additional information directly propagated from the corresponding encoder layer helps the reconstruction. We implement this kind of skip connections in the form of additional autoencoders, each of which is a small codec that compresses the massive data transfer between the paired encoder-decoder layers. We empirically verify that the proposed hyper-autoencoded architecture improves perceptual audio quality compared to an ordinary autoencoder baseline.", "authors": ["Darius Petermann", "Seungkwon Beack", "Minje Kim"], "published": "2021-07-22T17:57:53Z", "updated": "2021-07-23T14:33:04Z", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2107.10843v2", "landing_url": "https://arxiv.org/abs/2107.10843v2", "doi": "https://doi.org/10.48550/arXiv.2107.10843"}, "round-A_JuniorNano_output": {"reasoning": "Although the abstract mentions quantization to discrete bitstrings, the work centers on hyper-autoencoded reconstruction for neural audio coding and does not propose or analyze discrete audio tokens, vocabularies, or token-based downstream modeling as required by the discrete audio tokens criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "Although the abstract mentions quantization to discrete bitstrings, the work centers on hyper-autoencoded reconstruction for neural audio coding and does not propose or analyze discrete audio tokens, vocabularies, or token-based downstream modeling as required by the discrete audio tokens criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The study proposes a neural audio codec architecture with additional autoencoders to improve audio reconstruction quality, but the abstract does not clearly describe the discretization of audio into discrete tokens, the design of tokenizers, codebooks, or related settings. It focuses on autoencoder-based compression and perceptual quality improvement rather than explicitly producing or analyzing discrete audio tokens used for sequence modeling, which is crucial to meet the inclusion criteria. Therefore, it likely does not meet the requirements focused on discrete token generation and analysis for audio generation or alignment tasks.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study proposes a neural audio codec architecture with additional autoencoders to improve audio reconstruction quality, but the abstract does not clearly describe the discretization of audio into discrete tokens, the design of tokenizers, codebooks, or related settings. It focuses on autoencoder-based compression and perceptual quality improvement rather than explicitly producing or analyzing discrete audio tokens used for sequence modeling, which is crucial to meet the inclusion criteria. Therefore, it likely does not meet the requirements focused on discrete token generation and analysis for audio generation or alignment tasks.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "Jiang2022EndtoEndNS", "query_title": "{End-to-End Neural Speech Coding for Real-Time Communications}", "normalized_title": "end to end neural speech coding for real time communications", "title": "End-to-End Neural Speech Coding for Real-Time Communications", "abstract": "Deep-learning based methods have shown their advantages in audio coding over traditional ones but limited attention has been paid on real-time communications (RTC). This paper proposes the TFNet, an end-to-end neural speech codec with low latency for RTC. It takes an encoder-temporal filtering-decoder paradigm that has seldom been investigated in audio coding. An interleaved structure is proposed for temporal filtering to capture both short-term and long-term temporal dependencies. Furthermore, with end-to-end optimization, the TFNet is jointly optimized with speech enhancement and packet loss concealment, yielding a one-for-all network for three tasks. Both subjective and objective results demonstrate the efficiency of the proposed TFNet.", "source": "arxiv", "source_id": "2201.09429v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "End-to-End Neural Speech Coding for Real-Time Communications", "abstract": "Deep-learning based methods have shown their advantages in audio coding over traditional ones but limited attention has been paid on real-time communications (RTC). This paper proposes the TFNet, an end-to-end neural speech codec with low latency for RTC. It takes an encoder-temporal filtering-decoder paradigm that has seldom been investigated in audio coding. An interleaved structure is proposed for temporal filtering to capture both short-term and long-term temporal dependencies. Furthermore, with end-to-end optimization, the TFNet is jointly optimized with speech enhancement and packet loss concealment, yielding a one-for-all network for three tasks. Both subjective and objective results demonstrate the efficiency of the proposed TFNet.", "metadata": {"arxiv_id": "2201.09429v3", "title": "End-to-End Neural Speech Coding for Real-Time Communications", "summary": "Deep-learning based methods have shown their advantages in audio coding over traditional ones but limited attention has been paid on real-time communications (RTC). This paper proposes the TFNet, an end-to-end neural speech codec with low latency for RTC. It takes an encoder-temporal filtering-decoder paradigm that has seldom been investigated in audio coding. An interleaved structure is proposed for temporal filtering to capture both short-term and long-term temporal dependencies. Furthermore, with end-to-end optimization, the TFNet is jointly optimized with speech enhancement and packet loss concealment, yielding a one-for-all network for three tasks. Both subjective and objective results demonstrate the efficiency of the proposed TFNet.", "authors": ["Xue Jiang", "Xiulian Peng", "Chengyu Zheng", "Huaying Xue", "Yuan Zhang", "Yan Lu"], "published": "2022-01-24T03:06:30Z", "updated": "2022-02-15T05:11:03Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2201.09429v3", "landing_url": "https://arxiv.org/abs/2201.09429v3", "doi": "https://doi.org/10.48550/arXiv.2201.09429"}, "round-A_JuniorNano_output": {"reasoning": "The study focuses on end-to-end neural speech coding with continuous representations and does not mention discrete tokens, tokenizers, vector quantization, or multi-codebook token designs; therefore it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The study focuses on end-to-end neural speech coding with continuous representations and does not mention discrete tokens, tokenizers, vector quantization, or multi-codebook token designs; therefore it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a neural speech codec (TFNet) optimized for real-time communication focusing on low latency, speech enhancement, and packet loss concealment. However, the abstract does not explicitly mention the generation of discrete audio tokens through discretization processes such as vector quantization, codebook design, or tokenizer training, nor does it indicate that the codec outputs discrete token sequences used for downstream sequence modeling or cross-modal tasks. The focus appears to be on end-to-end continuous neural speech coding rather than on discrete token representations or their systematic evaluation, which is central to the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper proposes a neural speech codec (TFNet) optimized for real-time communication focusing on low latency, speech enhancement, and packet loss concealment. However, the abstract does not explicitly mention the generation of discrete audio tokens through discretization processes such as vector quantization, codebook design, or tokenizer training, nor does it indicate that the codec outputs discrete token sequences used for downstream sequence modeling or cross-modal tasks. The focus appears to be on end-to-end continuous neural speech coding rather than on discrete token representations or their systematic evaluation, which is central to the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "wu2024codecslt", "query_title": "{Codec-{SUPERB}@ {SLT} 2024: A lightweight benchmark for neural audio codec models}", "normalized_title": "codec superb at slt 2024 a lightweight benchmark for neural audio codec models", "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models", "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.", "source": "arxiv", "source_id": "2409.14085v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models", "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.", "metadata": {"arxiv_id": "2409.14085v1", "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models", "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.", "authors": ["Haibin Wu", "Xuanjun Chen", "Yi-Cheng Lin", "Kaiwei Chang", "Jiawei Du", "Ke-Han Lu", "Alexander H. Liu", "Ho-Lam Chung", "Yuan-Kuei Wu", "Dongchao Yang", "Songxiang Liu", "Yi-Chiao Wu", "Xu Tan", "James Glass", "Shinji Watanabe", "Hung-yi Lee"], "published": "2024-09-21T09:39:36Z", "updated": "2024-09-21T09:39:36Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2409.14085v1", "landing_url": "https://arxiv.org/abs/2409.14085v1", "doi": "https://doi.org/10.48550/arXiv.2409.14085"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on benchmarking neural audio codecs and does not discuss discrete audio tokens, tokenizers, or token-based modeling as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on benchmarking neural audio codecs and does not discuss discrete audio tokens, tokenizers, or token-based modeling as required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a benchmark challenge for neural audio codecs, which serve as tokenizers that discretize continuous audio into tokens for downstream modeling; it discusses codec design choices, datasets, metrics, and results, aligning well with the inclusion criteria focused on discrete audio tokens generated by neural audio codecs and their evaluation, and provides sufficient methodological detail in English to judge the discrete token representations involved.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents a benchmark challenge for neural audio codecs, which serve as tokenizers that discretize continuous audio into tokens for downstream modeling; it discusses codec design choices, datasets, metrics, and results, aligning well with the inclusion criteria focused on discrete audio tokens generated by neural audio codecs and their evaluation, and provides sufficient methodological detail in English to judge the discrete token representations involved.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": {"reasoning": "I reviewed the abstract and the two junior reviewers' comments: JuniorNano argued to exclude because the abstract does not detail discrete token generation, while JuniorMini argued to include because the work is a benchmark of neural audio codecs that serve as tokenizers and compares design/metrics. The paper explicitly frames neural audio codecs as tokenizers for audio and describes a systematic benchmark with datasets, metrics, participating systems, and results—this strongly aligns with the inclusion group covering systematic comparisons/analyses of codec-based discrete token designs and their evaluations; however the abstract lacks explicit quantization/codebook details (so there is some residual uncertainty until the full methods are inspected). Therefore I judge this as better to include. One-sentence explanation: I score 4 because the work is a benchmark of neural audio codecs (tokenizers) with systematic evaluation that matches the discrete-audio-token topic, though the abstract omits low-level quantization/codebook details so I stop short of a definitive \"absolutely include.\"", "evaluation": 4}, "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' comments: JuniorNano argued to exclude because the abstract does not detail discrete token generation, while JuniorMini argued to include because the work is a benchmark of neural audio codecs that serve as tokenizers and compares design/metrics. The paper explicitly frames neural audio codecs as tokenizers for audio and describes a systematic benchmark with datasets, metrics, participating systems, and results—this strongly aligns with the inclusion group covering systematic comparisons/analyses of codec-based discrete token designs and their evaluations; however the abstract lacks explicit quantization/codebook details (so there is some residual uncertainty until the full methods are inspected). Therefore I judge this as better to include. One-sentence explanation: I score 4 because the work is a benchmark of neural audio codecs (tokenizers) with systematic evaluation that matches the discrete-audio-token topic, though the abstract omits low-level quantization/codebook details so I stop short of a definitive \"absolutely include.\"", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "yang2021source", "query_title": "{Source-aware neural speech coding for noisy speech compression}", "normalized_title": "source aware neural speech coding for noisy speech compression", "title": "Source-Aware Neural Speech Coding for Noisy Speech Compression", "abstract": "This paper introduces a novel neural network-based speech coding system that can process noisy speech effectively. The proposed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources so that the more important source sounds better in the decoded signal. We target a new use case where the user on the receiver side cares about the quality of the non-speech components in speech communication, while the speech source still carries the most crucial information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech better than the baseline neural audio coding system, which is with no source-aware coding mechanism, and two conventional codecs.", "source": "arxiv", "source_id": "2008.12889v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Source-Aware Neural Speech Coding for Noisy Speech Compression", "abstract": "This paper introduces a novel neural network-based speech coding system that can process noisy speech effectively. The proposed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources so that the more important source sounds better in the decoded signal. We target a new use case where the user on the receiver side cares about the quality of the non-speech components in speech communication, while the speech source still carries the most crucial information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech better than the baseline neural audio coding system, which is with no source-aware coding mechanism, and two conventional codecs.", "metadata": {"arxiv_id": "2008.12889v2", "title": "Source-Aware Neural Speech Coding for Noisy Speech Compression", "summary": "This paper introduces a novel neural network-based speech coding system that can process noisy speech effectively. The proposed source-aware neural audio coding (SANAC) system harmonizes a deep autoencoder-based source separation model and a neural coding system so that it can explicitly perform source separation and coding in the latent space. An added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources so that the more important source sounds better in the decoded signal. We target a new use case where the user on the receiver side cares about the quality of the non-speech components in speech communication, while the speech source still carries the most crucial information. Both objective and subjective evaluation tests show that SANAC can recover the original noisy speech better than the baseline neural audio coding system, which is with no source-aware coding mechanism, and two conventional codecs.", "authors": ["Haici Yang", "Kai Zhen", "Seungkwon Beack", "Minje Kim"], "published": "2020-08-29T01:22:12Z", "updated": "2020-11-10T15:00:57Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2008.12889v2", "landing_url": "https://arxiv.org/abs/2008.12889v2", "doi": "https://doi.org/10.48550/arXiv.2008.12889"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes a continuous latent space neural audio codec with source separation; there is no mention of discrete tokens, tokenization, codebooks, or explicit quantization needed for discrete audio tokens, so it does not meet the token/discrete representation criterion.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper describes a continuous latent space neural audio codec with source separation; there is no mention of discrete tokens, tokenization, codebooks, or explicit quantization needed for discrete audio tokens, so it does not meet the token/discrete representation criterion.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses a neural network-based speech coding system targeting noisy speech compression, focusing on source-aware coding that improves quality and bit allocation for sources. However, the abstract does not describe any discretization process that converts continuous audio waveforms into discrete tokens, nor does it mention vector quantization or other discrete token generation methods essential to the inclusion criteria. The focus is on neural coding and source separation for compression quality, which aligns more with continuous audio representation or neural codec but lacks evidence of discrete token sequences or a tokenization process required by the topic definition. Therefore, this work does not meet the core inclusion criteria related to discrete audio tokens.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper discusses a neural network-based speech coding system targeting noisy speech compression, focusing on source-aware coding that improves quality and bit allocation for sources. However, the abstract does not describe any discretization process that converts continuous audio waveforms into discrete tokens, nor does it mention vector quantization or other discrete token generation methods essential to the inclusion criteria. The focus is on neural coding and source separation for compression quality, which aligns more with continuous audio representation or neural codec but lacks evidence of discrete token sequences or a tokenization process required by the topic definition. Therefore, this work does not meet the core inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "omran2023disentangling", "query_title": "{Disentangling speech from surroundings with neural embeddings}", "normalized_title": "disentangling speech from surroundings with neural embeddings", "title": "Disentangling speech from surroundings with neural embeddings", "abstract": "We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.", "source": "arxiv", "source_id": "2203.15578v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Disentangling speech from surroundings with neural embeddings", "abstract": "We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.", "metadata": {"arxiv_id": "2203.15578v2", "title": "Disentangling speech from surroundings with neural embeddings", "summary": "We present a method to separate speech signals from noisy environments in the embedding space of a neural audio codec. We introduce a new training procedure that allows our model to produce structured encodings of audio waveforms given by embedding vectors, where one part of the embedding vector represents the speech signal, and the rest represent the environment. We achieve this by partitioning the embeddings of different input waveforms and training the model to faithfully reconstruct audio from mixed partitions, thereby ensuring each partition encodes a separate audio attribute. As use cases, we demonstrate the separation of speech from background noise or from reverberation characteristics. Our method also allows for targeted adjustments of the audio output characteristics.", "authors": ["Ahmed Omran", "Neil Zeghidour", "Zalán Borsos", "Félix de Chaumont Quitry", "Malcolm Slaney", "Marco Tagliasacchi"], "published": "2022-03-29T13:58:33Z", "updated": "2023-06-04T18:08:38Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2203.15578v2", "landing_url": "https://arxiv.org/abs/2203.15578v2", "doi": "https://doi.org/10.48550/arXiv.2203.15578"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on separating speech from surrounding audio in an embedding space of a neural audio codec and does not describe discrete token generation/quantization or token-based modeling, hence it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on separating speech from surrounding audio in an embedding space of a neural audio codec and does not describe discrete token generation/quantization or token-based modeling, hence it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a method involving neural audio codec embeddings to separate speech from noise, but it does not clearly describe producing discrete audio tokens (i.e., token sequences derived via discretization such as vector quantization) that are central to the inclusion criteria, nor does it mention token design, tokenizers, or vocabulary/codebook details required to qualify as discrete audio tokens for downstream sequence modeling.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents a method involving neural audio codec embeddings to separate speech from noise, but it does not clearly describe producing discrete audio tokens (i.e., token sequences derived via discretization such as vector quantization) that are central to the inclusion criteria, nor does it mention token design, tokenizers, or vocabulary/codebook details required to qualify as discrete audio tokens for downstream sequence modeling.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "Ji2024LanguageCodecRT", "query_title": "{Language-Codec: Reducing the Gaps Between Discrete Codec Representation and Speech Language Models}", "normalized_title": "language codec reducing the gaps between discrete codec representation and speech language models", "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models", "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .", "source": "arxiv", "source_id": "2402.12208", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models", "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .", "metadata": {"arxiv_id": "2402.12208", "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models", "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .", "authors": ["Shengpeng Ji", "Minghui Fang", "Jialong Zuo", "Ziyue Jiang", "Dingdong Wang", "Hanting Wang", "Hai Huang", "Zhou Zhao"], "published": "2024-02-19T15:12:12Z", "updated": "2025-06-04T05:50:15Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2402.12208v4", "landing_url": "https://arxiv.org/abs/2402.12208v4", "doi": "https://doi.org/10.48550/arXiv.2402.12208"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on discrete audio tokens produced by neural codecs (discrete codebooks, residual vector quantization) and proposes a discrete-token mechanism (MCRVQ) to bridge these representations with downstream speech-language models, with evaluation on downstream tasks; it provides English title/abstract and discusses token design and downstream impact, meeting all inclusion criteria and none of the exclusions.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens produced by neural codecs (discrete codebooks, residual vector quantization) and proposes a discrete-token mechanism (MCRVQ) to bridge these representations with downstream speech-language models, with evaluation on downstream tasks; it provides English title/abstract and discusses token design and downstream impact, meeting all inclusion criteria and none of the exclusions.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper discusses a neural audio codec that produces discrete acoustic tokens via residual vector quantization and applies these tokens for improving downstream speech language models, aligning well with the inclusion criteria of discrete audio tokens generated by neural codecs with multi-codebook features and validated on downstream generative tasks. It provides design details, evaluations against other algorithms, and mentions code availability for reproducibility. Therefore, it clearly fits the topic of discrete audio tokens and their application in speech generation and language modeling without falling into any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper discusses a neural audio codec that produces discrete acoustic tokens via residual vector quantization and applies these tokens for improving downstream speech language models, aligning well with the inclusion criteria of discrete audio tokens generated by neural codecs with multi-codebook features and validated on downstream generative tasks. It provides design details, evaluations against other algorithms, and mentions code availability for reproducibility. Therefore, it clearly fits the topic of discrete audio tokens and their application in speech generation and language modeling without falling into any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "Pan2024PSCodecAS", "query_title": "{PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders}", "normalized_title": "pscodec a series of high fidelity low bitrate neural speech codecs leveraging prompt encoders", "title": "PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders", "abstract": "Neural speech codecs have recently emerged as a focal point in the fields of speech compression and generation. Despite this progress, achieving high-quality speech reconstruction under low-bitrate scenarios remains a significant challenge. In this paper, we propose PSCodec, a series of neural speech codecs based on prompt encoders, comprising PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN, which are capable of delivering high-performance speech reconstruction with low bandwidths. Specifically, we first introduce PSCodec-Base, which leverages a pretrained speaker verification model-based prompt encoder (VPP-Enc) and a learnable Mel-spectrogram-based prompt encoder (MelP-Enc) to effectively disentangle and integrate voiceprint and Mel-related features in utterances. To further enhance feature utilization efficiency, we propose PSCodec-DRL-ICT, incorporating a structural similarity (SSIM) based disentangled representation loss (DRL) and an incremental continuous training (ICT) strategy. While PSCodec-DRL-ICT demonstrates impressive performance, its reliance on extensive hyperparameter tuning and multi-stage training makes it somewhat labor-intensive. To circumvent these limitations, we propose PSCodec-CasAN, utilizing an advanced cascaded attention network (CasAN) to enhance representational capacity of the entire system. Extensive experiments show that our proposed PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN all significantly outperform several state-of-the-art neural codecs, exhibiting substantial improvements in both speech reconstruction quality and speaker similarity under low-bitrate conditions.", "source": "arxiv", "source_id": "2404.02702v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders", "abstract": "Neural speech codecs have recently emerged as a focal point in the fields of speech compression and generation. Despite this progress, achieving high-quality speech reconstruction under low-bitrate scenarios remains a significant challenge. In this paper, we propose PSCodec, a series of neural speech codecs based on prompt encoders, comprising PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN, which are capable of delivering high-performance speech reconstruction with low bandwidths. Specifically, we first introduce PSCodec-Base, which leverages a pretrained speaker verification model-based prompt encoder (VPP-Enc) and a learnable Mel-spectrogram-based prompt encoder (MelP-Enc) to effectively disentangle and integrate voiceprint and Mel-related features in utterances. To further enhance feature utilization efficiency, we propose PSCodec-DRL-ICT, incorporating a structural similarity (SSIM) based disentangled representation loss (DRL) and an incremental continuous training (ICT) strategy. While PSCodec-DRL-ICT demonstrates impressive performance, its reliance on extensive hyperparameter tuning and multi-stage training makes it somewhat labor-intensive. To circumvent these limitations, we propose PSCodec-CasAN, utilizing an advanced cascaded attention network (CasAN) to enhance representational capacity of the entire system. Extensive experiments show that our proposed PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN all significantly outperform several state-of-the-art neural codecs, exhibiting substantial improvements in both speech reconstruction quality and speaker similarity under low-bitrate conditions.", "metadata": {"arxiv_id": "2404.02702v3", "title": "PSCodec: A Series of High-Fidelity Low-bitrate Neural Speech Codecs Leveraging Prompt Encoders", "summary": "Neural speech codecs have recently emerged as a focal point in the fields of speech compression and generation. Despite this progress, achieving high-quality speech reconstruction under low-bitrate scenarios remains a significant challenge. In this paper, we propose PSCodec, a series of neural speech codecs based on prompt encoders, comprising PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN, which are capable of delivering high-performance speech reconstruction with low bandwidths. Specifically, we first introduce PSCodec-Base, which leverages a pretrained speaker verification model-based prompt encoder (VPP-Enc) and a learnable Mel-spectrogram-based prompt encoder (MelP-Enc) to effectively disentangle and integrate voiceprint and Mel-related features in utterances. To further enhance feature utilization efficiency, we propose PSCodec-DRL-ICT, incorporating a structural similarity (SSIM) based disentangled representation loss (DRL) and an incremental continuous training (ICT) strategy. While PSCodec-DRL-ICT demonstrates impressive performance, its reliance on extensive hyperparameter tuning and multi-stage training makes it somewhat labor-intensive. To circumvent these limitations, we propose PSCodec-CasAN, utilizing an advanced cascaded attention network (CasAN) to enhance representational capacity of the entire system. Extensive experiments show that our proposed PSCodec-Base, PSCodec-DRL-ICT, and PSCodec-CasAN all significantly outperform several state-of-the-art neural codecs, exhibiting substantial improvements in both speech reconstruction quality and speaker similarity under low-bitrate conditions.", "authors": ["Yu Pan", "Xiang Zhang", "Yuguang Yang", "Jixun Yao", "Yanni Hu", "Jianhao Ye", "Hongbin Zhou", "Lei Ma", "Jianjun Zhao"], "published": "2024-04-03T13:00:08Z", "updated": "2024-11-21T10:31:03Z", "categories": ["cs.SD", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2404.02702v3", "landing_url": "https://arxiv.org/abs/2404.02702v3", "doi": "https://doi.org/10.48550/arXiv.2404.02702"}, "round-A_JuniorNano_output": {"reasoning": "This paper discusses neural speech codecs with prompt encoders for low-bitrate speech reconstruction, but there is no discussion of discrete audio tokens, tokenizers, codebooks, or quantization methods; thus it does not meet the discrete audio token criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper discusses neural speech codecs with prompt encoders for low-bitrate speech reconstruction, but there is no discussion of discrete audio tokens, tokenizers, codebooks, or quantization methods; thus it does not meet the discrete audio token criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents neural speech codecs designed for low-bitrate speech reconstruction, focusing on speech compression and generation quality improvements. However, it does not explicitly mention the use or generation of discrete audio tokens, such as discretization into token sequences, vector quantization details, codebook settings, or using discrete tokens as core inputs for subsequent modeling, which are required by the inclusion criteria. The emphasis is on continuous neural codec performance rather than discrete token design or applications in generation, alignment, or cross-modal reasoning. Therefore, it does not sufficiently meet the inclusion criteria focused on discrete audio tokens and their systematic design or usage, nor does it provide detailed tokenization/tokenizer mechanisms as required.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents neural speech codecs designed for low-bitrate speech reconstruction, focusing on speech compression and generation quality improvements. However, it does not explicitly mention the use or generation of discrete audio tokens, such as discretization into token sequences, vector quantization details, codebook settings, or using discrete tokens as core inputs for subsequent modeling, which are required by the inclusion criteria. The emphasis is on continuous neural codec performance rather than discrete token design or applications in generation, alignment, or cross-modal reasoning. Therefore, it does not sufficiently meet the inclusion criteria focused on discrete audio tokens and their systematic design or usage, nor does it provide detailed tokenization/tokenizer mechanisms as required.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "yang24h_interspeech", "query_title": "{Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens}", "normalized_title": "genhancer high fidelity speech enhancement via generative modeling on discrete codec tokens", "title": "Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens", "abstract": "We present a high-fidelity generative speech enhancement model, Genhancer, which generates clean speech as discrete codec tokens while conditioning on the input speech features. Discrete codec tokens provide an efficient latent domain in place of the conventional time or time-frequency domain of signals, so as to enable complex modeling of speech and allow generative modeling to enforce speaker consistency and content continuity. We provide insights into the best-fit generation scheme for enhancement among parallel prediction, auto-regression, and masking to demonstrate the benefits of conditioning on both pre-trained and jointly learned speech features. Subjective and objective tests show that Genhancer significantly improves audio quality and speaker-identity retention over the SOTA baselines, including conventional and generative ones while preserving content accuracy. Audio samples and supplement materials are available at https://minjekim.com/", "source": "semantic_scholar", "source_id": "ed6b910c45a0ab819c98b5fd4f9b8e4ca2ac7482", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens", "abstract": "We present a high-fidelity generative speech enhancement model, Genhancer, which generates clean speech as discrete codec tokens while conditioning on the input speech features. Discrete codec tokens provide an efficient latent domain in place of the conventional time or time-frequency domain of signals, so as to enable complex modeling of speech and allow generative modeling to enforce speaker consistency and content continuity. We provide insights into the best-fit generation scheme for enhancement among parallel prediction, auto-regression, and masking to demonstrate the benefits of conditioning on both pre-trained and jointly learned speech features. Subjective and objective tests show that Genhancer significantly improves audio quality and speaker-identity retention over the SOTA baselines, including conventional and generative ones while preserving content accuracy. Audio samples and supplement materials are available at https://minjekim.com/", "metadata": {"arxiv_id": "semantic_scholar:ed6b910c45a0ab819c98b5fd4f9b8e4ca2ac7482", "title": "Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens", "summary": "We present a high-fidelity generative speech enhancement model, Genhancer, which generates clean speech as discrete codec tokens while conditioning on the input speech features. Discrete codec tokens provide an efficient latent domain in place of the conventional time or time-frequency domain of signals, so as to enable complex modeling of speech and allow generative modeling to enforce speaker consistency and content continuity. We provide insights into the best-fit generation scheme for enhancement among parallel prediction, auto-regression, and masking to demonstrate the benefits of conditioning on both pre-trained and jointly learned speech features. Subjective and objective tests show that Genhancer significantly improves audio quality and speaker-identity retention over the SOTA baselines, including conventional and generative ones while preserving content accuracy. Audio samples and supplement materials are available at https://minjekim.com/", "source": "semantic_scholar", "source_id": "ed6b910c45a0ab819c98b5fd4f9b8e4ca2ac7482", "publication_date": "2024-09-01"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on discrete audio tokens (discrete codec tokens) as the core latent representation for speech enhancement, discusses token generation/codec design, and provides English title/abstract—matching the 'Discrete Audio Tokens' criteria without falling under any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens (discrete codec tokens) as the core latent representation for speech enhancement, discusses token generation/codec design, and provides English title/abstract—matching the 'Discrete Audio Tokens' criteria without falling under any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper \"Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens\" clearly involves the use of discrete audio tokens generated by a neural audio codec for speech enhancement, a downstream generation task. It discusses the discrete codec tokens as a latent domain replacing conventional continuous domains, aligning well with the inclusion criteria for discrete token-based modeling, generation, and evaluation. The abstract also mentions the generative modeling approach that leverages these tokens and provides empirical results, satisfying the criterion on downstream application and system evaluation. The English title and abstract are provided with sufficient detail on token usage, meeting evaluation requirements and exclusion avoidance related to token definition and use. Thus, the paper fits well within the scope defined for inclusion.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper \"Genhancer: High-Fidelity Speech Enhancement via Generative Modeling on Discrete Codec Tokens\" clearly involves the use of discrete audio tokens generated by a neural audio codec for speech enhancement, a downstream generation task. It discusses the discrete codec tokens as a latent domain replacing conventional continuous domains, aligning well with the inclusion criteria for discrete token-based modeling, generation, and evaluation. The abstract also mentions the generative modeling approach that leverages these tokens and provides empirical results, satisfying the criterion on downstream application and system evaluation. The English title and abstract are provided with sufficient detail on token usage, meeting evaluation requirements and exclusion avoidance related to token definition and use. Thus, the paper fits well within the scope defined for inclusion.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "xue24lowlatency", "query_title": "{Low-Latency Speech Enhancement via Speech Token Generation}", "normalized_title": "low latency speech enhancement via speech token generation", "title": "Low-latency Speech Enhancement via Speech Token Generation", "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.", "source": "arxiv", "source_id": "2310.08981v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Low-latency Speech Enhancement via Speech Token Generation", "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.", "metadata": {"arxiv_id": "2310.08981v3", "title": "Low-latency Speech Enhancement via Speech Token Generation", "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.", "authors": ["Huaying Xue", "Xiulian Peng", "Yan Lu"], "published": "2023-10-13T09:57:09Z", "updated": "2024-01-23T06:13:04Z", "categories": ["cs.SD", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2310.08981v3", "landing_url": "https://arxiv.org/abs/2310.08981v3", "doi": "https://doi.org/10.48550/arXiv.2310.08981"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on generating and aligning discrete speech tokens/codes via a neural codec for low-latency speech enhancement, aligning with the discrete audio tokens theme; however, explicit token quantization details are not fully stated, but a token-based approach appears to be core.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract centers on generating and aligning discrete speech tokens/codes via a neural codec for low-latency speech enhancement, aligning with the discrete audio tokens theme; however, explicit token quantization details are not fully stated, but a token-based approach appears to be core.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper discusses using a neural speech codec to generate acoustic codes, which are discrete representations serving as tokens for speech generation. It addresses token generation via a neural codec, aligns with the discrete audio tokens concept, and provides methodological details in English. Therefore, it meets the inclusion criteria for studies focusing on discrete audio tokens generated by neural codecs and applied to speech generation, with clear descriptions suitable for reproducibility. No exclusion criteria are triggered.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper discusses using a neural speech codec to generate acoustic codes, which are discrete representations serving as tokens for speech generation. It addresses token generation via a neural codec, aligns with the discrete audio tokens concept, and provides methodological details in English. Therefore, it meets the inclusion criteria for studies focusing on discrete audio tokens generated by neural codecs and applied to speech generation, with clear descriptions suitable for reproducibility. No exclusion criteria are triggered.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "Ahn2024HILCodecHA", "query_title": "{HILCodec: High-Fidelity and Lightweight Neural Audio Codec}", "normalized_title": "hilcodec high fidelity and lightweight neural audio codec", "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec", "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.", "source": "arxiv", "source_id": "2405.04752v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec", "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.", "metadata": {"arxiv_id": "2405.04752v2", "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec", "summary": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.", "authors": ["Sunghwan Ahn", "Beom Jun Woo", "Min Hyun Han", "Chanyeong Moon", "Nam Soo Kim"], "published": "2024-05-08T01:40:13Z", "updated": "2024-09-24T07:40:54Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2405.04752v2", "landing_url": "https://arxiv.org/abs/2405.04752v2", "doi": "https://doi.org/10.48550/arXiv.2405.04752"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents a high-fidelity, lightweight neural audio codec and discriminators, but does not discuss discrete audio tokens, tokenizers, codebooks, or a token-based representation, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper presents a high-fidelity, lightweight neural audio codec and discriminators, but does not discuss discrete audio tokens, tokenizers, codebooks, or a token-based representation, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a neural audio codec focusing on high-fidelity audio compression and improved discriminator design but does not explicitly mention producing discrete audio tokens, quantization details, vocabularies, or token-based downstream tasks such as generation or alignment with sequences like Transformers/LLMs. It emphasizes codec performance and model complexity rather than token discretization or use of discrete token sequences as the core representation for modeling or synthesis.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes a neural audio codec focusing on high-fidelity audio compression and improved discriminator design but does not explicitly mention producing discrete audio tokens, quantization details, vocabularies, or token-based downstream tasks such as generation or alignment with sequences like Transformers/LLMs. It emphasizes codec performance and model complexity rather than token discretization or use of discrete token sequences as the core representation for modeling or synthesis.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "Ye2024CodecDM", "query_title": "{Codec does matter: Exploring the semantic shortcoming of codec for audio language model}", "normalized_title": "codec does matter exploring the semantic shortcoming of codec for audio language model", "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model", "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)", "source": "arxiv", "source_id": "2408.17175v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model", "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)", "metadata": {"arxiv_id": "2408.17175v3", "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model", "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)", "authors": ["Zhen Ye", "Peiwen Sun", "Jiahe Lei", "Hongzhan Lin", "Xu Tan", "Zheqi Dai", "Qiuqiang Kong", "Jianyi Chen", "Jiahao Pan", "Qifeng Liu", "Yike Guo", "Wei Xue"], "published": "2024-08-30T10:24:07Z", "updated": "2024-11-27T11:47:45Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2408.17175v3", "landing_url": "https://arxiv.org/abs/2408.17175v3", "doi": "https://doi.org/10.48550/arXiv.2408.17175"}, "round-A_JuniorNano_output": {"reasoning": "Reasoning: The paper explicitly discusses discrete audio tokens via a neural codec (RVQ) with semantic features, applies to TTS/music/sound generation, and provides an English title/abstract and methods, matching the inclusion criteria and not meeting any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Reasoning: The paper explicitly discusses discrete audio tokens via a neural codec (RVQ) with semantic features, applies to TTS/music/sound generation, and provides an English title/abstract and methods, matching the inclusion criteria and not meeting any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on improving an audio codec (X-Codec) for audio language models by enhancing semantic features and reducing word error rates in audio generation tasks including speech synthesis, music continuation, and text-to-sound, indicating it deals with discrete audio tokens from neural audio codecs used for sequence modeling; it provides detailed methodology and demonstrates evaluation results, aligning well with the inclusion criteria around discrete audio tokens, codec design, and downstream audio generation tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on improving an audio codec (X-Codec) for audio language models by enhancing semantic features and reducing word error rates in audio generation tasks including speech synthesis, music continuation, and text-to-sound, indicating it deals with discrete audio tokens from neural audio codecs used for sequence modeling; it provides detailed methodology and demonstrates evaluation results, aligning well with the inclusion criteria around discrete audio tokens, codec design, and downstream audio generation tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "Guo2024AddressingIC", "query_title": "{Addressing Index Collapse of Large-Codebook Speech Tokenizer With Dual-Decoding Product-Quantized Variational Auto-Encoder}", "normalized_title": "addressing index collapse of large codebook speech tokenizer with dual decoding product quantized variational auto encoder", "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder", "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.", "source": "arxiv", "source_id": "2406.02940v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder", "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.", "metadata": {"arxiv_id": "2406.02940v1", "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder", "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.", "authors": ["Haohan Guo", "Fenglong Xie", "Dongchao Yang", "Hui Lu", "Xixin Wu", "Helen Meng"], "published": "2024-06-05T04:54:49Z", "updated": "2024-06-05T04:54:49Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.02940v1", "landing_url": "https://arxiv.org/abs/2406.02940v1", "doi": "https://doi.org/10.48550/arXiv.2406.02940"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly develops a discrete speech tokenization approach using PQ-VAE with multiple codebooks to address index collapse and evaluates the tokens for downstream TTS, satisfying the discrete-audio-token criteria and providing English description of the tokenizer design and experiments.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly develops a discrete speech tokenization approach using PQ-VAE with multiple codebooks to address index collapse and evaluates the tokens for downstream TTS, satisfying the discrete-audio-token criteria and providing English description of the tokenizer design and experiments.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper clearly focuses on discrete audio tokens generated by a neural audio codec approach (VQ-VAE and product-quantized VAE), addressing token design issues (index collapse) and providing experimental results relevant to token quality and downstream TTS applications, fulfilling inclusion criteria and lacking exclusion issues.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper clearly focuses on discrete audio tokens generated by a neural audio codec approach (VQ-VAE and product-quantized VAE), addressing token design issues (index collapse) and providing experimental results relevant to token quality and downstream TTS applications, fulfilling inclusion criteria and lacking exclusion issues.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "Zhou2024WMCodecEN", "query_title": "{WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification}", "normalized_title": "wmcodec end to end neural speech codec with deep watermarking for authenticity verification", "title": "WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification", "abstract": "Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness.", "source": "arxiv", "source_id": "2409.12121v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification", "abstract": "Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness.", "metadata": {"arxiv_id": "2409.12121v3", "title": "WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification", "summary": "Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness.", "authors": ["Junzuo Zhou", "Jiangyan Yi", "Yong Ren", "Jianhua Tao", "Tao Wang", "Chu Yuan Zhang"], "published": "2024-09-18T16:45:09Z", "updated": "2024-12-27T07:42:35Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.12121v3", "landing_url": "https://arxiv.org/abs/2409.12121v3", "doi": "https://doi.org/10.48550/arXiv.2409.12121"}, "round-A_JuniorNano_output": {"reasoning": "The paper discusses an end-to-end neural speech codec with watermarking but does not introduce or analyze discrete audio tokens or tokenization/quantization schemes, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper discusses an end-to-end neural speech codec with watermarking but does not introduce or analyze discrete audio tokens or tokenization/quantization schemes, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a neural speech codec with integrated watermarking for authenticity verification, but it does not discuss discretizing continuous audio into token sequences for downstream modeling, nor does it provide details about discrete token generation, vocabulary, or codebooks related to 'Discrete Audio Tokens' as defined. The primary emphasis is on compression and watermark embedding rather than on producing or utilizing discrete audio tokens for generative or multimodal modeling tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on a neural speech codec with integrated watermarking for authenticity verification, but it does not discuss discretizing continuous audio into token sequences for downstream modeling, nor does it provide details about discrete token generation, vocabulary, or codebooks related to 'Discrete Audio Tokens' as defined. The primary emphasis is on compression and watermark embedding rather than on producing or utilizing discrete audio tokens for generative or multimodal modeling tasks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "Wu2024TS3CodecTS", "query_title": "{TS3-Codec: Transformer-Based Simple Streaming Single Codec}", "normalized_title": "ts3 codec transformer based simple streaming single codec", "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "abstract": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.", "source": "arxiv", "source_id": "2411.18803v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "abstract": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.", "metadata": {"arxiv_id": "2411.18803v2", "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec", "summary": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.", "authors": ["Haibin Wu", "Naoyuki Kanda", "Sefik Emre Eskimez", "Jinyu Li"], "published": "2024-11-27T23:07:52Z", "updated": "2025-04-27T16:37:28Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.18803v2", "landing_url": "https://arxiv.org/abs/2411.18803v2", "doi": "https://doi.org/10.48550/arXiv.2411.18803"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on a transformer-based neural audio codec for streaming and compression, with no discussion of discrete audio tokens, tokenizers, or quantization schemes, thus it does not meet the token-centric inclusion criteria and aligns with the exclusion criteria for continuous representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on a transformer-based neural audio codec for streaming and compression, with no discussion of discrete audio tokens, tokenizers, or quantization schemes, thus it does not meet the token-centric inclusion criteria and aligns with the exclusion criteria for continuous representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a purely transformer-based neural audio codec focusing on audio compression and representation, but it does not clearly describe the production of discrete audio tokens or their use as sequences for subsequent modeling, nor does it detail tokenizers, quantization methods, or vocabulary/codebook settings, making it difficult to classify it as a study centered on 'Discrete Audio Tokens' as defined in the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper proposes a purely transformer-based neural audio codec focusing on audio compression and representation, but it does not clearly describe the production of discrete audio tokens or their use as sequences for subsequent modeling, nor does it detail tokenizers, quantization methods, or vocabulary/codebook settings, making it difficult to classify it as a study centered on 'Discrete Audio Tokens' as defined in the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "Jiang2024MDCTCodecAL", "query_title": "{MDCTCodec: A Lightweight MDCT-Based Neural Audio Codec Towards High Sampling Rate and Low Bitrate Scenarios}", "normalized_title": "mdctcodec a lightweight mdct based neural audio codec towards high sampling rate and low bitrate scenarios", "title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios", "abstract": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.", "source": "arxiv", "source_id": "2411.00464v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios", "abstract": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.", "metadata": {"arxiv_id": "2411.00464v1", "title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios", "summary": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.", "authors": ["Xiao-Hang Jiang", "Yang Ai", "Rui-Chen Zheng", "Hui-Peng Du", "Ye-Xin Lu", "Zhen-Hua Ling"], "published": "2024-11-01T09:24:28Z", "updated": "2024-11-01T09:24:28Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.00464v1", "landing_url": "https://arxiv.org/abs/2411.00464v1", "doi": "https://doi.org/10.48550/arXiv.2411.00464"}, "round-A_JuniorNano_output": {"reasoning": "The study discretizes the MDCT spectrum via a residual vector quantizer to produce discrete audio tokens, aligning with the discrete-token criteria and enabling token-based modeling, thus it should be included.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The study discretizes the MDCT spectrum via a residual vector quantizer to produce discrete audio tokens, aligning with the discrete-token criteria and enabling token-based modeling, thus it should be included.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes MDCTCodec, a neural audio codec that uses residual vector quantization to discretize the MDCT spectrum into latent codes, which aligns with the inclusion criteria of neural audio codec-based discrete token generation. The study details the quantization method, bitrate, training, and evaluation metrics, supporting reproducibility and evaluation. Although the main focus is on codec efficiency and audio quality at low bitrates and high sampling rates, it inherently produces discrete latent codes usable as tokens in audio generation or modeling, fitting the theme of discrete audio tokens for downstream tasks. Therefore, it meets all inclusion criteria and no exclusion criteria apply.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes MDCTCodec, a neural audio codec that uses residual vector quantization to discretize the MDCT spectrum into latent codes, which aligns with the inclusion criteria of neural audio codec-based discrete token generation. The study details the quantization method, bitrate, training, and evaluation metrics, supporting reproducibility and evaluation. Although the main focus is on codec efficiency and audio quality at low bitrates and high sampling rates, it inherently produces discrete latent codes usable as tokens in audio generation or modeling, fitting the theme of discrete audio tokens for downstream tasks. Therefore, it meets all inclusion criteria and no exclusion criteria apply.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "Guo2024LSCodecLA", "query_title": "{LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec}", "normalized_title": "lscodec low bitrate and speaker decoupled discrete speech codec", "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.", "source": "arxiv", "source_id": "2410.15764v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.", "metadata": {"arxiv_id": "2410.15764v3", "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec", "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.", "authors": ["Yiwei Guo", "Zhihan Li", "Chenpeng Du", "Hankun Wang", "Xie Chen", "Kai Yu"], "published": "2024-10-21T08:23:31Z", "updated": "2025-05-21T16:46:32Z", "categories": ["eess.AS", "cs.AI", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2410.15764v3", "landing_url": "https://arxiv.org/abs/2410.15764v3", "doi": "https://doi.org/10.48550/arXiv.2410.15764"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on a neural discrete speech codec that produces discrete tokens via vector quantization (with a continuous bottleneck) and a token vocoder, and it demonstrates speaker disentanglement/voice conversion, directly addressing discrete audio tokens and their evaluation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work centers on a neural discrete speech codec that produces discrete tokens via vector quantization (with a continuous bottleneck) and a token vocoder, and it demonstrates speaker disentanglement/voice conversion, directly addressing discrete audio tokens and their evaluation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes LSCodec, a discrete speech codec that performs vector quantization to produce discrete tokens with speaker decoupling, clearly aligns with the definition of using discrete audio tokens through neural audio codec mechanisms. It includes detailed method description, codebook usage, and evaluations on token design impact on intelligibility and speaker disentanglement, meeting multiple inclusion criteria while none of the exclusion criteria apply.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes LSCodec, a discrete speech codec that performs vector quantization to produce discrete tokens with speaker decoupling, clearly aligns with the definition of using discrete audio tokens through neural audio codec mechanisms. It includes detailed method description, codebook usage, and evaluations on token design impact on intelligibility and speaker disentanglement, meeting multiple inclusion criteria while none of the exclusion criteria apply.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "zaiem2023icassp", "query_title": "{Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study}", "normalized_title": "fine tuning strategies for faster inference using speech self supervised models a comparative study", "title": "Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study", "abstract": "Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings. In this context, it has been demonstrated that larger self-supervised feature extractors are crucial for achieving lower downstream ASR error rates. Thus, better performance might be sanctioned with longer inferences. This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times. Interestingly, we found that given enough downstream data, a simple downsampling of the input sequences outperforms the other methods with both low performance drops and high computational savings, reducing computations by 61.3% with an WER increase of only 0.81. Finally, we analyze the robustness of the comparison to changes in dataset conditions, revealing sensitivity to dataset size.", "source": "arxiv", "source_id": "2303.06740v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study", "abstract": "Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings. In this context, it has been demonstrated that larger self-supervised feature extractors are crucial for achieving lower downstream ASR error rates. Thus, better performance might be sanctioned with longer inferences. This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times. Interestingly, we found that given enough downstream data, a simple downsampling of the input sequences outperforms the other methods with both low performance drops and high computational savings, reducing computations by 61.3% with an WER increase of only 0.81. Finally, we analyze the robustness of the comparison to changes in dataset conditions, revealing sensitivity to dataset size.", "metadata": {"arxiv_id": "2303.06740v1", "title": "Fine-tuning Strategies for Faster Inference using Speech Self-Supervised Models: A Comparative Study", "summary": "Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings. In this context, it has been demonstrated that larger self-supervised feature extractors are crucial for achieving lower downstream ASR error rates. Thus, better performance might be sanctioned with longer inferences. This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times. Interestingly, we found that given enough downstream data, a simple downsampling of the input sequences outperforms the other methods with both low performance drops and high computational savings, reducing computations by 61.3% with an WER increase of only 0.81. Finally, we analyze the robustness of the comparison to changes in dataset conditions, revealing sensitivity to dataset size.", "authors": ["Salah Zaiem", "Robin Algayres", "Titouan Parcollet", "Slim Essid", "Mirco Ravanelli"], "published": "2023-03-12T19:52:34Z", "updated": "2023-03-12T19:52:34Z", "categories": ["eess.AS", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2303.06740v1", "landing_url": "https://arxiv.org/abs/2303.06740v1", "doi": "https://doi.org/10.48550/arXiv.2303.06740"}, "round-A_JuniorNano_output": {"reasoning": "此論文聚焦於透過微調與輸入下采樣以加速 SSL 編碼器推理，未提出或分析離散音訊 token 的產生、碼本/字彙表設計，因此不符合離散 token 主題的包含條件。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此論文聚焦於透過微調與輸入下采樣以加速 SSL 編碼器推理，未提出或分析離散音訊 token 的產生、碼本/字彙表設計，因此不符合離散 token 主題的包含條件。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on fine-tuning strategies for self-supervised speech models to improve inference speed, but it does not explicitly discuss the production or use of discrete audio tokens through discretization processes like vector quantization, nor does it detail token vocabulary or codebook settings; thus, it does not clearly meet the inclusion criteria centered on discrete audio tokens as core representations used for modeling or generation.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning strategies for self-supervised speech models to improve inference speed, but it does not explicitly discuss the production or use of discrete audio tokens through discretization processes like vector quantization, nor does it detail token vocabulary or codebook settings; thus, it does not clearly meet the inclusion criteria centered on discrete audio tokens as core representations used for modeling or generation.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "yang2021superb", "query_title": "{{SUPERB: Speech Processing Universal PERformance Benchmark}}", "normalized_title": "superb speech processing universal performance benchmark", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.", "source": "arxiv", "source_id": "2105.01051v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SUPERB: Speech processing Universal PERformance Benchmark", "abstract": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.", "metadata": {"arxiv_id": "2105.01051v4", "title": "SUPERB: Speech processing Universal PERformance Benchmark", "summary": "Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.", "authors": ["Shu-wen Yang", "Po-Han Chi", "Yung-Sung Chuang", "Cheng-I Jeff Lai", "Kushal Lakhotia", "Yist Y. Lin", "Andy T. Liu", "Jiatong Shi", "Xuankai Chang", "Guan-Ting Lin", "Tzu-Hsien Huang", "Wei-Cheng Tseng", "Ko-tik Lee", "Da-Rong Liu", "Zili Huang", "Shuyan Dong", "Shang-Wen Li", "Shinji Watanabe", "Abdelrahman Mohamed", "Hung-yi Lee"], "published": "2021-05-03T17:51:09Z", "updated": "2021-10-15T22:04:39Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2105.01051v4", "landing_url": "https://arxiv.org/abs/2105.01051v4", "doi": "https://doi.org/10.48550/arXiv.2105.01051"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a benchmark for SSL-based speech representations without mentioning discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria focused on discrete token design and token-based modeling.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a benchmark for SSL-based speech representations without mentioning discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria focused on discrete token design and token-based modeling.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces SUPERB, a benchmark for speech processing mainly focusing on self-supervised learning representations for various speech tasks, but it does not explicitly discuss the generation or use of discrete audio tokens via discretization, vector quantization, or neural audio codecs as defined in the inclusion criteria; hence, it does not meet the key requirement of focusing on discrete token sequences as core representations for modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces SUPERB, a benchmark for speech processing mainly focusing on self-supervised learning representations for various speech tasks, but it does not explicitly discuss the generation or use of discrete audio tokens via discretization, vector quantization, or neural audio codecs as defined in the inclusion criteria; hence, it does not meet the key requirement of focusing on discrete token sequences as core representations for modeling or generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "Rabiner:1993dq", "query_title": "{Fundamentals of Speech Recognition}", "normalized_title": "fundamentals of speech recognition", "title": "Fundamentals of speech recognition", "abstract": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition.", "source": "semantic_scholar", "source_id": "2392e94df520e707e8b1422311bfdc552954dea9", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Fundamentals of speech recognition", "abstract": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition.", "metadata": {"arxiv_id": "semantic_scholar:2392e94df520e707e8b1422311bfdc552954dea9", "title": "Fundamentals of speech recognition", "summary": "1. Fundamentals of Speech Recognition. 2. The Speech Signal: Production, Perception, and Acoustic-Phonetic Characterization. 3. Signal Processing and Analysis Methods for Speech Recognition. 4. Pattern Comparison Techniques. 5. Speech Recognition System Design and Implementation Issues. 6. Theory and Implementation of Hidden Markov Models. 7. Speech Recognition Based on Connected Word Models. 8. Large Vocabulary Continuous Speech Recognition. 9. Task-Oriented Applications of Automatic Speech Recognition.", "source": "semantic_scholar", "source_id": "2392e94df520e707e8b1422311bfdc552954dea9", "year": 1993}, "round-A_JuniorNano_output": {"reasoning": "The item is a general fundamentals/survey paper on speech recognition lacking a focus on discrete audio tokens, tokenization, or neural audio codecs; it does not meet the inclusion criteria and does meet at least one exclusion condition (not addressing discrete token representations).", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a general fundamentals/survey paper on speech recognition lacking a focus on discrete audio tokens, tokenization, or neural audio codecs; it does not meet the inclusion criteria and does meet at least one exclusion condition (not addressing discrete token representations).", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe foundational topics in speech recognition, focusing on signal processing, model techniques, and system design, but there is no mention of discrete audio tokenization methods, neural audio codecs, vector quantization, or any representation of speech as discrete tokens suitable for sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe foundational topics in speech recognition, focusing on signal processing, model techniques, and system design, but there is no mention of discrete audio tokenization methods, neural audio codecs, vector quantization, or any representation of speech as discrete tokens suitable for sequence modeling; thus, it does not meet the inclusion criteria related to discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "busso2008iemocap", "query_title": "{{IEMOCAP}: Interactive emotional dyadic motion capture database}", "normalized_title": "iemocap interactive emotional dyadic motion capture database", "title": "IEMOCAP: interactive emotional dyadic motion capture database", "abstract": "Speech is the most significant mode of communication among human beings and a potential method for human-computer interaction (HCI) by using a microphone sensor. Quantifiable emotion recognition using these sensors from speech signals is an emerging area of research in HCI, which applies to multiple applications such as human-reboot interaction, virtual reality, behavior assessment, healthcare, and emergency call centers to determine the speaker’s emotional state from an individual’s speech. In this paper, we present major contributions for; (i) increasing the accuracy of speech emotion recognition (SER) compared to state of the art and (ii) reducing the computational complexity of the presented SER model. We propose an artificial intelligence-assisted deep stride convolutional neural network (DSCNN) architecture using the plain nets strategy to learn salient and discriminative features from spectrogram of speech signals that are enhanced in prior steps to perform better. Local hidden patterns are learned in convolutional layers with special strides to down-sample the feature maps rather than pooling layer and global discriminative features are learned in fully connected layers. A SoftMax classifier is used for the classification of emotions in speech. The proposed technique is evaluated on Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) datasets to improve accuracy by 7.85% and 4.5%, respectively, with the model size reduced by 34.5 MB. It proves the effectiveness and significance of the proposed SER technique and reveals its applicability in real-world applications.", "source": "semantic_scholar", "source_id": "5cf0d213f3253cd46673d955209f8463db73cc51", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "IEMOCAP: interactive emotional dyadic motion capture database", "abstract": "Speech is the most significant mode of communication among human beings and a potential method for human-computer interaction (HCI) by using a microphone sensor. Quantifiable emotion recognition using these sensors from speech signals is an emerging area of research in HCI, which applies to multiple applications such as human-reboot interaction, virtual reality, behavior assessment, healthcare, and emergency call centers to determine the speaker’s emotional state from an individual’s speech. In this paper, we present major contributions for; (i) increasing the accuracy of speech emotion recognition (SER) compared to state of the art and (ii) reducing the computational complexity of the presented SER model. We propose an artificial intelligence-assisted deep stride convolutional neural network (DSCNN) architecture using the plain nets strategy to learn salient and discriminative features from spectrogram of speech signals that are enhanced in prior steps to perform better. Local hidden patterns are learned in convolutional layers with special strides to down-sample the feature maps rather than pooling layer and global discriminative features are learned in fully connected layers. A SoftMax classifier is used for the classification of emotions in speech. The proposed technique is evaluated on Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) datasets to improve accuracy by 7.85% and 4.5%, respectively, with the model size reduced by 34.5 MB. It proves the effectiveness and significance of the proposed SER technique and reveals its applicability in real-world applications.", "metadata": {"arxiv_id": "semantic_scholar:5cf0d213f3253cd46673d955209f8463db73cc51", "title": "IEMOCAP: interactive emotional dyadic motion capture database", "summary": "Speech is the most significant mode of communication among human beings and a potential method for human-computer interaction (HCI) by using a microphone sensor. Quantifiable emotion recognition using these sensors from speech signals is an emerging area of research in HCI, which applies to multiple applications such as human-reboot interaction, virtual reality, behavior assessment, healthcare, and emergency call centers to determine the speaker’s emotional state from an individual’s speech. In this paper, we present major contributions for; (i) increasing the accuracy of speech emotion recognition (SER) compared to state of the art and (ii) reducing the computational complexity of the presented SER model. We propose an artificial intelligence-assisted deep stride convolutional neural network (DSCNN) architecture using the plain nets strategy to learn salient and discriminative features from spectrogram of speech signals that are enhanced in prior steps to perform better. Local hidden patterns are learned in convolutional layers with special strides to down-sample the feature maps rather than pooling layer and global discriminative features are learned in fully connected layers. A SoftMax classifier is used for the classification of emotions in speech. The proposed technique is evaluated on Interactive Emotional Dyadic Motion Capture (IEMOCAP) and Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) datasets to improve accuracy by 7.85% and 4.5%, respectively, with the model size reduced by 34.5 MB. It proves the effectiveness and significance of the proposed SER technique and reveals its applicability in real-world applications.", "source": "semantic_scholar", "source_id": "5cf0d213f3253cd46673d955209f8463db73cc51", "publication_date": "2008-11-05"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on speech emotion recognition using CNNs on spectrograms and datasets (IEMOCAP/RAVDESS) and does not discuss discrete audio tokens, tokenization, or neural audio codecs; it does not meet the discrete-token inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on speech emotion recognition using CNNs on spectrograms and datasets (IEMOCAP/RAVDESS) and does not discuss discrete audio tokens, tokenization, or neural audio codecs; it does not meet the discrete-token inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on speech emotion recognition using deep convolutional neural networks and evaluates their method on emotional speech datasets; however, it does not discuss discretization of continuous audio waveforms into discrete tokens, neural audio codecs, nor provides any details on token vocabularies or quantization schemes. Therefore, it does not meet the inclusion criteria related to discrete audio token generation and modeling, nor is discrete token use central to its methodology or evaluation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on speech emotion recognition using deep convolutional neural networks and evaluates their method on emotional speech datasets; however, it does not discuss discretization of continuous audio waveforms into discrete tokens, neural audio codecs, nor provides any details on token vocabularies or quantization schemes. Therefore, it does not meet the inclusion criteria related to discrete audio token generation and modeling, nor is discrete token use central to its methodology or evaluation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "lecun2015deep", "query_title": "{Deep learning}", "normalized_title": "deep learning", "title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "source": "arxiv", "source_id": "1807.07987v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "metadata": {"arxiv_id": "1807.07987v2", "title": "Deep Learning", "summary": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "authors": ["Nicholas G. Polson", "Vadim O. Sokolov"], "published": "2018-07-20T18:20:34Z", "updated": "2018-08-03T11:27:28Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1807.07987v2", "landing_url": "https://arxiv.org/abs/1807.07987v2", "doi": "https://doi.org/10.48550/arXiv.1807.07987"}, "round-A_JuniorNano_output": {"reasoning": "The item is a broad Deep Learning survey with no discussion of discrete audio tokens, tokenization processes, or related downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a broad Deep Learning survey with no discussion of discrete audio tokens, tokenization processes, or related downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract discuss deep learning in a broad sense, focusing on high-dimensional data and AI applications but do not mention discrete audio tokens, neural audio codecs, quantization, or related tokenization processes for audio data as required by the inclusion criteria. It lacks any detail on discrete token generation or use in audio modeling or cross-modal alignment, failing to meet the core subject requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract discuss deep learning in a broad sense, focusing on high-dimensional data and AI applications but do not mention discrete audio tokens, neural audio codecs, quantization, or related tokenization processes for audio data as required by the inclusion criteria. It lacks any detail on discrete token generation or use in audio modeling or cross-modal alignment, failing to meet the core subject requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "metadata": {"arxiv_id": "1807.07987v2", "title": "Deep Learning", "summary": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "authors": ["Nicholas G. Polson", "Vadim O. Sokolov"], "published": "2018-07-20T18:20:34Z", "updated": "2018-08-03T11:27:28Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1807.07987v2", "landing_url": "https://arxiv.org/abs/1807.07987v2", "doi": "https://doi.org/10.48550/arXiv.1807.07987"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses general deep learning without addressing discrete audio tokens, tokenization methods, or downstream tasks specified in the inclusion criteria, so it fails to meet all inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses general deep learning without addressing discrete audio tokens, tokenization methods, or downstream tasks specified in the inclusion criteria, so it fails to meet all inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article reviews deep learning from a broad modeling and algorithmic perspective and mentions applications in AI, image processing, robotics, and automation; however, it does not mention discrete audio tokens, their generation, discretization processes, tokenizers, neural audio codecs, or any related evaluation of token design choices specific to audio data. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens, nor does it provide any relevant details for evaluation or reproduction in this field.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article reviews deep learning from a broad modeling and algorithmic perspective and mentions applications in AI, image processing, robotics, and automation; however, it does not mention discrete audio tokens, their generation, discretization processes, tokenizers, neural audio codecs, or any related evaluation of token design choices specific to audio data. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens, nor does it provide any relevant details for evaluation or reproduction in this field.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "GoodBengCour16", "query_title": "{Deep Learning}", "normalized_title": "deep learning", "title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "source": "arxiv", "source_id": "1807.07987v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "metadata": {"arxiv_id": "1807.07987v2", "title": "Deep Learning", "summary": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "authors": ["Nicholas G. Polson", "Vadim O. Sokolov"], "published": "2018-07-20T18:20:34Z", "updated": "2018-08-03T11:27:28Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1807.07987v2", "landing_url": "https://arxiv.org/abs/1807.07987v2", "doi": "https://doi.org/10.48550/arXiv.1807.07987"}, "round-A_JuniorNano_output": {"reasoning": "The item is a broad Deep Learning survey with no discussion of discrete audio tokens, tokenization processes, or related downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a broad Deep Learning survey with no discussion of discrete audio tokens, tokenization processes, or related downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract discuss deep learning in a broad sense, focusing on high-dimensional data and AI applications but do not mention discrete audio tokens, neural audio codecs, quantization, or related tokenization processes for audio data as required by the inclusion criteria. It lacks any detail on discrete token generation or use in audio modeling or cross-modal alignment, failing to meet the core subject requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract discuss deep learning in a broad sense, focusing on high-dimensional data and AI applications but do not mention discrete audio tokens, neural audio codecs, quantization, or related tokenization processes for audio data as required by the inclusion criteria. It lacks any detail on discrete token generation or use in audio modeling or cross-modal alignment, failing to meet the core subject requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "Deep Learning", "abstract": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "metadata": {"arxiv_id": "1807.07987v2", "title": "Deep Learning", "summary": "Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.", "authors": ["Nicholas G. Polson", "Vadim O. Sokolov"], "published": "2018-07-20T18:20:34Z", "updated": "2018-08-03T11:27:28Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1807.07987v2", "landing_url": "https://arxiv.org/abs/1807.07987v2", "doi": "https://doi.org/10.48550/arXiv.1807.07987"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses general deep learning without addressing discrete audio tokens, tokenization methods, or downstream tasks specified in the inclusion criteria, so it fails to meet all inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses general deep learning without addressing discrete audio tokens, tokenization methods, or downstream tasks specified in the inclusion criteria, so it fails to meet all inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article reviews deep learning from a broad modeling and algorithmic perspective and mentions applications in AI, image processing, robotics, and automation; however, it does not mention discrete audio tokens, their generation, discretization processes, tokenizers, neural audio codecs, or any related evaluation of token design choices specific to audio data. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens, nor does it provide any relevant details for evaluation or reproduction in this field.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article reviews deep learning from a broad modeling and algorithmic perspective and mentions applications in AI, image processing, robotics, and automation; however, it does not mention discrete audio tokens, their generation, discretization processes, tokenizers, neural audio codecs, or any related evaluation of token design choices specific to audio data. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens, nor does it provide any relevant details for evaluation or reproduction in this field.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "geminiteam2023gemini", "query_title": "{Gemini: A Family of Highly Capable Multimodal Models}", "normalized_title": "gemini a family of highly capable multimodal models", "title": "Gemini: A Family of Highly Capable Multimodal Models", "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.", "source": "arxiv", "source_id": "2312.11805v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Gemini: A Family of Highly Capable Multimodal Models", "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.", "metadata": {"arxiv_id": "2312.11805v5", "title": "Gemini: A Family of Highly Capable Multimodal Models", "summary": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.", "authors": ["Gemini Team", "Rohan Anil", "Sebastian Borgeaud", "Jean-Baptiste Alayrac", "Jiahui Yu", "Radu Soricut", "Johan Schalkwyk", "Andrew M. Dai", "Anja Hauth", "Katie Millican", "David Silver", "Melvin Johnson", "Ioannis Antonoglou", "Julian Schrittwieser", "Amelia Glaese", "Jilin Chen", "Emily Pitler", "Timothy Lillicrap", "Angeliki Lazaridou", "Orhan Firat", "James Molloy", "Michael Isard", "Paul R. Barham", "Tom Hennigan", "Benjamin Lee", "Fabio Viola", "Malcolm Reynolds", "Yuanzhong Xu", "Ryan Doherty", "Eli Collins", "Clemens Meyer", "Eliza Rutherford", "Erica Moreira", "Kareem Ayoub", "Megha Goel", "Jack Krawczyk", "Cosmo Du", "Ed Chi", "Heng-Tze Cheng", "Eric Ni", "Purvi Shah", "Patrick Kane", "Betty Chan", "Manaal Faruqui", "Aliaksei Severyn", "Hanzhao Lin", "YaGuang Li", "Yong Cheng", "Abe Ittycheriah", "Mahdis Mahdieh", "Mia Chen", "Pei Sun", "Dustin Tran", "Sumit Bagri", "Balaji Lakshminarayanan", "Jeremiah Liu", "Andras Orban", "Fabian Güra", "Hao Zhou", "Xinying Song", "Aurelien Boffy", "Harish Ganapathy", "Steven Zheng", "HyunJeong Choe", "Ágoston Weisz", "Tao Zhu", "Yifeng Lu", "Siddharth Gopal", "Jarrod Kahn", "Maciej Kula", "Jeff Pitman", "Rushin Shah", "Emanuel Taropa", "Majd Al Merey", "Martin Baeuml", "Zhifeng Chen", "Laurent El Shafey", "Yujing Zhang", "Olcan Sercinoglu", "George Tucker", "Enrique Piqueras", "Maxim Krikun", "Iain Barr", "Nikolay Savinov", "Ivo Danihelka", "Becca Roelofs", "Anaïs White", "Anders Andreassen", "Tamara von Glehn", "Lakshman Yagati", "Mehran Kazemi", "Lucas Gonzalez", "Misha Khalman", "Jakub Sygnowski", "Alexandre Frechette", "Charlotte Smith", "Laura Culp", "Lev Proleev", "Yi Luan", "Xi Chen", "James Lottes", "Nathan Schucher", "Federico Lebron", "Alban Rrustemi", "Natalie Clay", "Phil Crone", "Tomas Kocisky", "Jeffrey Zhao", "Bartek Perz", "Dian Yu", "Heidi Howard", "Adam Bloniarz", "Jack W. Rae", "Han Lu", "Laurent Sifre", "Marcello Maggioni", "Fred Alcober", "Dan Garrette", "Megan Barnes", "Shantanu Thakoor", "Jacob Austin", "Gabriel Barth-Maron", "William Wong", "Rishabh Joshi", "Rahma Chaabouni", "Deeni Fatiha", "Arun Ahuja", "Gaurav Singh Tomar", "Evan Senter", "Martin Chadwick", "Ilya Kornakov", "Nithya Attaluri", "Iñaki Iturrate", "Ruibo Liu", "Yunxuan Li", "Sarah Cogan", "Jeremy Chen", "Chao Jia", "Chenjie Gu", "Qiao Zhang", "Jordan Grimstad", "Ale Jakse Hartman", "Xavier Garcia", "Thanumalayan Sankaranarayana Pillai", "Jacob Devlin", "Michael Laskin", "Diego de Las Casas", "Dasha Valter", "Connie Tao", "Lorenzo Blanco", "Adrià Puigdomènech Badia", "David Reitter", "Mianna Chen", "Jenny Brennan", "Clara Rivera", "Sergey Brin", "Shariq Iqbal", "Gabriela Surita", "Jane Labanowski", "Abhi Rao", "Stephanie Winkler", "Emilio Parisotto", "Yiming Gu", "Kate Olszewska", "Ravi Addanki", "Antoine Miech", "Annie Louis", "Denis Teplyashin", "Geoff Brown", "Elliot Catt", "Jan Balaguer", "Jackie Xiang", "Pidong Wang", "Zoe Ashwood", "Anton Briukhov", "Albert Webson", "Sanjay Ganapathy", "Smit Sanghavi", "Ajay Kannan", "Ming-Wei Chang", "Axel Stjerngren", "Josip Djolonga", "Yuting Sun", "Ankur Bapna", "Matthew Aitchison", "Pedram Pejman", "Henryk Michalewski", "Tianhe Yu", "Cindy Wang", "Juliette Love", "Junwhan Ahn", "Dawn Bloxwich", "Kehang Han", "Peter Humphreys", "Thibault Sellam", "James Bradbury", "Varun Godbole", "Sina Samangooei", "Bogdan Damoc", "Alex Kaskasoli", "Sébastien M. R. Arnold", "Vijay Vasudevan", "Shubham Agrawal", "Jason Riesa", "Dmitry Lepikhin", "Richard Tanburn", "Srivatsan Srinivasan", "Hyeontaek Lim", "Sarah Hodkinson", "Pranav Shyam", "Johan Ferret", "Steven Hand", "Ankush Garg", "Tom Le Paine", "Jian Li", "Yujia Li", "Minh Giang", "Alexander Neitz", "Zaheer Abbas", "Sarah York", "Machel Reid", "Elizabeth Cole", "Aakanksha Chowdhery", "Dipanjan Das", "Dominika Rogozińska", "Vitaliy Nikolaev", "Pablo Sprechmann", "Zachary Nado", "Lukas Zilka", "Flavien Prost", "Luheng He", "Marianne Monteiro", "Gaurav Mishra", "Chris Welty", "Josh Newlan", "Dawei Jia", "Miltiadis Allamanis", "Clara Huiyi Hu", "Raoul de Liedekerke", "Justin Gilmer", "Carl Saroufim", "Shruti Rijhwani", "Shaobo Hou", "Disha Shrivastava", "Anirudh Baddepudi", "Alex Goldin", "Adnan Ozturel", "Albin Cassirer", "Yunhan Xu", "Daniel Sohn", "Devendra Sachan", "Reinald Kim Amplayo", "Craig Swanson", "Dessie Petrova", "Shashi Narayan", "Arthur Guez", "Siddhartha Brahma", "Jessica Landon", "Miteyan Patel", "Ruizhe Zhao", "Kevin Villela", "Luyu Wang", "Wenhao Jia", "Matthew Rahtz", "Mai Giménez", "Legg Yeung", "James Keeling", "Petko Georgiev", "Diana Mincu", "Boxi Wu", "Salem Haykal", "Rachel Saputro", "Kiran Vodrahalli", "James Qin", "Zeynep Cankara", "Abhanshu Sharma", "Nick Fernando", "Will Hawkins", "Behnam Neyshabur", "Solomon Kim", "Adrian Hutter", "Priyanka Agrawal", "Alex Castro-Ros", "George van den Driessche", "Tao Wang", "Fan Yang", "Shuo-yiin Chang", "Paul Komarek", "Ross McIlroy", "Mario Lučić", "Guodong Zhang", "Wael Farhan", "Michael Sharman", "Paul Natsev", "Paul Michel", "Yamini Bansal", "Siyuan Qiao", "Kris Cao", "Siamak Shakeri", "Christina Butterfield", "Justin Chung", "Paul Kishan Rubenstein", "Shivani Agrawal", "Arthur Mensch", "Kedar Soparkar", "Karel Lenc", "Timothy Chung", "Aedan Pope", "Loren Maggiore", "Jackie Kay", "Priya Jhakra", "Shibo Wang", "Joshua Maynez", "Mary Phuong", "Taylor Tobin", "Andrea Tacchetti", "Maja Trebacz", "Kevin Robinson", "Yash Katariya", "Sebastian Riedel", "Paige Bailey", "Kefan Xiao", "Nimesh Ghelani", "Lora Aroyo", "Ambrose Slone", "Neil Houlsby", "Xuehan Xiong", "Zhen Yang", "Elena Gribovskaya", "Jonas Adler", "Mateo Wirth", "Lisa Lee", "Music Li", "Thais Kagohara", "Jay Pavagadhi", "Sophie Bridgers", "Anna Bortsova", "Sanjay Ghemawat", "Zafarali Ahmed", "Tianqi Liu", "Richard Powell", "Vijay Bolina", "Mariko Iinuma", "Polina Zablotskaia", "James Besley", "Da-Woon Chung", "Timothy Dozat", "Ramona Comanescu", "Xiance Si", "Jeremy Greer", "Guolong Su", "Martin Polacek", "Raphaël Lopez Kaufman", "Simon Tokumine", "Hexiang Hu", "Elena Buchatskaya", "Yingjie Miao", "Mohamed Elhawaty", "Aditya Siddhant", "Nenad Tomasev", "Jinwei Xing", "Christina Greer", "Helen Miller", "Shereen Ashraf", "Aurko Roy", "Zizhao Zhang", "Ada Ma", "Angelos Filos", "Milos Besta", "Rory Blevins", "Ted Klimenko", "Chih-Kuan Yeh", "Soravit Changpinyo", "Jiaqi Mu", "Oscar Chang", "Mantas Pajarskas", "Carrie Muir", "Vered Cohen", "Charline Le Lan", "Krishna Haridasan", "Amit Marathe", "Steven Hansen", "Sholto Douglas", "Rajkumar Samuel", "Mingqiu Wang", "Sophia Austin", "Chang Lan", "Jiepu Jiang", "Justin Chiu", "Jaime Alonso Lorenzo", "Lars Lowe Sjösund", "Sébastien Cevey", "Zach Gleicher", "Thi Avrahami", "Anudhyan Boral", "Hansa Srinivasan", "Vittorio Selo", "Rhys May", "Konstantinos Aisopos", "Léonard Hussenot", "Livio Baldini Soares", "Kate Baumli", "Michael B. Chang", "Adrià Recasens", "Ben Caine", "Alexander Pritzel", "Filip Pavetic", "Fabio Pardo", "Anita Gergely", "Justin Frye", "Vinay Ramasesh", "Dan Horgan", "Kartikeya Badola", "Nora Kassner", "Subhrajit Roy", "Ethan Dyer", "Víctor Campos Campos", "Alex Tomala", "Yunhao Tang", "Dalia El Badawy", "Elspeth White", "Basil Mustafa", "Oran Lang", "Abhishek Jindal", "Sharad Vikram", "Zhitao Gong", "Sergi Caelles", "Ross Hemsley", "Gregory Thornton", "Fangxiaoyu Feng", "Wojciech Stokowiec", "Ce Zheng", "Phoebe Thacker", "Çağlar Ünlü", "Zhishuai Zhang", "Mohammad Saleh", "James Svensson", "Max Bileschi", "Piyush Patil", "Ankesh Anand", "Roman Ring", "Katerina Tsihlas", "Arpi Vezer", "Marco Selvi", "Toby Shevlane", "Mikel Rodriguez", "Tom Kwiatkowski", "Samira Daruki", "Keran Rong", "Allan Dafoe", "Nicholas FitzGerald", "Keren Gu-Lemberg", "Mina Khan", "Lisa Anne Hendricks", "Marie Pellat", "Vladimir Feinberg", "James Cobon-Kerr", "Tara Sainath", "Maribeth Rauh", "Sayed Hadi Hashemi", "Richard Ives", "Yana Hasson", "Eric Noland", "Yuan Cao", "Nathan Byrd", "Le Hou", "Qingze Wang", "Thibault Sottiaux", "Michela Paganini", "Jean-Baptiste Lespiau", "Alexandre Moufarek", "Samer Hassan", "Kaushik Shivakumar", "Joost van Amersfoort", "Amol Mandhane", "Pratik Joshi", "Anirudh Goyal", "Matthew Tung", "Andrew Brock", "Hannah Sheahan", "Vedant Misra", "Cheng Li", "Nemanja Rakićević", "Mostafa Dehghani", "Fangyu Liu", "Sid Mittal", "Junhyuk Oh", "Seb Noury", "Eren Sezener", "Fantine Huot", "Matthew Lamm", "Nicola De Cao", "Charlie Chen", "Sidharth Mudgal", "Romina Stella", "Kevin Brooks", "Gautam Vasudevan", "Chenxi Liu", "Mainak Chain", "Nivedita Melinkeri", "Aaron Cohen", "Venus Wang", "Kristie Seymore", "Sergey Zubkov", "Rahul Goel", "Summer Yue", "Sai Krishnakumaran", "Brian Albert", "Nate Hurley", "Motoki Sano", "Anhad Mohananey", "Jonah Joughin", "Egor Filonov", "Tomasz Kępa", "Yomna Eldawy", "Jiawern Lim", "Rahul Rishi", "Shirin Badiezadegan", "Taylor Bos", "Jerry Chang", "Sanil Jain", "Sri Gayatri Sundara Padmanabhan", "Subha Puttagunta", "Kalpesh Krishna", "Leslie Baker", "Norbert Kalb", "Vamsi Bedapudi", "Adam Kurzrok", "Shuntong Lei", "Anthony Yu", "Oren Litvin", "Xiang Zhou", "Zhichun Wu", "Sam Sobell", "Andrea Siciliano", "Alan Papir", "Robby Neale", "Jonas Bragagnolo", "Tej Toor", "Tina Chen", "Valentin Anklin", "Feiran Wang", "Richie Feng", "Milad Gholami", "Kevin Ling", "Lijuan Liu", "Jules Walter", "Hamid Moghaddam", "Arun Kishore", "Jakub Adamek", "Tyler Mercado", "Jonathan Mallinson", "Siddhinita Wandekar", "Stephen Cagle", "Eran Ofek", "Guillermo Garrido", "Clemens Lombriser", "Maksim Mukha", "Botu Sun", "Hafeezul Rahman Mohammad", "Josip Matak", "Yadi Qian", "Vikas Peswani", "Pawel Janus", "Quan Yuan", "Leif Schelin", "Oana David", "Ankur Garg", "Yifan He", "Oleksii Duzhyi", "Anton Älgmyr", "Timothée Lottaz", "Qi Li", "Vikas Yadav", "Luyao Xu", "Alex Chinien", "Rakesh Shivanna", "Aleksandr Chuklin", "Josie Li", "Carrie Spadine", "Travis Wolfe", "Kareem Mohamed", "Subhabrata Das", "Zihang Dai", "Kyle He", "Daniel von Dincklage", "Shyam Upadhyay", "Akanksha Maurya", "Luyan Chi", "Sebastian Krause", "Khalid Salama", "Pam G Rabinovitch", "Pavan Kumar Reddy M", "Aarush Selvan", "Mikhail Dektiarev", "Golnaz Ghiasi", "Erdem Guven", "Himanshu Gupta", "Boyi Liu", "Deepak Sharma", "Idan Heimlich Shtacher", "Shachi Paul", "Oscar Akerlund", "François-Xavier Aubet", "Terry Huang", "Chen Zhu", "Eric Zhu", "Elico Teixeira", "Matthew Fritze", "Francesco Bertolini", "Liana-Eleonora Marinescu", "Martin Bölle", "Dominik Paulus", "Khyatti Gupta", "Tejasi Latkar", "Max Chang", "Jason Sanders", "Roopa Wilson", "Xuewei Wu", "Yi-Xuan Tan", "Lam Nguyen Thiet", "Tulsee Doshi", "Sid Lall", "Swaroop Mishra", "Wanming Chen", "Thang Luong", "Seth Benjamin", "Jasmine Lee", "Ewa Andrejczuk", "Dominik Rabiej", "Vipul Ranjan", "Krzysztof Styrc", "Pengcheng Yin", "Jon Simon", "Malcolm Rose Harriott", "Mudit Bansal", "Alexei Robsky", "Geoff Bacon", "David Greene", "Daniil Mirylenka", "Chen Zhou", "Obaid Sarvana", "Abhimanyu Goyal", "Samuel Andermatt", "Patrick Siegler", "Ben Horn", "Assaf Israel", "Francesco Pongetti", "Chih-Wei \"Louis\" Chen", "Marco Selvatici", "Pedro Silva", "Kathie Wang", "Jackson Tolins", "Kelvin Guu", "Roey Yogev", "Xiaochen Cai", "Alessandro Agostini", "Maulik Shah", "Hung Nguyen", "Noah Ó Donnaile", "Sébastien Pereira", "Linda Friso", "Adam Stambler", "Adam Kurzrok", "Chenkai Kuang", "Yan Romanikhin", "Mark Geller", "ZJ Yan", "Kane Jang", "Cheng-Chun Lee", "Wojciech Fica", "Eric Malmi", "Qijun Tan", "Dan Banica", "Daniel Balle", "Ryan Pham", "Yanping Huang", "Diana Avram", "Hongzhi Shi", "Jasjot Singh", "Chris Hidey", "Niharika Ahuja", "Pranab Saxena", "Dan Dooley", "Srividya Pranavi Potharaju", "Eileen O'Neill", "Anand Gokulchandran", "Ryan Foley", "Kai Zhao", "Mike Dusenberry", "Yuan Liu", "Pulkit Mehta", "Ragha Kotikalapudi", "Chalence Safranek-Shrader", "Andrew Goodman", "Joshua Kessinger", "Eran Globen", "Prateek Kolhar", "Chris Gorgolewski", "Ali Ibrahim", "Yang Song", "Ali Eichenbaum", "Thomas Brovelli", "Sahitya Potluri", "Preethi Lahoti", "Cip Baetu", "Ali Ghorbani", "Charles Chen", "Andy Crawford", "Shalini Pal", "Mukund Sridhar", "Petru Gurita", "Asier Mujika", "Igor Petrovski", "Pierre-Louis Cedoz", "Chenmei Li", "Shiyuan Chen", "Niccolò Dal Santo", "Siddharth Goyal", "Jitesh Punjabi", "Karthik Kappaganthu", "Chester Kwak", "Pallavi LV", "Sarmishta Velury", "Himadri Choudhury", "Jamie Hall", "Premal Shah", "Ricardo Figueira", "Matt Thomas", "Minjie Lu", "Ting Zhou", "Chintu Kumar", "Thomas Jurdi", "Sharat Chikkerur", "Yenai Ma", "Adams Yu", "Soo Kwak", "Victor Ähdel", "Sujeevan Rajayogam", "Travis Choma", "Fei Liu", "Aditya Barua", "Colin Ji", "Ji Ho Park", "Vincent Hellendoorn", "Alex Bailey", "Taylan Bilal", "Huanjie Zhou", "Mehrdad Khatir", "Charles Sutton", "Wojciech Rzadkowski", "Fiona Macintosh", "Roopali Vij", "Konstantin Shagin", "Paul Medina", "Chen Liang", "Jinjing Zhou", "Pararth Shah", "Yingying Bi", "Attila Dankovics", "Shipra Banga", "Sabine Lehmann", "Marissa Bredesen", "Zifan Lin", "John Eric Hoffmann", "Jonathan Lai", "Raynald Chung", "Kai Yang", "Nihal Balani", "Arthur Bražinskas", "Andrei Sozanschi", "Matthew Hayes", "Héctor Fernández Alcalde", "Peter Makarov", "Will Chen", "Antonio Stella", "Liselotte Snijders", "Michael Mandl", "Ante Kärrman", "Paweł Nowak", "Xinyi Wu", "Alex Dyck", "Krishnan Vaidyanathan", "Raghavender R", "Jessica Mallet", "Mitch Rudominer", "Eric Johnston", "Sushil Mittal", "Akhil Udathu", "Janara Christensen", "Vishal Verma", "Zach Irving", "Andreas Santucci", "Gamaleldin Elsayed", "Elnaz Davoodi", "Marin Georgiev", "Ian Tenney", "Nan Hua", "Geoffrey Cideron", "Edouard Leurent", "Mahmoud Alnahlawi", "Ionut Georgescu", "Nan Wei", "Ivy Zheng", "Dylan Scandinaro", "Heinrich Jiang", "Jasper Snoek", "Mukund Sundararajan", "Xuezhi Wang", "Zack Ontiveros", "Itay Karo", "Jeremy Cole", "Vinu Rajashekhar", "Lara Tumeh", "Eyal Ben-David", "Rishub Jain", "Jonathan Uesato", "Romina Datta", "Oskar Bunyan", "Shimu Wu", "John Zhang", "Piotr Stanczyk", "Ye Zhang", "David Steiner", "Subhajit Naskar", "Michael Azzam", "Matthew Johnson", "Adam Paszke", "Chung-Cheng Chiu", "Jaume Sanchez Elias", "Afroz Mohiuddin", "Faizan Muhammad", "Jin Miao", "Andrew Lee", "Nino Vieillard", "Jane Park", "Jiageng Zhang", "Jeff Stanway", "Drew Garmon", "Abhijit Karmarkar", "Zhe Dong", "Jong Lee", "Aviral Kumar", "Luowei Zhou", "Jonathan Evens", "William Isaac", "Geoffrey Irving", "Edward Loper", "Michael Fink", "Isha Arkatkar", "Nanxin Chen", "Izhak Shafran", "Ivan Petrychenko", "Zhe Chen", "Johnson Jia", "Anselm Levskaya", "Zhenkai Zhu", "Peter Grabowski", "Yu Mao", "Alberto Magni", "Kaisheng Yao", "Javier Snaider", "Norman Casagrande", "Evan Palmer", "Paul Suganthan", "Alfonso Castaño", "Irene Giannoumis", "Wooyeol Kim", "Mikołaj Rybiński", "Ashwin Sreevatsa", "Jennifer Prendki", "David Soergel", "Adrian Goedeckemeyer", "Willi Gierke", "Mohsen Jafari", "Meenu Gaba", "Jeremy Wiesner", "Diana Gage Wright", "Yawen Wei", "Harsha Vashisht", "Yana Kulizhskaya", "Jay Hoover", "Maigo Le", "Lu Li", "Chimezie Iwuanyanwu", "Lu Liu", "Kevin Ramirez", "Andrey Khorlin", "Albert Cui", "Tian LIN", "Marcus Wu", "Ricardo Aguilar", "Keith Pallo", "Abhishek Chakladar", "Ginger Perng", "Elena Allica Abellan", "Mingyang Zhang", "Ishita Dasgupta", "Nate Kushman", "Ivo Penchev", "Alena Repina", "Xihui Wu", "Tom van der Weide", "Priya Ponnapalli", "Caroline Kaplan", "Jiri Simsa", "Shuangfeng Li", "Olivier Dousse", "Fan Yang", "Jeff Piper", "Nathan Ie", "Rama Pasumarthi", "Nathan Lintz", "Anitha Vijayakumar", "Daniel Andor", "Pedro Valenzuela", "Minnie Lui", "Cosmin Paduraru", "Daiyi Peng", "Katherine Lee", "Shuyuan Zhang", "Somer Greene", "Duc Dung Nguyen", "Paula Kurylowicz", "Cassidy Hardin", "Lucas Dixon", "Lili Janzer", "Kiam Choo", "Ziqiang Feng", "Biao Zhang", "Achintya Singhal", "Dayou Du", "Dan McKinnon", "Natasha Antropova", "Tolga Bolukbasi", "Orgad Keller", "David Reid", "Daniel Finchelstein", "Maria Abi Raad", "Remi Crocker", "Peter Hawkins", "Robert Dadashi", "Colin Gaffney", "Ken Franko", "Anna Bulanova", "Rémi Leblond", "Shirley Chung", "Harry Askham", "Luis C. Cobo", "Kelvin Xu", "Felix Fischer", "Jun Xu", "Christina Sorokin", "Chris Alberti", "Chu-Cheng Lin", "Colin Evans", "Alek Dimitriev", "Hannah Forbes", "Dylan Banarse", "Zora Tung", "Mark Omernick", "Colton Bishop", "Rachel Sterneck", "Rohan Jain", "Jiawei Xia", "Ehsan Amid", "Francesco Piccinno", "Xingyu Wang", "Praseem Banzal", "Daniel J. Mankowitz", "Alex Polozov", "Victoria Krakovna", "Sasha Brown", "MohammadHossein Bateni", "Dennis Duan", "Vlad Firoiu", "Meghana Thotakuri", "Tom Natan", "Matthieu Geist", "Ser tan Girgin", "Hui Li", "Jiayu Ye", "Ofir Roval", "Reiko Tojo", "Michael Kwong", "James Lee-Thorp", "Christopher Yew", "Danila Sinopalnikov", "Sabela Ramos", "John Mellor", "Abhishek Sharma", "Kathy Wu", "David Miller", "Nicolas Sonnerat", "Denis Vnukov", "Rory Greig", "Jennifer Beattie", "Emily Caveness", "Libin Bai", "Julian Eisenschlos", "Alex Korchemniy", "Tomy Tsai", "Mimi Jasarevic", "Weize Kong", "Phuong Dao", "Zeyu Zheng", "Frederick Liu", "Fan Yang", "Rui Zhu", "Tian Huey Teh", "Jason Sanmiya", "Evgeny Gladchenko", "Nejc Trdin", "Daniel Toyama", "Evan Rosen", "Sasan Tavakkol", "Linting Xue", "Chen Elkind", "Oliver Woodman", "John Carpenter", "George Papamakarios", "Rupert Kemp", "Sushant Kafle", "Tanya Grunina", "Rishika Sinha", "Alice Talbert", "Diane Wu", "Denese Owusu-Afriyie", "Cosmo Du", "Chloe Thornton", "Jordi Pont-Tuset", "Pradyumna Narayana", "Jing Li", "Saaber Fatehi", "John Wieting", "Omar Ajmeri", "Benigno Uria", "Yeongil Ko", "Laura Knight", "Amélie Héliou", "Ning Niu", "Shane Gu", "Chenxi Pang", "Yeqing Li", "Nir Levine", "Ariel Stolovich", "Rebeca Santamaria-Fernandez", "Sonam Goenka", "Wenny Yustalim", "Robin Strudel", "Ali Elqursh", "Charlie Deck", "Hyo Lee", "Zonglin Li", "Kyle Levin", "Raphael Hoffmann", "Dan Holtmann-Rice", "Olivier Bachem", "Sho Arora", "Christy Koh", "Soheil Hassas Yeganeh", "Siim Põder", "Mukarram Tariq", "Yanhua Sun", "Lucian Ionita", "Mojtaba Seyedhosseini", "Pouya Tafti", "Zhiyu Liu", "Anmol Gulati", "Jasmine Liu", "Xinyu Ye", "Bart Chrzaszcz", "Lily Wang", "Nikhil Sethi", "Tianrun Li", "Ben Brown", "Shreya Singh", "Wei Fan", "Aaron Parisi", "Joe Stanton", "Vinod Koverkathu", "Christopher A. Choquette-Choo", "Yunjie Li", "TJ Lu", "Abe Ittycheriah", "Prakash Shroff", "Mani Varadarajan", "Sanaz Bahargam", "Rob Willoughby", "David Gaddy", "Guillaume Desjardins", "Marco Cornero", "Brona Robenek", "Bhavishya Mittal", "Ben Albrecht", "Ashish Shenoy", "Fedor Moiseev", "Henrik Jacobsson", "Alireza Ghaffarkhah", "Morgane Rivière", "Alanna Walton", "Clément Crepy", "Alicia Parrish", "Zongwei Zhou", "Clement Farabet", "Carey Radebaugh", "Praveen Srinivasan", "Claudia van der Salm", "Andreas Fidjeland", "Salvatore Scellato", "Eri Latorre-Chimoto", "Hanna Klimczak-Plucińska", "David Bridson", "Dario de Cesare", "Tom Hudson", "Piermaria Mendolicchio", "Lexi Walker", "Alex Morris", "Matthew Mauger", "Alexey Guseynov", "Alison Reid", "Seth Odoom", "Lucia Loher", "Victor Cotruta", "Madhavi Yenugula", "Dominik Grewe", "Anastasia Petrushkina", "Tom Duerig", "Antonio Sanchez", "Steve Yadlowsky", "Amy Shen", "Amir Globerson", "Lynette Webb", "Sahil Dua", "Dong Li", "Surya Bhupatiraju", "Dan Hurt", "Haroon Qureshi", "Ananth Agarwal", "Tomer Shani", "Matan Eyal", "Anuj Khare", "Shreyas Rammohan Belle", "Lei Wang", "Chetan Tekur", "Mihir Sanjay Kale", "Jinliang Wei", "Ruoxin Sang", "Brennan Saeta", "Tyler Liechty", "Yi Sun", "Yao Zhao", "Stephan Lee", "Pandu Nayak", "Doug Fritz", "Manish Reddy Vuyyuru", "John Aslanides", "Nidhi Vyas", "Martin Wicke", "Xiao Ma", "Evgenii Eltyshev", "Nina Martin", "Hardie Cate", "James Manyika", "Keyvan Amiri", "Yelin Kim", "Xi Xiong", "Kai Kang", "Florian Luisier", "Nilesh Tripuraneni", "David Madras", "Mandy Guo", "Austin Waters", "Oliver Wang", "Joshua Ainslie", "Jason Baldridge", "Han Zhang", "Garima Pruthi", "Jakob Bauer", "Feng Yang", "Riham Mansour", "Jason Gelman", "Yang Xu", "George Polovets", "Ji Liu", "Honglong Cai", "Warren Chen", "XiangHai Sheng", "Emily Xue", "Sherjil Ozair", "Christof Angermueller", "Xiaowei Li", "Anoop Sinha", "Weiren Wang", "Julia Wiesinger", "Emmanouil Koukoumidis", "Yuan Tian", "Anand Iyer", "Madhu Gurumurthy", "Mark Goldenson", "Parashar Shah", "MK Blake", "Hongkun Yu", "Anthony Urbanowicz", "Jennimaria Palomaki", "Chrisantha Fernando", "Ken Durden", "Harsh Mehta", "Nikola Momchev", "Elahe Rahimtoroghi", "Maria Georgaki", "Amit Raul", "Sebastian Ruder", "Morgan Redshaw", "Jinhyuk Lee", "Denny Zhou", "Komal Jalan", "Dinghua Li", "Blake Hechtman", "Parker Schuh", "Milad Nasr", "Kieran Milan", "Vladimir Mikulik", "Juliana Franco", "Tim Green", "Nam Nguyen", "Joe Kelley", "Aroma Mahendru", "Andrea Hu", "Joshua Howland", "Ben Vargas", "Jeffrey Hui", "Kshitij Bansal", "Vikram Rao", "Rakesh Ghiya", "Emma Wang", "Ke Ye", "Jean Michel Sarr", "Melanie Moranski Preston", "Madeleine Elish", "Steve Li", "Aakash Kaku", "Jigar Gupta", "Ice Pasupat", "Da-Cheng Juan", "Milan Someswar", "Tejvi M.", "Xinyun Chen", "Aida Amini", "Alex Fabrikant", "Eric Chu", "Xuanyi Dong", "Amruta Muthal", "Senaka Buthpitiya", "Sarthak Jauhari", "Nan Hua", "Urvashi Khandelwal", "Ayal Hitron", "Jie Ren", "Larissa Rinaldi", "Shahar Drath", "Avigail Dabush", "Nan-Jiang Jiang", "Harshal Godhia", "Uli Sachs", "Anthony Chen", "Yicheng Fan", "Hagai Taitelbaum", "Hila Noga", "Zhuyun Dai", "James Wang", "Chen Liang", "Jenny Hamer", "Chun-Sung Ferng", "Chenel Elkind", "Aviel Atias", "Paulina Lee", "Vít Listík", "Mathias Carlen", "Jan van de Kerkhof", "Marcin Pikus", "Krunoslav Zaher", "Paul Müller", "Sasha Zykova", "Richard Stefanec", "Vitaly Gatsko", "Christoph Hirnschall", "Ashwin Sethi", "Xingyu Federico Xu", "Chetan Ahuja", "Beth Tsai", "Anca Stefanoiu", "Bo Feng", "Keshav Dhandhania", "Manish Katyal", "Akshay Gupta", "Atharva Parulekar", "Divya Pitta", "Jing Zhao", "Vivaan Bhatia", "Yashodha Bhavnani", "Omar Alhadlaq", "Xiaolin Li", "Peter Danenberg", "Dennis Tu", "Alex Pine", "Vera Filippova", "Abhipso Ghosh", "Ben Limonchik", "Bhargava Urala", "Chaitanya Krishna Lanka", "Derik Clive", "Yi Sun", "Edward Li", "Hao Wu", "Kevin Hongtongsak", "Ianna Li", "Kalind Thakkar", "Kuanysh Omarov", "Kushal Majmundar", "Michael Alverson", "Michael Kucharski", "Mohak Patel", "Mudit Jain", "Maksim Zabelin", "Paolo Pelagatti", "Rohan Kohli", "Saurabh Kumar", "Joseph Kim", "Swetha Sankar", "Vineet Shah", "Lakshmi Ramachandruni", "Xiangkai Zeng", "Ben Bariach", "Laura Weidinger", "Tu Vu", "Alek Andreev", "Antoine He", "Kevin Hui", "Sheleem Kashem", "Amar Subramanya", "Sissie Hsiao", "Demis Hassabis", "Koray Kavukcuoglu", "Adam Sadovsky", "Quoc Le", "Trevor Strohman", "Yonghui Wu", "Slav Petrov", "Jeffrey Dean", "Oriol Vinyals"], "published": "2023-12-19T02:39:27Z", "updated": "2025-05-09T21:04:06Z", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf_url": "https://arxiv.org/pdf/2312.11805v5", "landing_url": "https://arxiv.org/abs/2312.11805v5", "doi": "https://doi.org/10.48550/arXiv.2312.11805"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses Gemini multimodal models and deployment; there is no focus on discrete audio tokens or tokenization/discretization of audio, so it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses Gemini multimodal models and deployment; there is no focus on discrete audio tokens or tokenization/discretization of audio, so it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper Gemini introduces a family of multimodal models with strong performance on benchmarks involving image, audio, video, and text, but does not provide specific details on discrete audio tokenization processes, token design, quantization methods, or tokenizer configuration; therefore, it does not meet the core inclusion criteria focused on discrete audio tokens and their design and evaluation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper Gemini introduces a family of multimodal models with strong performance on benchmarks involving image, audio, video, and text, but does not provide specific details on discrete audio tokenization processes, token design, quantization methods, or tokenizer configuration; therefore, it does not meet the core inclusion criteria focused on discrete audio tokens and their design and evaluation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wu2025esi", "query_title": "{Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model}", "normalized_title": "towards efficient speech text jointly decoding within one speech language model", "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model", "abstract": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.", "source": "arxiv", "source_id": "2506.04518v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model", "abstract": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.", "metadata": {"arxiv_id": "2506.04518v2", "title": "Towards Efficient Speech-Text Jointly Decoding within One Speech Language Model", "summary": "Speech language models (Speech LMs) enable end-to-end speech-text modelling within a single model, offering a promising direction for spoken dialogue systems. The choice of speech-text jointly decoding paradigm plays a critical role in performance, efficiency, and alignment quality. In this work, we systematically compare representative joint speech-text decoding strategies-including the interleaved, and parallel generation paradigms-under a controlled experimental setup using the same base language model, speech tokenizer and training data. Our results show that the interleaved approach achieves the best alignment. However it suffers from slow inference due to long token sequence length. To address this, we propose a novel early-stop interleaved (ESI) pattern that not only significantly accelerates decoding but also yields slightly better performance. Additionally, we curate high-quality question answering (QA) datasets to further improve speech QA performance.", "authors": ["Haibin Wu", "Yuxuan Hu", "Ruchao Fan", "Xiaofei Wang", "Kenichi Kumatani", "Bo Ren", "Jianwei Yu", "Heng Lu", "Lijuan Wang", "Yao Qian", "Jinyu Li"], "published": "2025-06-04T23:53:49Z", "updated": "2025-06-13T03:55:18Z", "categories": ["eess.AS", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2506.04518v2", "landing_url": "https://arxiv.org/abs/2506.04518v2", "doi": "https://doi.org/10.48550/arXiv.2506.04518"}, "round-A_JuniorNano_output": {"reasoning": "The article studies end-to-end speech-text joint decoding within one model and compares decoding paradigms, but does not define, generate, or evaluate discrete audio tokens/tokenization as a core representation, thus failing the inclusion criteria centered on discrete audio tokens and aligning with the exclusion criterion for continuous audio representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article studies end-to-end speech-text joint decoding within one model and compares decoding paradigms, but does not define, generate, or evaluate discrete audio tokens/tokenization as a core representation, thus failing the inclusion criteria centered on discrete audio tokens and aligning with the exclusion criterion for continuous audio representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on speech language models enabling end-to-end speech-text modeling and compares joint decoding strategies, but it does not explicitly describe the generation or use of discrete audio tokens via neural audio codecs or quantization methods, nor provide technical details on tokenization or codec architecture. Without clear evidence of discrete token design, generation process, or analysis associated with discrete audio tokens, it does not fully meet the inclusion criteria centered on discrete audio token research.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study focuses on speech language models enabling end-to-end speech-text modeling and compares joint decoding strategies, but it does not explicitly describe the generation or use of discrete audio tokens via neural audio codecs or quantization methods, nor provide technical details on tokenization or codec architecture. Without clear evidence of discrete token design, generation process, or analysis associated with discrete audio tokens, it does not fully meet the inclusion criteria centered on discrete audio token research.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "kumar2023high", "query_title": "{High-Fidelity Audio Compression with Improved {RVQGAN}}", "normalized_title": "high fidelity audio compression with improved rvqgan", "title": "High-Fidelity Audio Compression with Improved RVQGAN", "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.", "source": "arxiv", "source_id": "2306.06546v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "High-Fidelity Audio Compression with Improved RVQGAN", "abstract": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.", "metadata": {"arxiv_id": "2306.06546v2", "title": "High-Fidelity Audio Compression with Improved RVQGAN", "summary": "Language models have been successfully used to model natural signals, such as images, speech, and music. A key component of these models is a high quality neural compression model that can compress high-dimensional natural signals into lower dimensional discrete tokens. To that end, we introduce a high-fidelity universal neural audio compression algorithm that achieves ~90x compression of 44.1 KHz audio into tokens at just 8kbps bandwidth. We achieve this by combining advances in high-fidelity audio generation with better vector quantization techniques from the image domain, along with improved adversarial and reconstruction losses. We compress all domains (speech, environment, music, etc.) with a single universal model, making it widely applicable to generative modeling of all audio. We compare with competing audio compression algorithms, and find our method outperforms them significantly. We provide thorough ablations for every design choice, as well as open-source code and trained model weights. We hope our work can lay the foundation for the next generation of high-fidelity audio modeling.", "authors": ["Rithesh Kumar", "Prem Seetharaman", "Alejandro Luebs", "Ishaan Kumar", "Kundan Kumar"], "published": "2023-06-11T00:13:00Z", "updated": "2023-10-26T22:17:49Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2306.06546v2", "landing_url": "https://arxiv.org/abs/2306.06546v2", "doi": "https://doi.org/10.48550/arXiv.2306.06546"}, "round-A_JuniorNano_output": {"reasoning": "Contains a neural audio codec that produces discrete tokens via vector quantization (tokens, codebooks) and evaluates a token-based, multi-domain audio compression system, aligning with the discrete audio tokens topic and providing design/experimental details.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Contains a neural audio codec that produces discrete tokens via vector quantization (tokens, codebooks) and evaluates a token-based, multi-domain audio compression system, aligning with the discrete audio tokens topic and providing design/experimental details.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a high-fidelity neural audio compression algorithm that compresses continuous audio into discrete tokens using vector quantization techniques, providing details on compression rate, codec design, and open-source reproducibility; the tokens are central to the method, applicable across various audio domains, and support generative modeling, which aligns well with the inclusion criteria focusing on discrete audio tokens generated by neural audio codecs for modeling and generation purposes.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a high-fidelity neural audio compression algorithm that compresses continuous audio into discrete tokens using vector quantization techniques, providing details on compression rate, codec design, and open-source reproducibility; the tokens are central to the method, applicable across various audio domains, and support generative modeling, which aligns well with the inclusion criteria focusing on discrete audio tokens generated by neural audio codecs for modeling and generation purposes.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "zhang2023speechtokenizer", "query_title": "{{SpeechTokenizer}: Unified speech tokenizer for speech large language models}", "normalized_title": "speechtokenizer unified speech tokenizer for speech large language models", "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models", "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.", "source": "arxiv", "source_id": "2308.16692v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models", "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.", "metadata": {"arxiv_id": "2308.16692v2", "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models", "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.", "authors": ["Xin Zhang", "Dong Zhang", "Shimin Li", "Yaqian Zhou", "Xipeng Qiu"], "published": "2023-08-31T12:53:09Z", "updated": "2024-01-23T01:56:57Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.16692v2", "landing_url": "https://arxiv.org/abs/2308.16692v2", "doi": "https://doi.org/10.48550/arXiv.2308.16692"}, "round-A_JuniorNano_output": {"reasoning": "This item clearly targets discrete audio tokens: it introduces SpeechTokenizer (a discrete tokenization scheme using RVQ with multi-codebooks) and validates it on speech modeling tasks, provides English abstract with token-generation details, and reports experimental results—covering token design, generation, and downstream evaluation. It provides sufficient English content and implementation details to assess discreteness and reproducibility, and does not fall under the exclusion criteria. 1-sentence explanation: This study clearly proposes and evaluates a discrete speech token scheme suitable for inclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "This item clearly targets discrete audio tokens: it introduces SpeechTokenizer (a discrete tokenization scheme using RVQ with multi-codebooks) and validates it on speech modeling tasks, provides English abstract with token-generation details, and reports experimental results—covering token design, generation, and downstream evaluation. It provides sufficient English content and implementation details to assess discreteness and reproducibility, and does not fall under the exclusion criteria. 1-sentence explanation: This study clearly proposes and evaluates a discrete speech token scheme suitable for inclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a unified speech tokenizer based on an encoder-decoder architecture with residual vector quantization, combining semantic and acoustic tokens into discrete token sequences designed for speech large language models, supported by detailed benchmarking and evaluations for speech generation and understanding tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens, their design, evaluation, and application.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a unified speech tokenizer based on an encoder-decoder architecture with residual vector quantization, combining semantic and acoustic tokens into discrete token sequences designed for speech large language models, supported by detailed benchmarking and evaluations for speech generation and understanding tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens, their design, evaluation, and application.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2023audiodec", "query_title": "{Audiodec: An Open-Source Streaming High-Fidelity Neural Audio Codec}", "normalized_title": "audiodec an open source streaming high fidelity neural audio codec", "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec", "abstract": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.", "source": "arxiv", "source_id": "2305.16608v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec", "abstract": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.", "metadata": {"arxiv_id": "2305.16608v1", "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec", "summary": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.", "authors": ["Yi-Chiao Wu", "Israel D. Gebru", "Dejan Marković", "Alexander Richard"], "published": "2023-05-26T04:01:16Z", "updated": "2023-05-26T04:01:16Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.16608v1", "landing_url": "https://arxiv.org/abs/2305.16608v1", "doi": "https://doi.org/10.1109/ICASSP49357.2023.10096509"}, "round-A_JuniorNano_output": {"reasoning": "本文僅描述神經音訊編碼器的性能、延遲與壓縮率，未提及離散音訊 token 的設計/碼本/字彙表或以 token 作為核心的下游任務，因此不符合本研究的離散 token 主題與評估要求。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "本文僅描述神經音訊編碼器的性能、延遲與壓縮率，未提及離散音訊 token 的設計/碼本/字彙表或以 token 作為核心的下游任務，因此不符合本研究的離散 token 主題與評估要求。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study proposes a neural audio codec focusing on live transmission with low bitrate and latency, but the abstract does not explicitly mention discretization into token sequences, vector quantization details, or usage of discrete tokens for downstream modeling or cross-modal reasoning, making it unclear if it meets the core inclusion criteria about discrete audio tokens and their systemic evaluation.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study proposes a neural audio codec focusing on live transmission with low bitrate and latency, but the abstract does not explicitly mention discretization into token sequences, vector quantization details, or usage of discrete tokens for downstream modeling or cross-modal reasoning, making it unclear if it meets the core inclusion criteria about discrete audio tokens and their systemic evaluation.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "yang2023hifi", "query_title": "{{HiFi-Codec}: Group-residual Vector quantization for High Fidelity Audio Codec}", "normalized_title": "hifi codec group residual vector quantization for high fidelity audio codec", "title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec", "abstract": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}", "source": "arxiv", "source_id": "2305.02765v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec", "abstract": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}", "metadata": {"arxiv_id": "2305.02765v2", "title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec", "summary": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}", "authors": ["Dongchao Yang", "Songxiang Liu", "Rongjie Huang", "Jinchuan Tian", "Chao Weng", "Yuexian Zou"], "published": "2023-05-04T12:11:13Z", "updated": "2023-05-07T09:22:04Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.02765v2", "landing_url": "https://arxiv.org/abs/2305.02765v2", "doi": "https://doi.org/10.48550/arXiv.2305.02765"}, "round-A_JuniorNano_output": {"reasoning": "The paper proposes a discrete token-like representation via GRVQ with 4 codebooks for HiFi-Codec, discusses tokenization-like discretization, and evaluates on reconstruction with downstream generation context, meeting the discrete-audio-token criteria and providing English content.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper proposes a discrete token-like representation via GRVQ with 4 codebooks for HiFi-Codec, discusses tokenization-like discretization, and evaluates on reconstruction with downstream generation context, meeting the discrete-audio-token criteria and providing English content.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a HiFi-Codec using group-residual vector quantization, explicitly describing the use of discrete audio tokens generated by a neural audio codec with a low number of codebooks (4). It discusses token design choices impacting fidelity and provides detailed methodology, training data, and evaluation, fitting the inclusion criteria of research on discrete audio tokens used for audio generation and modeling, with clear English descriptions and verifiable tokenization processes. It does not fall into any exclusion criteria such as continuous representation only, traditional codec without neural discrete tokens, or lack of enough English detail.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a HiFi-Codec using group-residual vector quantization, explicitly describing the use of discrete audio tokens generated by a neural audio codec with a low number of codebooks (4). It discusses token design choices impacting fidelity and provides detailed methodology, training data, and evaluation, fitting the inclusion criteria of research on discrete audio tokens used for audio generation and modeling, with clear English descriptions and verifiable tokenization processes. It does not fall into any exclusion criteria such as continuous representation only, traditional codec without neural discrete tokens, or lack of enough English detail.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "du2023funcodec", "query_title": "{{FunCodec}: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec}", "normalized_title": "funcodec a fundamental reproducible and integrable open source toolkit for neural speech codec", "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec", "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.", "source": "arxiv", "source_id": "2309.07405v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec", "abstract": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.", "metadata": {"arxiv_id": "2309.07405v2", "title": "FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec", "summary": "This paper presents FunCodec, a fundamental neural speech codec toolkit, which is an extension of the open-source speech processing toolkit FunASR. FunCodec provides reproducible training recipes and inference scripts for the latest neural speech codec models, such as SoundStream and Encodec. Thanks to the unified design with FunASR, FunCodec can be easily integrated into downstream tasks, such as speech recognition. Along with FunCodec, pre-trained models are also provided, which can be used for academic or generalized purposes. Based on the toolkit, we further propose the frequency-domain codec models, FreqCodec, which can achieve comparable speech quality with much lower computation and parameter complexity. Experimental results show that, under the same compression ratio, FunCodec can achieve better reconstruction quality compared with other toolkits and released models. We also demonstrate that the pre-trained models are suitable for downstream tasks, including automatic speech recognition and personalized text-to-speech synthesis. This toolkit is publicly available at https://github.com/alibaba-damo-academy/FunCodec.", "authors": ["Zhihao Du", "Shiliang Zhang", "Kai Hu", "Siqi Zheng"], "published": "2023-09-14T03:18:24Z", "updated": "2023-10-07T02:56:00Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2309.07405v2", "landing_url": "https://arxiv.org/abs/2309.07405v2", "doi": "https://doi.org/10.48550/arXiv.2309.07405"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on a neural speech codec toolkit and integration (FunCodec) rather than on discrete audio tokens, tokenization methods, or multi-codebook/token design; it does not clearly address discrete token generation or evaluation, so it likely does not meet the token-centric inclusion criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The abstract centers on a neural speech codec toolkit and integration (FunCodec) rather than on discrete audio tokens, tokenization methods, or multi-codebook/token design; it does not clearly address discrete token generation or evaluation, so it likely does not meet the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit that includes neural audio codec models like SoundStream and Encodec, which are relevant for producing discrete audio tokens via vector quantization; it discusses design choices impacting quality and computation, provides pre-trained models and reproducible training/inference scripts, and evaluates reconstruction quality and downstream speech tasks, meeting the inclusion criteria of focusing on discrete token sequences from neural audio codecs with clear reproducibility and experimental validation, thus fitting well within the defined topic \"Discrete Audio Tokens\".", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents FunCodec, an open-source neural speech codec toolkit that includes neural audio codec models like SoundStream and Encodec, which are relevant for producing discrete audio tokens via vector quantization; it discusses design choices impacting quality and computation, provides pre-trained models and reproducible training/inference scripts, and evaluates reconstruction quality and downstream speech tasks, meeting the inclusion criteria of focusing on discrete token sequences from neural audio codecs with clear reproducibility and experimental validation, thus fitting well within the defined topic \"Discrete Audio Tokens\".", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": {"reasoning": "Junior reviewers disagreed: one excluded it for not explicitly describing discrete tokens, the other included it as clearly relevant. The abstract presents an open-source neural speech codec toolkit (FunCodec) implementing models like SoundStream and Encodec (which are neural codecs that use vector-quantized / multi-codebook discrete representations), provides reproducible training/inference recipes and pre-trained models, and demonstrates downstream use (ASR, personalized TTS) and empirical evaluation — all of which align with the “Discrete Audio Tokens” inclusion group; however, the abstract itself lacks explicit low-level tokenization/quantization parameter details, so inclusion is slightly cautious. Score 4: better to include (relevant toolkit and pre-trained neural codecs that produce discrete tokens and support downstream tasks, though the abstract could be more explicit about token/codec settings).", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: one excluded it for not explicitly describing discrete tokens, the other included it as clearly relevant. The abstract presents an open-source neural speech codec toolkit (FunCodec) implementing models like SoundStream and Encodec (which are neural codecs that use vector-quantized / multi-codebook discrete representations), provides reproducible training/inference recipes and pre-trained models, and demonstrates downstream use (ASR, personalized TTS) and empirical evaluation — all of which align with the “Discrete Audio Tokens” inclusion group; however, the abstract itself lacks explicit low-level tokenization/quantization parameter details, so inclusion is slightly cautious. Score 4: better to include (relevant toolkit and pre-trained neural codecs that produce discrete tokens and support downstream tasks, though the abstract could be more explicit about token/codec settings).", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "borsos2023audiolm", "query_title": "{{AudioLM}: A Language Modeling Approach to Audio Generation}", "normalized_title": "audiolm a language modeling approach to audio generation", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.", "source": "arxiv", "source_id": "2209.03143v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioLM: a Language Modeling Approach to Audio Generation", "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.", "metadata": {"arxiv_id": "2209.03143v2", "title": "AudioLM: a Language Modeling Approach to Audio Generation", "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.", "authors": ["Zalán Borsos", "Raphaël Marinier", "Damien Vincent", "Eugene Kharitonov", "Olivier Pietquin", "Matt Sharifi", "Dominik Roblek", "Olivier Teboul", "David Grangier", "Marco Tagliasacchi", "Neil Zeghidour"], "published": "2022-09-07T13:40:08Z", "updated": "2023-07-26T03:52:36Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2209.03143v2", "landing_url": "https://arxiv.org/abs/2209.03143v2", "doi": "https://doi.org/10.48550/arXiv.2209.03143"}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly frames audio generation via discrete token sequences, using tokenizer/discretized activations and neural audio codecs to produce discrete tokens, and discusses downstream generation tasks (speech and music) and evaluation, fitting the 'Discrete Audio Tokens' topic.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly frames audio generation via discrete token sequences, using tokenizer/discretized activations and neural audio codecs to produce discrete tokens, and discusses downstream generation tasks (speech and music) and evaluation, fitting the 'Discrete Audio Tokens' topic.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes AudioLM which uses a hybrid tokenization scheme combining discretized activations of a masked language model pre-trained on audio and discrete codes from a neural audio codec, mapping audio to discrete token sequences for language modeling-based audio generation encompassing speech and music; this aligns well with the inclusion criteria focusing on neural codec-based discrete audio tokens used for generation with detailed tokenization design and empirical evaluations, thus meeting all inclusion and no exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes AudioLM which uses a hybrid tokenization scheme combining discretized activations of a masked language model pre-trained on audio and discrete codes from a neural audio codec, mapping audio to discrete token sequences for language modeling-based audio generation encompassing speech and music; this aligns well with the inclusion criteria focusing on neural codec-based discrete audio tokens used for generation with detailed tokenization design and empirical evaluations, thus meeting all inclusion and no exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "rubenstein2023audiopalm", "query_title": "{{AudioPaLM}: A Large Language Model That Can Speak and Listen}", "normalized_title": "audiopalm a large language model that can speak and listen", "title": "AudioPaLM: A Large Language Model That Can Speak and Listen", "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples", "source": "arxiv", "source_id": "2306.12925v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioPaLM: A Large Language Model That Can Speak and Listen", "abstract": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples", "metadata": {"arxiv_id": "2306.12925v1", "title": "AudioPaLM: A Large Language Model That Can Speak and Listen", "summary": "We introduce AudioPaLM, a large language model for speech understanding and generation. AudioPaLM fuses text-based and speech-based language models, PaLM-2 [Anil et al., 2023] and AudioLM [Borsos et al., 2022], into a unified multimodal architecture that can process and generate text and speech with applications including speech recognition and speech-to-speech translation. AudioPaLM inherits the capability to preserve paralinguistic information such as speaker identity and intonation from AudioLM and the linguistic knowledge present only in text large language models such as PaLM-2. We demonstrate that initializing AudioPaLM with the weights of a text-only large language model improves speech processing, successfully leveraging the larger quantity of text training data used in pretraining to assist with the speech tasks. The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. We release examples of our method at https://google-research.github.io/seanet/audiopalm/examples", "authors": ["Paul K. Rubenstein", "Chulayuth Asawaroengchai", "Duc Dung Nguyen", "Ankur Bapna", "Zalán Borsos", "Félix de Chaumont Quitry", "Peter Chen", "Dalia El Badawy", "Wei Han", "Eugene Kharitonov", "Hannah Muckenhirn", "Dirk Padfield", "James Qin", "Danny Rozenberg", "Tara Sainath", "Johan Schalkwyk", "Matt Sharifi", "Michelle Tadmor Ramanovich", "Marco Tagliasacchi", "Alexandru Tudor", "Mihajlo Velimirović", "Damien Vincent", "Jiahui Yu", "Yongqiang Wang", "Vicky Zayats", "Neil Zeghidour", "Yu Zhang", "Zhishuai Zhang", "Lukas Zilka", "Christian Frank"], "published": "2023-06-22T14:37:54Z", "updated": "2023-06-22T14:37:54Z", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2306.12925v1", "landing_url": "https://arxiv.org/abs/2306.12925v1", "doi": "https://doi.org/10.48550/arXiv.2306.12925"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a multimodal LLM (AudioPaLM) for speech understanding and generation, but it does not discuss discrete audio tokens, tokenizers, or neural audio codecs as core representations, so it does not meet the inclusion criteria focused on discrete audio tokens and should be excluded per the exclusion criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The item describes a multimodal LLM (AudioPaLM) for speech understanding and generation, but it does not discuss discrete audio tokens, tokenizers, or neural audio codecs as core representations, so it does not meet the inclusion criteria focused on discrete audio tokens and should be excluded per the exclusion criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The abstract describes AudioPaLM, a large language model integrating text-based and speech-based models to perform speech understanding and generation tasks, including speech recognition and speech-to-speech translation. However, it does not explicitly mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised learning for representing continuous audio as discrete token sequences, nor does it provide details on tokenization processes, codebook settings, or quantization methods. Without clear evidence of discrete token mechanisms or related system design details, the paper does not meet the core inclusion criteria regarding discrete audio tokens.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes AudioPaLM, a large language model integrating text-based and speech-based models to perform speech understanding and generation tasks, including speech recognition and speech-to-speech translation. However, it does not explicitly mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised learning for representing continuous audio as discrete token sequences, nor does it provide details on tokenization processes, codebook settings, or quantization methods. Without clear evidence of discrete token mechanisms or related system design details, the paper does not meet the core inclusion criteria regarding discrete audio tokens.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "wang2024viola", "query_title": "{Viola: Conditional language models for speech recognition, synthesis, and translation}", "normalized_title": "viola conditional language models for speech recognition synthesis and translation", "title": "VioLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional language model task via multi-task learning framework. To accomplish this, we first convert the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence prediction problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID), language IDs (LID), and LSTM-based acoustic embedding into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "source": "semantic_scholar", "source_id": "66fd85f42596fcd018d517a4bf6acc75100e59da", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "VioLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional language model task via multi-task learning framework. To accomplish this, we first convert the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence prediction problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID), language IDs (LID), and LSTM-based acoustic embedding into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "metadata": {"arxiv_id": "semantic_scholar:66fd85f42596fcd018d517a4bf6acc75100e59da", "title": "VioLA: Conditional Language Models for Speech Recognition, Synthesis, and Translation", "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional language model task via multi-task learning framework. To accomplish this, we first convert the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence prediction problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID), language IDs (LID), and LSTM-based acoustic embedding into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "source": "semantic_scholar", "source_id": "66fd85f42596fcd018d517a4bf6acc75100e59da", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly uses discrete audio tokens via a neural codec to convert speech into token sequences and models cross-modal tasks (speech/text) with a decoder-only transformer, aligning with the 'Discrete Audio Tokens' criteria and enabling token-level generation and evaluation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete audio tokens via a neural codec to convert speech into token sequences and models cross-modal tasks (speech/text) with a decoder-only transformer, aligning with the 'Discrete Audio Tokens' criteria and enabling token-level generation and evaluation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes VioLA, a Transformer-based conditional language model that unifies speech and text tasks via converting speech utterances into discrete tokens generated by an offline neural codec encoder. This aligns with the inclusion criteria emphasizing the use of discrete audio tokens derived from neural audio codecs and their application in multi-task learning for speech generation and understanding. Although the abstract does not detail the tokenizer architecture, quantization specifics, or codebook settings, it clearly uses token-based sequence prediction with discrete tokens from a neural codec, supporting reproducibility and evaluation. Therefore, it meets the core criteria and should be included.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper proposes VioLA, a Transformer-based conditional language model that unifies speech and text tasks via converting speech utterances into discrete tokens generated by an offline neural codec encoder. This aligns with the inclusion criteria emphasizing the use of discrete audio tokens derived from neural audio codecs and their application in multi-task learning for speech generation and understanding. Although the abstract does not detail the tokenizer architecture, quantization specifics, or codebook settings, it clearly uses token-based sequence prediction with discrete tokens from a neural codec, supporting reproducibility and evaluation. Therefore, it meets the core criteria and should be included.", "round-A_JuniorMini_evaluation": 4, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "sicherman2023analysing", "query_title": "{Analysing discrete self supervised speech representation for spoken language modeling}", "normalized_title": "analysing discrete self supervised speech representation for spoken language modeling", "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling", "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations", "source": "arxiv", "source_id": "2301.00591v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling", "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations", "metadata": {"arxiv_id": "2301.00591v3", "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling", "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations", "authors": ["Amitay Sicherman", "Yossi Adi"], "published": "2023-01-02T10:36:40Z", "updated": "2023-03-01T09:59:54Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2301.00591v3", "landing_url": "https://arxiv.org/abs/2301.00591v3", "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"}, "round-A_JuniorNano_output": {"reasoning": "The abstract analyzes discrete self-supervised speech representations (units) as discrete tokens for modeling/analysis, proposes a new metric to quantify unit redundancies, and develops methods to improve clustering with evaluations on ABX, aligning with the ‘Discrete Audio Tokens’ criteria and providing English title/abstract.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract analyzes discrete self-supervised speech representations (units) as discrete tokens for modeling/analysis, proposes a new metric to quantify unit redundancies, and develops methods to improve clustering with evaluations on ABX, aligning with the ‘Discrete Audio Tokens’ criteria and providing English title/abstract.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper analyzes discrete self-supervised speech representations (units) used as tokens for generative spoken language modeling, clearly describing the production of discrete units from self-supervised learning models and evaluating their properties and improvements, fitting well within the inclusion criteria focused on discrete audio tokens produced by neural/self-supervised methods and used for modeling and generation; the abstract is in English and provides relevant methodological details, thus meeting inclusion and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper analyzes discrete self-supervised speech representations (units) used as tokens for generative spoken language modeling, clearly describing the production of discrete units from self-supervised learning models and evaluating their properties and improvements, fitting well within the inclusion criteria focused on discrete audio tokens produced by neural/self-supervised methods and used for modeling and generation; the abstract is in English and provides relevant methodological details, thus meeting inclusion and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "arora2025landscape", "query_title": "{On The Landscape of Spoken Language Models: A Comprehensive Survey}", "normalized_title": "on the landscape of spoken language models a comprehensive survey", "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "abstract": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.", "source": "arxiv", "source_id": "2504.08528v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "abstract": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.", "metadata": {"arxiv_id": "2504.08528v1", "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey", "summary": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.", "authors": ["Siddhant Arora", "Kai-Wei Chang", "Chung-Ming Chien", "Yifan Peng", "Haibin Wu", "Yossi Adi", "Emmanuel Dupoux", "Hung-Yi Lee", "Karen Livescu", "Shinji Watanabe"], "published": "2025-04-11T13:40:53Z", "updated": "2025-04-11T13:40:53Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2504.08528v1", "landing_url": "https://arxiv.org/abs/2504.08528v1", "doi": "https://doi.org/10.48550/arXiv.2504.08528"}, "round-A_JuniorNano_output": {"reasoning": "This item is a broad survey of Spoken Language Models and does not present discrete audio token generation/codec design, token vocabularies, or token-based analyses as its core contribution; it only mentions tokenized speech sequences in passing, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is a broad survey of Spoken Language Models and does not present discrete audio token generation/codec design, token vocabularies, or token-based analyses as its core contribution; it only mentions tokenized speech sequences in passing, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper is a comprehensive survey on spoken language models (SLMs), focusing on universal speech processing systems and language models for speech. However, it does not focus on discrete audio tokens or the discretization of continuous audio waveforms into token sequences, nor does it discuss neural audio codecs, token design choices, or evaluations related to discrete tokens, which are essential inclusion criteria. Therefore, it does not meet the core requirements related to discrete audio tokens and token-based modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper is a comprehensive survey on spoken language models (SLMs), focusing on universal speech processing systems and language models for speech. However, it does not focus on discrete audio tokens or the discretization of continuous audio waveforms into token sequences, nor does it discuss neural audio codecs, token design choices, or evaluations related to discrete tokens, which are essential inclusion criteria. Therefore, it does not meet the core requirements related to discrete audio tokens and token-based modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "chen2023lauragpt", "query_title": "{{LauraGPT}: Listen, attend, understand, and regenerate audio with {GPT}}", "normalized_title": "lauragpt listen attend understand and regenerate audio with gpt", "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT", "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.", "source": "arxiv", "source_id": "2310.04673v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT", "abstract": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.", "metadata": {"arxiv_id": "2310.04673v4", "title": "LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT", "summary": "Generative Pre-trained Transformer (GPT) models have achieved remarkable performance on various natural language processing tasks, and have shown great potential as backbones for audio-and-text large language models (LLMs). Previous mainstream audio-and-text LLMs use discrete audio tokens to represent both input and output audio; however, they suffer from performance degradation on tasks such as automatic speech recognition, speech-to-text translation, and speech enhancement over models using continuous speech features. In this paper, we propose LauraGPT, a novel unified audio-and-text GPT-based LLM for audio recognition, understanding, and generation. LauraGPT is a versatile LLM that can process both audio and text inputs and generate outputs in either modalities. We propose a novel data representation that combines continuous and discrete features for audio: LauraGPT encodes input audio into continuous representations using an audio encoder and generates output audio from discrete codec codes. We propose a one-step codec vocoder to overcome the prediction challenge caused by the multimodal distribution of codec tokens. We fine-tune LauraGPT using supervised multi-task learning. Extensive experiments show that LauraGPT consistently achieves comparable to superior performance compared to strong baselines on a wide range of audio tasks related to content, semantics, paralinguistics, and audio-signal analysis, such as automatic speech recognition, speech-to-text translation, text-to-speech synthesis, speech enhancement, automated audio captioning, speech emotion recognition, and spoken language understanding.", "authors": ["Zhihao Du", "Jiaming Wang", "Qian Chen", "Yunfei Chu", "Zhifu Gao", "Zerui Li", "Kai Hu", "Xiaohuan Zhou", "Jin Xu", "Ziyang Ma", "Wen Wang", "Siqi Zheng", "Chang Zhou", "Zhijie Yan", "Shiliang Zhang"], "published": "2023-10-07T03:17:59Z", "updated": "2024-07-03T02:38:03Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2310.04673v4", "landing_url": "https://arxiv.org/abs/2310.04673v4", "doi": "https://doi.org/10.48550/arXiv.2310.04673"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses discrete audio tokens (discrete codec codes) to generate audio in a multimodal audio-text LLM, discusses token-based representations and their impact on downstream tasks, and provides an English abstract addressing tokenization and evaluation, aligning with the inclusion criteria for discrete audio tokens and excluding purely continuous or survey-focused work.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper uses discrete audio tokens (discrete codec codes) to generate audio in a multimodal audio-text LLM, discusses token-based representations and their impact on downstream tasks, and provides an English abstract addressing tokenization and evaluation, aligning with the inclusion criteria for discrete audio tokens and excluding purely continuous or survey-focused work.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper presents LauraGPT, which uses a combination of continuous audio representations for input and discrete codec codes for output audio generation. It involves a neural audio codec producing discrete tokens (codec codes) used for audio generation, which aligns with the inclusion criteria of using discrete audio tokens derived from neural audio encoders/codecs. The study also includes experimental evaluations on multiple downstream tasks, indicating empirical assessment. Despite the novelty of combining continuous and discrete features, the use of discrete tokens as core output representation and detailed mention of codec vocoder and discrete codec codes fulfill the main inclusion requirements. Therefore, the paper meets the inclusion criteria and does not meet any exclusion criterion.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents LauraGPT, which uses a combination of continuous audio representations for input and discrete codec codes for output audio generation. It involves a neural audio codec producing discrete tokens (codec codes) used for audio generation, which aligns with the inclusion criteria of using discrete audio tokens derived from neural audio encoders/codecs. The study also includes experimental evaluations on multiple downstream tasks, indicating empirical assessment. Despite the novelty of combining continuous and discrete features, the use of discrete tokens as core output representation and detailed mention of codec vocoder and discrete codec codes fulfill the main inclusion requirements. Therefore, the paper meets the inclusion criteria and does not meet any exclusion criterion.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "borsos2023soundstorm", "query_title": "{{SoundStorm}: Efficient Parallel Audio Generation}", "normalized_title": "soundstorm efficient parallel audio generation", "title": "SoundStorm: Efficient Parallel Audio Generation", "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.", "source": "arxiv", "source_id": "2305.09636v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SoundStorm: Efficient Parallel Audio Generation", "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.", "metadata": {"arxiv_id": "2305.09636v1", "title": "SoundStorm: Efficient Parallel Audio Generation", "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.", "authors": ["Zalán Borsos", "Matt Sharifi", "Damien Vincent", "Eugene Kharitonov", "Neil Zeghidour", "Marco Tagliasacchi"], "published": "2023-05-16T17:41:25Z", "updated": "2023-05-16T17:41:25Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.09636v1", "landing_url": "https://arxiv.org/abs/2305.09636v1", "doi": "https://doi.org/10.48550/arXiv.2305.09636"}, "round-A_JuniorNano_output": {"reasoning": "SoundStorm uses discrete tokens from a neural audio codec (semantic tokens) for parallel audio generation, aligning with the Discrete Audio Tokens criterion and not matching any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "SoundStorm uses discrete tokens from a neural audio codec (semantic tokens) for parallel audio generation, aligning with the Discrete Audio Tokens criterion and not matching any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study presents SoundStorm, an efficient non-autoregressive audio generation model that inputs semantic tokens of AudioLM and outputs neural audio codec tokens. The presence of neural audio codec tokens indicates the usage of discrete audio tokens generated by a neural codec. However, the abstract does not detail the tokenization process, codec architecture, quantization, or codebook settings necessary to fully assess whether the token generation aligns with the inclusion criteria. It does focus on audio generation using these tokens, which is relevant. Given the lack of explicit information on the tokenization specifics, it is unclear if it fully meets all inclusion requirements.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study presents SoundStorm, an efficient non-autoregressive audio generation model that inputs semantic tokens of AudioLM and outputs neural audio codec tokens. The presence of neural audio codec tokens indicates the usage of discrete audio tokens generated by a neural codec. However, the abstract does not detail the tokenization process, codec architecture, quantization, or codebook settings necessary to fully assess whether the token generation aligns with the inclusion criteria. It does focus on audio generation using these tokens, which is relevant. Given the lack of explicit information on the tokenization specifics, it is unclear if it fully meets all inclusion requirements.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Junior reviewers: Reviewer A assigned 5 noting SoundStorm uses discrete semantic tokens and neural audio codec tokens and matches the Discrete Audio Tokens topic; Reviewer B assigned 3, cautious because the abstract does not detail the tokenizer/codec architecture or quantization specifics required by the inclusion checklist. My assessment: although the abstract lacks low-level implementation details, it explicitly states the use of semantic tokens (from AudioLM) as input and generation of neural audio codec tokens as the core representation for audio generation, and the paper focuses on using these discrete tokens for downstream generation tasks—this satisfies the core inclusion requirements (use of neural-audio-codec-derived discrete tokens and application to generation). One-sentence explanation: The abstract clearly describes using neural audio-codec discrete tokens as the central representation for generation, matching the Discrete Audio Tokens inclusion criteria, so include.", "evaluation": 5}, "round-B_SeniorLead_reasoning": "Junior reviewers: Reviewer A assigned 5 noting SoundStorm uses discrete semantic tokens and neural audio codec tokens and matches the Discrete Audio Tokens topic; Reviewer B assigned 3, cautious because the abstract does not detail the tokenizer/codec architecture or quantization specifics required by the inclusion checklist. My assessment: although the abstract lacks low-level implementation details, it explicitly states the use of semantic tokens (from AudioLM) as input and generation of neural audio codec tokens as the core representation for audio generation, and the paper focuses on using these discrete tokens for downstream generation tasks—this satisfies the core inclusion requirements (use of neural-audio-codec-derived discrete tokens and application to generation). One-sentence explanation: The abstract clearly describes using neural audio-codec discrete tokens as the central representation for generation, matching the Discrete Audio Tokens inclusion criteria, so include.", "round-B_SeniorLead_evaluation": 5, "final_verdict": "include (senior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2023viola", "query_title": "{{VioLA}: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation}", "normalized_title": "viola unified codec language models for speech recognition synthesis and translation", "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "source": "arxiv", "source_id": "2305.16107v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation", "abstract": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "metadata": {"arxiv_id": "2305.16107v1", "title": "VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation", "summary": "Recent research shows a big convergence in model architecture, training objectives, and inference methods across various tasks for different modalities. In this paper, we propose VioLA, a single auto-regressive Transformer decoder-only network that unifies various cross-modal tasks involving speech and text, such as speech-to-text, text-to-text, text-to-speech, and speech-to-speech tasks, as a conditional codec language model task via multi-task learning framework. To accomplish this, we first convert all the speech utterances to discrete tokens (similar to the textual data) using an offline neural codec encoder. In such a way, all these tasks are converted to token-based sequence conversion problems, which can be naturally handled with one conditional language model. We further integrate task IDs (TID) and language IDs (LID) into the proposed model to enhance the modeling capability of handling different languages and tasks. Experimental results demonstrate that the proposed VioLA model can support both single-modal and cross-modal tasks well, and the decoder-only model achieves a comparable and even better performance than the strong baselines.", "authors": ["Tianrui Wang", "Long Zhou", "Ziqiang Zhang", "Yu Wu", "Shujie Liu", "Yashesh Gaur", "Zhuo Chen", "Jinyu Li", "Furu Wei"], "published": "2023-05-25T14:39:47Z", "updated": "2023-05-25T14:39:47Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.16107v1", "landing_url": "https://arxiv.org/abs/2305.16107v1", "doi": "https://doi.org/10.48550/arXiv.2305.16107"}, "round-A_JuniorNano_output": {"reasoning": "Explanation: The paper discretizes speech into discrete tokens via an offline neural codec encoder and uses a decoder-only Transformer to model token sequences for speech recognition, synthesis, and translation, i.e., cross-modal tasks. This aligns with the discrete audio token criteria and reports token-based modeling details; no exclusion criteria are evident.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Explanation: The paper discretizes speech into discrete tokens via an offline neural codec encoder and uses a decoder-only Transformer to model token sequences for speech recognition, synthesis, and translation, i.e., cross-modal tasks. This aligns with the discrete audio token criteria and reports token-based modeling details; no exclusion criteria are evident.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes VioLA, which uses an offline neural codec encoder to convert speech into discrete tokens, fitting the 'Discrete Audio Tokens' theme by focusing on token-based sequence conversion of speech data. It clearly describes the use of a neural audio codec for token generation, applicable for multiple speech and text tasks with experimental results, thus meeting the inclusion criteria regarding token source, method, and downstream tasks. The work is detailed in English with abstract and methods, and focuses on generating and leveraging discrete audio tokens for multi-modal modeling, not just continuous representations or traditional audio compression. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes VioLA, which uses an offline neural codec encoder to convert speech into discrete tokens, fitting the 'Discrete Audio Tokens' theme by focusing on token-based sequence conversion of speech data. It clearly describes the use of a neural audio codec for token generation, applicable for multiple speech and text tasks with experimental results, thus meeting the inclusion criteria regarding token source, method, and downstream tasks. The work is detailed in English with abstract and methods, and focuses on generating and leveraging discrete audio tokens for multi-modal modeling, not just continuous representations or traditional audio compression. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2023speechx", "query_title": "{SpeechX: Neural Codec Language Model as a Versatile Speech Transformer}", "normalized_title": "speechx neural codec language model as a versatile speech transformer", "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer", "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.", "source": "arxiv", "source_id": "2308.06873v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer", "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.", "metadata": {"arxiv_id": "2308.06873v2", "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer", "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.", "authors": ["Xiaofei Wang", "Manthan Thakker", "Zhuo Chen", "Naoyuki Kanda", "Sefik Emre Eskimez", "Sanyuan Chen", "Min Tang", "Shujie Liu", "Jinyu Li", "Takuya Yoshioka"], "published": "2023-08-14T01:01:19Z", "updated": "2024-06-25T18:38:28Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2308.06873v2", "landing_url": "https://arxiv.org/abs/2308.06873v2", "doi": "https://doi.org/10.48550/arXiv.2308.06873"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on neural codec language modeling for versatile speech tasks but does not clearly discuss discrete audio tokens, tokenization, codebooks, or the use of discrete tokens as core representation, so it likely does not meet the discrete audio tokens inclusion criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The abstract centers on neural codec language modeling for versatile speech tasks but does not clearly discuss discrete audio tokens, tokenization, codebooks, or the use of discrete tokens as core representation, so it likely does not meet the discrete audio tokens inclusion criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper proposes SpeechX, a neural codec language model that utilizes neural audio codec techniques presumably involving quantization to generate discrete audio tokens, which is directly relevant to discrete audio token research for speech generation and transformation tasks like TTS, noise suppression, and speech editing. However, the abstract does not provide sufficient explicit detail about the discrete token generation process, codec architecture, codebook configurations, or quantization specifics, which are required to confirm compliance with the inclusion criteria fully. Nevertheless, given the direct mention of neural codec language modeling and multi-task learning on discrete representations for diverse audio tasks, it likely aligns with the theme of discrete audio tokens modeling and application in audio generation and transformation. Therefore, the inclusion is favored but not decisively confirmed due to insufficient granularity in the abstract regarding discrete token design and evaluation.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper proposes SpeechX, a neural codec language model that utilizes neural audio codec techniques presumably involving quantization to generate discrete audio tokens, which is directly relevant to discrete audio token research for speech generation and transformation tasks like TTS, noise suppression, and speech editing. However, the abstract does not provide sufficient explicit detail about the discrete token generation process, codec architecture, codebook configurations, or quantization specifics, which are required to confirm compliance with the inclusion criteria fully. Nevertheless, given the direct mention of neural codec language modeling and multi-task learning on discrete representations for diverse audio tasks, it likely aligns with the theme of discrete audio tokens modeling and application in audio generation and transformation. Therefore, the inclusion is favored but not decisively confirmed due to insufficient granularity in the abstract regarding discrete token design and evaluation.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": {"reasoning": "I reviewed the abstract and the two junior reviewers' outputs: the paper proposes SpeechX, a “neural codec language model” for zero-shot TTS and various speech-transformation tasks (suggesting use of discrete codec tokens), JuniorNano recommended exclusion (2) citing lack of explicit tokenization details, and JuniorMini leaned toward inclusion (4) but also noted insufficient granularity in the abstract. The inclusion criteria require clear, inspectable descriptions of how discrete audio tokens are produced (tokenizer/codec architecture, quantization, codebook settings, bitrate/downsampling) to confirm the work centers on discrete audio tokens; the abstract mentions a neural codec LM but does not provide these required details. Therefore, based only on the title and abstract it is impossible to confirm that ALL inclusion criteria are met and NONE of the exclusion criteria apply. Score explanation (1 sentence): Ambiguous — the abstract implies discrete tokens via a neural codec LM but lacks the explicit tokenizer/quantization/codebook details required by the inclusion criteria, so inclusion cannot be confirmed without the full paper.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "I reviewed the abstract and the two junior reviewers' outputs: the paper proposes SpeechX, a “neural codec language model” for zero-shot TTS and various speech-transformation tasks (suggesting use of discrete codec tokens), JuniorNano recommended exclusion (2) citing lack of explicit tokenization details, and JuniorMini leaned toward inclusion (4) but also noted insufficient granularity in the abstract. The inclusion criteria require clear, inspectable descriptions of how discrete audio tokens are produced (tokenizer/codec architecture, quantization, codebook settings, bitrate/downsampling) to confirm the work centers on discrete audio tokens; the abstract mentions a neural codec LM but does not provide these required details. Therefore, based only on the title and abstract it is impossible to confirm that ALL inclusion criteria are met and NONE of the exclusion criteria apply. Score explanation (1 sentence): Ambiguous — the abstract implies discrete tokens via a neural codec LM but lacks the explicit tokenizer/quantization/codebook details required by the inclusion criteria, so inclusion cannot be confirmed without the full paper.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "kreuk2022audiogen", "query_title": "{{AudioGen}: Textually Guided Audio Generation}", "normalized_title": "audiogen textually guided audio generation", "title": "AudioGen: Textually Guided Audio Generation", "abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen", "source": "arxiv", "source_id": "2209.15352v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioGen: Textually Guided Audio Generation", "abstract": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen", "metadata": {"arxiv_id": "2209.15352v2", "title": "AudioGen: Textually Guided Audio Generation", "summary": "We tackle the problem of generating audio samples conditioned on descriptive text captions. In this work, we propose AaudioGen, an auto-regressive generative model that generates audio samples conditioned on text inputs. AudioGen operates on a learnt discrete audio representation. The task of text-to-audio generation poses multiple challenges. Due to the way audio travels through a medium, differentiating ``objects'' can be a difficult task (e.g., separating multiple people simultaneously speaking). This is further complicated by real-world recording conditions (e.g., background noise, reverberation, etc.). Scarce text annotations impose another constraint, limiting the ability to scale models. Finally, modeling high-fidelity audio requires encoding audio at high sampling rate, leading to extremely long sequences. To alleviate the aforementioned challenges we propose an augmentation technique that mixes different audio samples, driving the model to internally learn to separate multiple sources. We curated 10 datasets containing different types of audio and text annotations to handle the scarcity of text-audio data points. For faster inference, we explore the use of multi-stream modeling, allowing the use of shorter sequences while maintaining a similar bitrate and perceptual quality. We apply classifier-free guidance to improve adherence to text. Comparing to the evaluated baselines, AudioGen outperforms over both objective and subjective metrics. Finally, we explore the ability of the proposed method to generate audio continuation conditionally and unconditionally. Samples: https://felixkreuk.github.io/audiogen", "authors": ["Felix Kreuk", "Gabriel Synnaeve", "Adam Polyak", "Uriel Singer", "Alexandre Défossez", "Jade Copet", "Devi Parikh", "Yaniv Taigman", "Yossi Adi"], "published": "2022-09-30T10:17:05Z", "updated": "2023-03-05T09:14:16Z", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2209.15352v2", "landing_url": "https://arxiv.org/abs/2209.15352v2", "doi": "https://doi.org/10.48550/arXiv.2209.15352"}, "round-A_JuniorNano_output": {"reasoning": "Abstract states 'AudioGen operates on a learnt discrete audio representation', indicating discrete tokens as the core representation for text-conditioned audio generation, which aligns with the inclusion criteria and there are no clear exclusion cues in the abstract.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Abstract states 'AudioGen operates on a learnt discrete audio representation', indicating discrete tokens as the core representation for text-conditioned audio generation, which aligns with the inclusion criteria and there are no clear exclusion cues in the abstract.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The abstract describes AudioGen, an auto-regressive generative model for text-to-audio generation operating on a learnt discrete audio representation. It mentions the use of discrete tokens derived from neural audio encoding and vector quantization techniques, clearly aligning with the inclusion criteria about using discrete audio tokens for modeling and generation. The study provides details about the discrete audio representations, addresses challenges related to audio tokenization, and applies these tokens for downstream audio generation tasks. It is written in English with an English abstract, fulfilling language requirements, and the focus is on discrete token-based modeling rather than continuous audio waveform modeling or traditional audio compression. Therefore, the study meets the inclusion criteria without hitting any exclusion points.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The abstract describes AudioGen, an auto-regressive generative model for text-to-audio generation operating on a learnt discrete audio representation. It mentions the use of discrete tokens derived from neural audio encoding and vector quantization techniques, clearly aligning with the inclusion criteria about using discrete audio tokens for modeling and generation. The study provides details about the discrete audio representations, addresses challenges related to audio tokenization, and applies these tokens for downstream audio generation tasks. It is written in English with an English abstract, fulfilling language requirements, and the focus is on discrete token-based modeling rather than continuous audio waveform modeling or traditional audio compression. Therefore, the study meets the inclusion criteria without hitting any exclusion points.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "desplanques2020ecapa", "query_title": "{{ECAPA-TDNN}: Emphasized channel attention, propagation and aggregation in {TDNN} based speaker verification}", "normalized_title": "ecapa tdnn emphasized channel attention propagation and aggregation in tdnn based speaker verification", "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification", "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.", "source": "arxiv", "source_id": "2005.07143v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification", "abstract": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.", "metadata": {"arxiv_id": "2005.07143v3", "title": "ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification", "summary": "Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.", "authors": ["Brecht Desplanques", "Jenthe Thienpondt", "Kris Demuynck"], "published": "2020-05-14T17:02:15Z", "updated": "2020-08-10T13:50:24Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2005.07143v3", "landing_url": "https://arxiv.org/abs/2005.07143v3", "doi": "https://doi.org/10.21437/Interspeech.2020-2650"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on improving a TDNN-based speaker verification system using channel attention and aggregation; there is no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on improving a TDNN-based speaker verification system using channel attention and aggregation; there is no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on speaker verification using an enhanced TDNN architecture and does not describe the use or generation of discrete audio tokens via discretization methods such as neural audio codecs or quantization; it also lacks discussion on token vocabularies or token-based modeling for generation or cross-modal tasks, thus it does not meet the inclusion criteria related to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on speaker verification using an enhanced TDNN architecture and does not describe the use or generation of discrete audio tokens via discretization methods such as neural audio codecs or quantization; it also lacks discussion on token vocabularies or token-based modeling for generation or cross-modal tasks, thus it does not meet the inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ardila2019common", "query_title": "\"{Common Voice}: A Massively-Multilingual Speech Corpus\"", "normalized_title": "common voice a massively multilingual speech corpus", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.", "source": "arxiv", "source_id": "1912.06670v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Common Voice: A Massively-Multilingual Speech Corpus", "abstract": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.", "metadata": {"arxiv_id": "1912.06670v2", "title": "Common Voice: A Massively-Multilingual Speech Corpus", "summary": "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.", "authors": ["Rosana Ardila", "Megan Branson", "Kelly Davis", "Michael Henretty", "Michael Kohler", "Josh Meyer", "Reuben Morais", "Lindsay Saunders", "Francis M. Tyers", "Gregor Weber"], "published": "2019-12-13T19:22:44Z", "updated": "2020-03-05T20:37:08Z", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1912.06670v2", "landing_url": "https://arxiv.org/abs/1912.06670v2", "doi": "https://doi.org/10.48550/arXiv.1912.06670"}, "round-A_JuniorNano_output": {"reasoning": "此研究聚焦於一個大規模語音語料庫的資料集與實驗，未描述離散音訊 token 的產生、碼本設定或對下游任務的離散 token 研究，因此不符合本主題的納入條件。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此研究聚焦於一個大規模語音語料庫的資料集與實驗，未描述離散音訊 token 的產生、碼本設定或對下游任務的離散 token 研究，因此不符合本主題的納入條件。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents the Common Voice corpus, a multilingual speech dataset designed for Automatic Speech Recognition and related tasks; however, it does not focus on discrete audio tokens or the process of discretizing continuous audio waveforms into token sequences, nor does it describe neural audio codecs, vector quantization, or tokenization schemes; it primarily describes a dataset and its use in speech recognition experiments, which falls outside the defined inclusion criteria and aligns more with continuous audio representation and ASR tasks rather than discrete token-based modeling or generation frameworks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents the Common Voice corpus, a multilingual speech dataset designed for Automatic Speech Recognition and related tasks; however, it does not focus on discrete audio tokens or the process of discretizing continuous audio waveforms into token sequences, nor does it describe neural audio codecs, vector quantization, or tokenization schemes; it primarily describes a dataset and its use in speech recognition experiments, which falls outside the defined inclusion criteria and aligns more with continuous audio representation and ASR tasks rather than discrete token-based modeling or generation frameworks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "agostinelli2023musiclm", "query_title": "{{MusicLM}: Generating music from text}", "normalized_title": "musiclm generating music from text", "title": "MusicLM: Generating Music From Text", "abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.", "source": "arxiv", "source_id": "2301.11325v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MusicLM: Generating Music From Text", "abstract": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.", "metadata": {"arxiv_id": "2301.11325v1", "title": "MusicLM: Generating Music From Text", "summary": "We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.", "authors": ["Andrea Agostinelli", "Timo I. Denk", "Zalán Borsos", "Jesse Engel", "Mauro Verzetti", "Antoine Caillon", "Qingqing Huang", "Aren Jansen", "Adam Roberts", "Marco Tagliasacchi", "Matt Sharifi", "Neil Zeghidour", "Christian Frank"], "published": "2023-01-26T18:58:53Z", "updated": "2023-01-26T18:58:53Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2301.11325v1", "landing_url": "https://arxiv.org/abs/2301.11325v1", "doi": "https://doi.org/10.48550/arXiv.2301.11325"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on text-to-music generation (MusicLM) and does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on text-to-music generation (MusicLM) and does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes MusicLM as a model generating music from text and mentions its hierarchical sequence-to-sequence approach but does not provide any details on discrete audio tokens, tokenization, neural audio codec or vector quantization, vocabulary/codebook settings, or any discrete token generation details. It appears to operate on continuous waveforms or features rather than discrete tokens as core representations, thus failing key inclusion criteria concerning discrete audio token generation and usage. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking explicit discrete token methodology or analysis.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes MusicLM as a model generating music from text and mentions its hierarchical sequence-to-sequence approach but does not provide any details on discrete audio tokens, tokenization, neural audio codec or vector quantization, vocabulary/codebook settings, or any discrete token generation details. It appears to operate on continuous waveforms or features rather than discrete tokens as core representations, thus failing key inclusion criteria concerning discrete audio token generation and usage. Therefore, it does not meet the inclusion criteria and falls under exclusion for lacking explicit discrete token methodology or analysis.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yang2023uniaudio", "query_title": "{{UniAudio}: Towards universal audio generation with large language models}", "normalized_title": "uniaudio towards universal audio generation with large language models", "title": "UniAudio: Towards Universal Audio Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.", "source": "semantic_scholar", "source_id": "61632c78b26ca366b5a1c8cbf3d0f50981126e8e", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "UniAudio: Towards Universal Audio Generation with Large Language Models", "abstract": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.", "metadata": {"arxiv_id": "semantic_scholar:61632c78b26ca366b5a1c8cbf3d0f50981126e8e", "title": "UniAudio: Towards Universal Audio Generation with Large Language Models", "summary": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.", "source": "semantic_scholar", "source_id": "61632c78b26ca366b5a1c8cbf3d0f50981126e8e", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "The item is a survey on universal audio generation with LLMs, focusing on time-series analysis and multimodal methods rather than designing or evaluating discrete audio token representations or tokenizers/codebooks for audio; it does not meet the core criteria for discrete audio tokens.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The item is a survey on universal audio generation with LLMs, focusing on time-series analysis and multimodal methods rather than designing or evaluating discrete audio token representations or tokenizers/codebooks for audio; it does not meet the core criteria for discrete audio tokens.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a survey on the use of LLMs for time series analysis including audio but does not focus specifically on discrete audio tokens generated by neural audio codecs or self-supervised models; it lacks details on discrete token generation processes, vocabulary settings, or experiments on discrete tokens, and is primarily a broad survey rather than an original or in-depth study focused on discrete audio token design, evaluation, or application to audio generation or alignment as required by the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes a survey on the use of LLMs for time series analysis including audio but does not focus specifically on discrete audio tokens generated by neural audio codecs or self-supervised models; it lacks details on discrete token generation processes, vocabulary settings, or experiments on discrete tokens, and is primarily a broad survey rather than an original or in-depth study focused on discrete audio token design, evaluation, or application to audio generation or alignment as required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "wells2022phonetic", "query_title": "{Phonetic analysis of self-supervised representations of {E}nglish speech}", "normalized_title": "phonetic analysis of self supervised representations of english speech", "title": "Phonetic Analysis of Self-supervised Representations of English Speech", "abstract": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when consid-ering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.", "source": "semantic_scholar", "source_id": "fc5c506887b53d0e132cb6a4fb239c92cd5c9830", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Phonetic Analysis of Self-supervised Representations of English Speech", "abstract": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when consid-ering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.", "metadata": {"arxiv_id": "semantic_scholar:fc5c506887b53d0e132cb6a4fb239c92cd5c9830", "title": "Phonetic Analysis of Self-supervised Representations of English Speech", "summary": "We present an analysis of discrete units discovered via self-supervised representation learning on English speech. We focus on units produced by a pre-trained HuBERT model due to its wide adoption in ASR, speech synthesis, and many other tasks. Whereas previous work has evaluated the quality of such quantization models in aggregate over all phones for a given language, we break our analysis down into broad phonetic classes, taking into account specific aspects of their articulation when consid-ering their alignment to discrete units. We find that these units correspond to sub-phonetic events, and that fine dynamics such as the distinct closure and release portions of plosives tend to be represented by sequences of discrete units. Our work provides a reference for the phonetic properties of discrete units discovered by HuBERT, facilitating analyses of many speech applications based on this model.", "source": "semantic_scholar", "source_id": "fc5c506887b53d0e132cb6a4fb239c92cd5c9830", "year": 2022}, "round-A_JuniorNano_output": {"reasoning": "The abstract analyzes discrete units produced by a self-supervised model (HuBERT) and studies their phonetic properties, aligning with the discrete audio tokens criteria, and the English title/abstract are provided; thus it meets inclusion criteria for token-based discrete representations, though it is more analytical than proposing new token designs.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract analyzes discrete units produced by a self-supervised model (HuBERT) and studies their phonetic properties, aligning with the discrete audio tokens criteria, and the English title/abstract are provided; thus it meets inclusion criteria for token-based discrete representations, though it is more analytical than proposing new token designs.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study analyzes discrete units generated via self-supervised learning (HuBERT) applied to English speech, focusing on phonetic properties of these discrete tokens, which aligns with the inclusion criteria of using discrete audio tokens derived from self-supervised representations and analysis of their design and effects in downstream tasks; the abstract provides enough information about token generation and analysis, and the study is written in English with an English abstract, meeting the evaluation criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study analyzes discrete units generated via self-supervised learning (HuBERT) applied to English speech, focusing on phonetic properties of these discrete tokens, which aligns with the inclusion criteria of using discrete audio tokens derived from self-supervised representations and analysis of their design and effects in downstream tasks; the abstract provides enough information about token generation and analysis, and the study is written in English with an English abstract, meeting the evaluation criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "polyak2021speech", "query_title": "{Speech Resynthesis from Discrete Disentangled Self-Supervised Representations}", "normalized_title": "speech resynthesis from discrete disentangled self supervised representations", "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.", "source": "arxiv", "source_id": "2104.00355v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "abstract": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.", "metadata": {"arxiv_id": "2104.00355v3", "title": "Speech Resynthesis from Discrete Disentangled Self-Supervised Representations", "summary": "We propose using self-supervised discrete representations for the task of speech resynthesis. To generate disentangled representation, we separately extract low-bitrate representations for speech content, prosodic information, and speaker identity. This allows to synthesize speech in a controllable manner. We analyze various state-of-the-art, self-supervised representation learning methods and shed light on the advantages of each method while considering reconstruction quality and disentanglement properties. Specifically, we evaluate the F0 reconstruction, speaker identification performance (for both resynthesis and voice conversion), recordings' intelligibility, and overall quality using subjective human evaluation. Lastly, we demonstrate how these representations can be used for an ultra-lightweight speech codec. Using the obtained representations, we can get to a rate of 365 bits per second while providing better speech quality than the baseline methods. Audio samples can be found under the following link: speechbot.github.io/resynthesis.", "authors": ["Adam Polyak", "Yossi Adi", "Jade Copet", "Eugene Kharitonov", "Kushal Lakhotia", "Wei-Ning Hsu", "Abdelrahman Mohamed", "Emmanuel Dupoux"], "published": "2021-04-01T09:20:33Z", "updated": "2021-07-27T14:27:27Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2104.00355v3", "landing_url": "https://arxiv.org/abs/2104.00355v3", "doi": "https://doi.org/10.48550/arXiv.2104.00355"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on using discrete self-supervised representations as tokens for speech resynthesis and discusses their use in controllable synthesis, evaluation of token-level properties and a lightweight codec, aligning with the discrete audio token criteria; although the abstract doesn't detail the tokenizer/codec architecture, the core concept is token-based discrete representations for downstream generation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item centers on using discrete self-supervised representations as tokens for speech resynthesis and discusses their use in controllable synthesis, evaluation of token-level properties and a lightweight codec, aligning with the discrete audio token criteria; although the abstract doesn't detail the tokenizer/codec architecture, the core concept is token-based discrete representations for downstream generation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes using self-supervised discrete representations to resynthesize speech, extracting disentangled discrete tokens for speech content, prosody, and speaker identity, thus fitting the definition of using discrete audio tokens derived from self-supervised learning methods. It reports clear design details, evaluation on quality and intelligibility, and discusses bitrate and codec aspects, aligning with inclusion criteria regarding discrete token generation, usage in audio generation, and comprehensive evaluation. The abstract is in English and provides sufficient methodological details to support reproducibility, and the study focuses on generating discrete tokens for speech resynthesis rather than continuous representations or traditional codecs. Therefore, it meets all inclusion and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes using self-supervised discrete representations to resynthesize speech, extracting disentangled discrete tokens for speech content, prosody, and speaker identity, thus fitting the definition of using discrete audio tokens derived from self-supervised learning methods. It reports clear design details, evaluation on quality and intelligibility, and discusses bitrate and codec aspects, aligning with inclusion criteria regarding discrete token generation, usage in audio generation, and comprehensive evaluation. The abstract is in English and provides sufficient methodological details to support reproducibility, and the study focuses on generating discrete tokens for speech resynthesis rather than continuous representations or traditional codecs. Therefore, it meets all inclusion and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "du2025codecfake", "query_title": "{CodecFake-Omni: A Large-Scale Codec-based Deepfake Speech Dataset}", "normalized_title": "codecfake omni a large scale codec based deepfake speech dataset", "title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset", "abstract": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.", "source": "arxiv", "source_id": "2501.08238", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset", "abstract": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.", "metadata": {"arxiv_id": "2501.08238", "title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset", "summary": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.", "authors": ["Xuanjun Chen", "Jiawei Du", "Haibin Wu", "Lin Zhang", "I-Ming Lin", "I-Hsiang Chiu", "Wenze Ren", "Yuan Tseng", "Yu Tsao", "Jyh-Shing Roger Jang", "Hung-yi Lee"], "published": "2025-01-14T16:26:14Z", "updated": "2025-03-17T22:22:05Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2501.08238v2", "landing_url": "https://arxiv.org/abs/2501.08238v2", "doi": "https://doi.org/10.48550/arXiv.2501.08238"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on a CodecFake dataset and taxonomy of neural audio codecs, not on discrete audio tokens as primary representations or tokenization details necessary for replication, so it does not meet the token-centric inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on a CodecFake dataset and taxonomy of neural audio codecs, not on discrete audio tokens as primary representations or tokenization details necessary for replication, so it does not meet the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on a large-scale dataset and taxonomy for detecting neural audio codec-based deepfake speech. Although it centers on neural audio codecs, the paper emphasizes detection of synthetic speech rather than the design, generation, or use of discrete audio tokens for modeling or generation tasks. It does not discuss the specifics of discrete token vocabularies, quantization schemes, or token-based representation details essential for inclusion, nor does it address systematic analysis of token designs or downstream tasks beyond detection. Thus, it lacks a core focus on discrete token sequence representation as required by the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on a large-scale dataset and taxonomy for detecting neural audio codec-based deepfake speech. Although it centers on neural audio codecs, the paper emphasizes detection of synthetic speech rather than the design, generation, or use of discrete audio tokens for modeling or generation tasks. It does not discuss the specifics of discrete token vocabularies, quantization schemes, or token-based representation details essential for inclusion, nor does it address systematic analysis of token designs or downstream tasks beyond detection. Thus, it lacks a core focus on discrete token sequence representation as required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "touvron2023llama", "query_title": "{Llama 2: Open foundation and fine-tuned chat models}", "normalized_title": "llama 2 open foundation and fine tuned chat models", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "source": "arxiv", "source_id": "2307.09288v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "metadata": {"arxiv_id": "2307.09288v2", "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models", "summary": "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.", "authors": ["Hugo Touvron", "Louis Martin", "Kevin Stone", "Peter Albert", "Amjad Almahairi", "Yasmine Babaei", "Nikolay Bashlykov", "Soumya Batra", "Prajjwal Bhargava", "Shruti Bhosale", "Dan Bikel", "Lukas Blecher", "Cristian Canton Ferrer", "Moya Chen", "Guillem Cucurull", "David Esiobu", "Jude Fernandes", "Jeremy Fu", "Wenyin Fu", "Brian Fuller", "Cynthia Gao", "Vedanuj Goswami", "Naman Goyal", "Anthony Hartshorn", "Saghar Hosseini", "Rui Hou", "Hakan Inan", "Marcin Kardas", "Viktor Kerkez", "Madian Khabsa", "Isabel Kloumann", "Artem Korenev", "Punit Singh Koura", "Marie-Anne Lachaux", "Thibaut Lavril", "Jenya Lee", "Diana Liskovich", "Yinghai Lu", "Yuning Mao", "Xavier Martinet", "Todor Mihaylov", "Pushkar Mishra", "Igor Molybog", "Yixin Nie", "Andrew Poulton", "Jeremy Reizenstein", "Rashi Rungta", "Kalyan Saladi", "Alan Schelten", "Ruan Silva", "Eric Michael Smith", "Ranjan Subramanian", "Xiaoqing Ellen Tan", "Binh Tang", "Ross Taylor", "Adina Williams", "Jian Xiang Kuan", "Puxin Xu", "Zheng Yan", "Iliyan Zarov", "Yuchen Zhang", "Angela Fan", "Melanie Kambadur", "Sharan Narang", "Aurelien Rodriguez", "Robert Stojnic", "Sergey Edunov", "Thomas Scialom"], "published": "2023-07-18T14:31:57Z", "updated": "2023-07-19T17:08:59Z", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2307.09288v2", "landing_url": "https://arxiv.org/abs/2307.09288v2", "doi": "https://doi.org/10.48550/arXiv.2307.09288"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on Llama 2 language models and chat fine-tuning, not on discrete audio tokens; it does not meet inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on Llama 2 language models and chat fine-tuning, not on discrete audio tokens; it does not meet inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on the development of Llama 2, a language model collection for dialogue use, without involving discrete audio tokenization, neural audio codecs, or discretization of continuous audio signals into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens and their associated modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on the development of Llama 2, a language model collection for dialogue use, without involving discrete audio tokenization, neural audio codecs, or discretization of continuous audio signals into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens and their associated modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "defossez2024moshi", "query_title": "{Moshi: a speech-text foundation model for real-time dialogue}", "normalized_title": "moshi a speech text foundation model for real time dialogue", "title": "Moshi: a speech-text foundation model for real-time dialogue", "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.", "source": "arxiv", "source_id": "2410.00037v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Moshi: a speech-text foundation model for real-time dialogue", "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.", "metadata": {"arxiv_id": "2410.00037v2", "title": "Moshi: a speech-text foundation model for real-time dialogue", "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.", "authors": ["Alexandre Défossez", "Laurent Mazaré", "Manu Orsini", "Amélie Royer", "Patrick Pérez", "Hervé Jégou", "Edouard Grave", "Neil Zeghidour"], "published": "2024-09-17T17:55:39Z", "updated": "2024-10-02T09:11:45Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2410.00037v2", "landing_url": "https://arxiv.org/abs/2410.00037v2", "doi": "https://doi.org/10.48550/arXiv.2410.00037"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes using discrete audio tokens generated from a neural audio codec’s residual quantizer and token-based modeling (including audio tokens and time-aligned text tokens) for real-time, full-duplex speech, aligning with the discrete audio token criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract describes using discrete audio tokens generated from a neural audio codec’s residual quantizer and token-based modeling (including audio tokens and time-aligned text tokens) for real-time, full-duplex speech, aligning with the discrete audio token criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The abstract explicitly states that Moshi uses a neural audio codec with residual quantization to generate speech tokens, modeling speech-to-speech generation with discrete token streams representing both the user's and the model's speech. This aligns with the inclusion criteria of using discrete audio tokens from a neural audio codec for downstream generation tasks such as dialogue and TTS, with details on quantization and token hierarchy provided, fulfilling the requirements for an eligible discrete audio token study. The model is also applied to real-time spoken large language modeling, which is a downstream application involving generation and understanding based on discrete tokens, supported by English title and abstract with sufficient method insights for reproducibility assessment. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The abstract explicitly states that Moshi uses a neural audio codec with residual quantization to generate speech tokens, modeling speech-to-speech generation with discrete token streams representing both the user's and the model's speech. This aligns with the inclusion criteria of using discrete audio tokens from a neural audio codec for downstream generation tasks such as dialogue and TTS, with details on quantization and token hierarchy provided, fulfilling the requirements for an eligible discrete audio token study. The model is also applied to real-time spoken large language modeling, which is a downstream application involving generation and understanding based on discrete tokens, supported by English title and abstract with sufficient method insights for reproducibility assessment. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "kharitonov2021text", "query_title": "\"Text-Free Prosody-Aware Generative Spoken Language Modeling\"", "normalized_title": "text free prosody aware generative spoken language modeling", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "source": "arxiv", "source_id": "2109.03264v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "metadata": {"arxiv_id": "2109.03264v2", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "authors": ["Eugene Kharitonov", "Ann Lee", "Adam Polyak", "Yossi Adi", "Jade Copet", "Kushal Lakhotia", "Tu-Anh Nguyen", "Morgane Rivière", "Abdelrahman Mohamed", "Emmanuel Dupoux", "Wei-Ning Hsu"], "published": "2021-09-07T18:03:21Z", "updated": "2022-05-10T16:41:20Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2109.03264v2", "landing_url": "https://arxiv.org/abs/2109.03264v2", "doi": "https://doi.org/10.48550/arXiv.2109.03264"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents a text-free, prosody-aware generative spoken language model that uses 'discovered unit streams' (token-like discrete units) alongside prosodic features to model and generate speech, which aligns with discrete audio token representations and downstream generation tasks though exact token design details are not fully specified in the abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper presents a text-free, prosody-aware generative spoken language model that uses 'discovered unit streams' (token-like discrete units) alongside prosodic features to model and generate speech, which aligns with discrete audio token representations and downstream generation tasks though exact token design details are not fully specified in the abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study presents a prosody-aware generative spoken language model that uses discovered unit streams—which are discrete and phone-like—as tokens for modeling and generating speech, aligning with the discrete audio tokens topic; it also provides codes, models, and detailed methodology supportive of reproducibility, and focuses on generative tasks rather than pure classification, satisfying all inclusion criteria without triggering any exclusions.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a prosody-aware generative spoken language model that uses discovered unit streams—which are discrete and phone-like—as tokens for modeling and generating speech, aligning with the discrete audio tokens topic; it also provides codes, models, and detailed methodology supportive of reproducibility, and focuses on generative tasks rather than pure classification, satisfying all inclusion criteria without triggering any exclusions.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}, {"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "metadata": {"arxiv_id": "2109.03264v2", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "authors": ["Eugene Kharitonov", "Ann Lee", "Adam Polyak", "Yossi Adi", "Jade Copet", "Kushal Lakhotia", "Tu-Anh Nguyen", "Morgane Rivière", "Abdelrahman Mohamed", "Emmanuel Dupoux", "Wei-Ning Hsu"], "published": "2021-09-07T18:03:21Z", "updated": "2022-05-10T16:41:20Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2109.03264v2", "landing_url": "https://arxiv.org/abs/2109.03264v2", "doi": "https://doi.org/10.48550/arXiv.2109.03264"}, "round-A_JuniorNano_output": {"reasoning": "The abstract uses 'discovered unit' streams as discrete input to a text-free generative speech model, implying discrete audio tokens suitable for token-based modeling, which aligns with the inclusion criteria, though explicit tokenizer/codec details are not given in the abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract uses 'discovered unit' streams as discrete input to a text-free generative speech model, implying discrete audio tokens suitable for token-based modeling, which aligns with the inclusion criteria, though explicit tokenizer/codec details are not given in the abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study presents a generative spoken language model utilizing discovered phone-like units as tokens, which are discrete audio units derived from speech for language modeling; it details the representation methods and provides links to code and models, fulfilling the inclusion criteria related to discrete audio tokens and sufficient English technical details, while none of the exclusion criteria apply.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a generative spoken language model utilizing discovered phone-like units as tokens, which are discrete audio units derived from speech for language modeling; it details the representation methods and provides links to code and models, fulfilling the inclusion criteria related to discrete audio tokens and sufficient English technical details, while none of the exclusion criteria apply.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "nguyen2023generative", "query_title": "{Generative spoken dialogue language modeling}", "normalized_title": "generative spoken dialogue language modeling", "title": "Generative Spoken Dialogue Language Modeling", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn-taking compared to a text-based cascaded model.", "source": "arxiv", "source_id": "2203.16502v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Generative Spoken Dialogue Language Modeling", "abstract": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn-taking compared to a text-based cascaded model.", "metadata": {"arxiv_id": "2203.16502v2", "title": "Generative Spoken Dialogue Language Modeling", "summary": "We introduce dGSLM, the first \"textless\" model able to generate audio samples of naturalistic spoken dialogues. It uses recent work on unsupervised spoken unit discovery coupled with a dual-tower transformer architecture with cross-attention trained on 2000 hours of two-channel raw conversational audio (Fisher dataset) without any text or labels. We show that our model is able to generate speech, laughter and other paralinguistic signals in the two channels simultaneously and reproduces more naturalistic and fluid turn-taking compared to a text-based cascaded model.", "authors": ["Tu Anh Nguyen", "Eugene Kharitonov", "Jade Copet", "Yossi Adi", "Wei-Ning Hsu", "Ali Elkahky", "Paden Tomasello", "Robin Algayres", "Benoit Sagot", "Abdelrahman Mohamed", "Emmanuel Dupoux"], "published": "2022-03-30T17:39:45Z", "updated": "2022-11-22T19:20:29Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2203.16502v2", "landing_url": "https://arxiv.org/abs/2203.16502v2", "doi": "https://doi.org/10.48550/arXiv.2203.16502"}, "round-A_JuniorNano_output": {"reasoning": "Although the work involves textless speech generation and mentions unsupervised spoken unit discovery, the abstract does not explicitly define or use discrete audio tokens, tokenizers, or codebooks as core representations or for downstream modeling, so it likely does not meet the discrete audio token inclusion criteria and may fall under exclusion.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "Although the work involves textless speech generation and mentions unsupervised spoken unit discovery, the abstract does not explicitly define or use discrete audio tokens, tokenizers, or codebooks as core representations or for downstream modeling, so it likely does not meet the discrete audio token inclusion criteria and may fall under exclusion.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The study introduces dGSLM, a 'textless' spoken dialogue generation model trained on raw audio without text or labels, which implies the use of discrete audio tokenization likely via unsupervised spoken unit discovery; however, the abstract lacks explicit details on the discrete tokenization process, such as tokenizer architecture, vocabulary, quantization methods, or codebooks, making it difficult to fully verify if discrete audio tokens as defined in the inclusion criteria are utilized and documented thoroughly for reproducibility. Nevertheless, the use of unsupervised spoken unit discovery strongly suggests the generation of discrete tokens usable for sequence modeling, which aligns closely with the discrete audio tokens topic. Given the uncertainty but overall relevance, the evaluation leans towards inclusion but with some caution due to incomplete tokenization details.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study introduces dGSLM, a 'textless' spoken dialogue generation model trained on raw audio without text or labels, which implies the use of discrete audio tokenization likely via unsupervised spoken unit discovery; however, the abstract lacks explicit details on the discrete tokenization process, such as tokenizer architecture, vocabulary, quantization methods, or codebooks, making it difficult to fully verify if discrete audio tokens as defined in the inclusion criteria are utilized and documented thoroughly for reproducibility. Nevertheless, the use of unsupervised spoken unit discovery strongly suggests the generation of discrete tokens usable for sequence modeling, which aligns closely with the discrete audio tokens topic. Given the uncertainty but overall relevance, the evaluation leans towards inclusion but with some caution due to incomplete tokenization details.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the paper is highly relevant: the abstract explicitly states a “textless” model trained with unsupervised spoken unit discovery to generate dialogue audio, which strongly suggests the use of discrete audio units/tokens (JuniorMini). However, the abstract does not explicitly describe the discrete-token generation details (tokenizer/codec architecture, quantization or codebook/vocabulary, bitrate or sampling of tokens) required by the inclusion criteria, and JuniorNano flagged this lack of explicit tokenization details as a reason to exclude. Because inclusion requires clear evidence that discrete audio tokens and their generation/setting are central and documented, and the title/abstract alone are ambiguous on those required methodological details, I judge this item as uncertain for inclusion pending inspection of the full paper/methods.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper is highly relevant: the abstract explicitly states a “textless” model trained with unsupervised spoken unit discovery to generate dialogue audio, which strongly suggests the use of discrete audio units/tokens (JuniorMini). However, the abstract does not explicitly describe the discrete-token generation details (tokenizer/codec architecture, quantization or codebook/vocabulary, bitrate or sampling of tokens) required by the inclusion criteria, and JuniorNano flagged this lack of explicit tokenization details as a reason to exclude. Because inclusion requires clear evidence that discrete audio tokens and their generation/setting are central and documented, and the title/abstract alone are ambiguous on those required methodological details, I judge this item as uncertain for inclusion pending inspection of the full paper/methods.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "popuri2022enhanced", "query_title": "{Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation}", "normalized_title": "enhanced direct speech to speech translation using self supervised pre training and data augmentation", "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation", "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "source": "arxiv", "source_id": "2204.02967v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation", "abstract": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "metadata": {"arxiv_id": "2204.02967v3", "title": "Enhanced Direct Speech-to-Speech Translation Using Self-supervised Pre-training and Data Augmentation", "summary": "Direct speech-to-speech translation (S2ST) models suffer from data scarcity issues as there exists little parallel S2ST data, compared to the amount of data available for conventional cascaded systems that consist of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS) synthesis. In this work, we explore self-supervised pre-training with unlabeled speech data and data augmentation to tackle this issue. We take advantage of a recently proposed speech-to-unit translation (S2UT) framework that encodes target speech into discrete representations, and transfer pre-training and efficient partial finetuning techniques that work well for speech-to-text translation (S2T) to the S2UT domain by studying both speech encoder and discrete unit decoder pre-training. Our experiments on Spanish-English translation show that self-supervised pre-training consistently improves model performance compared with multitask learning with an average 6.6-12.1 BLEU gain, and it can be further combined with data augmentation techniques that apply MT to create weakly supervised training data. Audio samples are available at: https://facebookresearch.github.io/speech_translation/enhanced_direct_s2st_units/index.html .", "authors": ["Sravya Popuri", "Peng-Jen Chen", "Changhan Wang", "Juan Pino", "Yossi Adi", "Jiatao Gu", "Wei-Ning Hsu", "Ann Lee"], "published": "2022-04-06T17:59:22Z", "updated": "2022-09-13T17:00:55Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2204.02967v3", "landing_url": "https://arxiv.org/abs/2204.02967v3", "doi": "https://doi.org/10.48550/arXiv.2204.02967"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly encodes target speech into discrete representations via a speech-to-unit framework and studies pre-training/fine-tuning in that discrete-token setup, aligning with the Discrete Audio Tokens inclusion criteria and not falling under the exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly encodes target speech into discrete representations via a speech-to-unit framework and studies pre-training/fine-tuning in that discrete-token setup, aligning with the Discrete Audio Tokens inclusion criteria and not falling under the exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study employs a speech-to-unit translation framework that encodes target speech into discrete representations (tokens), leveraging self-supervised pre-training and data augmentation for speech-to-speech translation, which aligns with the inclusion criteria requiring discrete audio tokens as core representations. The work provides experimental details and evaluation metrics (BLEU scores), and is presented with English title and abstract. However, the abstract does not explicitly detail the tokenizer architecture, quantization methods, or token vocabulary/codebook settings, which are necessary for clear reproducibility and confirmation of discrete token design, leaving some ambiguity.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study employs a speech-to-unit translation framework that encodes target speech into discrete representations (tokens), leveraging self-supervised pre-training and data augmentation for speech-to-speech translation, which aligns with the inclusion criteria requiring discrete audio tokens as core representations. The work provides experimental details and evaluation metrics (BLEU scores), and is presented with English title and abstract. However, the abstract does not explicitly detail the tokenizer architecture, quantization methods, or token vocabulary/codebook settings, which are necessary for clear reproducibility and confirmation of discrete token design, leaving some ambiguity.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the paper explicitly uses a speech-to-unit (S2UT) framework that encodes target speech into discrete representations and applies those units in downstream S2ST modeling, which aligns with the inclusion group of self-supervised-derived discrete audio tokens; JuniorNano recommended inclusion while JuniorMini flagged ambiguity because the abstract does not detail the tokenizer/quantization/codebook specifics required for full reproducibility. Given the clear use of discrete units as core representations and empirical downstream evaluation (BLEU) in a S2UT S2ST setting, this likely fits the “Discrete Audio Tokens” topic, though the abstract leaves some implementation details unspecified (which should be checked in the full paper). Score: 4 — better to include (reasonable evidence of discrete token use, but minor uncertainty due to missing tokenizer/quantization details in the abstract).", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper explicitly uses a speech-to-unit (S2UT) framework that encodes target speech into discrete representations and applies those units in downstream S2ST modeling, which aligns with the inclusion group of self-supervised-derived discrete audio tokens; JuniorNano recommended inclusion while JuniorMini flagged ambiguity because the abstract does not detail the tokenizer/quantization/codebook specifics required for full reproducibility. Given the clear use of discrete units as core representations and empirical downstream evaluation (BLEU) in a S2UT S2ST setting, this likely fits the “Discrete Audio Tokens” topic, though the abstract leaves some implementation details unspecified (which should be checked in the full paper). Score: 4 — better to include (reasonable evidence of discrete token use, but minor uncertainty due to missing tokenizer/quantization details in the abstract).", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "inaguma2022unity", "query_title": "{UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units}", "normalized_title": "unity two pass direct speech to speech translation with discrete units", "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units", "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.", "source": "arxiv", "source_id": "2212.08055v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units", "abstract": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.", "metadata": {"arxiv_id": "2212.08055v2", "title": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units", "summary": "Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.", "authors": ["Hirofumi Inaguma", "Sravya Popuri", "Ilia Kulikov", "Peng-Jen Chen", "Changhan Wang", "Yu-An Chung", "Yun Tang", "Ann Lee", "Shinji Watanabe", "Juan Pino"], "published": "2022-12-15T18:58:28Z", "updated": "2023-05-26T16:07:54Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2212.08055v2", "landing_url": "https://arxiv.org/abs/2212.08055v2", "doi": "https://doi.org/10.48550/arXiv.2212.08055"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly uses discrete acoustic units as core representation and discusses tokenization/codec design, aligning with discrete audio tokens criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete acoustic units as core representation and discusses tokenization/codec design, aligning with discrete audio tokens criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study clearly discusses the use of discrete acoustic units as tokens generated in a two-pass speech-to-speech translation model, detailing improvements in token prediction and decoding speed, which fits the core definition of discrete audio tokens and meets the inclusion criteria; sufficient technical details about token prediction and the discrete nature of tokens are provided in English, and the study focuses on token design impact on performance rather than continuous audio representation or traditional compression methods, hence no exclusion criteria are triggered.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study clearly discusses the use of discrete acoustic units as tokens generated in a two-pass speech-to-speech translation model, detailing improvements in token prediction and decoding speed, which fits the core definition of discrete audio tokens and meets the inclusion criteria; sufficient technical details about token prediction and the discrete nature of tokens are provided in English, and the study focuses on token design impact on performance rather than continuous audio representation or traditional compression methods, hence no exclusion criteria are triggered.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "chang2022speechprompt", "query_title": "{SpeechPrompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks}", "normalized_title": "speechprompt an exploration of prompt tuning on generative spoken language model for speech processing tasks", "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks", "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.", "source": "arxiv", "source_id": "2203.16773v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks", "abstract": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.", "metadata": {"arxiv_id": "2203.16773v3", "title": "SpeechPrompt: An Exploration of Prompt Tuning on Generative Spoken Language Model for Speech Processing Tasks", "summary": "Speech representations learned from Self-supervised learning (SSL) models can benefit various speech processing tasks. However, utilizing SSL representations usually requires fine-tuning the pre-trained models or designing task-specific downstream models and loss functions, causing much memory usage and human labor. Recently, prompting in Natural Language Processing (NLP) has been found to be an efficient technique to leverage pre-trained language models (LMs). Specifically, prompt tuning optimizes a limited number of task-specific parameters with a fixed pre-trained model; as a result, only a small set of parameters is needed to be stored for each task. Prompt tuning improves computation and memory efficiency by leveraging the pre-trained LM's prediction ability. Nevertheless, such a paradigm is little studied in the speech community. We report in this paper the first exploration of the prompt tuning paradigm for speech processing tasks based on Generative Spoken Language Model (GSLM). Experiment results show that the prompt tuning technique achieves competitive performance in speech classification tasks with fewer trainable parameters than fine-tuning specialized downstream models. We further study the technique in challenging sequence generation tasks. Prompt tuning also demonstrates its potential, while the limitation and possible research directions are discussed in this paper. The source code is available on https://github.com/ga642381/SpeechPrompt.", "authors": ["Kai-Wei Chang", "Wei-Cheng Tseng", "Shang-Wen Li", "Hung-yi Lee"], "published": "2022-03-31T03:26:55Z", "updated": "2022-07-10T19:30:18Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2203.16773v3", "landing_url": "https://arxiv.org/abs/2203.16773v3", "doi": "https://doi.org/10.48550/arXiv.2203.16773"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on prompt tuning for Generative Spoken Language Models and SSL representations, with no emphasis on discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the discrete audio token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on prompt tuning for Generative Spoken Language Models and SSL representations, with no emphasis on discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the discrete audio token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study explores prompt tuning on a Generative Spoken Language Model for speech processing tasks and discusses using self-supervised learning speech representations; however, it does not explicitly describe the generation or use of discrete audio tokens derived from discretization processes like vector quantization or neural audio codecs, nor does it detail token vocabularies or quantization methods that meet the inclusion criteria focused on discrete token representations. Therefore, it appears focused on continuous speech representations and downstream prompt tuning without meeting the strict discrete token core representation requirement specified.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study explores prompt tuning on a Generative Spoken Language Model for speech processing tasks and discusses using self-supervised learning speech representations; however, it does not explicitly describe the generation or use of discrete audio tokens derived from discretization processes like vector quantization or neural audio codecs, nor does it detail token vocabularies or quantization methods that meet the inclusion criteria focused on discrete token representations. Therefore, it appears focused on continuous speech representations and downstream prompt tuning without meeting the strict discrete token core representation requirement specified.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "chang2023speechprompt", "query_title": "{Speechprompt v2: Prompt tuning for speech classification tasks}", "normalized_title": "speechprompt v2 prompt tuning for speech classification tasks", "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks", "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.", "source": "arxiv", "source_id": "2303.00733v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks", "abstract": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.", "metadata": {"arxiv_id": "2303.00733v1", "title": "SpeechPrompt v2: Prompt Tuning for Speech Classification Tasks", "summary": "Prompt tuning is a technology that tunes a small set of parameters to steer a pre-trained language model (LM) to directly generate the output for downstream tasks. Recently, prompt tuning has demonstrated its storage and computation efficiency in both natural language processing (NLP) and speech processing fields. These advantages have also revealed prompt tuning as a candidate approach to serving pre-trained LM for multiple tasks in a unified manner. For speech processing, SpeechPrompt shows its high parameter efficiency and competitive performance on a few speech classification tasks. However, whether SpeechPrompt is capable of serving a large number of tasks is unanswered. In this work, we propose SpeechPrompt v2, a prompt tuning framework capable of performing a wide variety of speech classification tasks, covering multiple languages and prosody-related tasks. The experiment result shows that SpeechPrompt v2 achieves performance on par with prior works with less than 0.15M trainable parameters in a unified framework.", "authors": ["Kai-Wei Chang", "Yu-Kai Wang", "Hua Shen", "Iu-thing Kang", "Wei-Cheng Tseng", "Shang-Wen Li", "Hung-yi Lee"], "published": "2023-03-01T18:47:41Z", "updated": "2023-03-01T18:47:41Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2303.00733v1", "landing_url": "https://arxiv.org/abs/2303.00733v1", "doi": "https://doi.org/10.48550/arXiv.2303.00733"}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on prompt tuning for speech classification and does not involve discrete audio tokens, tokenization, or token-based modeling, which are central to the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work focuses on prompt tuning for speech classification and does not involve discrete audio tokens, tokenization, or token-based modeling, which are central to the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article focuses on prompt tuning for speech classification tasks without mentioning the use or design of discrete audio tokens, their generation process, or related codebooks/vocabularies; it centers on discriminative classification rather than generative or cross-modal modeling involving discrete audio tokens, so it does not meet the inclusion criteria and falls under exclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article focuses on prompt tuning for speech classification tasks without mentioning the use or design of discrete audio tokens, their generation process, or related codebooks/vocabularies; it centers on discriminative classification rather than generative or cross-modal modeling involving discrete audio tokens, so it does not meet the inclusion criteria and falls under exclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "hsu2023exploration", "query_title": "{An exploration of in-context learning for speech language model}", "normalized_title": "an exploration of in context learning for speech language model", "title": "An Exploration of In-Context Learning for Speech Language Model", "abstract": "Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner.", "source": "semantic_scholar", "source_id": "d628f3c65e0c8c3024d9ff0678d44383fc9fe27c", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "An Exploration of In-Context Learning for Speech Language Model", "abstract": "Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner.", "metadata": {"arxiv_id": "semantic_scholar:d628f3c65e0c8c3024d9ff0678d44383fc9fe27c", "title": "An Exploration of In-Context Learning for Speech Language Model", "summary": "Ever since the development of GPT-3 in the natural language processing (NLP) field, in-context learning (ICL) has played an essential role in utilizing large language models (LLMs). By presenting the LM utterance-label demonstrations at the input, the LM can accomplish few-shot learning without relying on gradient descent or requiring explicit modification of its parameters. This enables the LM to perform various downstream tasks in a black-box manner. Despite the success of ICL in NLP, little work is exploring the possibility of ICL in speech processing. This study is the first work exploring ICL for speech classification tasks with textless speech LM. We first show that the current speech LM lacks the ICL capability. We then perform warmup training on the speech LM, equipping the LM with demonstration learning capability. This paper explores and proposes the first speech LM capable of performing unseen classification tasks in an ICL manner.", "source": "semantic_scholar", "source_id": "d628f3c65e0c8c3024d9ff0678d44383fc9fe27c", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "The item discusses in-context learning for speech language models and does not address discrete audio tokens, tokenizers, or token-level representations, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses in-context learning for speech language models and does not address discrete audio tokens, tokenizers, or token-level representations, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study explores in-context learning for speech language models focusing on speech classification tasks; however, the abstract does not mention the use of discrete audio tokens produced by neural audio codecs or self-supervised learning models, nor does it detail any tokenization, codebook, or quantization methods. The focus is on in-context learning capabilities of speech LMs rather than on discretization of audio waveforms into tokens used for generative or cross-modal tasks as required by the inclusion criteria. Thus, the paper does not satisfy the core criteria regarding discrete audio token representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study explores in-context learning for speech language models focusing on speech classification tasks; however, the abstract does not mention the use of discrete audio tokens produced by neural audio codecs or self-supervised learning models, nor does it detail any tokenization, codebook, or quantization methods. The focus is on in-context learning capabilities of speech LMs rather than on discretization of audio waveforms into tokens used for generative or cross-modal tasks as required by the inclusion criteria. Thus, the paper does not satisfy the core criteria regarding discrete audio token representation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kuan2023towards", "query_title": "{Towards General-Purpose Text-Instruction-Guided Voice Conversion}", "normalized_title": "towards general purpose text instruction guided voice conversion", "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion", "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.", "source": "arxiv", "source_id": "2309.14324v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion", "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.", "metadata": {"arxiv_id": "2309.14324v2", "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion", "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.", "authors": ["Chun-Yi Kuan", "Chen An Li", "Tsu-Yuan Hsu", "Tse-Yang Lin", "Ho-Lam Chung", "Kai-Wei Chang", "Shuo-yiin Chang", "Hung-yi Lee"], "published": "2023-09-25T17:52:09Z", "updated": "2024-01-16T13:53:56Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2309.14324v2", "landing_url": "https://arxiv.org/abs/2309.14324v2", "doi": "https://doi.org/10.48550/arXiv.2309.14324"}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly describes a VC model that uses a sequence of discrete codes (tokens) produced by a neural codec language model, i.e., discrete audio tokens, and applies token-based modeling for speech generation, satisfying the token-source and downstream modeling criteria; it is not a pure continuous representation and provides token-based methodology and application, meeting inclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly describes a VC model that uses a sequence of discrete codes (tokens) produced by a neural codec language model, i.e., discrete audio tokens, and applies token-based modeling for speech generation, satisfying the token-source and downstream modeling criteria; it is not a pure continuous representation and provides token-based methodology and application, meeting inclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a neural codec language model for text-instruction-guided voice conversion that processes sequences of discrete codes representing converted speech, which fits the core inclusion criterion of using discrete audio tokens from a neural codec as the main representation; however, the abstract lacks explicit details on the tokenizer/codec architecture, quantization methods, codebook vocabulary, or bitrate, limiting the ability to verify reproducibility and fully assess token design details as required. Therefore, it meets key thematic criteria but fails to fully meet the evaluation criteria due to insufficient methodological detail in the abstract.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study proposes a neural codec language model for text-instruction-guided voice conversion that processes sequences of discrete codes representing converted speech, which fits the core inclusion criterion of using discrete audio tokens from a neural codec as the main representation; however, the abstract lacks explicit details on the tokenizer/codec architecture, quantization methods, codebook vocabulary, or bitrate, limiting the ability to verify reproducibility and fully assess token design details as required. Therefore, it meets key thematic criteria but fails to fully meet the evaluation criteria due to insufficient methodological detail in the abstract.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "JuniorNano rated this a 5, noting the abstract explicitly states a neural codec language model that processes a sequence of discrete codes for voice conversion; JuniorMini rated it a 3, pointing out the abstract lacks concrete tokenizer/codec details (quantization, codebook, bitrate, time downsampling) required by the inclusion criteria for reproducibility. My assessment: the paper clearly targets discrete audio tokens (neural codec LM) and applies them to downstream generation (voice conversion), so it meets the thematic inclusion, but the provided abstract does not describe the discrete-token generation process or codec/tokenizer settings required by the English assessability criterion—so based on the abstract alone it is uncertain whether the paper satisfies the requirement to clearly describe token generation and reproducibility-relevant details.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "JuniorNano rated this a 5, noting the abstract explicitly states a neural codec language model that processes a sequence of discrete codes for voice conversion; JuniorMini rated it a 3, pointing out the abstract lacks concrete tokenizer/codec details (quantization, codebook, bitrate, time downsampling) required by the inclusion criteria for reproducibility. My assessment: the paper clearly targets discrete audio tokens (neural codec LM) and applies them to downstream generation (voice conversion), so it meets the thematic inclusion, but the provided abstract does not describe the discrete-token generation process or codec/tokenizer settings required by the English assessability criterion—so based on the abstract alone it is uncertain whether the paper satisfies the requirement to clearly describe token generation and reproducibility-relevant details.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "zeng2024glm", "query_title": "{G{LM}-4-voice: Towards intelligent and human-like end-to-end spoken chatbot}", "normalized_title": "glm 4 voice towards intelligent and human like end to end spoken chatbot", "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.", "source": "arxiv", "source_id": "2412.02612", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.", "metadata": {"arxiv_id": "2412.02612", "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot", "summary": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.", "authors": ["Aohan Zeng", "Zhengxiao Du", "Mingdao Liu", "Kedong Wang", "Shengmin Jiang", "Lei Zhao", "Yuxiao Dong", "Jie Tang"], "published": "2024-12-03T17:41:24Z", "updated": "2024-12-03T17:41:24Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2412.02612v1", "landing_url": "https://arxiv.org/abs/2412.02612v1", "doi": "https://doi.org/10.48550/arXiv.2412.02612"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes a discrete audio tokenization approach (vector-quantized bottleneck, a 175 bps single-codebook speech tokenizer) used for end-to-end spoken chatbot development, including token-level modeling and pretraining with speech tokens, which aligns with the topic of Discrete Audio Tokens and covers token generation/architecture details in English; it therefore meets inclusion criteria and does not appear to trigger any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract describes a discrete audio tokenization approach (vector-quantized bottleneck, a 175 bps single-codebook speech tokenizer) used for end-to-end spoken chatbot development, including token-level modeling and pretraining with speech tokens, which aligns with the topic of Discrete Audio Tokens and covers token generation/architecture details in English; it therefore meets inclusion criteria and does not appear to trigger any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a neural audio codec using vector quantization for speech tokenization at an ultra-low bitrate, and leverages these discrete tokens for modeling speech with a large language model, aligning well with the theme of discrete audio tokens used for generation and understanding; detailed descriptions of the tokenizer architecture, bitrate, and token quantization are provided, satisfying inclusion criteria, and the focus is on discrete tokens rather than continuous audio representations, making it relevant and includable.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper introduces a neural audio codec using vector quantization for speech tokenization at an ultra-low bitrate, and leverages these discrete tokens for modeling speech with a large language model, aligning well with the theme of discrete audio tokens used for generation and understanding; detailed descriptions of the tokenizer architecture, bitrate, and token quantization are provided, satisfying inclusion criteria, and the focus is on discrete tokens rather than continuous audio representations, making it relevant and includable.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "huang2023dynamic", "query_title": "{Dynamic-{SUPERB}: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech}", "normalized_title": "dynamic superb towards a dynamic collaborative and comprehensive instruction tuning benchmark for speech", "title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech", "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.", "source": "arxiv", "source_id": "2309.09510v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech", "abstract": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.", "metadata": {"arxiv_id": "2309.09510v2", "title": "Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech", "summary": "Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.", "authors": ["Chien-yu Huang", "Ke-Han Lu", "Shih-Heng Wang", "Chi-Yuan Hsiao", "Chun-Yi Kuan", "Haibin Wu", "Siddhant Arora", "Kai-Wei Chang", "Jiatong Shi", "Yifan Peng", "Roshan Sharma", "Shinji Watanabe", "Bhiksha Ramakrishnan", "Shady Shehata", "Hung-yi Lee"], "published": "2023-09-18T06:43:30Z", "updated": "2024-03-22T15:25:04Z", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2309.09510v2", "landing_url": "https://arxiv.org/abs/2309.09510v2", "doi": "https://doi.org/10.48550/arXiv.2309.09510"}, "round-A_JuniorNano_output": {"reasoning": "The item describes Dynamic-SUPERB, a benchmark for instruction-tuning in speech tasks, which does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes Dynamic-SUPERB, a benchmark for instruction-tuning in speech tasks, which does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a benchmark for instruction tuning in speech tasks but does not discuss or describe any discrete audio tokenization process, neural audio codecs, vector quantization, or other methods to convert continuous audio into discrete token sequences; thus, it lacks the core focus on discrete audio tokens required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a benchmark for instruction tuning in speech tasks but does not discuss or describe any discrete audio tokenization process, neural audio codecs, vector quantization, or other methods to convert continuous audio into discrete token sequences; thus, it lacks the core focus on discrete audio tokens required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "huang2024dynamic", "query_title": "{Dynamic-superb phase-2: A collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks}", "normalized_title": "dynamic superb phase 2 a collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks", "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.", "source": "arxiv", "source_id": "2411.05361v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.", "metadata": {"arxiv_id": "2411.05361v2", "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks", "summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.", "authors": ["Chien-yu Huang", "Wei-Chih Chen", "Shu-wen Yang", "Andy T. Liu", "Chen-An Li", "Yu-Xiang Lin", "Wei-Cheng Tseng", "Anuj Diwan", "Yi-Jen Shih", "Jiatong Shi", "William Chen", "Chih-Kai Yang", "Wenze Ren", "Xuanjun Chen", "Chi-Yuan Hsiao", "Puyuan Peng", "Shih-Heng Wang", "Chun-Yi Kuan", "Ke-Han Lu", "Kai-Wei Chang", "Fabian Ritter-Gutierrez", "Kuan-Po Huang", "Siddhant Arora", "You-Kuan Lin", "Ming To Chuang", "Eunjung Yeo", "Kalvin Chang", "Chung-Ming Chien", "Kwanghee Choi", "Jun-You Wang", "Cheng-Hsiu Hsieh", "Yi-Cheng Lin", "Chee-En Yu", "I-Hsiang Chiu", "Heitor R. Guimarães", "Jionghao Han", "Tzu-Quan Lin", "Tzu-Yuan Lin", "Homu Chang", "Ting-Wu Chang", "Chun Wei Chen", "Shou-Jen Chen", "Yu-Hua Chen", "Hsi-Chun Cheng", "Kunal Dhawan", "Jia-Lin Fang", "Shi-Xin Fang", "Kuan-Yu Fang Chiang", "Chi An Fu", "Hsien-Fu Hsiao", "Ching Yu Hsu", "Shao-Syuan Huang", "Lee Chen Wei", "Hsi-Che Lin", "Hsuan-Hao Lin", "Hsuan-Ting Lin", "Jian-Ren Lin", "Ting-Chun Liu", "Li-Chun Lu", "Tsung-Min Pai", "Ankita Pasad", "Shih-Yun Shan Kuan", "Suwon Shon", "Yuxun Tang", "Yun-Shao Tsai", "Jui-Chiang Wei", "Tzu-Chieh Wei", "Chengxi Wu", "Dien-Ruei Wu", "Chao-Han Huck Yang", "Chieh-Chi Yang", "Jia Qi Yip", "Shao-Xiang Yuan", "Vahid Noroozi", "Zhehuai Chen", "Haibin Wu", "Karen Livescu", "David Harwath", "Shinji Watanabe", "Hung-yi Lee"], "published": "2024-11-08T06:33:22Z", "updated": "2025-06-09T16:36:12Z", "categories": ["cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.05361v2", "landing_url": "https://arxiv.org/abs/2411.05361v2", "doi": "https://doi.org/10.48550/arXiv.2411.05361"}, "round-A_JuniorNano_output": {"reasoning": "The item is about Dynamic-SUPERB Phase-2, a benchmark for evaluating spoken language models; it does not discuss discrete audio tokens, tokenization, neural audio codecs, or token-based representations, and therefore does not meet the topic criteria for 'Discrete Audio Tokens' and aligns with exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is about Dynamic-SUPERB Phase-2, a benchmark for evaluating spoken language models; it does not discuss discrete audio tokens, tokenization, neural audio codecs, or token-based representations, and therefore does not meet the topic criteria for 'Discrete Audio Tokens' and aligns with exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study Dynamic-SUPERB Phase-2 focuses on an evaluation benchmark for spoken language models and includes a wide array of tasks across speech, music, and environmental audio but does not specifically discuss the generation or use of discrete audio tokens through neural audio codecs or self-supervised discrete unit extraction, nor does it detail tokenization processes like codebook design or quantization schemes. It primarily centers on benchmarking models rather than proposing or analyzing discrete token schemes or their influence on generation or cross-modal tasks, which are core to the inclusion criteria. Consequently, it does not meet the key inclusion criteria related to discrete audio tokens and tokenization details essential for inclusion in this topic.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study Dynamic-SUPERB Phase-2 focuses on an evaluation benchmark for spoken language models and includes a wide array of tasks across speech, music, and environmental audio but does not specifically discuss the generation or use of discrete audio tokens through neural audio codecs or self-supervised discrete unit extraction, nor does it detail tokenization processes like codebook design or quantization schemes. It primarily centers on benchmarking models rather than proposing or analyzing discrete token schemes or their influence on generation or cross-modal tasks, which are core to the inclusion criteria. Consequently, it does not meet the key inclusion criteria related to discrete audio tokens and tokenization details essential for inclusion in this topic.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "xu2025qwen2", "query_title": "{Qwen2. 5-omni technical report}", "normalized_title": "qwen2 5 omni technical report", "title": "Qwen2.5-Omni Technical Report", "abstract": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", "source": "arxiv", "source_id": "2503.20215", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Qwen2.5-Omni Technical Report", "abstract": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", "metadata": {"arxiv_id": "2503.20215", "title": "Qwen2.5-Omni Technical Report", "summary": "In this report, we present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. To synchronize the timestamps of video inputs with audio, we organize the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE(Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose \\textbf{Thinker-Talker} architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni is comparable with the similarly sized Qwen2.5-VL and outperforms Qwen2-Audio. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni's performance in end-to-end speech instruction following is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperforms most existing streaming and non-streaming alternatives in robustness and naturalness.", "authors": ["Jin Xu", "Zhifang Guo", "Jinzheng He", "Hangrui Hu", "Ting He", "Shuai Bai", "Keqin Chen", "Jialin Wang", "Yang Fan", "Kai Dang", "Bin Zhang", "Xiong Wang", "Yunfei Chu", "Junyang Lin"], "published": "2025-03-26T04:17:55Z", "updated": "2025-03-26T04:17:55Z", "categories": ["cs.CL", "cs.CV", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2503.20215v1", "landing_url": "https://arxiv.org/abs/2503.20215v1", "doi": "https://doi.org/10.48550/arXiv.2503.20215"}, "round-A_JuniorNano_output": {"reasoning": "The abstract mentions 'audio tokens' in the context of a streaming multimodal model, but it does not provide explicit details on discrete token generation, tokenizers/codebooks, or a sole focus on discrete audio tokens as core representation—thus it does not clearly satisfy the core criteria for 'Discrete Audio Tokens' despite some token usage.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The abstract mentions 'audio tokens' in the context of a streaming multimodal model, but it does not provide explicit details on discrete token generation, tokenizers/codebooks, or a sole focus on discrete audio tokens as core representation—thus it does not clearly satisfy the core criteria for 'Discrete Audio Tokens' despite some token usage.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a multimodal model handling various modalities including text, images, audio, and video, with a focus on simultaneous generation of text and natural speech responses using an architecture called Thinker-Talker. However, it lacks clear and detailed description of discrete audio tokenization, such as neural audio codec-based vector quantization or discrete token vocabulary design, which is essential for meeting the inclusion criteria. The study focuses more on system-level architecture and multimodal integration rather than specifically on discrete audio tokens or their design/analysis, hence it does not meet the core inclusion criteria related to discrete audio tokens.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes a multimodal model handling various modalities including text, images, audio, and video, with a focus on simultaneous generation of text and natural speech responses using an architecture called Thinker-Talker. However, it lacks clear and detailed description of discrete audio tokenization, such as neural audio codec-based vector quantization or discrete token vocabulary design, which is essential for meeting the inclusion criteria. The study focuses more on system-level architecture and multimodal integration rather than specifically on discrete audio tokens or their design/analysis, hence it does not meet the core inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "liu2023pre", "query_title": "{Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing}", "normalized_title": "pre train prompt and predict a systematic survey of prompting methods in natural language processing", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "abstract": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.", "source": "arxiv", "source_id": "2107.13586v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "abstract": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.", "metadata": {"arxiv_id": "2107.13586v1", "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing", "summary": "This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub \"prompt-based learning\". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.", "authors": ["Pengfei Liu", "Weizhe Yuan", "Jinlan Fu", "Zhengbao Jiang", "Hiroaki Hayashi", "Graham Neubig"], "published": "2021-07-28T18:09:46Z", "updated": "2021-07-28T18:09:46Z", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2107.13586v1", "landing_url": "https://arxiv.org/abs/2107.13586v1", "doi": "https://doi.org/10.48550/arXiv.2107.13586"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses prompt-based learning in NLP and does not involve discrete audio tokens or tokenization/codec design; it fails to meet the 'Discrete Audio Tokens' topic criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses prompt-based learning in NLP and does not involve discrete audio tokens or tokenization/codec design; it fails to meet the 'Discrete Audio Tokens' topic criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper is a systematic survey on prompt-based learning methods in natural language processing, focusing on textual token manipulation and language model adaptation, with no mention of discrete audio tokens, neural audio codecs, or any audio-related discrete tokenization or generation. It does not meet the inclusion criteria related to discrete audio tokens, their design, evaluation, or application in downstream audio tasks, and thus should be excluded.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper is a systematic survey on prompt-based learning methods in natural language processing, focusing on textual token manipulation and language model adaptation, with no mention of discrete audio tokens, neural audio codecs, or any audio-related discrete tokenization or generation. It does not meet the inclusion criteria related to discrete audio tokens, their design, evaluation, or application in downstream audio tasks, and thus should be excluded.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "lester2021power", "query_title": "{The power of scale for parameter-efficient prompt tuning}", "normalized_title": "the power of scale for parameter efficient prompt tuning", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.", "source": "arxiv", "source_id": "2104.08691v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "abstract": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.", "metadata": {"arxiv_id": "2104.08691v2", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "summary": "In this work, we explore \"prompt tuning\", a simple yet effective mechanism for learning \"soft prompts\" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \"closes the gap\" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \"prefix tuning\" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.", "authors": ["Brian Lester", "Rami Al-Rfou", "Noah Constant"], "published": "2021-04-18T03:19:26Z", "updated": "2021-09-02T17:34:41Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2104.08691v2", "landing_url": "https://arxiv.org/abs/2104.08691v2", "doi": "https://doi.org/10.48550/arXiv.2104.08691"}, "round-A_JuniorNano_output": {"reasoning": "The item is about parameter-efficient prompt tuning for frozen language models and does not involve discrete audio tokens, tokenizers, or any analysis of discrete audio token design or usage; thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is about parameter-efficient prompt tuning for frozen language models and does not involve discrete audio tokens, tokenizers, or any analysis of discrete audio token design or usage; thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on prompt tuning for large language models and does not discuss discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are central to the inclusion criteria. It also lacks details on token design choices related to audio tasks like TTS or speech generation, making it irrelevant to the discrete audio tokens topic.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on prompt tuning for large language models and does not discuss discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are central to the inclusion criteria. It also lacks details on token design choices related to audio tasks like TTS or speech generation, making it irrelevant to the discrete audio tokens topic.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "chang2023exploring", "query_title": "{Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study}", "normalized_title": "exploring speech recognition translation and understanding with discrete speech units a comparative study", "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study", "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.", "source": "arxiv", "source_id": "2309.15800v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study", "abstract": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.", "metadata": {"arxiv_id": "2309.15800v1", "title": "Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study", "summary": "Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.", "authors": ["Xuankai Chang", "Brian Yan", "Kwanghee Choi", "Jeeweon Jung", "Yichen Lu", "Soumi Maiti", "Roshan Sharma", "Jiatong Shi", "Jinchuan Tian", "Shinji Watanabe", "Yuya Fujita", "Takashi Maekaku", "Pengcheng Guo", "Yao-Fei Cheng", "Pavel Denisov", "Kohei Saijo", "Hsiu-Hsuan Wang"], "published": "2023-09-27T17:21:13Z", "updated": "2023-09-27T17:21:13Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2309.15800v1", "landing_url": "https://arxiv.org/abs/2309.15800v1", "doi": "https://doi.org/10.48550/arXiv.2309.15800"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly investigates discrete audio tokens derived from self-supervised representations, applies them to multiple downstream tasks (ASR, speech translation, SLU), is presented with an English title and abstract, and discusses token generation/representation with downstream evaluation, aligning with all inclusion criteria and no exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly investigates discrete audio tokens derived from self-supervised representations, applies them to multiple downstream tasks (ASR, speech translation, SLU), is presented with an English title and abstract, and discusses token generation/representation with downstream evaluation, aligning with all inclusion criteria and no exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on discrete speech units derived from self-supervised learning representations, which fit the inclusion criteria of discrete audio tokens generated by neural audio encoding or self-supervised models. It systematically explores their use in downstream applications like speech recognition, translation, and understanding, with detailed experiments and the intention to release configurations and models, indicating transparency and reproducibility. The abstract suggests the study addresses the design and comparative effectiveness of discrete tokens, aligning well with the inclusion criteria involving systematical analysis and downstream task use. There is no indication of exclusion criteria such as purely continuous audio modeling or lack of token design details.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on discrete speech units derived from self-supervised learning representations, which fit the inclusion criteria of discrete audio tokens generated by neural audio encoding or self-supervised models. It systematically explores their use in downstream applications like speech recognition, translation, and understanding, with detailed experiments and the intention to release configurations and models, indicating transparency and reproducibility. The abstract suggests the study addresses the design and comparative effectiveness of discrete tokens, aligning well with the inclusion criteria involving systematical analysis and downstream task use. There is no indication of exclusion criteria such as purely continuous audio modeling or lack of token design details.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2023selm", "query_title": "{{SELM}: Speech Enhancement Using Discrete Tokens and Language Models}", "normalized_title": "selm speech enhancement using discrete tokens and language models", "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models", "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.", "source": "arxiv", "source_id": "2312.09747v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models", "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.", "metadata": {"arxiv_id": "2312.09747v2", "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models", "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.", "authors": ["Ziqian Wang", "Xinfa Zhu", "Zihan Zhang", "YuanJun Lv", "Ning Jiang", "Guoqing Zhao", "Lei Xie"], "published": "2023-12-15T12:36:05Z", "updated": "2024-01-07T09:02:52Z", "categories": ["eess.AS", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2312.09747v2", "landing_url": "https://arxiv.org/abs/2312.09747v2", "doi": "https://doi.org/10.48550/arXiv.2312.09747"}, "round-A_JuniorNano_output": {"reasoning": "Abstract explicitly uses discrete audio tokens produced by SSL models and a k-means tokenizer, models them with language models, and reconstructs audio, aligning with discrete-token audio representations and token-based downstream processing.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Abstract explicitly uses discrete audio tokens produced by SSL models and a k-means tokenizer, models them with language models, and reconstructs audio, aligning with discrete-token audio representations and token-based downstream processing.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study clearly uses discrete tokens derived from continuous waveform signals through self-supervised learning models and a k-means tokenizer, aligning with the inclusion criteria of using discrete audio tokens for speech enhancement. It also discusses the token generation process and application of language models on these tokens, fitting the core theme of discrete token-based audio modeling and enhancement, with available English abstract and detailed methodology. Therefore, it meets all inclusion and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study clearly uses discrete tokens derived from continuous waveform signals through self-supervised learning models and a k-means tokenizer, aligning with the inclusion criteria of using discrete audio tokens for speech enhancement. It also discusses the token generation process and application of language models on these tokens, fitting the core theme of discrete token-based audio modeling and enhancement, with available English abstract and detailed methodology. Therefore, it meets all inclusion and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2024codec", "query_title": "{{Codec-SUPERB}: An In-Depth Analysis of Sound Codec Models}", "normalized_title": "codec superb an in depth analysis of sound codec models", "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.", "source": "arxiv", "source_id": "2402.13071v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "abstract": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.", "metadata": {"arxiv_id": "2402.13071v3", "title": "Codec-SUPERB: An In-Depth Analysis of Sound Codec Models", "summary": "The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.", "authors": ["Haibin Wu", "Ho-Lam Chung", "Yi-Cheng Lin", "Yuan-Kuei Wu", "Xuanjun Chen", "Yu-Chi Pai", "Hsiu-Hsuan Wang", "Kai-Wei Chang", "Alexander H. Liu", "Hung-yi Lee"], "published": "2024-02-20T15:13:38Z", "updated": "2024-09-18T12:02:47Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2402.13071v3", "landing_url": "https://arxiv.org/abs/2402.13071v3", "doi": "https://doi.org/10.48550/arXiv.2402.13071"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on general sound codecs and benchmarking (Codec-SUPERB) rather than on discrete audio token design/tokenizers or token-based downstream tasks, so it does not meet the discrete-token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper centers on general sound codecs and benchmarking (Codec-SUPERB) rather than on discrete audio token design/tokenizers or token-based downstream tasks, so it does not meet the discrete-token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper describes Codec-SUPERB, a benchmark ecosystem evaluating neural audio codec models for sound which likely involve discrete audio tokenization, considering codebooks and signal-level metrics relevant to tokenization and audio modeling, but it does not explicitly clarify the discrete token generation process, token vocabulary, quantization details, or specific usage of discrete tokens for downstream tasks as required by the inclusion criteria; hence, it partially matches the domain but lacks full confirmation of key discrete token technical details and their usage, leading to uncertainty about full inclusion.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The paper describes Codec-SUPERB, a benchmark ecosystem evaluating neural audio codec models for sound which likely involve discrete audio tokenization, considering codebooks and signal-level metrics relevant to tokenization and audio modeling, but it does not explicitly clarify the discrete token generation process, token vocabulary, quantization details, or specific usage of discrete tokens for downstream tasks as required by the inclusion criteria; hence, it partially matches the domain but lacks full confirmation of key discrete token technical details and their usage, leading to uncertainty about full inclusion.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Junior reviewers: JuniorNano recommended exclusion (1) noting the paper focuses on general codec benchmarking rather than discrete-token design; JuniorMini was uncertain (3) because the work concerns neural audio codecs but lacks explicit tokenization details. My assessment: although the abstract mentions codecs' role as tokenizers and introduces a benchmark for codec models, it does not provide the required explicit English details about discrete token generation (e.g., tokenizer/codec architecture, quantization/codebook settings, vocabulary, time downsampling or bitrate) nor does it state use of discrete token sequences for downstream sequence modeling or generation—requirements in the inclusion criteria. Therefore it fails the criterion that the paper must clearly define and evaluate discrete audio tokens as core representations. One-sentence explanation: The abstract lacks explicit, reproducible details on discrete token generation and use, so this paper should be excluded (better to exclude).", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommended exclusion (1) noting the paper focuses on general codec benchmarking rather than discrete-token design; JuniorMini was uncertain (3) because the work concerns neural audio codecs but lacks explicit tokenization details. My assessment: although the abstract mentions codecs' role as tokenizers and introduces a benchmark for codec models, it does not provide the required explicit English details about discrete token generation (e.g., tokenizer/codec architecture, quantization/codebook settings, vocabulary, time downsampling or bitrate) nor does it state use of discrete token sequences for downstream sequence modeling or generation—requirements in the inclusion criteria. Therefore it fails the criterion that the paper must clearly define and evaluate discrete audio tokens as core representations. One-sentence explanation: The abstract lacks explicit, reproducible details on discrete token generation and use, so this paper should be excluded (better to exclude).", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "zhang2023dub", "query_title": "\"{DUB}: Discrete Unit Back-translation for Speech Translation\"", "normalized_title": "dub discrete unit back translation for speech translation", "title": "DUB: Discrete Unit Back-translation for Speech Translation", "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST. Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation (DUB) to answer two questions: (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://github.com/0nutation/DUB.", "source": "arxiv", "source_id": "2305.11411v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "DUB: Discrete Unit Back-translation for Speech Translation", "abstract": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST. Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation (DUB) to answer two questions: (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://github.com/0nutation/DUB.", "metadata": {"arxiv_id": "2305.11411v1", "title": "DUB: Discrete Unit Back-translation for Speech Translation", "summary": "How can speech-to-text translation (ST) perform as well as machine translation (MT)? The key point is to bridge the modality gap between speech and text so that useful MT techniques can be applied to ST. Recently, the approach of representing speech with unsupervised discrete units yields a new way to ease the modality problem. This motivates us to propose Discrete Unit Back-translation (DUB) to answer two questions: (1) Is it better to represent speech with discrete units than with continuous features in direct ST? (2) How much benefit can useful MT techniques bring to ST? With DUB, the back-translation technique can successfully be applied on direct ST and obtains an average boost of 5.5 BLEU on MuST-C En-De/Fr/Es. In the low-resource language scenario, our method achieves comparable performance to existing methods that rely on large-scale external data. Code and models are available at https://github.com/0nutation/DUB.", "authors": ["Dong Zhang", "Rong Ye", "Tom Ko", "Mingxuan Wang", "Yaqian Zhou"], "published": "2023-05-19T03:48:16Z", "updated": "2023-05-19T03:48:16Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.11411v1", "landing_url": "https://arxiv.org/abs/2305.11411v1", "doi": "https://doi.org/10.48550/arXiv.2305.11411"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on discrete audio tokens (discrete unit representations for speech) used to improve speech translation via back-translation, presenting concrete tokens/codec-based generation and empirical gains, which fully aligns with the inclusion criteria and contains no exclusion indicators.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens (discrete unit representations for speech) used to improve speech translation via back-translation, presenting concrete tokens/codec-based generation and empirical gains, which fully aligns with the inclusion criteria and contains no exclusion indicators.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper clearly discusses the use of discrete units derived from speech for speech translation, which aligns with the theme of discrete audio tokens representing continuous audio as token sequences for modeling. It mentions discrete unit back-translation employing these discrete units and provides code/models, indicating reproducibility and design transparency. The study uses discrete speech units as core inputs, thus meeting the inclusion criteria. There is no indication it focuses on continuous features only or excludes discrete token analysis. Therefore, it fits well within the inclusion criteria and excludes are not applicable.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper clearly discusses the use of discrete units derived from speech for speech translation, which aligns with the theme of discrete audio tokens representing continuous audio as token sequences for modeling. It mentions discrete unit back-translation employing these discrete units and provides code/models, indicating reproducibility and design transparency. The study uses discrete speech units as core inputs, thus meeting the inclusion criteria. There is no indication it focuses on continuous features only or excludes discrete token analysis. Therefore, it fits well within the inclusion criteria and excludes are not applicable.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2024toksing", "query_title": "{TokSing: Singing Voice Synthesis based on Discrete Tokens}", "normalized_title": "toksing singing voice synthesis based on discrete tokens", "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens", "abstract": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.", "source": "arxiv", "source_id": "2406.08416v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "TokSing: Singing Voice Synthesis based on Discrete Tokens", "abstract": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.", "metadata": {"arxiv_id": "2406.08416v2", "title": "TokSing: Singing Voice Synthesis based on Discrete Tokens", "summary": "Recent advancements in speech synthesis witness significant benefits by leveraging discrete tokens extracted from self-supervised learning (SSL) models. Discrete tokens offer higher storage efficiency and greater operability in intermediate representations compared to traditional continuous Mel spectrograms. However, when it comes to singing voice synthesis(SVS), achieving higher levels of melody expression poses a great challenge for utilizing discrete tokens. In this paper, we introduce TokSing, a discrete-based SVS system equipped with a token formulator that offers flexible token blendings. We observe a melody degradation during discretization, prompting us to integrate a melody signal with the discrete token and incorporate a specially-designed melody enhancement strategy in the musical encoder. Extensive experiments demonstrate that our TokSing achieves better performance against the Mel spectrogram baselines while offering advantages in intermediate representation space cost and convergence speed.", "authors": ["Yuning Wu", "Chunlei zhang", "Jiatong Shi", "Yuxun Tang", "Shan Yang", "Qin Jin"], "published": "2024-06-12T16:59:24Z", "updated": "2024-06-20T16:46:14Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.08416v2", "landing_url": "https://arxiv.org/abs/2406.08416v2", "doi": "https://doi.org/10.48550/arXiv.2406.08416"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on discrete audio tokens (from SSL) as the core representation for singing voice synthesis and discusses token design/formation and downstream SVS performance, aligning with the inclusion criteria and not falling into exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper centers on discrete audio tokens (from SSL) as the core representation for singing voice synthesis and discusses token design/formation and downstream SVS performance, aligning with the inclusion criteria and not falling into exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper addresses singing voice synthesis using discrete tokens derived from self-supervised learning models, which aligns with the inclusion criteria of employing discrete audio tokens generated by neural audio encoding or SSL models. The abstract indicates the use of discrete tokens as core representations for generating singing voice and discusses improvements on token design, performance, and efficiency, fitting the theme of discrete audio tokens and their application to downstream generation tasks. The English abstract clearly describes the token generation and usage in the system, supporting reproducibility and clarity. There is no sign of exclusion criteria such as focusing solely on continuous representations or lacking details on tokenization. Therefore, the study meets all inclusion and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper addresses singing voice synthesis using discrete tokens derived from self-supervised learning models, which aligns with the inclusion criteria of employing discrete audio tokens generated by neural audio encoding or SSL models. The abstract indicates the use of discrete tokens as core representations for generating singing voice and discusses improvements on token design, performance, and efficiency, fitting the theme of discrete audio tokens and their application to downstream generation tasks. The English abstract clearly describes the token generation and usage in the system, supporting reproducibility and clarity. There is no sign of exclusion criteria such as focusing solely on continuous representations or lacking details on tokenization. Therefore, the study meets all inclusion and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "yip2024towards", "query_title": "{Towards audio codec-based speech separation}", "normalized_title": "towards audio codec based speech separation", "title": "Towards Audio Codec-based Speech Separation", "abstract": "Recent improvements in neural audio codec (NAC) models have generated interest in adopting pre-trained codecs for a variety of speech processing applications to take advantage of the efficiencies gained from high compression, but these have yet been applied to the speech separation (SS) task. SS can benefit from high compression because the compute required for traditional SS models makes them impractical for many edge computing use cases. However, SS is a waveform-masking task where compression tends to introduce distortions that severely impact performance. Here we propose a novel task of Audio Codec-based SS, where SS is performed within the embedding space of a NAC, and propose a new model, Codecformer, to address this task. At inference, Codecformer achieves a 52x reduction in MAC while producing separation performance comparable to a cloud deployment of Sepformer. This method charts a new direction for performing efficient SS in practical scenarios.", "source": "arxiv", "source_id": "2406.12434v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards Audio Codec-based Speech Separation", "abstract": "Recent improvements in neural audio codec (NAC) models have generated interest in adopting pre-trained codecs for a variety of speech processing applications to take advantage of the efficiencies gained from high compression, but these have yet been applied to the speech separation (SS) task. SS can benefit from high compression because the compute required for traditional SS models makes them impractical for many edge computing use cases. However, SS is a waveform-masking task where compression tends to introduce distortions that severely impact performance. Here we propose a novel task of Audio Codec-based SS, where SS is performed within the embedding space of a NAC, and propose a new model, Codecformer, to address this task. At inference, Codecformer achieves a 52x reduction in MAC while producing separation performance comparable to a cloud deployment of Sepformer. This method charts a new direction for performing efficient SS in practical scenarios.", "metadata": {"arxiv_id": "2406.12434v2", "title": "Towards Audio Codec-based Speech Separation", "summary": "Recent improvements in neural audio codec (NAC) models have generated interest in adopting pre-trained codecs for a variety of speech processing applications to take advantage of the efficiencies gained from high compression, but these have yet been applied to the speech separation (SS) task. SS can benefit from high compression because the compute required for traditional SS models makes them impractical for many edge computing use cases. However, SS is a waveform-masking task where compression tends to introduce distortions that severely impact performance. Here we propose a novel task of Audio Codec-based SS, where SS is performed within the embedding space of a NAC, and propose a new model, Codecformer, to address this task. At inference, Codecformer achieves a 52x reduction in MAC while producing separation performance comparable to a cloud deployment of Sepformer. This method charts a new direction for performing efficient SS in practical scenarios.", "authors": ["Jia Qi Yip", "Shengkui Zhao", "Dianwen Ng", "Eng Siong Chng", "Bin Ma"], "published": "2024-06-18T09:29:24Z", "updated": "2024-07-05T09:50:06Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.12434v2", "landing_url": "https://arxiv.org/abs/2406.12434v2", "doi": "https://doi.org/10.48550/arXiv.2406.12434"}, "round-A_JuniorNano_output": {"reasoning": "The paper targets audio codec-based speech separation and reports efficiency gains, but it does not discuss discrete audio tokens, tokenizers, or token-based representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper targets audio codec-based speech separation and reports efficiency gains, but it does not discuss discrete audio tokens, tokenizers, or token-based representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses using neural audio codecs for speech separation by processing in the embedding space of a neural audio codec. However, there is no explicit mention of discretizing audio into token sequences or providing details on tokenization mechanisms such as vector quantization or codebook configurations, which is crucial for meeting the inclusion criteria focused on discrete audio tokens. The emphasis is on compression efficiency and model performance rather than on discrete token design or their direct use in downstream generative or multimodal tasks.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper discusses using neural audio codecs for speech separation by processing in the embedding space of a neural audio codec. However, there is no explicit mention of discretizing audio into token sequences or providing details on tokenization mechanisms such as vector quantization or codebook configurations, which is crucial for meeting the inclusion criteria focused on discrete audio tokens. The emphasis is on compression efficiency and model performance rather than on discrete token design or their direct use in downstream generative or multimodal tasks.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "chung2021w2v", "query_title": "{{w2v-BERT}: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training}", "normalized_title": "w2v bert combining contrastive learning and masked language modeling for self supervised speech pre training", "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training", "abstract": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.", "source": "arxiv", "source_id": "2108.06209v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training", "abstract": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.", "metadata": {"arxiv_id": "2108.06209v2", "title": "W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training", "summary": "Motivated by the success of masked language modeling~(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks~(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\\% to~10\\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec~2.0 by more than~30\\% relatively.", "authors": ["Yu-An Chung", "Yu Zhang", "Wei Han", "Chung-Cheng Chiu", "James Qin", "Ruoming Pang", "Yonghui Wu"], "published": "2021-08-07T06:29:36Z", "updated": "2021-09-13T18:29:12Z", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2108.06209v2", "landing_url": "https://arxiv.org/abs/2108.06209v2", "doi": "https://doi.org/10.48550/arXiv.2108.06209"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly uses discretized speech tokens (via a contrastive discretization module) and applies masked language modeling on those discrete tokens, aligning with the discrete audio token criteria and enabling downstream modeling, with English title/abstract provided, thus meeting inclusion and not fitting any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly uses discretized speech tokens (via a contrastive discretization module) and applies masked language modeling on those discrete tokens, aligning with the discrete audio token criteria and enabling downstream modeling, with English title/abstract provided, thus meeting inclusion and not fitting any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes w2v-BERT, a self-supervised speech pre-training model combining contrastive learning and masked language modeling to discretize continuous speech into finite token sets, which aligns with the inclusion criteria focusing on discrete audio tokens derived from self-supervised learning models; however, the abstract does not detail the exact discrete token generation process (e.g., tokenizer architecture, quantization specifics, vocabulary size) or provide explicit design choices and evaluations related to token characteristics which are required to clearly assess reproducibility and token definition as discrete audio tokens.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The paper proposes w2v-BERT, a self-supervised speech pre-training model combining contrastive learning and masked language modeling to discretize continuous speech into finite token sets, which aligns with the inclusion criteria focusing on discrete audio tokens derived from self-supervised learning models; however, the abstract does not detail the exact discrete token generation process (e.g., tokenizer architecture, quantization specifics, vocabulary size) or provide explicit design choices and evaluations related to token characteristics which are required to clearly assess reproducibility and token definition as discrete audio tokens.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "The abstract explicitly states the model discretizes continuous speech into a finite set of discriminative speech tokens via a contrastive discretization module and uses those tokens for masked language modeling, which matches the inclusion group of self-supervised-derived discrete audio units; both junior reviewers noted this alignment (one recommended inclusion, the other flagged that the abstract lacks low-level tokenizer/codebook specifics). While the abstract does not enumerate tokenizer/quantization details (vocab/codebook settings) needed for full reproducibility, the core contribution clearly centers on discrete tokens from a self-supervised pipeline rather than purely continuous representations, so it should be included for this collection but with the noted caveat to check the full paper for detailed tokenization settings.", "evaluation": 4}, "round-B_SeniorLead_reasoning": "The abstract explicitly states the model discretizes continuous speech into a finite set of discriminative speech tokens via a contrastive discretization module and uses those tokens for masked language modeling, which matches the inclusion group of self-supervised-derived discrete audio units; both junior reviewers noted this alignment (one recommended inclusion, the other flagged that the abstract lacks low-level tokenizer/codebook specifics). While the abstract does not enumerate tokenizer/quantization details (vocab/codebook settings) needed for full reproducibility, the core contribution clearly centers on discrete tokens from a self-supervised pipeline rather than purely continuous representations, so it should be included for this collection but with the noted caveat to check the full paper for detailed tokenization settings.", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "hsu2021hubert", "query_title": "{{HuBERT}: Self-supervised speech representation learning by masked prediction of hidden units}", "normalized_title": "hubert self supervised speech representation learning by masked prediction of hidden units", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.", "source": "arxiv", "source_id": "2106.07447v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.", "metadata": {"arxiv_id": "2106.07447v1", "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units", "summary": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.", "authors": ["Wei-Ning Hsu", "Benjamin Bolte", "Yao-Hung Hubert Tsai", "Kushal Lakhotia", "Ruslan Salakhutdinov", "Abdelrahman Mohamed"], "published": "2021-06-14T14:14:28Z", "updated": "2021-06-14T14:14:28Z", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2106.07447v1", "landing_url": "https://arxiv.org/abs/2106.07447v1", "doi": "https://doi.org/10.48550/arXiv.2106.07447"}, "round-A_JuniorNano_output": {"reasoning": "HuBERT employs offline clustering to produce discrete target units (tokens) used in a BERT-like self-supervised loss, aligning with the discrete audio token criteria, and the English abstract supports evaluation of token generation/usage.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "HuBERT employs offline clustering to produce discrete target units (tokens) used in a BERT-like self-supervised loss, aligning with the discrete audio token criteria, and the English abstract supports evaluation of token generation/usage.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes HuBERT, a self-supervised speech representation learning model that uses offline clustering to generate discrete hidden units (tokens) from continuous speech input, which aligns with the topic of discrete audio tokens used for modeling; the abstract describes the token generation via clustering and their use as prediction targets, indicating a discrete tokenization process, fulfilling inclusion requirements on token source and modeling approach. Therefore, it fits the criteria of using discrete tokens derived from self-supervised learning for downstream modeling, with sufficient English information and explicit mention of token clusters for evaluation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes HuBERT, a self-supervised speech representation learning model that uses offline clustering to generate discrete hidden units (tokens) from continuous speech input, which aligns with the topic of discrete audio tokens used for modeling; the abstract describes the token generation via clustering and their use as prediction targets, indicating a discrete tokenization process, fulfilling inclusion requirements on token source and modeling approach. Therefore, it fits the criteria of using discrete tokens derived from self-supervised learning for downstream modeling, with sufficient English information and explicit mention of token clusters for evaluation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "chen2022wavlm", "query_title": "{{WavLM}: Large-scale self-supervised pre-training for full stack speech processing}", "normalized_title": "wavlm large scale self supervised pre training for full stack speech processing", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing", "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.", "source": "arxiv", "source_id": "2110.13900v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing", "abstract": "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.", "metadata": {"arxiv_id": "2110.13900v5", "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing", "summary": "Self-supervised learning (SSL) achieves great success in speech recognition, while limited exploration has been attempted for other speech processing tasks. As speech signal contains multi-faceted information including speaker identity, paralinguistics, spoken content, etc., learning universal representations for all speech tasks is challenging. To tackle the problem, we propose a new pre-trained model, WavLM, to solve full-stack downstream speech tasks. WavLM jointly learns masked speech prediction and denoising in pre-training. By this means, WavLM does not only keep the speech content modeling capability by the masked speech prediction, but also improves the potential to non-ASR tasks by the speech denoising. In addition, WavLM employs gated relative position bias for the Transformer structure to better capture the sequence ordering of input speech. We also scale up the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks. The code and pre-trained models are available at https://aka.ms/wavlm.", "authors": ["Sanyuan Chen", "Chengyi Wang", "Zhengyang Chen", "Yu Wu", "Shujie Liu", "Zhuo Chen", "Jinyu Li", "Naoyuki Kanda", "Takuya Yoshioka", "Xiong Xiao", "Jian Wu", "Long Zhou", "Shuo Ren", "Yanmin Qian", "Yao Qian", "Jian Wu", "Michael Zeng", "Xiangzhan Yu", "Furu Wei"], "published": "2021-10-26T17:55:19Z", "updated": "2022-06-17T05:44:33Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2110.13900v5", "landing_url": "https://arxiv.org/abs/2110.13900v5", "doi": "https://doi.org/10.1109/JSTSP.2022.3188113"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe WavLM pre-training for full-stack speech processing with masked prediction and denoising, but there is no discussion of discrete audio tokens, tokenizers, or neural codecs as core representations; therefore it does not meet the discrete-audio-token inclusion criteria and aligns with the exclusion criterion (continuous representations without tokenization).", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe WavLM pre-training for full-stack speech processing with masked prediction and denoising, but there is no discussion of discrete audio tokens, tokenizers, or neural codecs as core representations; therefore it does not meet the discrete-audio-token inclusion criteria and aligns with the exclusion criterion (continuous representations without tokenization).", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract discusses a self-supervised pre-trained model for speech processing that improves various downstream tasks but does not explicitly describe the use or generation of discrete audio tokens through discretization, vector quantization, or related tokenization mechanisms—nor does it detail token vocabularies or quantization procedures necessary to confirm it supports the inclusion criteria that focus on discrete audio tokens as core representations.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract discusses a self-supervised pre-trained model for speech processing that improves various downstream tasks but does not explicitly describe the use or generation of discrete audio tokens through discretization, vector quantization, or related tokenization mechanisms—nor does it detail token vocabularies or quantization procedures necessary to confirm it supports the inclusion criteria that focus on discrete audio tokens as core representations.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "baevski2020wav2vec", "query_title": "{wav2vec 2.0: A framework for self-supervised learning of speech representations}", "normalized_title": "wav2vec 2 0 a framework for self supervised learning of speech representations", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.", "source": "arxiv", "source_id": "2006.11477v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.", "metadata": {"arxiv_id": "2006.11477v3", "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations", "summary": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.", "authors": ["Alexei Baevski", "Henry Zhou", "Abdelrahman Mohamed", "Michael Auli"], "published": "2020-06-20T02:35:02Z", "updated": "2020-10-22T06:09:10Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2006.11477v3", "landing_url": "https://arxiv.org/abs/2006.11477v3", "doi": "https://doi.org/10.48550/arXiv.2006.11477"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses a vector-quantized latent space to produce discrete audio tokens as part of wav2vec 2.0, aligning with the discrete audio token criterion, though the work centers on self-supervised learning rather than a token-design framework.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper uses a vector-quantized latent space to produce discrete audio tokens as part of wav2vec 2.0, aligning with the discrete audio token criterion, though the work centers on self-supervised learning rather than a token-design framework.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study presents a self-supervised learning method for speech representations using wav2vec 2.0, which involves quantization of latent speech representations, effectively producing discrete tokens from continuous audio input for downstream modeling and speech recognition tasks. However, there is no explicit detailed information on the discrete token design, tokenizer or codec architecture, codebook settings, or the degree of discretization, which are necessary for confirming the use of discrete audio tokens per the inclusion criteria. Therefore, while the method involves discrete representations, the lack of clear, reproducible discrete token definitions and explanations in the abstract leads to uncertainty about whether it fully meets the topic requirements focused on discrete audio tokens for generative or multimodal tasks.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study presents a self-supervised learning method for speech representations using wav2vec 2.0, which involves quantization of latent speech representations, effectively producing discrete tokens from continuous audio input for downstream modeling and speech recognition tasks. However, there is no explicit detailed information on the discrete token design, tokenizer or codec architecture, codebook settings, or the degree of discretization, which are necessary for confirming the use of discrete audio tokens per the inclusion criteria. Therefore, while the method involves discrete representations, the lack of clear, reproducible discrete token definitions and explanations in the abstract leads to uncertainty about whether it fully meets the topic requirements focused on discrete audio tokens for generative or multimodal tasks.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Junior reviews: one rated 4 noting wav2vec 2.0 uses vector-quantized latent representations (discrete units), the other rated 3 citing the abstract's lack of tokenizer/codebook details and uncertainty about treating those quantized units as discrete audio tokens for downstream generative/multimodal tasks. My assessment: the abstract explicitly mentions quantization of latent representations (implying discrete units), but the paper's focus is self-supervised ASR rather than token-design, generation, or systematic analysis of discrete-token design choices, and the abstract lacks the necessary details (tokenizer/codec architecture, codebook settings, bitrate/time downsampling) required by the inclusion criteria to confirm it should be treated as a discrete-audio-token contribution—therefore inclusion is uncertain based on title/abstract alone.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "Junior reviews: one rated 4 noting wav2vec 2.0 uses vector-quantized latent representations (discrete units), the other rated 3 citing the abstract's lack of tokenizer/codebook details and uncertainty about treating those quantized units as discrete audio tokens for downstream generative/multimodal tasks. My assessment: the abstract explicitly mentions quantization of latent representations (implying discrete units), but the paper's focus is self-supervised ASR rather than token-design, generation, or systematic analysis of discrete-token design choices, and the abstract lacks the necessary details (tokenizer/codec architecture, codebook settings, bitrate/time downsampling) required by the inclusion criteria to confirm it should be treated as a discrete-audio-token contribution—therefore inclusion is uncertain based on title/abstract alone.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "chang2023exploration", "query_title": "{Exploration of Efficient End-to-End {ASR} using Discretized Input from Self-Supervised Learning}", "normalized_title": "exploration of efficient end to end asr using discretized input from self supervised learning", "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.", "source": "arxiv", "source_id": "2305.18108v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning", "abstract": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.", "metadata": {"arxiv_id": "2305.18108v1", "title": "Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning", "summary": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.", "authors": ["Xuankai Chang", "Brian Yan", "Yuya Fujita", "Takashi Maekaku", "Shinji Watanabe"], "published": "2023-05-29T14:23:28Z", "updated": "2023-05-29T14:23:28Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.18108v1", "landing_url": "https://arxiv.org/abs/2305.18108v1", "doi": "https://doi.org/10.48550/arXiv.2305.18108"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract explicitly discuss discretized token sequences as input for ASR, including tokenization/quantization concepts, and report reductions in sequence length and storage with competitive results on LibriSpeech, aligning with the discrete audio tokens inclusion criteria and not falling under any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The title/abstract explicitly discuss discretized token sequences as input for ASR, including tokenization/quantization concepts, and report reductions in sequence length and storage with competitive results on LibriSpeech, aligning with the discrete audio tokens inclusion criteria and not falling under any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes using discretized token sequences derived from self-supervised learning models for end-to-end ASR, involving operations like deduplication and sub-word modeling to reduce sequence length and computation, fitting well the inclusion criteria around discrete audio tokens generated by SSL models and used as core input representations with evaluations on downstream tasks; however, the abstract does not detail the exact tokenizer architecture, quantization, codebook settings, or token generation specifics, which slightly limits reproducibility assessment but the core approach clearly involves discrete token sequences for audio modeling and downstream usage, aligning it with the theme and systematic consideration of discrete tokens' impact on computation and performance.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper proposes using discretized token sequences derived from self-supervised learning models for end-to-end ASR, involving operations like deduplication and sub-word modeling to reduce sequence length and computation, fitting well the inclusion criteria around discrete audio tokens generated by SSL models and used as core input representations with evaluations on downstream tasks; however, the abstract does not detail the exact tokenizer architecture, quantization, codebook settings, or token generation specifics, which slightly limits reproducibility assessment but the core approach clearly involves discrete token sequences for audio modeling and downstream usage, aligning it with the theme and systematic consideration of discrete tokens' impact on computation and performance.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "pasad2023comparative", "query_title": "{Comparative layer-wise analysis of self-supervised speech models}", "normalized_title": "comparative layer wise analysis of self supervised speech models", "title": "Comparative layer-wise analysis of self-supervised speech models", "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models.", "source": "arxiv", "source_id": "2211.03929v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Comparative layer-wise analysis of self-supervised speech models", "abstract": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models.", "metadata": {"arxiv_id": "2211.03929v3", "title": "Comparative layer-wise analysis of self-supervised speech models", "summary": "Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose layers of interest for downstream tasks and that single-layer performance often matches or improves upon using all layers, suggesting implications for more efficient use of pre-trained models.", "authors": ["Ankita Pasad", "Bowen Shi", "Karen Livescu"], "published": "2022-11-08T00:59:05Z", "updated": "2023-03-16T22:57:25Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2211.03929v3", "landing_url": "https://arxiv.org/abs/2211.03929v3", "doi": "https://doi.org/10.48550/arXiv.2211.03929"}, "round-A_JuniorNano_output": {"reasoning": "該論文聚焦自監督語音模型的中間表示與層級分析，未提出或分析離散音訊 token/碼本的產生與設計，因此不符合「離散音訊 token」主題的核心與設計評估要點。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "該論文聚焦自監督語音模型的中間表示與層級分析，未提出或分析離散音訊 token/碼本的產生與設計，因此不符合「離散音訊 token」主題的核心與設計評估要點。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study analyzes intermediate representations of self-supervised speech models focusing on acoustic, phonetic, and word-level properties; however, it does not describe using discrete audio tokens derived by discretization (e.g., vector quantization) nor provide specifics about tokenizers, codebooks, or discrete token design and evaluation relevant to audio tokenization. The work is more about analyzing continuous feature representations rather than discrete token sequences used for generation, understanding, or cross-modal modeling, so it does not meet the inclusion criteria centered on discrete audio tokens.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study analyzes intermediate representations of self-supervised speech models focusing on acoustic, phonetic, and word-level properties; however, it does not describe using discrete audio tokens derived by discretization (e.g., vector quantization) nor provide specifics about tokenizers, codebooks, or discrete token design and evaluation relevant to audio tokenization. The work is more about analyzing continuous feature representations rather than discrete token sequences used for generation, understanding, or cross-modal modeling, so it does not meet the inclusion criteria centered on discrete audio tokens.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "goodfellow2020generative", "query_title": "{Generative adversarial networks}", "normalized_title": "generative adversarial networks", "title": "Generative Adversarial Networks", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "source": "arxiv", "source_id": "1406.2661v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Generative Adversarial Networks", "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "metadata": {"arxiv_id": "1406.2661v1", "title": "Generative Adversarial Networks", "summary": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.", "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "published": "2014-06-10T18:58:17Z", "updated": "2014-06-10T18:58:17Z", "categories": ["stat.ML", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1406.2661v1", "landing_url": "https://arxiv.org/abs/1406.2661v1", "doi": "https://doi.org/10.48550/arXiv.1406.2661"}, "round-A_JuniorNano_output": {"reasoning": "This item describes a generic GAN framework for modeling data (likely images) and contains no mention of discrete audio tokens, tokenization, neural audio codecs, or audio-specific downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item describes a generic GAN framework for modeling data (likely images) and contains no mention of discrete audio tokens, tokenization, neural audio codecs, or audio-specific downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces Generative Adversarial Networks focusing on generative modeling and adversarial training without mentioning discrete audio tokens, neural audio codec, vector quantization, or tokenization processes related to audio waveform discretization, which are essential inclusion criteria; thus, it does not meet the topic definition for Discrete Audio Tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces Generative Adversarial Networks focusing on generative modeling and adversarial training without mentioning discrete audio tokens, neural audio codec, vector quantization, or tokenization processes related to audio waveform discretization, which are essential inclusion criteria; thus, it does not meet the topic definition for Discrete Audio Tokens and should be excluded.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kong2020hifigan", "query_title": "{{HiFi-GAN}: generative adversarial networks for efficient and high fidelity speech synthesis}", "normalized_title": "hifi gan generative adversarial networks for efficient and high fidelity speech synthesis", "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis", "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.", "source": "arxiv", "source_id": "2010.05646v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis", "abstract": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.", "metadata": {"arxiv_id": "2010.05646v2", "title": "HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis", "summary": "Several recent work on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this work, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real-time on CPU with comparable quality to an autoregressive counterpart.", "authors": ["Jungil Kong", "Jaehyeon Kim", "Jaekyoung Bae"], "published": "2020-10-12T12:33:43Z", "updated": "2020-10-23T09:12:04Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2010.05646v2", "landing_url": "https://arxiv.org/abs/2010.05646v2", "doi": "https://doi.org/10.48550/arXiv.2010.05646"}, "round-A_JuniorNano_output": {"reasoning": "The HiFi-GAN paper focuses on GAN-based speech synthesis and waveform generation without discussing discrete audio tokens, tokenizers, or neural codecs, so it does not meet the inclusion criteria that require discrete token representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The HiFi-GAN paper focuses on GAN-based speech synthesis and waveform generation without discussing discrete audio tokens, tokenizers, or neural codecs, so it does not meet the inclusion criteria that require discrete token representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on using GANs to generate raw speech waveforms efficiently and with high fidelity, but it does not mention any discretization process mapping continuous audio into discrete token sequences or neural audio codecs producing discrete tokens for downstream modeling; therefore, it does not meet the key inclusion criterion of discrete audio token usage as core representations for modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on using GANs to generate raw speech waveforms efficiently and with high fidelity, but it does not mention any discretization process mapping continuous audio into discrete token sequences or neural audio codecs producing discrete tokens for downstream modeling; therefore, it does not meet the key inclusion criterion of discrete audio token usage as core representations for modeling or generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "oord2016wavenet", "query_title": "{WaveNet: A Generative Model for Raw Audio}", "normalized_title": "wavenet a generative model for raw audio", "title": "WaveNet: A Generative Model for Raw Audio", "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "source": "arxiv", "source_id": "1609.03499v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WaveNet: A Generative Model for Raw Audio", "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "metadata": {"arxiv_id": "1609.03499v2", "title": "WaveNet: A Generative Model for Raw Audio", "summary": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "authors": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "published": "2016-09-12T17:29:40Z", "updated": "2016-09-19T18:04:35Z", "categories": ["cs.SD", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1609.03499v2", "landing_url": "https://arxiv.org/abs/1609.03499v2", "doi": "https://doi.org/10.48550/arXiv.1609.03499"}, "round-A_JuniorNano_output": {"reasoning": "The paper models continuous raw audio with WaveNet and does not discuss discrete tokenization or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper models continuous raw audio with WaveNet and does not discuss discrete tokenization or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper WaveNet describes a generative model for raw audio waveforms and focuses on directly modeling continuous audio samples rather than discrete audio tokens obtained through discretization processes like neural audio codecs or vector quantization. It does not discuss the creation or use of discrete token sequences as a core representation, nor provide design details of tokenizers, codecs, vocabularies, or codebooks required by the inclusion criteria. Therefore, it does not meet the core requirements related to discrete audio tokens for this task.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper WaveNet describes a generative model for raw audio waveforms and focuses on directly modeling continuous audio samples rather than discrete audio tokens obtained through discretization processes like neural audio codecs or vector quantization. It does not discuss the creation or use of discrete token sequences as a core representation, nor provide design details of tokenizers, codecs, vocabularies, or codebooks required by the inclusion criteria. Therefore, it does not meet the core requirements related to discrete audio tokens for this task.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "prenger2018waveglow", "query_title": "{Waveglow: A Flow-based Generative Network for Speech Synthesis}", "normalized_title": "waveglow a flow based generative network for speech synthesis", "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "abstract": "In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.", "source": "arxiv", "source_id": "1811.00002v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "abstract": "In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.", "metadata": {"arxiv_id": "1811.00002v1", "title": "WaveGlow: A Flow-based Generative Network for Speech Synthesis", "summary": "In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.", "authors": ["Ryan Prenger", "Rafael Valle", "Bryan Catanzaro"], "published": "2018-10-31T03:22:25Z", "updated": "2018-10-31T03:22:25Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1811.00002v1", "landing_url": "https://arxiv.org/abs/1811.00002v1", "doi": "https://doi.org/10.48550/arXiv.1811.00002"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents a continuous-flow speech synthesis model (WaveGlow) without discrete tokenization or any discrete audio token design, thus failing the inclusion criteria that require discrete audio tokens.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper presents a continuous-flow speech synthesis model (WaveGlow) without discrete tokenization or any discrete audio token design, thus failing the inclusion criteria that require discrete audio tokens.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper WaveGlow focuses on a flow-based generative network for speech synthesis from mel-spectrograms without mentioning any discretization process or the generation of discrete audio tokens; it does not discuss tokenizers, vector quantization, or discrete codebooks essential to the inclusion criteria. Hence, it aligns with continuous audio representation modeling and does not meet the core inclusion criteria for discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper WaveGlow focuses on a flow-based generative network for speech synthesis from mel-spectrograms without mentioning any discretization process or the generation of discrete audio tokens; it does not discuss tokenizers, vector quantization, or discrete codebooks essential to the inclusion criteria. Hence, it aligns with continuous audio representation modeling and does not meet the core inclusion criteria for discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "transformer", "query_title": "{Attention is All You Need}", "normalized_title": "attention is all you need", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "source": "arxiv", "source_id": "1706.03762v7", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "metadata": {"arxiv_id": "1706.03762v7", "title": "Attention Is All You Need", "summary": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "published": "2017-06-12T17:57:34Z", "updated": "2023-08-02T00:41:18Z", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/1706.03762v7", "landing_url": "https://arxiv.org/abs/1706.03762v7", "doi": "https://doi.org/10.48550/arXiv.1706.03762"}, "round-A_JuniorNano_output": {"reasoning": "This item analyzes the Transformer architecture for text-based machine translation and does not involve discrete audio tokens, neural audio codecs, tokenization schemes, or audio-centric downstream tasks; it fails the inclusion criteria and does not trigger the stated exclusions.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item analyzes the Transformer architecture for text-based machine translation and does not involve discrete audio tokens, neural audio codecs, tokenization schemes, or audio-centric downstream tasks; it fails the inclusion criteria and does not trigger the stated exclusions.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe the Transformer model for machine translation and parsing tasks, focusing on attention-based neural architectures without any mention of discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization of continuous audio signals. The research is primarily on NLP tasks involving text translation and parsing rather than audio discretization, generation, or multimodal alignment involving discrete audio tokens. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens representation or token-based audio modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe the Transformer model for machine translation and parsing tasks, focusing on attention-based neural architectures without any mention of discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization of continuous audio signals. The research is primarily on NLP tasks involving text translation and parsing rather than audio discretization, generation, or multimodal alignment involving discrete audio tokens. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens representation or token-based audio modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "somos", "query_title": "{{SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis}}", "normalized_title": "somos the samsung open mos dataset for the evaluation of neural text to speech synthesis", "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis", "abstract": "In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the samples' variation depends only on the acoustic models. The synthesized utterances provide balanced and adequate domain and length coverage. We collect MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and share practices leading to reliable crowdsourced annotations for this task. We provide baseline results of state-of-the-art MOS prediction models on the SOMOS dataset and show the limitations that such models face when assigned to evaluate TTS utterances.", "source": "arxiv", "source_id": "2204.03040v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis", "abstract": "In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the samples' variation depends only on the acoustic models. The synthesized utterances provide balanced and adequate domain and length coverage. We collect MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and share practices leading to reliable crowdsourced annotations for this task. We provide baseline results of state-of-the-art MOS prediction models on the SOMOS dataset and show the limitations that such models face when assigned to evaluate TTS utterances.", "metadata": {"arxiv_id": "2204.03040v2", "title": "SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis", "summary": "In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 TTS systems including vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the samples' variation depends only on the acoustic models. The synthesized utterances provide balanced and adequate domain and length coverage. We collect MOS naturalness evaluations on 3 English Amazon Mechanical Turk locales and share practices leading to reliable crowdsourced annotations for this task. We provide baseline results of state-of-the-art MOS prediction models on the SOMOS dataset and show the limitations that such models face when assigned to evaluate TTS utterances.", "authors": ["Georgia Maniati", "Alexandra Vioni", "Nikolaos Ellinas", "Karolos Nikitaras", "Konstantinos Klapsas", "June Sig Sung", "Gunu Jho", "Aimilios Chalamandaris", "Pirros Tsiakoulis"], "published": "2022-04-06T18:45:20Z", "updated": "2022-08-24T14:24:57Z", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2204.03040v2", "landing_url": "https://arxiv.org/abs/2204.03040v2", "doi": "https://doi.org/10.21437/Interspeech.2022-10922"}, "round-A_JuniorNano_output": {"reasoning": "The item is a MOS dataset for evaluating neural TTS and does not discuss discrete audio tokens, tokenization, or token-based modelling, thus it fails the inclusion criteria and meets exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a MOS dataset for evaluating neural TTS and does not discuss discrete audio tokens, tokenization, or token-based modelling, thus it fails the inclusion criteria and meets exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a large-scale MOS dataset for neural TTS evaluation but does not involve the creation or use of discrete audio tokens derived from discretization processes such as neural audio codecs or self-supervised models, nor does it address token design, encoding, or downstream usage in generative or alignment tasks; it rather focuses on subjective evaluation metrics and model performance on speech naturalness assessment, which does not meet the inclusion criteria related to discrete audio tokens usage or analysis.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a large-scale MOS dataset for neural TTS evaluation but does not involve the creation or use of discrete audio tokens derived from discretization processes such as neural audio codecs or self-supervised models, nor does it address token design, encoding, or downstream usage in generative or alignment tasks; it rather focuses on subjective evaluation metrics and model performance on speech naturalness assessment, which does not meet the inclusion criteria related to discrete audio tokens usage or analysis.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yang2023towards", "query_title": "{Towards Universal Speech Discrete Tokens: A Case Study for {ASR} and {TTS}}", "normalized_title": "towards universal speech discrete tokens a case study for asr and tts", "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS", "abstract": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.", "source": "arxiv", "source_id": "2309.07377v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS", "abstract": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.", "metadata": {"arxiv_id": "2309.07377v2", "title": "Towards Universal Speech Discrete Tokens: A Case Study for ASR and TTS", "summary": "Self-supervised learning (SSL) proficiency in speech-related tasks has driven research into utilizing discrete tokens for speech tasks like recognition and translation, which offer lower storage requirements and great potential to employ natural language processing techniques. However, these studies, mainly single-task focused, faced challenges like overfitting and performance degradation in speech recognition tasks, often at the cost of sacrificing performance in multi-task scenarios. This study presents a comprehensive comparison and optimization of discrete tokens generated by various leading SSL models in speech recognition and synthesis tasks. We aim to explore the universality of speech discrete tokens across multiple speech tasks. Experimental results demonstrate that discrete tokens achieve comparable results against systems trained on FBank features in speech recognition tasks and outperform mel-spectrogram features in speech synthesis in subjective and objective metrics. These findings suggest that universal discrete tokens have enormous potential in various speech-related tasks. Our work is open-source and publicly available at https://github.com/k2-fsa/icefall.", "authors": ["Yifan Yang", "Feiyu Shen", "Chenpeng Du", "Ziyang Ma", "Kai Yu", "Daniel Povey", "Xie Chen"], "published": "2023-09-14T01:39:43Z", "updated": "2023-12-14T13:18:35Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2309.07377v2", "landing_url": "https://arxiv.org/abs/2309.07377v2", "doi": "https://doi.org/10.48550/arXiv.2309.07377"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on discrete audio tokens produced by neural encoders or SSL models and evaluates these tokens for ASR and TTS with explicit English abstract and experimental results, aligning with the inclusion criteria and not fitting the exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work centers on discrete audio tokens produced by neural encoders or SSL models and evaluates these tokens for ASR and TTS with explicit English abstract and experimental results, aligning with the inclusion criteria and not fitting the exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study investigates discrete tokens generated by self-supervised learning (SSL) models for speech recognition and synthesis tasks, discussing token design, performance comparison, and universality across tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens derived from neural audio encoders or SSL models, and their use in downstream tasks with comprehensive evaluation; it provides detailed information on token generation and evaluation, and includes an English title and abstract, supporting reproducibility and assessment. Hence, it meets the inclusion criteria and does not fall under exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study investigates discrete tokens generated by self-supervised learning (SSL) models for speech recognition and synthesis tasks, discussing token design, performance comparison, and universality across tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens derived from neural audio encoders or SSL models, and their use in downstream tasks with comprehensive evaluation; it provides detailed information on token generation and evaluation, and includes an English title and abstract, supporting reproducibility and assessment. Hence, it meets the inclusion criteria and does not fall under exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2021dwer", "query_title": "{Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement}", "normalized_title": "sequential multi frame neural beamforming for speech separation and enhancement", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "source": "arxiv", "source_id": "1911.07953v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "metadata": {"arxiv_id": "1911.07953v3", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "summary": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "authors": ["Zhong-Qiu Wang", "Hakan Erdogan", "Scott Wisdom", "Kevin Wilson", "Desh Raj", "Shinji Watanabe", "Zhuo Chen", "John R. Hershey"], "published": "2019-11-18T20:59:03Z", "updated": "2020-11-03T19:35:49Z", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1911.07953v3", "landing_url": "https://arxiv.org/abs/1911.07953v3", "doi": "https://doi.org/10.48550/arXiv.1911.07953"}, "round-A_JuniorNano_output": {"reasoning": "No discrete audio tokens or tokenization are discussed; the study is about neural beamforming for speech separation, so it does not meet the discrete audio tokens inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "No discrete audio tokens or tokenization are discussed; the study is about neural beamforming for speech separation, so it does not meet the discrete audio tokens inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on neural beamforming methods for speech separation and enhancement without discussing or proposing any discretization process to convert continuous audio into discrete tokens; it neither mentions neural audio codecs nor discrete token vocabularies or tokenization strategies essential to the inclusion criteria related to 'Discrete Audio Tokens'. Therefore, it does not meet the inclusion criteria of using discrete audio token sequences or token generation mechanisms.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on neural beamforming methods for speech separation and enhancement without discussing or proposing any discretization process to convert continuous audio into discrete tokens; it neither mentions neural audio codecs nor discrete token vocabularies or tokenization strategies essential to the inclusion criteria related to 'Discrete Audio Tokens'. Therefore, it does not meet the inclusion criteria of using discrete audio token sequences or token generation mechanisms.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wichern2019wham", "query_title": "{{WHAM!}: Extending Speech Separation to Noisy Environments}", "normalized_title": "wham extending speech separation to noisy environments", "title": "WHAM!: Extending Speech Separation to Noisy Environments", "abstract": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches.", "source": "arxiv", "source_id": "1907.01160v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WHAM!: Extending Speech Separation to Noisy Environments", "abstract": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches.", "metadata": {"arxiv_id": "1907.01160v1", "title": "WHAM!: Extending Speech Separation to Noisy Environments", "summary": "Recent progress in separating the speech signals from multiple overlapping speakers using a single audio channel has brought us closer to solving the cocktail party problem. However, most studies in this area use a constrained problem setup, comparing performance when speakers overlap almost completely, at artificially low sampling rates, and with no external background noise. In this paper, we strive to move the field towards more realistic and challenging scenarios. To that end, we created the WSJ0 Hipster Ambient Mixtures (WHAM!) dataset, consisting of two speaker mixtures from the wsj0-2mix dataset combined with real ambient noise samples. The samples were collected in coffee shops, restaurants, and bars in the San Francisco Bay Area, and are made publicly available. We benchmark various speech separation architectures and objective functions to evaluate their robustness to noise. While separation performance decreases as a result of noise, we still observe substantial gains relative to the noisy signals for most approaches.", "authors": ["Gordon Wichern", "Joe Antognini", "Michael Flynn", "Licheng Richard Zhu", "Emmett McQuinn", "Dwight Crow", "Ethan Manilow", "Jonathan Le Roux"], "published": "2019-07-02T04:27:55Z", "updated": "2019-07-02T04:27:55Z", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1907.01160v1", "landing_url": "https://arxiv.org/abs/1907.01160v1", "doi": "https://doi.org/10.48550/arXiv.1907.01160"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on a dataset and benchmarking for speech separation in noisy environments and does not discuss discrete audio tokens, tokenization architectures, or token-based downstream tasks, thus failing the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work centers on a dataset and benchmarking for speech separation in noisy environments and does not discuss discrete audio tokens, tokenization architectures, or token-based downstream tasks, thus failing the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on speech separation in noisy environments and introduces a new dataset and evaluations of separation architectures. However, it does not discuss or use any form of discrete audio tokens, neural audio codecs, or tokenization processes to discretize audio signals. The study primarily deals with continuous audio processing and noise robustness rather than discrete token-based modeling or generation, so it does not meet the inclusion criteria that require discrete token representations and related analyses.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on speech separation in noisy environments and introduces a new dataset and evaluations of separation architectures. However, it does not discuss or use any form of discrete audio tokens, neural audio codecs, or tokenization processes to discretize audio signals. The study primarily deals with continuous audio processing and noise robustness rather than discrete token-based modeling or generation, so it does not meet the inclusion criteria that require discrete token representations and related analyses.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "le2019sdr", "query_title": "{{SDR}--half-baked or well done?}", "normalized_title": "sdr half baked or well done", "title": "SDR - half-baked or well done?", "abstract": "In speech enhancement and source separation, signal-to-noise ratio is a ubiquitous objective measure of denoising/separation quality. A decade ago, the BSS_eval toolkit was developed to give researchers worldwide a way to evaluate the quality of their algorithms in a simple, fair, and hopefully insightful way: it attempted to account for channel variations, and to not only evaluate the total distortion in the estimated signal but also split it in terms of various factors such as remaining interference, newly added artifacts, and channel errors. In recent years, hundreds of papers have been relying on this toolkit to evaluate their proposed methods and compare them to previous works, often arguing that differences on the order of 0.1 dB proved the effectiveness of a method over others. We argue here that the signal-to-distortion ratio (SDR) implemented in the BSS_eval toolkit has generally been improperly used and abused, especially in the case of single-channel separation, resulting in misleading results. We propose to use a slightly modified definition, resulting in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We present various examples of critical failure of the original SDR that SI-SDR overcomes.", "source": "arxiv", "source_id": "1811.02508v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SDR - half-baked or well done?", "abstract": "In speech enhancement and source separation, signal-to-noise ratio is a ubiquitous objective measure of denoising/separation quality. A decade ago, the BSS_eval toolkit was developed to give researchers worldwide a way to evaluate the quality of their algorithms in a simple, fair, and hopefully insightful way: it attempted to account for channel variations, and to not only evaluate the total distortion in the estimated signal but also split it in terms of various factors such as remaining interference, newly added artifacts, and channel errors. In recent years, hundreds of papers have been relying on this toolkit to evaluate their proposed methods and compare them to previous works, often arguing that differences on the order of 0.1 dB proved the effectiveness of a method over others. We argue here that the signal-to-distortion ratio (SDR) implemented in the BSS_eval toolkit has generally been improperly used and abused, especially in the case of single-channel separation, resulting in misleading results. We propose to use a slightly modified definition, resulting in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We present various examples of critical failure of the original SDR that SI-SDR overcomes.", "metadata": {"arxiv_id": "1811.02508v1", "title": "SDR - half-baked or well done?", "summary": "In speech enhancement and source separation, signal-to-noise ratio is a ubiquitous objective measure of denoising/separation quality. A decade ago, the BSS_eval toolkit was developed to give researchers worldwide a way to evaluate the quality of their algorithms in a simple, fair, and hopefully insightful way: it attempted to account for channel variations, and to not only evaluate the total distortion in the estimated signal but also split it in terms of various factors such as remaining interference, newly added artifacts, and channel errors. In recent years, hundreds of papers have been relying on this toolkit to evaluate their proposed methods and compare them to previous works, often arguing that differences on the order of 0.1 dB proved the effectiveness of a method over others. We argue here that the signal-to-distortion ratio (SDR) implemented in the BSS_eval toolkit has generally been improperly used and abused, especially in the case of single-channel separation, resulting in misleading results. We propose to use a slightly modified definition, resulting in a simpler, more robust measure, called scale-invariant SDR (SI-SDR). We present various examples of critical failure of the original SDR that SI-SDR overcomes.", "authors": ["Jonathan Le Roux", "Scott Wisdom", "Hakan Erdogan", "John R. Hershey"], "published": "2018-11-06T17:20:05Z", "updated": "2018-11-06T17:20:05Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1811.02508v1", "landing_url": "https://arxiv.org/abs/1811.02508v1", "doi": "https://doi.org/10.48550/arXiv.1811.02508"}, "round-A_JuniorNano_output": {"reasoning": "The article evaluates SDR/SI-SDR as objective metrics for speech enhancement and source separation, not addressing discrete audio tokens, tokenization, or token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article evaluates SDR/SI-SDR as objective metrics for speech enhancement and source separation, not addressing discrete audio tokens, tokenization, or token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses the evaluation metric SDR and proposes a modification, SI-SDR, for better quality assessment in speech enhancement and source separation. However, it does not relate to discrete audio tokens, neural audio codecs, tokenization processes, or modeling using discrete token sequences, which are the core inclusion criteria. It focuses on continuous audio signal evaluation rather than discrete token representations or generation, thus not meeting the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses the evaluation metric SDR and proposes a modification, SI-SDR, for better quality assessment in speech enhancement and source separation. However, it does not relate to discrete audio tokens, neural audio codecs, tokenization processes, or modeling using discrete token sequences, which are the core inclusion criteria. It focuses on continuous audio signal evaluation rather than discrete token representations or generation, thus not meeting the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ljspeech17", "query_title": "{The {LJ} Speech Dataset}", "normalized_title": "the lj speech dataset", "title": "{The {LJ} Speech Dataset}", "abstract": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.", "source": "keithito", "source_id": "https://keithito.com/LJ-Speech-Dataset/", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{The {LJ} Speech Dataset}", "abstract": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.", "metadata": {"arxiv_id": "keithito:https://keithito.com/LJ-Speech-Dataset/", "title": "{The {LJ} Speech Dataset}", "summary": "This is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.", "source": "keithito", "source_id": "https://keithito.com/LJ-Speech-Dataset/", "year": 1964}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes a speech dataset without any discrete audio tokenization or token-based modeling aspects, thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes a speech dataset without any discrete audio tokenization or token-based modeling aspects, thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The item describes a public speech dataset providing audio clips and transcriptions but does not discuss the generation or use of discrete audio tokens, neural audio codecs, or any discretization processes, nor does it focus on modeling, generation, or cross-modal reasoning using such tokens, which are core to the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The item describes a public speech dataset providing audio clips and transcriptions but does not discuss the generation or use of discrete audio tokens, neural audio codecs, or any discretization processes, nor does it focus on modeling, generation, or cross-modal reasoning using such tokens, which are core to the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "srivastava2014dropout", "query_title": "{Dropout: a simple way to prevent neural networks from overfitting}", "normalized_title": "dropout a simple way to prevent neural networks from overfitting", "title": "Dropout: a simple way to prevent neural networks from overfitting", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "source": "semantic_scholar", "source_id": "34f25a8704614163c4095b3ee2fc969b60de4698", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Dropout: a simple way to prevent neural networks from overfitting", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "metadata": {"arxiv_id": "semantic_scholar:34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "summary": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned ” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "source": "semantic_scholar", "source_id": "34f25a8704614163c4095b3ee2fc969b60de4698", "year": 2014}, "round-A_JuniorNano_output": {"reasoning": "This item discusses dropout as a regularization technique for neural networks and does not involve discrete audio tokens, tokenizers, or token-based modeling as defined in the inclusion criteria, so it should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses dropout as a regularization technique for neural networks and does not involve discrete audio tokens, tokenizers, or token-based modeling as defined in the inclusion criteria, so it should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on dropout as a method to prevent overfitting in neural networks and does not discuss discrete audio tokens, their generation, or usage in audio modeling tasks. There is no mention of tokenization, quantization, or any form of discrete audio representation relevant to the inclusion criteria, thus failing to meet the core theme requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on dropout as a method to prevent overfitting in neural networks and does not discuss discrete audio tokens, their generation, or usage in audio modeling tasks. There is no mention of tokenization, quantization, or any form of discrete audio representation relevant to the inclusion criteria, thus failing to meet the core theme requirements.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "nagrani2017voxceleb", "query_title": "{{VoxCeleb}: A Large-Scale Speaker Identification Dataset}", "normalized_title": "voxceleb a large scale speaker identification dataset", "title": "VoxCeleb: a large-scale speaker identification dataset", "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.", "source": "arxiv", "source_id": "1706.08612v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "VoxCeleb: a large-scale speaker identification dataset", "abstract": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.", "metadata": {"arxiv_id": "1706.08612v2", "title": "VoxCeleb: a large-scale speaker identification dataset", "summary": "Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.", "authors": ["Arsha Nagrani", "Joon Son Chung", "Andrew Zisserman"], "published": "2017-06-26T21:42:27Z", "updated": "2018-05-30T06:52:06Z", "categories": ["cs.SD"], "pdf_url": "https://arxiv.org/pdf/1706.08612v2", "landing_url": "https://arxiv.org/abs/1706.08612v2", "doi": "https://doi.org/10.21437/Interspeech.2017-950"}, "round-A_JuniorNano_output": {"reasoning": "The article focuses on creating a large-scale speaker identification dataset with automated video-derived identities and does not discuss discrete audio tokens, tokenization methods, or token-based modeling, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article focuses on creating a large-scale speaker identification dataset with automated video-derived identities and does not discuss discrete audio tokens, tokenization methods, or token-based modeling, thus it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces VoxCeleb, a large-scale dataset for speaker identification, focusing on data collection and baseline benchmark models for speaker recognition. However, it does not discuss or propose discrete audio tokenization, discretization schemes, or any neural audio codec/tokenizer design that converts continuous audio waveforms into discrete token sequences for generative or cross-modal modeling, which is the core inclusion criterion. It primarily focuses on speaker identification in a continuous audio domain without addressing discrete audio token representations or their properties.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces VoxCeleb, a large-scale dataset for speaker identification, focusing on data collection and baseline benchmark models for speaker recognition. However, it does not discuss or propose discrete audio tokenization, discretization schemes, or any neural audio codec/tokenizer design that converts continuous audio waveforms into discrete token sequences for generative or cross-modal modeling, which is the core inclusion criterion. It primarily focuses on speaker identification in a continuous audio domain without addressing discrete audio token representations or their properties.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "1360861705599880960", "query_title": "\"{{UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022}}\"", "normalized_title": "utmos utokyo sarulab system for voicemos challenge 2022", "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022", "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests. Our system is based on ensemble learning of strong and weak learners. Strong learners incorporate several improvements to the previous fine-tuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features. In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks. In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.", "source": "arxiv", "source_id": "2204.02152v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022", "abstract": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests. Our system is based on ensemble learning of strong and weak learners. Strong learners incorporate several improvements to the previous fine-tuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features. In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks. In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.", "metadata": {"arxiv_id": "2204.02152v2", "title": "UTMOS: UTokyo-SaruLab System for VoiceMOS Challenge 2022", "summary": "We present the UTokyo-SaruLab mean opinion score (MOS) prediction system submitted to VoiceMOS Challenge 2022. The challenge is to predict the MOS values of speech samples collected from previous Blizzard Challenges and Voice Conversion Challenges for two tracks: a main track for in-domain prediction and an out-of-domain (OOD) track for which there is less labeled data from different listening tests. Our system is based on ensemble learning of strong and weak learners. Strong learners incorporate several improvements to the previous fine-tuning models of self-supervised learning (SSL) models, while weak learners use basic machine-learning methods to predict scores from SSL features. In the Challenge, our system had the highest score on several metrics for both the main and OOD tracks. In addition, we conducted ablation studies to investigate the effectiveness of our proposed methods.", "authors": ["Takaaki Saeki", "Detai Xin", "Wataru Nakata", "Tomoki Koriyama", "Shinnosuke Takamichi", "Hiroshi Saruwatari"], "published": "2022-04-05T12:23:51Z", "updated": "2022-06-29T13:42:05Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2204.02152v2", "landing_url": "https://arxiv.org/abs/2204.02152v2", "doi": "https://doi.org/10.48550/arXiv.2204.02152"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on MOS prediction for VoiceMOS Challenge without discussing discrete audio tokens or tokenization, thus it does not meet the token-based inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on MOS prediction for VoiceMOS Challenge without discussing discrete audio tokens or tokenization, thus it does not meet the token-based inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a MOS prediction system using ensemble learning with SSL models for speech quality assessment, but it does not mention generation of discrete audio tokens or discretization processes such as vector quantization, codebook design, or tokenization that map continuous audio waves to discrete token sequences for sequence modeling. The core method involves quality score prediction rather than discrete token design, analysis, or application for audio generation/understanding. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens, and lacks necessary details about tokenization or discretization.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on a MOS prediction system using ensemble learning with SSL models for speech quality assessment, but it does not mention generation of discrete audio tokens or discretization processes such as vector quantization, codebook design, or tokenization that map continuous audio waves to discrete token sequences for sequence modeling. The core method involves quality score prediction rather than discrete token design, analysis, or application for audio generation/understanding. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens, and lacks necessary details about tokenization or discretization.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "zaiem2025speech", "query_title": "{Speech self-supervised representations benchmarking: a case for larger probing heads}", "normalized_title": "speech self supervised representations benchmarking a case for larger probing heads", "title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads", "abstract": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference costs, generalization and multi-level feature exploitation.", "source": "arxiv", "source_id": "2308.14456v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads", "abstract": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference costs, generalization and multi-level feature exploitation.", "metadata": {"arxiv_id": "2308.14456v2", "title": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads", "summary": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data. The high number of proposed approaches fostered the emergence of comprehensive benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, while the number of considered tasks has been growing, most proposals rely upon a single downstream architecture that maps the frozen SSL representations to the task labels. This study examines how benchmarking results are affected by changes in the probing head architecture. Interestingly, we found that altering the downstream architecture structure leads to significant fluctuations in the performance ranking of the evaluated models. Against common practices in speech SSL benchmarking, we evaluate larger-capacity probing heads, showing their impact on performance, inference costs, generalization and multi-level feature exploitation.", "authors": ["Salah Zaiem", "Youcef Kemiche", "Titouan Parcollet", "Slim Essid", "Mirco Ravanelli"], "published": "2023-08-28T09:49:48Z", "updated": "2024-02-21T16:57:23Z", "categories": ["eess.AS", "cs.LG", "cs.SD", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2308.14456v2", "landing_url": "https://arxiv.org/abs/2308.14456v2", "doi": "https://doi.org/10.48550/arXiv.2308.14456"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on benchmarking self-supervised speech representations and effects of larger probing heads, with no discussion of discrete audio token generation or tokenization; thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work centers on benchmarking self-supervised speech representations and effects of larger probing heads, with no discussion of discrete audio token generation or tokenization; thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses self-supervised learning representations of speech and evaluates different architectures for downstream probing heads, but it does not describe or focus on discrete audio tokens obtained through discretization (e.g., vector quantization, neural audio codec), nor does it address token design choices or their impact on generation, understanding, or cross-modal modeling. Therefore, it lacks the core elements required by the inclusion criteria related to discrete token generation and usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses self-supervised learning representations of speech and evaluates different architectures for downstream probing heads, but it does not describe or focus on discrete audio tokens obtained through discretization (e.g., vector quantization, neural audio codec), nor does it address token design choices or their impact on generation, understanding, or cross-modal modeling. Therefore, it lacks the core elements required by the inclusion criteria related to discrete token generation and usage.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "defossez2022high", "query_title": "{High Fidelity Neural Audio Compression}", "normalized_title": "high fidelity neural audio compression", "title": "High Fidelity Neural Audio Compression", "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.", "source": "arxiv", "source_id": "2210.13438v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "High Fidelity Neural Audio Compression", "abstract": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.", "metadata": {"arxiv_id": "2210.13438v1", "title": "High Fidelity Neural Audio Compression", "summary": "We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.", "authors": ["Alexandre Défossez", "Jade Copet", "Gabriel Synnaeve", "Yossi Adi"], "published": "2022-10-24T17:52:02Z", "updated": "2022-10-24T17:52:02Z", "categories": ["eess.AS", "cs.AI", "cs.SD", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2210.13438v1", "landing_url": "https://arxiv.org/abs/2210.13438v1", "doi": "https://doi.org/10.48550/arXiv.2210.13438"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes a neural audio codec with a quantized latent space, implying discrete token–like representations that could serve as tokens for downstream modeling, and it involves neural modeling with evaluation, aligning with the discrete audio token criteria; while explicit tokenization details are not provided, the discrete latent codes likely satisfy the token requirement.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract describes a neural audio codec with a quantized latent space, implying discrete token–like representations that could serve as tokens for downstream modeling, and it involves neural modeling with evaluation, aligning with the discrete audio token criteria; while explicit tokenization details are not provided, the discrete latent codes likely satisfy the token requirement.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper presents a neural audio codec with a quantized latent space and a detailed description of encoder-decoder architecture, loss balancing, and compression using lightweight Transformers. However, the abstract does not clearly specify the discrete tokenization process, codebook settings, or vocabulary details needed to confirm that discrete audio tokens are used as core representations for subsequent modeling. While it involves neural audio compression and mentions quantization, it lacks explicit information on token design choices, tokenization procedures, or applications of discrete tokens in audio generation, understanding, or cross-modal tasks, which are critical inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents a neural audio codec with a quantized latent space and a detailed description of encoder-decoder architecture, loss balancing, and compression using lightweight Transformers. However, the abstract does not clearly specify the discrete tokenization process, codebook settings, or vocabulary details needed to confirm that discrete audio tokens are used as core representations for subsequent modeling. While it involves neural audio compression and mentions quantization, it lacks explicit information on token design choices, tokenization procedures, or applications of discrete tokens in audio generation, understanding, or cross-modal tasks, which are critical inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the paper’s relevance: JuniorNano judged it likely included because it describes a neural codec with a quantized latent space (discrete-like codes), while JuniorMini hesitated because the abstract lacks explicit tokenization/codebook details. The abstract, however, clearly presents a neural audio codec with a quantized latent space and further compression via Transformers, which aligns with the “neural audio codec producing discrete tokens” inclusion group and does not meet any exclusion criteria; because the abstract omits low-level codebook/vocabulary details I don’t mark it as an absolute include. (1-sentence explanation: Include — the paper describes a neural codec with quantized latent representations suitable as discrete audio tokens, but missing explicit codebook/token settings prevents a definitive 5.)", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper’s relevance: JuniorNano judged it likely included because it describes a neural codec with a quantized latent space (discrete-like codes), while JuniorMini hesitated because the abstract lacks explicit tokenization/codebook details. The abstract, however, clearly presents a neural audio codec with a quantized latent space and further compression via Transformers, which aligns with the “neural audio codec producing discrete tokens” inclusion group and does not meet any exclusion criteria; because the abstract omits low-level codebook/vocabulary details I don’t mark it as an absolute include. (1-sentence explanation: Include — the paper describes a neural codec with quantized latent representations suitable as discrete audio tokens, but missing explicit codebook/token settings prevents a definitive 5.)", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "DevlinCLT19", "query_title": "\"{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding\"", "normalized_title": "bert pre training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "source": "arxiv", "source_id": "1810.04805v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "metadata": {"arxiv_id": "1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "published": "2018-10-11T00:50:01Z", "updated": "2019-05-24T20:37:26Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1810.04805v2", "landing_url": "https://arxiv.org/abs/1810.04805v2", "doi": "https://doi.org/10.48550/arXiv.1810.04805"}, "round-A_JuniorNano_output": {"reasoning": "此項目為BERT的英文語言模型，與離散音訊 token 的主題不符，且未涉及離散音訊 token 的產生、設計或下游任務，因此不符合納入標準。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此項目為BERT的英文語言模型，與離散音訊 token 的主題不符，且未涉及離散音訊 token 的產生、設計或下游任務，因此不符合納入標準。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents BERT, a language representation model for text, focusing on natural language processing tasks without any mention of processing audio signals or discrete audio tokens as defined in the inclusion criteria. There is no indication the study involves discretization of audio waveforms, neural audio codecs, or tokenization related to audio, thus not meeting the core requirements for inclusion focused on discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents BERT, a language representation model for text, focusing on natural language processing tasks without any mention of processing audio signals or discrete audio tokens as defined in the inclusion criteria. There is no indication the study involves discretization of audio waveforms, neural audio codecs, or tokenization related to audio, thus not meeting the core requirements for inclusion focused on discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "metadata": {"arxiv_id": "1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "published": "2018-10-11T00:50:01Z", "updated": "2019-05-24T20:37:26Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1810.04805v2", "landing_url": "https://arxiv.org/abs/1810.04805v2", "doi": "https://doi.org/10.48550/arXiv.1810.04805"}, "round-A_JuniorNano_output": {"reasoning": "The article is about BERT for language understanding and does not discuss discrete audio tokens, tokenizers for audio, or audio generation/understanding, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article is about BERT for language understanding and does not discuss discrete audio tokens, tokenizers for audio, or audio generation/understanding, thus it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" focuses exclusively on natural language processing and language representation using bidirectional transformers; it does not discuss or involve discrete audio tokens, audio tokenization methods, or any audio-related token discretization required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for lack of relevance to discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" focuses exclusively on natural language processing and language representation using bidirectional transformers; it does not discuss or involve discrete audio tokens, audio tokenization methods, or any audio-related token discretization required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for lack of relevance to discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "newell2020useful", "query_title": "{How useful is self-supervised pretraining for visual tasks?}", "normalized_title": "how useful is self supervised pretraining for visual tasks", "title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "abstract": "Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}.", "source": "arxiv", "source_id": "2003.14323v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "abstract": "Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}.", "metadata": {"arxiv_id": "2003.14323v1", "title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "summary": "Recent advances have spurred incredible progress in self-supervised pretraining for vision. We investigate what factors may play a role in the utility of these pretraining methods for practitioners. To do this, we evaluate various self-supervised algorithms across a comprehensive array of synthetic datasets and downstream tasks. We prepare a suite of synthetic data that enables an endless supply of annotated images as well as full control over dataset difficulty. Our experiments offer insights into how the utility of self-supervision changes as the number of available labels grows as well as how the utility changes as a function of the downstream task and the properties of the training data. We also find that linear evaluation does not correlate with finetuning performance. Code and data is available at \\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}.", "authors": ["Alejandro Newell", "Jia Deng"], "published": "2020-03-31T16:03:22Z", "updated": "2020-03-31T16:03:22Z", "categories": ["cs.CV", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2003.14323v1", "landing_url": "https://arxiv.org/abs/2003.14323v1", "doi": "https://doi.org/10.48550/arXiv.2003.14323"}, "round-A_JuniorNano_output": {"reasoning": "This item does not address discrete audio tokens and focuses on visual self-supervised pretraining, so it should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item does not address discrete audio tokens and focuses on visual self-supervised pretraining, so it should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on self-supervised pretraining for visual tasks using synthetic image data, without addressing audio data, discrete audio tokens, or any tokenization process for audio signals. It does not discuss neural audio codecs, vector quantization, or audio-related discrete token representations as required by the inclusion criteria, and thus does not meet the thematic inclusion requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised pretraining for visual tasks using synthetic image data, without addressing audio data, discrete audio tokens, or any tokenization process for audio signals. It does not discuss neural audio codecs, vector quantization, or audio-related discrete token representations as required by the inclusion criteria, and thus does not meet the thematic inclusion requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "liu2021self", "query_title": "{Self-supervised learning: Generative or contrastive}", "normalized_title": "self supervised learning generative or contrastive", "title": "Self-supervised Learning: Generative or Contrastive", "abstract": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.", "source": "arxiv", "source_id": "2006.08218v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Self-supervised Learning: Generative or Contrastive", "abstract": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.", "metadata": {"arxiv_id": "2006.08218v5", "title": "Self-supervised Learning: Generative or Contrastive", "summary": "Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.", "authors": ["Xiao Liu", "Fanjin Zhang", "Zhenyu Hou", "Zhaoyu Wang", "Li Mian", "Jing Zhang", "Jie Tang"], "published": "2020-06-15T08:40:03Z", "updated": "2021-03-20T05:07:03Z", "categories": ["cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2006.08218v5", "landing_url": "https://arxiv.org/abs/2006.08218v5", "doi": "https://doi.org/10.1109/TKDE.2021.3090866"}, "round-A_JuniorNano_output": {"reasoning": "This item is a general self-supervised learning survey across computer vision, natural language processing, and graphs and does not discuss discrete audio tokens, tokenization, or token-based audio representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is a general self-supervised learning survey across computer vision, natural language processing, and graphs and does not discuss discrete audio tokens, tokenization, or token-based audio representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract discuss self-supervised learning methods across vision, NLP, and graph learning, focusing on representation learning, but they do not address discrete audio tokens or the discretization of continuous audio waveforms into token sequences as defined in the inclusion criteria. Therefore, this material is not relevant to the topic of discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract discuss self-supervised learning methods across vision, NLP, and graph learning, focusing on representation learning, but they do not address discrete audio tokens or the discretization of continuous audio waveforms into token sequences as defined in the inclusion criteria. Therefore, this material is not relevant to the topic of discrete audio tokens and should be excluded.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "JMLR:v24:22-1144", "query_title": "{{PaLM}: scaling language modeling with pathways}", "normalized_title": "palm scaling language modeling with pathways", "title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "source": "arxiv", "source_id": "2204.02311v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "metadata": {"arxiv_id": "2204.02311v5", "title": "PaLM: Scaling Language Modeling with Pathways", "summary": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "authors": ["Aakanksha Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "Paul Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "Michael Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "Sanjay Ghemawat", "Sunipa Dev", "Henryk Michalewski", "Xavier Garcia", "Vedant Misra", "Kevin Robinson", "Liam Fedus", "Denny Zhou", "Daphne Ippolito", "David Luan", "Hyeontaek Lim", "Barret Zoph", "Alexander Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "Rewon Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Diaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "Kathy Meier-Hellstern", "Douglas Eck", "Jeff Dean", "Slav Petrov", "Noah Fiedel"], "published": "2022-04-05T16:11:45Z", "updated": "2022-10-05T06:02:24Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2204.02311v5", "landing_url": "https://arxiv.org/abs/2204.02311v5", "doi": "https://doi.org/10.48550/arXiv.2204.02311"}, "round-A_JuniorNano_output": {"reasoning": "This item is about scaling language models (PaLM) and few-shot learning; there is no discussion of discrete audio tokens, tokenizers, or audio token representations, so it does not meet inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is about scaling language models (PaLM) and few-shot learning; there is no discussion of discrete audio tokens, tokenizers, or audio token representations, so it does not meet inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe a study on a large language model (PaLM) that focuses on natural language processing tasks including few-shot learning, multilingual tasks, and code generation. However, there is no mention of discrete audio tokens, discretization of audio waveforms, neural audio codecs, or related tokenization processes for audio. The study does not meet any of the inclusion criteria that require discrete audio tokens as core representations or focus on audio tasks, and it falls outside the defined topic of discrete audio token research. Furthermore, the abstract explicitly focuses on text-based language modeling and ethical considerations therein, not on audio or tokenization of audio waveforms.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe a study on a large language model (PaLM) that focuses on natural language processing tasks including few-shot learning, multilingual tasks, and code generation. However, there is no mention of discrete audio tokens, discretization of audio waveforms, neural audio codecs, or related tokenization processes for audio. The study does not meet any of the inclusion criteria that require discrete audio tokens as core representations or focus on audio tasks, and it falls outside the defined topic of discrete audio token research. Furthermore, the abstract explicitly focuses on text-based language modeling and ethical considerations therein, not on audio or tokenization of audio waveforms.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "liu2023gpt", "query_title": "{{GPT} understands, too}", "normalized_title": "gpt understands too", "title": "GPT Understands, Too", "abstract": "Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.", "source": "arxiv", "source_id": "2103.10385v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "GPT Understands, Too", "abstract": "Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.", "metadata": {"arxiv_id": "2103.10385v2", "title": "GPT Understands, Too", "summary": "Prompting a pretrained language model with natural language patterns has been proved effective for natural language understanding (NLU). However, our preliminary study reveals that manual discrete prompts often lead to unstable performance -- e.g., changing a single word in the prompt might result in substantial performance drop. We propose a novel method P-Tuning that employs trainable continuous prompt embeddings in concatenation with discrete prompts. Empirically, P-Tuning not only stabilizes training by minimizing the gap between various discrete prompts, but also improves performance by a sizeable margin on a wide range of NLU tasks including LAMA and SuperGLUE. P-Tuning is generally effective for both frozen and tuned language models, under both the fully-supervised and few-shot settings.", "authors": ["Xiao Liu", "Yanan Zheng", "Zhengxiao Du", "Ming Ding", "Yujie Qian", "Zhilin Yang", "Jie Tang"], "published": "2021-03-18T17:13:50Z", "updated": "2023-10-25T06:15:58Z", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2103.10385v2", "landing_url": "https://arxiv.org/abs/2103.10385v2", "doi": "https://doi.org/10.48550/arXiv.2103.10385"}, "round-A_JuniorNano_output": {"reasoning": "The item deals with discrete prompts and P-Tuning for NLP models, not with discrete audio tokens or audio-token generation/analysis, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item deals with discrete prompts and P-Tuning for NLP models, not with discrete audio tokens or audio-token generation/analysis, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract focus on prompting techniques for pretrained language models in natural language understanding, with no mention or indication of discrete audio tokens, audio discretization processes, neural audio codecs, or audio-related token generation or modeling. Therefore, it does not meet any of the inclusion criteria centered around discrete audio tokens and related audio processing, and falls under exclusion criteria as it is unrelated to discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract focus on prompting techniques for pretrained language models in natural language understanding, with no mention or indication of discrete audio tokens, audio discretization processes, neural audio codecs, or audio-related token generation or modeling. Therefore, it does not meet any of the inclusion criteria centered around discrete audio tokens and related audio processing, and falls under exclusion criteria as it is unrelated to discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "speechbrain_ravanelli", "query_title": "{Open-source conversational ai with speechbrain 1.0}", "normalized_title": "open source conversational ai with speechbrain 1 0", "title": "Open-Source Conversational AI with SpeechBrain 1.0", "abstract": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete \"recipes\" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.", "source": "arxiv", "source_id": "2407.00463v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Open-Source Conversational AI with SpeechBrain 1.0", "abstract": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete \"recipes\" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.", "metadata": {"arxiv_id": "2407.00463v5", "title": "Open-Source Conversational AI with SpeechBrain 1.0", "summary": "SpeechBrain is an open-source Conversational AI toolkit based on PyTorch, focused particularly on speech processing tasks such as speech recognition, speech enhancement, speaker recognition, text-to-speech, and much more. It promotes transparency and replicability by releasing both the pre-trained models and the complete \"recipes\" of code and algorithms required for training them. This paper presents SpeechBrain 1.0, a significant milestone in the evolution of the toolkit, which now has over 200 recipes for speech, audio, and language processing tasks, and more than 100 models available on Hugging Face. SpeechBrain 1.0 introduces new technologies to support diverse learning modalities, Large Language Model (LLM) integration, and advanced decoding strategies, along with novel models, tasks, and modalities. It also includes a new benchmark repository, offering researchers a unified platform for evaluating models across diverse tasks.", "authors": ["Mirco Ravanelli", "Titouan Parcollet", "Adel Moumen", "Sylvain de Langen", "Cem Subakan", "Peter Plantinga", "Yingzhi Wang", "Pooneh Mousavi", "Luca Della Libera", "Artem Ploujnikov", "Francesco Paissan", "Davide Borra", "Salah Zaiem", "Zeyu Zhao", "Shucong Zhang", "Georgios Karakasidis", "Sung-Lin Yeh", "Pierre Champion", "Aku Rouhe", "Rudolf Braun", "Florian Mai", "Juan Zuluaga-Gomez", "Seyed Mahed Mousavi", "Andreas Nautsch", "Ha Nguyen", "Xuechen Liu", "Sangeet Sagar", "Jarod Duret", "Salima Mdhaffar", "Gaelle Laperriere", "Mickael Rouvier", "Renato De Mori", "Yannick Esteve"], "published": "2024-06-29T15:20:11Z", "updated": "2024-10-16T16:13:32Z", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.HC", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.00463v5", "landing_url": "https://arxiv.org/abs/2407.00463v5", "doi": "https://doi.org/10.48550/arXiv.2407.00463"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on an open-source speech processing toolkit and its features, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it meets the exclusion criteria for lacking discrete-token focus.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on an open-source speech processing toolkit and its features, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it meets the exclusion criteria for lacking discrete-token focus.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes SpeechBrain 1.0, an open-source Conversational AI toolkit supporting various speech processing tasks and integrating Large Language Models, but it does not mention the use or design of discrete audio tokens, neural audio codecs producing discrete token sequences, nor does it provide details on tokenization schemes or quantization methods relevant to discrete audio tokenization. Therefore, it does not meet the inclusion criteria focused on discrete audio token representation and modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes SpeechBrain 1.0, an open-source Conversational AI toolkit supporting various speech processing tasks and integrating Large Language Models, but it does not mention the use or design of discrete audio tokens, neural audio codecs producing discrete token sequences, nor does it provide details on tokenization schemes or quantization methods relevant to discrete audio tokenization. Therefore, it does not meet the inclusion criteria focused on discrete audio token representation and modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wang2024evaluating", "query_title": "{Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model}", "normalized_title": "evaluating text to speech synthesis from a large discrete token based speech language model", "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model", "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.", "source": "arxiv", "source_id": "2405.09768v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model", "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.", "metadata": {"arxiv_id": "2405.09768v1", "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model", "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.", "authors": ["Siyang Wang", "Éva Székely"], "published": "2024-05-16T02:18:41Z", "updated": "2024-05-16T02:18:41Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2405.09768v1", "landing_url": "https://arxiv.org/abs/2405.09768v1", "doi": "https://doi.org/10.48550/arXiv.2405.09768"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly centers on discrete token-based speech language models for TTS and reports empirical evaluation using discrete tokens as core representation (token-based SLMs, intelligibility, prosody, robustness), aligning with the discrete audio tokens scope and presenting downstream TTS assessment without any exclusion criteria evident.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly centers on discrete token-based speech language models for TTS and reports empirical evaluation using discrete tokens as core representation (token-based SLMs, intelligibility, prosody, robustness), aligning with the discrete audio tokens scope and presenting downstream TTS assessment without any exclusion criteria evident.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study evaluates text-to-speech synthesis using a discrete token-based speech language model, indicating the use of discrete audio tokens derived likely from neural audio codecs as core representations for generation, meeting the inclusion criteria on discrete tokens as the paper discusses token-based modeling, evaluation, and improvement of TTS systems; the abstract provides English title and summary with evaluation results, satisfying the clarity and detail needed for assessment.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study evaluates text-to-speech synthesis using a discrete token-based speech language model, indicating the use of discrete audio tokens derived likely from neural audio codecs as core representations for generation, meeting the inclusion criteria on discrete tokens as the paper discusses token-based modeling, evaluation, and improvement of TTS systems; the abstract provides English title and summary with evaluation results, satisfying the clarity and detail needed for assessment.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2023neural", "query_title": "{Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers}", "normalized_title": "neural codec language models are zero shot text to speech synthesizers", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.", "source": "arxiv", "source_id": "2301.02111v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.", "metadata": {"arxiv_id": "2301.02111v1", "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers", "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.", "authors": ["Chengyi Wang", "Sanyuan Chen", "Yu Wu", "Ziqiang Zhang", "Long Zhou", "Shujie Liu", "Zhuo Chen", "Yanqing Liu", "Huaming Wang", "Jinyu Li", "Lei He", "Sheng Zhao", "Furu Wei"], "published": "2023-01-05T15:37:15Z", "updated": "2023-01-05T15:37:15Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2301.02111v1", "landing_url": "https://arxiv.org/abs/2301.02111v1", "doi": "https://doi.org/10.48550/arXiv.2301.02111"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on discrete audio tokens produced by a neural codec and models TTS as a conditional language modeling task, meeting the discrete-token criteria and addressing token design and downstream generation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work centers on discrete audio tokens produced by a neural codec and models TTS as a conditional language modeling task, meeting the discrete-token criteria and addressing token design and downstream generation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents Vall-E, a neural codec language model for text-to-speech that uses discrete codes derived from an off-the-shelf neural audio codec, fitting the definition of discrete audio tokens generated via neural codec; it discusses the training data scale, in-context learning, and provides speech synthesis evaluation, fulfilling inclusion criteria related to using discrete token sequences for TTS generation with verifiable token generation details, and has an English title and abstract; therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents Vall-E, a neural codec language model for text-to-speech that uses discrete codes derived from an off-the-shelf neural audio codec, fitting the definition of discrete audio tokens generated via neural codec; it discusses the training data scale, in-context learning, and provides speech synthesis evaluation, fulfilling inclusion criteria related to using discrete token sequences for TTS generation with verifiable token generation details, and has an English title and abstract; therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "kharitonov2023speak", "query_title": "{Speak, read and prompt: High-fidelity text-to-speech with minimal supervision}", "normalized_title": "speak read and prompt high fidelity text to speech with minimal supervision", "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision", "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.", "source": "arxiv", "source_id": "2302.03540v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision", "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.", "metadata": {"arxiv_id": "2302.03540v1", "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision", "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.", "authors": ["Eugene Kharitonov", "Damien Vincent", "Zalán Borsos", "Raphaël Marinier", "Sertan Girgin", "Olivier Pietquin", "Matt Sharifi", "Marco Tagliasacchi", "Neil Zeghidour"], "published": "2023-02-07T15:48:31Z", "updated": "2023-02-07T15:48:31Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2302.03540v1", "landing_url": "https://arxiv.org/abs/2302.03540v1", "doi": "https://doi.org/10.48550/arXiv.2302.03540"}, "round-A_JuniorNano_output": {"reasoning": "The study explicitly centers on discrete speech representations (semantic and acoustic tokens) as core components of a sequence-to-sequence TTS pipeline, discusses token-based modeling and training with minimal supervision, and provides an English title/abstract suitable for evaluation, meeting inclusion criteria and not matching any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The study explicitly centers on discrete speech representations (semantic and acoustic tokens) as core components of a sequence-to-sequence TTS pipeline, discusses token-based modeling and training with minimal supervision, and provides an English title/abstract suitable for evaluation, meeting inclusion criteria and not matching any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study introduces SPEAR-TTS, a text-to-speech system that uses discrete speech representations separated into semantic tokens and acoustic tokens. This aligns well with the inclusion criteria of utilizing discrete audio tokens derived from neural audio codecs or self-supervised models and applying them in a downstream generative task (TTS). However, although the paper mentions a composition of sequence-to-sequence tasks using discrete tokens, it does not provide explicit detailed descriptions of the tokenizer/codec architecture, quantization methods, codebook settings, or bitrate to fully ascertain reproducibility and discrete token design specifics as required. Nevertheless, the method centers on discrete tokens for audio generation and shows empirical evaluation, which are key aspects favoring inclusion.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study introduces SPEAR-TTS, a text-to-speech system that uses discrete speech representations separated into semantic tokens and acoustic tokens. This aligns well with the inclusion criteria of utilizing discrete audio tokens derived from neural audio codecs or self-supervised models and applying them in a downstream generative task (TTS). However, although the paper mentions a composition of sequence-to-sequence tasks using discrete tokens, it does not provide explicit detailed descriptions of the tokenizer/codec architecture, quantization methods, codebook settings, or bitrate to fully ascertain reproducibility and discrete token design specifics as required. Nevertheless, the method centers on discrete tokens for audio generation and shows empirical evaluation, which are key aspects favoring inclusion.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "shi2021discretization", "query_title": "{Discretization and re-synthesis: an alternative method to solve the cocktail party problem}", "normalized_title": "discretization and re synthesis an alternative method to solve the cocktail party problem", "title": "Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem", "abstract": "Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.", "source": "arxiv", "source_id": "2112.09382v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem", "abstract": "Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.", "metadata": {"arxiv_id": "2112.09382v2", "title": "Discretization and Re-synthesis: an alternative method to solve the Cocktail Party Problem", "summary": "Deep learning based models have significantly improved the performance of speech separation with input mixtures like the cocktail party. Prominent methods (e.g., frequency-domain and time-domain speech separation) usually build regression models to predict the ground-truth speech from the mixture, using the masking-based design and the signal-level loss criterion (e.g., MSE or SI-SNR). This study demonstrates, for the first time, that the synthesis-based approach can also perform well on this problem, with great flexibility and strong potential. Specifically, we propose a novel speech separation/enhancement model based on the recognition of discrete symbols, and convert the paradigm of the speech separation/enhancement related tasks from regression to classification. By utilizing the synthesis model with the input of discrete symbols, after the prediction of discrete symbol sequence, each target speech could be re-synthesized. Evaluation results based on the WSJ0-2mix and VCTK-noisy corpora in various settings show that our proposed method can steadily synthesize the separated speech with high speech quality and without any interference, which is difficult to avoid in regression-based methods. In addition, with negligible loss of listening quality, the speaker conversion of enhanced/separated speech could be easily realized through our method.", "authors": ["Jing Shi", "Xuankai Chang", "Tomoki Hayashi", "Yen-Ju Lu", "Shinji Watanabe", "Bo Xu"], "published": "2021-12-17T08:35:40Z", "updated": "2022-01-09T14:28:38Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2112.09382v2", "landing_url": "https://arxiv.org/abs/2112.09382v2", "doi": "https://doi.org/10.48550/arXiv.2112.09382"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on a discrete-symbol (token) based approach for speech separation, using synthesis from discrete tokens, which aligns with the discrete audio tokens criteria and token-based modeling/downstream applicability, with no clear exclusion evident.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract centers on a discrete-symbol (token) based approach for speech separation, using synthesis from discrete tokens, which aligns with the discrete audio tokens criteria and token-based modeling/downstream applicability, with no clear exclusion evident.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a speech separation model that converts regression tasks into classification by recognizing discrete symbols and re-synthesizing speech, indicating the use of discrete token sequences as core representations generated via discretization. The abstract, written in English, discusses generating discrete symbol sequences as intermediate representations, aligning with the theme of discrete audio tokens. However, the abstract lacks explicit detail on the tokenizer/codec architecture, quantization methods, or vocabulary/codebook settings, making reproducibility and precise classification difficult. Despite some missing explicit technical details on the discrete tokenization process, the core idea of treating audio as discrete sequences for synthesis aligns well with the topic and inclusion criteria, but the absence of thorough methodology details suggests caution in inclusion.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study proposes a speech separation model that converts regression tasks into classification by recognizing discrete symbols and re-synthesizing speech, indicating the use of discrete token sequences as core representations generated via discretization. The abstract, written in English, discusses generating discrete symbol sequences as intermediate representations, aligning with the theme of discrete audio tokens. However, the abstract lacks explicit detail on the tokenizer/codec architecture, quantization methods, or vocabulary/codebook settings, making reproducibility and precise classification difficult. Despite some missing explicit technical details on the discrete tokenization process, the core idea of treating audio as discrete sequences for synthesis aligns well with the topic and inclusion criteria, but the absence of thorough methodology details suggests caution in inclusion.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "erdogan2023tokensplit", "query_title": "{{TokenSplit}: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition}", "normalized_title": "tokensplit using discrete speech representations for direct refined and transcript conditioned speech separation and recognition", "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition", "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.", "source": "arxiv", "source_id": "2308.10415v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition", "abstract": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.", "metadata": {"arxiv_id": "2308.10415v1", "title": "TokenSplit: Using Discrete Speech Representations for Direct, Refined, and Transcript-Conditioned Speech Separation and Recognition", "summary": "We present TokenSplit, a speech separation model that acts on discrete token sequences. The model is trained on multiple tasks simultaneously: separate and transcribe each speech source, and generate speech from text. The model operates on transcripts and audio token sequences and achieves multiple tasks through masking of inputs. The model is a sequence-to-sequence encoder-decoder model that uses the Transformer architecture. We also present a \"refinement\" version of the model that predicts enhanced audio tokens from the audio tokens of speech separated by a conventional separation model. Using both objective metrics and subjective MUSHRA listening tests, we show that our model achieves excellent performance in terms of separation, both with or without transcript conditioning. We also measure the automatic speech recognition (ASR) performance and provide audio samples of speech synthesis to demonstrate the additional utility of our model.", "authors": ["Hakan Erdogan", "Scott Wisdom", "Xuankai Chang", "Zalán Borsos", "Marco Tagliasacchi", "Neil Zeghidour", "John R. Hershey"], "published": "2023-08-21T01:52:01Z", "updated": "2023-08-21T01:52:01Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.10415v1", "landing_url": "https://arxiv.org/abs/2308.10415v1", "doi": "https://doi.org/10.48550/arXiv.2308.10415"}, "round-A_JuniorNano_output": {"reasoning": "該論文明確以離散語音表徵（token）進行分離與識別，並以 ASR、MUSHRA 等下游任務與評估展現其離散 token 的應用價值，符合離散音訊 token 的研究方向；但摘要未具體說明 tokens 的產生流程/編碼器或碼本設定等實作細節，可能影響可重現性與嚴格的可評估性要求。", "evaluation": 4}, "round-A_JuniorNano_reasoning": "該論文明確以離散語音表徵（token）進行分離與識別，並以 ASR、MUSHRA 等下游任務與評估展現其離散 token 的應用價值，符合離散音訊 token 的研究方向；但摘要未具體說明 tokens 的產生流程/編碼器或碼本設定等實作細節，可能影響可重現性與嚴格的可評估性要求。", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study uses discrete token sequences derived from speech audio for tasks including speech separation, transcription, and synthesis, employing a Transformer-based sequence-to-sequence model. It explicitly mentions operating on discrete audio tokens, which aligns with the inclusion criteria of using neural audio codec-derived discrete tokens for modeling, and provides evaluation results demonstrating the utility in downstream tasks. It thus meets key inclusion criteria about discrete audio token usage for generation and understanding, with clear description and empirical evaluation. No exclusion criteria such as lacking discrete token details or only continuous audio representations are met.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study uses discrete token sequences derived from speech audio for tasks including speech separation, transcription, and synthesis, employing a Transformer-based sequence-to-sequence model. It explicitly mentions operating on discrete audio tokens, which aligns with the inclusion criteria of using neural audio codec-derived discrete tokens for modeling, and provides evaluation results demonstrating the utility in downstream tasks. It thus meets key inclusion criteria about discrete audio token usage for generation and understanding, with clear description and empirical evaluation. No exclusion criteria such as lacking discrete token details or only continuous audio representations are met.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "puvvada2023discrete", "query_title": "{Discrete Audio Representation as an Alternative to {M}el-Spectrograms for Speaker and Speech Recognition}", "normalized_title": "discrete audio representation as an alternative to mel spectrograms for speaker and speech recognition", "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition", "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.", "source": "semantic_scholar", "source_id": "c78d2a875f6cfd3a7b70c8d4c03a828894146892", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition", "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.", "metadata": {"arxiv_id": "semantic_scholar:c78d2a875f6cfd3a7b70c8d4c03a828894146892", "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition", "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within 1% of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.", "source": "semantic_scholar", "source_id": "c78d2a875f6cfd3a7b70c8d4c03a828894146892", "publication_date": "2023-09-19"}, "round-A_JuniorNano_output": {"reasoning": "The item clearly uses discrete audio tokens, provides English abstract, and evaluates on downstream speaker/speech tasks, aligning with inclusion criteria; no exclusion criteria apparent.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item clearly uses discrete audio tokens, provides English abstract, and evaluates on downstream speaker/speech tasks, aligning with inclusion criteria; no exclusion criteria apparent.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper evaluates compression-based discrete audio tokens produced via Residual Vector Quantization for speaker and speech recognition tasks, comparing them to mel-spectrograms and detailing token compression rates, robustness, and frequency response characteristics, thereby fulfilling the inclusion criteria of introducing and analyzing discrete audio tokens derived from neural audio codecs with detailed methodology and experimental results.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper evaluates compression-based discrete audio tokens produced via Residual Vector Quantization for speaker and speech recognition tasks, comparing them to mel-spectrograms and detailing token compression rates, robustness, and frequency response characteristics, thereby fulfilling the inclusion criteria of introducing and analyzing discrete audio tokens derived from neural audio codecs with detailed methodology and experimental results.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2024codecfake", "query_title": "{Codecfake: Enhancing anti-spoofing models against deepfake audios from codec-based speech synthesis systems}", "normalized_title": "codecfake enhancing anti spoofing models against deepfake audios from codec based speech synthesis systems", "title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems", "abstract": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.", "source": "arxiv", "source_id": "2406.07237v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems", "abstract": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.", "metadata": {"arxiv_id": "2406.07237v1", "title": "CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems", "summary": "Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker. Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech. However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech. This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset. Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.", "authors": ["Haibin Wu", "Yuan Tseng", "Hung-yi Lee"], "published": "2024-06-11T13:16:09Z", "updated": "2024-06-11T13:16:09Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2406.07237v1", "landing_url": "https://arxiv.org/abs/2406.07237v1", "doi": "https://doi.org/10.48550/arXiv.2406.07237"}, "round-A_JuniorNano_output": {"reasoning": "The article centers on codec-based deepfake audio and anti-spoofing detection, with no discussion of discrete audio tokens, tokenization methods, or token-based representations, so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article centers on codec-based deepfake audio and anti-spoofing detection, with no discussion of discrete audio tokens, tokenization methods, or token-based representations, so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on anti-spoofing models for detecting deepfake audios synthesized by codec-based speech synthesis systems and introduces a dataset (CodecFake) for this purpose. However, it does not describe the generation or use of discrete audio tokens, neural audio codecs with vector quantization, or the design of token vocabularies or quantization that aligns with the theme of discrete audio tokens for generative or cross-modal modeling. The focus is on detection rather than encoding, tokenization, or generation with discrete tokens, and it does not discuss token design choices or downstream generation tasks. Therefore, it does not meet the inclusion criteria related to the use or analysis of discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on anti-spoofing models for detecting deepfake audios synthesized by codec-based speech synthesis systems and introduces a dataset (CodecFake) for this purpose. However, it does not describe the generation or use of discrete audio tokens, neural audio codecs with vector quantization, or the design of token vocabularies or quantization that aligns with the theme of discrete audio tokens for generative or cross-modal modeling. The focus is on detection rather than encoding, tokenization, or generation with discrete tokens, and it does not discuss token design choices or downstream generation tasks. Therefore, it does not meet the inclusion criteria related to the use or analysis of discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ren2024emo", "query_title": "{EMO-Codec: An In-Depth Look at Emotion Preservation Capacity of Legacy and Neural Codec Models with Subjective and Objective Evaluations}", "normalized_title": "emo codec an in depth look at emotion preservation capacity of legacy and neural codec models with subjective and objective evaluations", "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations", "abstract": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.", "source": "arxiv", "source_id": "2407.15458v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations", "abstract": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.", "metadata": {"arxiv_id": "2407.15458v4", "title": "EMO-Codec: An In-Depth Look at Emotion Preservation capacity of Legacy and Neural Codec Models With Subjective and Objective Evaluations", "summary": "The neural codec model reduces speech data transmission delay and serves as the foundational tokenizer for speech language models (speech LMs). Preserving emotional information in codecs is crucial for effective communication and context understanding. However, there is a lack of studies on emotion loss in existing codecs. This paper evaluates neural and legacy codecs using subjective and objective methods on emotion datasets like IEMOCAP. Our study identifies which codecs best preserve emotional information under various bitrate scenarios. We found that training codec models with both English and Chinese data had limited success in retaining emotional information in Chinese. Additionally, resynthesizing speech through these codecs degrades the performance of speech emotion recognition (SER), particularly for emotions like sadness, depression, fear, and disgust. Human listening tests confirmed these findings. This work guides future speech technology developments to ensure new codecs maintain the integrity of emotional information in speech.", "authors": ["Wenze Ren", "Yi-Cheng Lin", "Huang-Cheng Chou", "Haibin Wu", "Yi-Chiao Wu", "Chi-Chun Lee", "Hung-yi Lee", "Yu Tsao"], "published": "2024-07-22T08:14:16Z", "updated": "2024-07-30T12:37:35Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2407.15458v4", "landing_url": "https://arxiv.org/abs/2407.15458v4", "doi": "https://doi.org/10.48550/arXiv.2407.15458"}, "round-A_JuniorNano_output": {"reasoning": "This paper focuses on emotion preservation in neural and legacy codecs and evaluates speech emotion recognition, but it does not present discrete audio tokens, tokenization schemes, or token-based representations as a core method, so it does not meet the discrete audio tokens inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper focuses on emotion preservation in neural and legacy codecs and evaluates speech emotion recognition, but it does not present discrete audio tokens, tokenization schemes, or token-based representations as a core method, so it does not meet the discrete audio tokens inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on evaluating neural and legacy codec models in terms of emotion preservation in speech, mentioning neural codecs and bitrate scenarios; however, it does not explicitly describe the discrete token generation process, such as tokenizer architecture, vector quantization details, codebook settings, or discrete token usage for downstream modeling, which are required to confirm inclusion under the topic \"Discrete Audio Tokens.\" Therefore, it likely studies continuous codec performance rather than discrete token-based representations as the core method or object of analysis.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on evaluating neural and legacy codec models in terms of emotion preservation in speech, mentioning neural codecs and bitrate scenarios; however, it does not explicitly describe the discrete token generation process, such as tokenizer architecture, vector quantization details, codebook settings, or discrete token usage for downstream modeling, which are required to confirm inclusion under the topic \"Discrete Audio Tokens.\" Therefore, it likely studies continuous codec performance rather than discrete token-based representations as the core method or object of analysis.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "mousavi2024", "query_title": "{Semantic Token Tuning: How Should We Extract Discrete Audio Tokens from Self-Supervised Models?}", "normalized_title": "semantic token tuning how should we extract discrete audio tokens from self supervised models", "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.", "source": "semantic_scholar", "source_id": "40875b1e950fe6636ef277df4ed6d501f4933307", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.", "metadata": {"arxiv_id": "semantic_scholar:40875b1e950fe6636ef277df4ed6d501f4933307", "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?", "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.", "source": "semantic_scholar", "source_id": "40875b1e950fe6636ef277df4ed6d501f4933307", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on discrete audio tokens (semantic tokens from SSL or codec-based tokens), tokenization design, and evaluation across downstream tasks, aligning with inclusion criteria and not falling under exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work focuses on discrete audio tokens (semantic tokens from SSL or codec-based tokens), tokenization design, and evaluation across downstream tasks, aligning with inclusion criteria and not falling under exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper explicitly addresses discrete audio tokens derived from self-supervised learning models, discussing their extraction, quantization details, and usage in multiple audio tasks, which aligns with the inclusion criteria related to token source, design choices, and downstream applications; it also provides English abstract and technical insights necessary for reproducibility, and does not fall under any exclusion criteria such as only continuous representations or merely discriminative tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper explicitly addresses discrete audio tokens derived from self-supervised learning models, discussing their extraction, quantization details, and usage in multiple audio tasks, which aligns with the inclusion criteria related to token source, design choices, and downstream applications; it also provides English abstract and technical insights necessary for reproducibility, and does not fall under any exclusion criteria such as only continuous representations or merely discriminative tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "korvas_2014", "query_title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "normalized_title": "free english and czech telephone speech corpus shared under the cc by sa 3 0 license", "title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "abstract": "We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish also the trained models described in this paper. The data are released under the CC-BY-SA~3.0 license, the scripts are licensed under Apache~2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.", "source": "lrec_conf", "source_id": "lrec2014:535", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "abstract": "We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish also the trained models described in this paper. The data are released under the CC-BY-SA~3.0 license, the scripts are licensed under Apache~2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.", "metadata": {"arxiv_id": "lrec_conf:lrec2014:535", "title": "\"Free {E}nglish and {C}zech telephone speech corpus shared under the {CC}-{BY}-{SA} 3.0 license\"", "summary": "We present a dataset of telephone conversations in English and Czech, developed for training acoustic models for automatic speech recognition (ASR) in spoken dialogue systems (SDSs). The data comprise 45 hours of speech in English and over 18 hours in Czech. Large part of the data, both audio and transcriptions, was collected using crowdsourcing, the rest are transcriptions by hired transcribers. We release the data together with scripts for data pre-processing and building acoustic models using the HTK and Kaldi ASR toolkits. We publish also the trained models described in this paper. The data are released under the CC-BY-SA~3.0 license, the scripts are licensed under Apache~2.0. In the paper, we report on the methodology of collecting the data, on the size and properties of the data, and on the scripts and their use. We verify the usability of the datasets by training and evaluating acoustic models using the presented data and scripts.", "source": "lrec_conf", "source_id": "lrec2014:535", "year": 2014}, "round-A_JuniorNano_output": {"reasoning": "The item is a dataset release for ASR with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a dataset release for ASR with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a telephone speech corpus aimed at training acoustic models for ASR, but it does not mention any discretization process of audio into token sequences, neural audio codecs, vector quantization, or use of discrete audio tokens for modeling or generation, which are core to the inclusion criteria. Therefore, it lacks essential components related to discrete audio tokens as required and focuses on speech data collection and ASR model training instead.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a telephone speech corpus aimed at training acoustic models for ASR, but it does not mention any discretization process of audio into token sequences, neural audio codecs, vector quantization, or use of discrete audio tokens for modeling or generation, which are core to the inclusion criteria. Therefore, it lacks essential components related to discrete audio tokens as required and focuses on speech data collection and ASR model training instead.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "han2020contextnet", "query_title": "{{ContextNet}: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context}", "normalized_title": "contextnet improving convolutional neural networks for automatic speech recognition with global context", "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context", "abstract": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.", "source": "arxiv", "source_id": "2005.03191v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context", "abstract": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.", "metadata": {"arxiv_id": "2005.03191v3", "title": "ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context", "summary": "Convolutional neural networks (CNN) have shown promising results for end-to-end speech recognition, albeit still behind other state-of-the-art methods in performance. In this paper, we study how to bridge this gap and go beyond with a novel CNN-RNN-transducer architecture, which we call ContextNet. ContextNet features a fully convolutional encoder that incorporates global context information into convolution layers by adding squeeze-and-excitation modules. In addition, we propose a simple scaling method that scales the widths of ContextNet that achieves good trade-off between computation and accuracy. We demonstrate that on the widely used LibriSpeech benchmark, ContextNet achieves a word error rate (WER) of 2.1%/4.6% without external language model (LM), 1.9%/4.1% with LM and 2.9%/7.0% with only 10M parameters on the clean/noisy LibriSpeech test sets. This compares to the previous best published system of 2.0%/4.6% with LM and 3.9%/11.3% with 20M parameters. The superiority of the proposed ContextNet model is also verified on a much larger internal dataset.", "authors": ["Wei Han", "Zhengdong Zhang", "Yu Zhang", "Jiahui Yu", "Chung-Cheng Chiu", "James Qin", "Anmol Gulati", "Ruoming Pang", "Yonghui Wu"], "published": "2020-05-07T01:03:18Z", "updated": "2020-05-16T00:49:21Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2005.03191v3", "landing_url": "https://arxiv.org/abs/2005.03191v3", "doi": "https://doi.org/10.48550/arXiv.2005.03191"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on ContextNet architecture for ASR (CNN-RNN-transducer with squeeze-and-excitation and scaling) and evaluates WER on LibriSpeech; it does not present or analyze discrete audio tokens, tokenizers, codecs, or token-based downstream tasks, failing the inclusion criteria for 'Discrete Audio Tokens' and meeting any exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on ContextNet architecture for ASR (CNN-RNN-transducer with squeeze-and-excitation and scaling) and evaluates WER on LibriSpeech; it does not present or analyze discrete audio tokens, tokenizers, codecs, or token-based downstream tasks, failing the inclusion criteria for 'Discrete Audio Tokens' and meeting any exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on improving convolutional neural networks for automatic speech recognition, but it does not mention any discretization of continuous audio into discrete tokens, nor does it describe generating or using discrete audio tokens as defined in the inclusion criteria. It centers on continuous signal modeling and CNN-RNN architectures without addressing neural audio codecs, vector quantization, or discrete token sequences relevant to the topic.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on improving convolutional neural networks for automatic speech recognition, but it does not mention any discretization of continuous audio into discrete tokens, nor does it describe generating or using discrete audio tokens as defined in the inclusion criteria. It centers on continuous signal modeling and CNN-RNN architectures without addressing neural audio codecs, vector quantization, or discrete token sequences relevant to the topic.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "snyder2018x", "query_title": "{X-vectors: Robust {DNN} embeddings for speaker recognition}", "normalized_title": "x vectors robust dnn embeddings for speaker recognition", "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition", "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.", "source": "semantic_scholar", "source_id": "389cd9824428be98a710f5f4de67121a70c15fd3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition", "abstract": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.", "metadata": {"arxiv_id": "semantic_scholar:389cd9824428be98a710f5f4de67121a70c15fd3", "title": "X-Vectors: Robust DNN Embeddings for Speaker Recognition", "summary": "In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.", "source": "semantic_scholar", "source_id": "389cd9824428be98a710f5f4de67121a70c15fd3", "publication_date": "2018-04-15"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on x-vectors for speaker recognition and data augmentation, with no discussion of discrete audio tokens, tokenizers, or discretization, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on x-vectors for speaker recognition and data augmentation, with no discussion of discrete audio tokens, tokenizers, or discretization, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on using deep neural network embeddings (x-vectors) for speaker recognition, involving variable-length utterance mapping to fixed-dimensional embeddings, and data augmentation to improve performance. However, it does not discuss discretizing continuous audio waveforms into discrete token sequences, nor does it describe any neural audio codec, vector quantization, or tokenizer architectures that produce discrete tokens as defined by the inclusion criteria. The study is centered on speaker recognition (a discriminative task), without involving the generation, understanding, or alignment of discrete audio tokens for use in sequence models like Transformers or LLMs. Therefore, it does not meet the inclusion criteria related to discrete audio token research and falls under exclusion criteria for studying continuous audio representations and discriminative tasks without discrete token modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on using deep neural network embeddings (x-vectors) for speaker recognition, involving variable-length utterance mapping to fixed-dimensional embeddings, and data augmentation to improve performance. However, it does not discuss discretizing continuous audio waveforms into discrete token sequences, nor does it describe any neural audio codec, vector quantization, or tokenizer architectures that produce discrete tokens as defined by the inclusion criteria. The study is centered on speaker recognition (a discriminative task), without involving the generation, understanding, or alignment of discrete audio tokens for use in sequence models like Transformers or LLMs. Therefore, it does not meet the inclusion criteria related to discrete audio token research and falls under exclusion criteria for studying continuous audio representations and discriminative tasks without discrete token modeling.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wang2018additive", "query_title": "{Additive margin softmax for face verification}", "normalized_title": "additive margin softmax for face verification", "title": "Additive Margin Softmax for Face Verification", "abstract": "In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax", "source": "arxiv", "source_id": "1801.05599v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Additive Margin Softmax for Face Verification", "abstract": "In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax", "metadata": {"arxiv_id": "1801.05599v4", "title": "Additive Margin Softmax for Face Verification", "summary": "In this paper, we propose a conceptually simple and geometrically interpretable objective function, i.e. additive margin Softmax (AM-Softmax), for deep face verification. In general, the face verification task can be viewed as a metric learning problem, so learning large-margin face features whose intra-class variation is small and inter-class difference is large is of great importance in order to achieve good performance. Recently, Large-margin Softmax and Angular Softmax have been proposed to incorporate the angular margin in a multiplicative manner. In this work, we introduce a novel additive angular margin for the Softmax loss, which is intuitively appealing and more interpretable than the existing works. We also emphasize and discuss the importance of feature normalization in the paper. Most importantly, our experiments on LFW BLUFR and MegaFace show that our additive margin softmax loss consistently performs better than the current state-of-the-art methods using the same network architecture and training dataset. Our code has also been made available at https://github.com/happynear/AMSoftmax", "authors": ["Feng Wang", "Weiyang Liu", "Haijun Liu", "Jian Cheng"], "published": "2018-01-17T09:13:05Z", "updated": "2018-05-30T12:35:03Z", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/1801.05599v4", "landing_url": "https://arxiv.org/abs/1801.05599v4", "doi": "https://doi.org/10.1109/LSP.2018.2822810"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses Additive Margin Softmax for face verification, which concerns visual/midge vision tasks rather than discrete audio tokens; it fails the inclusion criteria (discrete audio tokens) and thus should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses Additive Margin Softmax for face verification, which concerns visual/midge vision tasks rather than discrete audio tokens; it fails the inclusion criteria (discrete audio tokens) and thus should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes an additive margin Softmax loss for face verification, which focuses on metric learning for face features; it does not address discrete audio tokens, audio waveform discretization, or neural audio coding, nor does it discuss token design, audio generation, or cross-modal modeling related to audio tokens, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper proposes an additive margin Softmax loss for face verification, which focuses on metric learning for face features; it does not address discrete audio tokens, audio waveform discretization, or neural audio coding, nor does it discuss token design, audio generation, or cross-modal modeling related to audio tokens, thus it does not meet the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "bastianelli2020slurp", "query_title": "\"{SLURP}: A Spoken Language Understanding Resource Package\"", "normalized_title": "slurp a spoken language understanding resource package", "title": "SLURP: A Spoken Language Understanding Resource Package", "abstract": "Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp.", "source": "arxiv", "source_id": "2011.13205v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SLURP: A Spoken Language Understanding Resource Package", "abstract": "Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp.", "metadata": {"arxiv_id": "2011.13205v1", "title": "SLURP: A Spoken Language Understanding Resource Package", "summary": "Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at https: //github.com/pswietojanski/slurp.", "authors": ["Emanuele Bastianelli", "Andrea Vanzo", "Pawel Swietojanski", "Verena Rieser"], "published": "2020-11-26T09:58:20Z", "updated": "2020-11-26T09:58:20Z", "categories": ["cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2011.13205v1", "landing_url": "https://arxiv.org/abs/2011.13205v1", "doi": "https://doi.org/10.48550/arXiv.2011.13205"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on SLU resources and a dataset/package rather than discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on SLU resources and a dataset/package rather than discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a resource package for spoken language understanding, focusing on semantic inference from audio data, dataset provision, and baseline systems; however, it does not mention any process of discretization of audio into tokens, nor neural audio codecs or tokenization schemes relevant for discrete audio token representation as outlined in the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes a resource package for spoken language understanding, focusing on semantic inference from audio data, dataset provision, and baseline systems; however, it does not mention any process of discretization of audio into tokens, nor neural audio codecs or tokenization schemes relevant for discrete audio token representation as outlined in the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "warden2017speech", "query_title": "{{Speech Commands}: A Dataset for Limited-Vocabulary Speech Recognition}", "normalized_title": "speech commands a dataset for limited vocabulary speech recognition", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "abstract": "Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.", "source": "arxiv", "source_id": "1804.03209v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "abstract": "Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.", "metadata": {"arxiv_id": "1804.03209v1", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "summary": "Describes an audio dataset of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that is different from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and verified, what it contains, previous versions and properties. Concludes by reporting baseline results of models trained on this dataset.", "authors": ["Pete Warden"], "published": "2018-04-09T19:58:17Z", "updated": "2018-04-09T19:58:17Z", "categories": ["cs.CL", "cs.HC"], "pdf_url": "https://arxiv.org/pdf/1804.03209v1", "landing_url": "https://arxiv.org/abs/1804.03209v1", "doi": "https://doi.org/10.48550/arXiv.1804.03209"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a dataset for keyword spotting without any discussion of discrete audio tokens, tokenization, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a dataset for keyword spotting without any discussion of discrete audio tokens, tokenization, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a dataset for limited-vocabulary speech recognition focusing on keyword spotting, discussing data collection and baselines but does not describe any discrete audio tokenization methods, neural audio codec usage, or discretization processes that produce discrete token sequences for downstream modeling as required by the inclusion criteria. It mostly concerns continuous audio data and classification tasks without addressing any novel or analyzed discrete token design or application for generative or cross-modal modeling, and no clear tokenization or codebook details are given.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a dataset for limited-vocabulary speech recognition focusing on keyword spotting, discussing data collection and baselines but does not describe any discrete audio tokenization methods, neural audio codec usage, or discretization processes that produce discrete token sequences for downstream modeling as required by the inclusion criteria. It mostly concerns continuous audio data and classification tasks without addressing any novel or analyzed discrete token design or application for generative or cross-modal modeling, and no clear tokenization or codebook details are given.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "tacotron2", "query_title": "{Natural {TTS} Synthesis by Conditioning {WaveNet} on {Mel} Spectrogram Predictions}", "normalized_title": "natural tts synthesis by conditioning wavenet on mel spectrogram predictions", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "source": "arxiv", "source_id": "1712.05884v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions", "abstract": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "metadata": {"arxiv_id": "1712.05884v2", "title": "Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions", "summary": "This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of $4.53$ comparable to a MOS of $4.58$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and $F_0$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.", "authors": ["Jonathan Shen", "Ruoming Pang", "Ron J. Weiss", "Mike Schuster", "Navdeep Jaitly", "Zongheng Yang", "Zhifeng Chen", "Yu Zhang", "Yuxuan Wang", "RJ Skerry-Ryan", "Rif A. Saurous", "Yannis Agiomyrgiannakis", "Yonghui Wu"], "published": "2017-12-16T00:51:40Z", "updated": "2018-02-16T01:28:23Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1712.05884v2", "landing_url": "https://arxiv.org/abs/1712.05884v2", "doi": "https://doi.org/10.48550/arXiv.1712.05884"}, "round-A_JuniorNano_output": {"reasoning": "The study describes continuous-signal TTS using mel spectrograms and a WaveNet vocoder; it does not involve discrete audio tokens, tokenization, or token-based modeling, thus it fails the discrete audio token inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The study describes continuous-signal TTS using mel spectrograms and a WaveNet vocoder; it does not involve discrete audio tokens, tokenization, or token-based modeling, thus it fails the discrete audio token inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents Tacotron 2, a TTS system converting text to speech via mel spectrograms and WaveNet vocoder, but does not describe any discrete audio tokenization processes such as vector quantization or discrete token sequence generation; it focuses on continuous acoustic feature prediction and waveform synthesis without discrete token modeling or related analyses, thus not meeting the inclusion criteria concerning discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents Tacotron 2, a TTS system converting text to speech via mel spectrograms and WaveNet vocoder, but does not describe any discrete audio tokenization processes such as vector quantization or discrete token sequence generation; it focuses on continuous acoustic feature prediction and waveform synthesis without discrete token modeling or related analyses, thus not meeting the inclusion criteria concerning discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "guided-attention", "query_title": "{Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention}", "normalized_title": "efficiently trainable text to speech system based on deep convolutional networks with guided attention", "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention", "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks. Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.", "source": "arxiv", "source_id": "1710.08969v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention", "abstract": "This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks. Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.", "metadata": {"arxiv_id": "1710.08969v2", "title": "Efficiently Trainable Text-to-Speech System Based on Deep Convolutional Networks with Guided Attention", "summary": "This paper describes a novel text-to-speech (TTS) technique based on deep convolutional neural networks (CNN), without use of any recurrent units. Recurrent neural networks (RNN) have become a standard technique to model sequential data recently, and this technique has been used in some cutting-edge neural TTS techniques. However, training RNN components often requires a very powerful computer, or a very long time, typically several days or weeks. Recent other studies, on the other hand, have shown that CNN-based sequence synthesis can be much faster than RNN-based techniques, because of high parallelizability. The objective of this paper is to show that an alternative neural TTS based only on CNN alleviate these economic costs of training. In our experiment, the proposed Deep Convolutional TTS was sufficiently trained overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while the quality of the synthesized speech was almost acceptable.", "authors": ["Hideyuki Tachibana", "Katsuya Uenoyama", "Shunsuke Aihara"], "published": "2017-10-24T19:56:32Z", "updated": "2020-09-30T05:41:53Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1710.08969v2", "landing_url": "https://arxiv.org/abs/1710.08969v2", "doi": "https://doi.org/10.1109/ICASSP.2018.8461829"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes CNN-based TTS training efficiency without discrete audio tokens or tokenization, failing to address discrete token design or token-based modeling required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes CNN-based TTS training efficiency without discrete audio tokens or tokenization, failing to address discrete token design or token-based modeling required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study describes a text-to-speech system based on deep convolutional neural networks and does not mention the use of discrete audio tokens or any discretization process mapping continuous audio into token sequences, nor does it detail tokenizer/codec architectures or quantization methods; therefore, it does not meet the inclusion criteria focusing on discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study describes a text-to-speech system based on deep convolutional neural networks and does not mention the use of discrete audio tokens or any discretization process mapping continuous audio into token sequences, nor does it detail tokenizer/codec architectures or quantization methods; therefore, it does not meet the inclusion criteria focusing on discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "levenshtein", "query_title": "{Binary Codes Capable of Correcting Deletions, Insertions and Reversals}", "normalized_title": "binary codes capable of correcting deletions insertions and reversals", "title": "Binary codes capable of correcting deletions, insertions, and reversals", "abstract": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.", "source": "semantic_scholar", "source_id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Binary codes capable of correcting deletions, insertions, and reversals", "abstract": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.", "metadata": {"arxiv_id": "semantic_scholar:b2f8876482c97e804bb50a5e2433881ae31d0cdd", "title": "Binary codes capable of correcting deletions, insertions, and reversals", "summary": "We investigate unsupervised techniques for acquiring monolingual sentence-level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web-based news sources. Two techniques are employed: (1) simple string edit distance, and (2) a heuristic strategy that pairs initial (presumably summary) sentences from different news stories in the same cluster. We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation. Results show that edit distance data is cleaner and more easily-aligned than the heuristic data, with an overall alignment error rate (AER) of 11.58% on a similarly-extracted test set. On test data extracted by the heuristic strategy, however, performance of the two training sets is similar, with AERs of 13.2% and 14.7% respectively. Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase. The summary sentences, while less readily alignable, retain more of the non-trivial alternations that are of greatest interest learning paraphrase relationships.", "source": "semantic_scholar", "source_id": "b2f8876482c97e804bb50a5e2433881ae31d0cdd", "year": 1965}, "round-A_JuniorNano_output": {"reasoning": "The item concerns text paraphrase data and edit-distance based alignment, with no discrete audio tokenization, codec, or token design discussion, so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item concerns text paraphrase data and edit-distance based alignment, with no discrete audio tokenization, codec, or token design discussion, so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract focus on unsupervised techniques for monolingual sentence-level paraphrase acquisition in text data, with no mention of audio discretization, neural audio codecs, or discrete tokens from continuous audio waveforms, which are core inclusion criteria. The study does not discuss any discrete audio token creation, codec architecture, quantization, or multi-codebook design, nor does it involve audio generation, understanding, or cross-modal modeling. Therefore, it fails all the key inclusion criteria and meets exclusion criteria of being text-based with no audio token discretization relevance.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract focus on unsupervised techniques for monolingual sentence-level paraphrase acquisition in text data, with no mention of audio discretization, neural audio codecs, or discrete tokens from continuous audio waveforms, which are core inclusion criteria. The study does not discuss any discrete audio token creation, codec architecture, quantization, or multi-codebook design, nor does it involve audio generation, understanding, or cross-modal modeling. Therefore, it fails all the key inclusion criteria and meets exclusion criteria of being text-based with no audio token discretization relevance.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "dwer", "query_title": "{Alternating Between Spectral and Spatial Estimation for Speech Separation and Enhancement}", "normalized_title": "alternating between spectral and spatial estimation for speech separation and enhancement", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "source": "arxiv", "source_id": "1911.07953", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "abstract": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "metadata": {"arxiv_id": "1911.07953v3", "title": "Sequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement", "summary": "This work introduces sequential neural beamforming, which alternates between neural network based spectral separation and beamforming based spatial separation. Our neural networks for separation use an advanced convolutional architecture trained with a novel stabilized signal-to-noise ratio loss function. For beamforming, we explore multiple ways of computing time-varying covariance matrices, including factorizing the spatial covariance into a time-varying amplitude component and a time-invariant spatial component, as well as using block-based techniques. In addition, we introduce a multi-frame beamforming method which improves the results significantly by adding contextual frames to the beamforming formulations. We extensively evaluate and analyze the effects of window size, block size, and multi-frame context size for these methods. Our best method utilizes a sequence of three neural separation and multi-frame time-invariant spatial beamforming stages, and demonstrates an average improvement of 2.75 dB in scale-invariant signal-to-noise ratio and 14.2% absolute reduction in a comparative speech recognition metric across four challenging reverberant speech enhancement and separation tasks. We also use our three-speaker separation model to separate real recordings in the LibriCSS evaluation set into non-overlapping tracks, and achieve a better word error rate as compared to a baseline mask based beamformer.", "authors": ["Zhong-Qiu Wang", "Hakan Erdogan", "Scott Wisdom", "Kevin Wilson", "Desh Raj", "Shinji Watanabe", "Zhuo Chen", "John R. Hershey"], "published": "2019-11-18T20:59:03Z", "updated": "2020-11-03T19:35:49Z", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1911.07953v3", "landing_url": "https://arxiv.org/abs/1911.07953v3", "doi": "https://doi.org/10.48550/arXiv.1911.07953"}, "round-A_JuniorNano_output": {"reasoning": "No discrete audio tokens or tokenization are discussed; the study is about neural beamforming for speech separation, so it does not meet the discrete audio tokens inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "No discrete audio tokens or tokenization are discussed; the study is about neural beamforming for speech separation, so it does not meet the discrete audio tokens inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on neural beamforming methods for speech separation and enhancement without discussing or proposing any discretization process to convert continuous audio into discrete tokens; it neither mentions neural audio codecs nor discrete token vocabularies or tokenization strategies essential to the inclusion criteria related to 'Discrete Audio Tokens'. Therefore, it does not meet the inclusion criteria of using discrete audio token sequences or token generation mechanisms.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on neural beamforming methods for speech separation and enhancement without discussing or proposing any discretization process to convert continuous audio into discrete tokens; it neither mentions neural audio codecs nor discrete token vocabularies or tokenization strategies essential to the inclusion criteria related to 'Discrete Audio Tokens'. Therefore, it does not meet the inclusion criteria of using discrete audio token sequences or token generation mechanisms.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "abouelenin2025phi", "query_title": "{Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras}", "normalized_title": "phi 4 mini technical report compact yet powerful multimodal language models via mixture of loras", "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "source": "arxiv", "source_id": "2503.01743v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "abstract": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "metadata": {"arxiv_id": "2503.01743v2", "title": "Phi-4-Mini Technical Report: Compact yet Powerful Multimodal Language Models via Mixture-of-LoRAs", "summary": "We introduce Phi-4-Mini and Phi-4-Multimodal, compact yet highly capable language and multimodal models. Phi-4-Mini is a 3.8-billion-parameter language model trained on high-quality web and synthetic data, significantly outperforming recent open-source models of similar size and matching the performance of models twice its size on math and coding tasks requiring complex reasoning. This achievement is driven by a carefully curated synthetic data recipe emphasizing high-quality math and coding datasets. Compared to its predecessor, Phi-3.5-Mini, Phi-4-Mini features an expanded vocabulary size of 200K tokens to better support multilingual applications, as well as group query attention for more efficient long-sequence generation. Phi-4-Multimodal is a multimodal model that integrates text, vision, and speech/audio input modalities into a single model. Its novel modality extension approach leverages LoRA adapters and modality-specific routers to allow multiple inference modes combining various modalities without interference. For example, it now ranks first in the OpenASR leaderboard to date, although the LoRA component of the speech/audio modality has just 460 million parameters. Phi-4-Multimodal supports scenarios involving (vision + language), (vision + speech), and (speech/audio) inputs, outperforming larger vision-language and speech-language models on a wide range of tasks. Additionally, we experiment to further train Phi-4-Mini to enhance its reasoning capabilities. Despite its compact 3.8-billion-parameter size, this experimental version achieves reasoning performance on par with or surpassing significantly larger models, including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.", "authors": ["Microsoft", ":", "Abdelrahman Abouelenin", "Atabak Ashfaq", "Adam Atkinson", "Hany Awadalla", "Nguyen Bach", "Jianmin Bao", "Alon Benhaim", "Martin Cai", "Vishrav Chaudhary", "Congcong Chen", "Dong Chen", "Dongdong Chen", "Junkun Chen", "Weizhu Chen", "Yen-Chun Chen", "Yi-ling Chen", "Qi Dai", "Xiyang Dai", "Ruchao Fan", "Mei Gao", "Min Gao", "Amit Garg", "Abhishek Goswami", "Junheng Hao", "Amr Hendy", "Yuxuan Hu", "Xin Jin", "Mahmoud Khademi", "Dongwoo Kim", "Young Jin Kim", "Gina Lee", "Jinyu Li", "Yunsheng Li", "Chen Liang", "Xihui Lin", "Zeqi Lin", "Mengchen Liu", "Yang Liu", "Gilsinia Lopez", "Chong Luo", "Piyush Madan", "Vadim Mazalov", "Arindam Mitra", "Ali Mousavi", "Anh Nguyen", "Jing Pan", "Daniel Perez-Becker", "Jacob Platin", "Thomas Portet", "Kai Qiu", "Bo Ren", "Liliang Ren", "Sambuddha Roy", "Ning Shang", "Yelong Shen", "Saksham Singhal", "Subhojit Som", "Xia Song", "Tetyana Sych", "Praneetha Vaddamanu", "Shuohang Wang", "Yiming Wang", "Zhenghao Wang", "Haibin Wu", "Haoran Xu", "Weijian Xu", "Yifan Yang", "Ziyi Yang", "Donghan Yu", "Ishmam Zabir", "Jianwen Zhang", "Li Lyna Zhang", "Yunan Zhang", "Xiren Zhou"], "published": "2025-03-03T17:05:52Z", "updated": "2025-03-07T09:05:58Z", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2503.01743v2", "landing_url": "https://arxiv.org/abs/2503.01743v2", "doi": "https://doi.org/10.48550/arXiv.2503.01743"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on Phi-4-Mini/Multimodal LMs with LoRA adapters and does not discuss discrete audio tokens, tokenizers, or token-based representations required by the Discrete Audio Tokens topic, thus it does not meet inclusion criteria and may violate the exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on Phi-4-Mini/Multimodal LMs with LoRA adapters and does not discuss discrete audio tokens, tokenizers, or token-based representations required by the Discrete Audio Tokens topic, thus it does not meet inclusion criteria and may violate the exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes Phi-4-Mini and Phi-4-Multimodal models, focusing on language and multimodal inputs including text, vision, and speech/audio, but there is no clear mention or description of discrete audio tokenization mechanisms such as neural audio codecs, vector quantization, or token vocabularies derived from discretizing continuous audio waveforms; thus, it does not meet the inclusion criteria requiring discrete audio tokens as a core representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes Phi-4-Mini and Phi-4-Multimodal models, focusing on language and multimodal inputs including text, vision, and speech/audio, but there is no clear mention or description of discrete audio tokenization mechanisms such as neural audio codecs, vector quantization, or token vocabularies derived from discretizing continuous audio waveforms; thus, it does not meet the inclusion criteria requiring discrete audio tokens as a core representation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "chang2024speechprompt", "query_title": "{Speechprompt: Prompting speech language models for speech processing tasks}", "normalized_title": "speechprompt prompting speech language models for speech processing tasks", "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "source": "arxiv", "source_id": "2408.13040v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "abstract": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "metadata": {"arxiv_id": "2408.13040v1", "title": "SpeechPrompt: Prompting Speech Language Models for Speech Processing Tasks", "summary": "Prompting has become a practical method for utilizing pre-trained language models (LMs). This approach offers several advantages. It allows an LM to adapt to new tasks with minimal training and parameter updates, thus achieving efficiency in both storage and computation. Additionally, prompting modifies only the LM's inputs and harnesses the generative capabilities of language models to address various downstream tasks in a unified manner. This significantly reduces the need for human labor in designing task-specific models. These advantages become even more evident as the number of tasks served by the LM scales up. Motivated by the strengths of prompting, we are the first to explore the potential of prompting speech LMs in the domain of speech processing. Recently, there has been a growing interest in converting speech into discrete units for language modeling. Our pioneer research demonstrates that these quantized speech units are highly versatile within our unified prompting framework. Not only can they serve as class labels, but they also contain rich phonetic information that can be re-synthesized back into speech signals for speech generation tasks. Specifically, we reformulate speech processing tasks into speech-to-unit generation tasks. As a result, we can seamlessly integrate tasks such as speech classification, sequence generation, and speech generation within a single, unified prompting framework. The experiment results show that the prompting method can achieve competitive performance compared to the strong fine-tuning method based on self-supervised learning models with a similar number of trainable parameters. The prompting method also shows promising results in the few-shot setting. Moreover, with the advanced speech LMs coming into the stage, the proposed prompting framework attains great potential.", "authors": ["Kai-Wei Chang", "Haibin Wu", "Yu-Kai Wang", "Yuan-Kuei Wu", "Hua Shen", "Wei-Cheng Tseng", "Iu-thing Kang", "Shang-Wen Li", "Hung-yi Lee"], "published": "2024-08-23T13:00:10Z", "updated": "2024-08-23T13:00:10Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2408.13040v1", "landing_url": "https://arxiv.org/abs/2408.13040v1", "doi": "https://doi.org/10.1109/TASLP.2024.3436618"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract explicitly discuss discrete speech units (quantized/discretized tokens) and reformulating speech tasks as speech-to-unit generation, aligning with the discrete audio token theme; however, the abstract lacks explicit methods for token production (tokenizer/codec architecture, quantization, codebook/vocabulary) and detailed token-level evaluations, so it meets inclusion criteria but without showcasing full methodological/executable details in the abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The title/abstract explicitly discuss discrete speech units (quantized/discretized tokens) and reformulating speech tasks as speech-to-unit generation, aligning with the discrete audio token theme; however, the abstract lacks explicit methods for token production (tokenizer/codec architecture, quantization, codebook/vocabulary) and detailed token-level evaluations, so it meets inclusion criteria but without showcasing full methodological/executable details in the abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study explores prompting speech language models using quantized speech units considered as discrete tokens with phonetic information, used in a unified framework for various speech processing tasks, showing competitive or promising results. However, the abstract lacks detailed descriptions of the tokenizer or codec architecture, quantization method, vocabulary/codebook settings, or rate/dimensionality, which are necessary to verify the discrete token production process and reproducibility as per inclusion criteria. While it aligns with the concept of discrete audio tokens and uses them at the core, the insufficient detail on discrete token generation and design limits assured inclusion.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study explores prompting speech language models using quantized speech units considered as discrete tokens with phonetic information, used in a unified framework for various speech processing tasks, showing competitive or promising results. However, the abstract lacks detailed descriptions of the tokenizer or codec architecture, quantization method, vocabulary/codebook settings, or rate/dimensionality, which are necessary to verify the discrete token production process and reproducibility as per inclusion criteria. While it aligns with the concept of discrete audio tokens and uses them at the core, the insufficient detail on discrete token generation and design limits assured inclusion.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers noted the paper explicitly uses quantized/discrete speech units and reframes tasks as speech-to-unit generation, which aligns well with the “Discrete Audio Tokens” topic; however, both also flagged the abstract’s lack of explicit tokenizer/codec/quantization details (codebook/vocabulary, bitrate/time-downsampling, or tokenizer training) required by the inclusion criteria to verify discrete token generation and reproducibility. Given this, the paper matches the thematic inclusion criteria but the available abstract does not provide sufficient methodological detail to confirm it satisfies the strict reproducibility/description requirements—so a conservative judgement is appropriate until the full paper/methods are inspected.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper explicitly uses quantized/discrete speech units and reframes tasks as speech-to-unit generation, which aligns well with the “Discrete Audio Tokens” topic; however, both also flagged the abstract’s lack of explicit tokenizer/codec/quantization details (codebook/vocabulary, bitrate/time-downsampling, or tokenizer training) required by the inclusion criteria to verify discrete token generation and reproducibility. Given this, the paper matches the thematic inclusion criteria but the available abstract does not provide sufficient methodological detail to confirm it satisfies the strict reproducibility/description requirements—so a conservative judgement is appropriate until the full paper/methods are inspected.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "cp-decomposition", "query_title": "{The Expression of a Tensor or a Polyadic as a Sum of Products}", "normalized_title": "the expression of a tensor or a polyadic as a sum of products", "title": "The Expression of a Tensor or a Polyadic as a Sum of Products", "abstract": "In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.", "source": "semantic_scholar", "source_id": "5265853dd172f6afe0ff1e820a0cc7bd0e58c765", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Expression of a Tensor or a Polyadic as a Sum of Products", "abstract": "In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.", "metadata": {"arxiv_id": "semantic_scholar:5265853dd172f6afe0ff1e820a0cc7bd0e58c765", "title": "The Expression of a Tensor or a Polyadic as a Sum of Products", "summary": "In this article, we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models, as well as languages used to query and validate knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We conclude with high-level future research directions for knowledge graphs.", "source": "semantic_scholar", "source_id": "5265853dd172f6afe0ff1e820a0cc7bd0e58c765", "publication_date": "1927-04-01"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on knowledge graphs and graph query/validation, with no discrete audio token generation, tokenizers, or token-based modeling described, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on knowledge graphs and graph query/validation, with no discrete audio token generation, tokenizers, or token-based modeling described, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article focuses on knowledge graphs and graph-based data models, with no mention of discretizing continuous audio waveforms into token sequences or using neural audio codecs or self-supervised audio models to produce discrete tokens; it also lacks any discussion on audio token design, audio generation, understanding, or cross-modal modeling, and thus does not meet the inclusion criteria related to 'Discrete Audio Tokens'.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article focuses on knowledge graphs and graph-based data models, with no mention of discretizing continuous audio waveforms into token sequences or using neural audio codecs or self-supervised audio models to produce discrete tokens; it also lacks any discussion on audio token design, audio generation, understanding, or cross-modal modeling, and thus does not meet the inclusion criteria related to 'Discrete Audio Tokens'.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "dubey2024icassp", "query_title": "{{ICASSP} 2023 deep noise suppression challenge}", "normalized_title": "icassp 2023 deep noise suppression challenge", "title": "ICASSP 2023 Deep Noise Suppression Challenge", "abstract": "Deep Speech Enhancement Challenge is the 5th edition of deep noise suppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand Challenges. DNS challenges were organized during 2019-2023 to stimulate research in deep speech enhancement (DSE). Previous DNS challenges were organized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022. From prior editions, we learnt that improving signal quality (SIG) is challenging particularly in presence of simultaneously active interfering talkers and noise. This challenge aims to develop models for joint denosing, dereverberation and suppression of interfering talkers. When primary talker wears a headphone, certain acoustic properties of their speech such as direct-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it possible to suppress neighboring talkers even without enrollment data for primary talker. This motivated us to create two tracks for this challenge: (i) Track-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz) training data and testset, and each testclips has a corresponding enrollment data (10-30s duration) for primary talker. Each track invited submissions of personalized and non-personalized models all of which are evaluated through same subjective evaluation. Most models submitted to challenge were personalized models, same team is winner in both tracks where the best models has improvement of 0.145 and 0.141 in challenge's Score as compared to noisy blind testset.", "source": "arxiv", "source_id": "2303.11510v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ICASSP 2023 Deep Noise Suppression Challenge", "abstract": "Deep Speech Enhancement Challenge is the 5th edition of deep noise suppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand Challenges. DNS challenges were organized during 2019-2023 to stimulate research in deep speech enhancement (DSE). Previous DNS challenges were organized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022. From prior editions, we learnt that improving signal quality (SIG) is challenging particularly in presence of simultaneously active interfering talkers and noise. This challenge aims to develop models for joint denosing, dereverberation and suppression of interfering talkers. When primary talker wears a headphone, certain acoustic properties of their speech such as direct-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it possible to suppress neighboring talkers even without enrollment data for primary talker. This motivated us to create two tracks for this challenge: (i) Track-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz) training data and testset, and each testclips has a corresponding enrollment data (10-30s duration) for primary talker. Each track invited submissions of personalized and non-personalized models all of which are evaluated through same subjective evaluation. Most models submitted to challenge were personalized models, same team is winner in both tracks where the best models has improvement of 0.145 and 0.141 in challenge's Score as compared to noisy blind testset.", "metadata": {"arxiv_id": "2303.11510v2", "title": "ICASSP 2023 Deep Noise Suppression Challenge", "summary": "Deep Speech Enhancement Challenge is the 5th edition of deep noise suppression (DNS) challenges organized at ICASSP 2023 Signal Processing Grand Challenges. DNS challenges were organized during 2019-2023 to stimulate research in deep speech enhancement (DSE). Previous DNS challenges were organized at INTERSPEECH 2020, ICASSP 2021, INTERSPEECH 2021, and ICASSP 2022. From prior editions, we learnt that improving signal quality (SIG) is challenging particularly in presence of simultaneously active interfering talkers and noise. This challenge aims to develop models for joint denosing, dereverberation and suppression of interfering talkers. When primary talker wears a headphone, certain acoustic properties of their speech such as direct-to-reverberation (DRR), signal to noise ratio (SNR) etc. make it possible to suppress neighboring talkers even without enrollment data for primary talker. This motivated us to create two tracks for this challenge: (i) Track-1 Headset; (ii) Track-2 Speakerphone. Both tracks has fullband (48kHz) training data and testset, and each testclips has a corresponding enrollment data (10-30s duration) for primary talker. Each track invited submissions of personalized and non-personalized models all of which are evaluated through same subjective evaluation. Most models submitted to challenge were personalized models, same team is winner in both tracks where the best models has improvement of 0.145 and 0.141 in challenge's Score as compared to noisy blind testset.", "authors": ["Harishchandra Dubey", "Ashkan Aazami", "Vishak Gopal", "Babak Naderi", "Sebastian Braun", "Ross Cutler", "Alex Ju", "Mehdi Zohourian", "Min Tang", "Hannes Gamper", "Mehrsa Golestaneh", "Robert Aichner"], "published": "2023-03-21T00:11:47Z", "updated": "2023-05-09T00:26:43Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2303.11510v2", "landing_url": "https://arxiv.org/abs/2303.11510v2", "doi": "https://doi.org/10.48550/arXiv.2303.11510"}, "round-A_JuniorNano_output": {"reasoning": "This item describes the ICASSP 2023 Deep Noise Suppression Challenge focusing on denoising/dereverberation and personalized speech models; it does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item describes the ICASSP 2023 Deep Noise Suppression Challenge focusing on denoising/dereverberation and personalized speech models; it does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe a deep noise suppression challenge focused on speech enhancement models, with no indication of generating or using discrete audio tokens through neural audio codecs or self-supervised models to transform continuous audio into discrete token sequences; the work centers on audio denoising and dereverberation rather than tokenization or discrete representation, thus it does not meet any inclusion criteria related to discrete audio tokens nor the detailed English tokenization process required.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe a deep noise suppression challenge focused on speech enhancement models, with no indication of generating or using discrete audio tokens through neural audio codecs or self-supervised models to transform continuous audio into discrete token sequences; the work centers on audio denoising and dereverberation rather than tokenization or discrete representation, thus it does not meet any inclusion criteria related to discrete audio tokens nor the detailed English tokenization process required.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "gemmeke2017audio", "query_title": "{{Audio Set}: An ontology and human-labeled dataset for audio events}", "normalized_title": "audio set an ontology and human labeled dataset for audio events", "title": "Audio Set: An ontology and human-labeled dataset for audio events", "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.", "source": "semantic_scholar", "source_id": "5ba2218b708ca64ab556e39d5997202e012717d5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Audio Set: An ontology and human-labeled dataset for audio events", "abstract": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.", "metadata": {"arxiv_id": "semantic_scholar:5ba2218b708ca64ab556e39d5997202e012717d5", "title": "Audio Set: An ontology and human-labeled dataset for audio events", "summary": "Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.", "source": "semantic_scholar", "source_id": "5ba2218b708ca64ab556e39d5997202e012717d5", "publication_date": "2017-03-05"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a large-scale audio event dataset with a hierarchical ontology; it does not discuss discrete audio tokens, tokenization, or token-based modeling, thus not meeting the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a large-scale audio event dataset with a hierarchical ontology; it does not discuss discrete audio tokens, tokenization, or token-based modeling, thus not meeting the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces Audio Set, a large-scale human-labeled dataset for audio event recognition, focusing on labeling audio classes within YouTube segments but does not describe any method of discretizing audio waveforms into discrete tokens or using neural audio codecs/vector quantization, nor does it discuss token designs or their impacts on audio generation or modeling; it mainly supports audio event classification, which based on the criteria is a discriminative task without core discrete token modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces Audio Set, a large-scale human-labeled dataset for audio event recognition, focusing on labeling audio classes within YouTube segments but does not describe any method of discretizing audio waveforms into discrete tokens or using neural audio codecs/vector quantization, nor does it discuss token designs or their impacts on audio generation or modeling; it mainly supports audio event classification, which based on the criteria is a discriminative task without core discrete token modeling.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "fonseca2021fsd50k", "query_title": "{{FSD50K}: an open dataset of human-labeled sound events}", "normalized_title": "fsd50k an open dataset of human labeled sound events", "title": "FSD50K: An Open Dataset of Human-Labeled Sound Events", "abstract": "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.", "source": "arxiv", "source_id": "2010.00475v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FSD50K: An Open Dataset of Human-Labeled Sound Events", "abstract": "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.", "metadata": {"arxiv_id": "2010.00475v2", "title": "FSD50K: An Open Dataset of Human-Labeled Sound Events", "summary": "Most existing datasets for sound event recognition (SER) are relatively small and/or domain-specific, with the exception of AudioSet, based on over 2M tracks from YouTube videos and encompassing over 500 sound classes. However, AudioSet is not an open dataset as its official release consists of pre-computed audio features. Downloading the original audio tracks can be problematic due to YouTube videos gradually disappearing and usage rights issues. To provide an alternative benchmark dataset and thus foster SER research, we introduce FSD50K, an open dataset containing over 51k audio clips totalling over 100h of audio manually labeled using 200 classes drawn from the AudioSet Ontology. The audio clips are licensed under Creative Commons licenses, making the dataset freely distributable (including waveforms). We provide a detailed description of the FSD50K creation process, tailored to the particularities of Freesound data, including challenges encountered and solutions adopted. We include a comprehensive dataset characterization along with discussion of limitations and key factors to allow its audio-informed usage. Finally, we conduct sound event classification experiments to provide baseline systems as well as insight on the main factors to consider when splitting Freesound audio data for SER. Our goal is to develop a dataset to be widely adopted by the community as a new open benchmark for SER research.", "authors": ["Eduardo Fonseca", "Xavier Favory", "Jordi Pons", "Frederic Font", "Xavier Serra"], "published": "2020-10-01T15:07:25Z", "updated": "2022-04-23T20:12:00Z", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2010.00475v2", "landing_url": "https://arxiv.org/abs/2010.00475v2", "doi": "https://doi.org/10.48550/arXiv.2010.00475"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes FSD50K as an open dataset for sound event recognition; it does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet inclusion criteria and should be excluded per the criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper describes FSD50K as an open dataset for sound event recognition; it does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet inclusion criteria and should be excluded per the criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces FSD50K, an open dataset for sound event recognition focusing on human-labeled audio event classification rather than generating or using discrete audio tokens derived from neural audio codec or self-supervised learning models. It primarily addresses dataset construction and classification baseline experiments, without describing discrete token generation, tokenizer architecture, quantization, or vocabulary/codebook design. The focus is on traditional classification tasks, which falls under exclusion criteria related to lacking core use or analysis of discrete audio tokens, and no description of discrete token settings or usage was presented, hence it fails to meet the inclusion criteria for the \"Discrete Audio Tokens\" topic.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces FSD50K, an open dataset for sound event recognition focusing on human-labeled audio event classification rather than generating or using discrete audio tokens derived from neural audio codec or self-supervised learning models. It primarily addresses dataset construction and classification baseline experiments, without describing discrete token generation, tokenizer architecture, quantization, or vocabulary/codebook design. The focus is on traditional classification tasks, which falls under exclusion criteria related to lacking core use or analysis of discrete audio tokens, and no description of discrete token settings or usage was presented, hence it fails to meet the inclusion criteria for the \"Discrete Audio Tokens\" topic.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "bogdanov2019mtg", "query_title": "{The {MTG-Jamendo} dataset for automatic music tagging}", "normalized_title": "the mtg jamendo dataset for automatic music tagging", "title": "The MTG-Jamendo Dataset for Automatic Music Tagging", "abstract": "Comunicació presentada a: ML4MD Machine Learning for Music Discovery Workshop del congrés ICML2019 celebrat el 15 de juny de 2019 a Long Beach, California.", "source": "semantic_scholar", "source_id": "23037085b0815455e6d47333089b925c8c0e21d5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The MTG-Jamendo Dataset for Automatic Music Tagging", "abstract": "Comunicació presentada a: ML4MD Machine Learning for Music Discovery Workshop del congrés ICML2019 celebrat el 15 de juny de 2019 a Long Beach, California.", "metadata": {"arxiv_id": "semantic_scholar:23037085b0815455e6d47333089b925c8c0e21d5", "title": "The MTG-Jamendo Dataset for Automatic Music Tagging", "summary": "Comunicació presentada a: ML4MD Machine Learning for Music Discovery Workshop del congrés ICML2019 celebrat el 15 de juny de 2019 a Long Beach, California.", "source": "semantic_scholar", "source_id": "23037085b0815455e6d47333089b925c8c0e21d5", "year": 2019}, "round-A_JuniorNano_output": {"reasoning": "The abstract only notes a workshop presentation about a dataset for automatic music tagging and contains no description of discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract only notes a workshop presentation about a dataset for automatic music tagging and contains no description of discrete audio tokens or tokenization methods, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title 'The MTG-Jamendo Dataset for Automatic Music Tagging' and the abstract only mention the presentation at a music discovery workshop, without any indication of discrete audio tokens, neural audio codecs, tokenizer details, or any discrete token-based modeling or generation. There is no evidence that the study involves the discretization of audio waveform into token sequences or discusses token design choices relevant to the given inclusion criteria. Hence, it lacks sufficient information to meet the inclusion criteria and likely falls under exclusion due to missing essential details about discrete tokens and their usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title 'The MTG-Jamendo Dataset for Automatic Music Tagging' and the abstract only mention the presentation at a music discovery workshop, without any indication of discrete audio tokens, neural audio codecs, tokenizer details, or any discrete token-based modeling or generation. There is no evidence that the study involves the discretization of audio waveform into token sequences or discusses token design choices relevant to the given inclusion criteria. Hence, it lacks sufficient information to meet the inclusion criteria and likely falls under exclusion due to missing essential details about discrete tokens and their usage.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mysore2014can", "query_title": "{Can we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?—{A} dataset, insights, and challenges}", "normalized_title": "can we automatically transform speech recorded on common consumer devices in real world environments into professional production quality speech a dataset insights and challenges", "title": "Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges", "abstract": "The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.", "source": "semantic_scholar", "source_id": "5aa5d627f4741eaeec41d59efe6589e71a3080c6", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges", "abstract": "The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.", "metadata": {"arxiv_id": "semantic_scholar:5aa5d627f4741eaeec41d59efe6589e71a3080c6", "title": "Can we Automatically Transform Speech Recorded on Common Consumer Devices in Real-World Environments into Professional Production Quality Speech?—A Dataset, Insights, and Challenges", "summary": "The goal of speech enhancement is typically to recover clean speech from noisy, reverberant, and often bandlimited speech in order to yield improved intelligibility, clarity, or automatic speech recognition performance. However, the acoustic goal for a great deal of speech content such as voice overs, podcasts, demo videos, lecture videos, and audio stories is often not merely clean speech, but speech that is aesthetically pleasing. This is achieved in professional recording studios by having a skilled sound engineer record clean speech in an acoustically treated room and then edit and process it with audio effects (which we refer to as production). A growing amount of speech content is being recorded on common consumer devices such as tablets, smartphones, and laptops. Moreover, it is typically recorded in common but non-acoustically treated environments such as homes and offices. We argue that the goal of enhancing such recordings should not only be to make it sound cleaner as would be done using traditional speech enhancement techniques, but to make it sound like it was recorded and produced in a professional recording studio. In this paper, we show why this can be beneficial, describe a new data set (a great deal of which was recorded in a professional recording studio) that we prepared to help in developing algorithms for this purpose, and discuss some insights and challenges associated with this problem.", "source": "semantic_scholar", "source_id": "5aa5d627f4741eaeec41d59efe6589e71a3080c6", "publication_date": "2015-08-01"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on transforming consumer-device recordings into studio-quality speech and provides a dataset, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based modeling as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on transforming consumer-device recordings into studio-quality speech and provides a dataset, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based modeling as required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on speech enhancement to make recordings from consumer devices sound like professional production quality. It does not mention discrete audio tokens, discretization of audio waveforms, neural audio codecs, or the generation and use of token sequences for modeling or generation. It lacks clear details about tokenizer or codec architectures, quantization methods, or token vocabulary, and does not address tasks involving discrete audio token design, analysis, or application. Thus, it does not meet the inclusion criteria focusing on discrete audio tokens and appears to fall under exclusion for continuous audio representation without discrete tokenization.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on speech enhancement to make recordings from consumer devices sound like professional production quality. It does not mention discrete audio tokens, discretization of audio waveforms, neural audio codecs, or the generation and use of token sequences for modeling or generation. It lacks clear details about tokenizer or codec architectures, quantization methods, or token vocabulary, and does not address tasks involving discrete audio token design, analysis, or application. Thus, it does not meet the inclusion criteria focusing on discrete audio tokens and appears to fall under exclusion for continuous audio representation without discrete tokenization.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "vctk2017", "query_title": "{{CSTR VCTK Corpus}: English Multi-speaker Corpus for {CSTR} Voice Cloning Toolkit (version 0.92)}", "normalized_title": "cstr vctk corpus english multi speaker corpus for cstr voice cloning toolkit version 0 92", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "metadata": {"arxiv_id": "semantic_scholar:df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "summary": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "publication_date": "2019-11-13"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a multi-speaker English speech corpus for TTS (no discussion of discrete audio tokens or tokenization), so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a multi-speaker English speech corpus for TTS (no discussion of discrete audio tokens or tokenization), so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study describes a multi-speaker speech corpus aimed at TTS synthesis, including data collection and recording details, but does not mention the creation or use of discrete audio tokens, tokenization methods, neural audio codecs, or vector quantization processes required by the inclusion criteria. It primarily focuses on continuous speech data collection rather than discrete token-based audio representation relevant for modeling and generation, and lacks any discrete token design, evaluation, or application as per the criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study describes a multi-speaker speech corpus aimed at TTS synthesis, including data collection and recording details, but does not mention the creation or use of discrete audio tokens, tokenization methods, neural audio codecs, or vector quantization processes required by the inclusion criteria. It primarily focuses on continuous speech data collection rather than discrete token-based audio representation relevant for modeling and generation, and lacks any discrete token design, evaluation, or application as per the criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "reddy2022dnsmos", "query_title": "{{DNSMOS P.835}: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors}", "normalized_title": "dnsmos p 835 a non intrusive perceptual objective speech quality metric to evaluate noise suppressors", "title": "DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors", "abstract": "Human subjective evaluation is the gold standard to evaluate speech quality optimized for human perception. Perceptual objective metrics serve as a proxy for subjective scores. We have recently developed a non-intrusive speech quality metric called Deep Noise Suppression Mean Opinion Score (DNSMOS) using the scores from ITU-T Rec. P.808 subjective evaluation. The P.808 scores reflect the overall quality of the audio clip. ITU-T Rec. P.835 subjective evaluation framework gives the standalone quality scores of speech and background noise in addition to the overall quality. In this work, we train an objective metric based on P.835 human ratings that outputs 3 scores: i) speech quality (SIG), ii) background noise quality (BAK), and iii) the overall quality (OVRL) of the audio. The developed metric is highly correlated with human ratings, with a Pearson's Correlation Coefficient (PCC)=0.94 for SIG and PCC=0.98 for BAK and OVRL. This is the first non-intrusive P.835 predictor we are aware of. DNSMOS P.835 is made publicly available as an Azure service.", "source": "arxiv", "source_id": "2110.01763v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors", "abstract": "Human subjective evaluation is the gold standard to evaluate speech quality optimized for human perception. Perceptual objective metrics serve as a proxy for subjective scores. We have recently developed a non-intrusive speech quality metric called Deep Noise Suppression Mean Opinion Score (DNSMOS) using the scores from ITU-T Rec. P.808 subjective evaluation. The P.808 scores reflect the overall quality of the audio clip. ITU-T Rec. P.835 subjective evaluation framework gives the standalone quality scores of speech and background noise in addition to the overall quality. In this work, we train an objective metric based on P.835 human ratings that outputs 3 scores: i) speech quality (SIG), ii) background noise quality (BAK), and iii) the overall quality (OVRL) of the audio. The developed metric is highly correlated with human ratings, with a Pearson's Correlation Coefficient (PCC)=0.94 for SIG and PCC=0.98 for BAK and OVRL. This is the first non-intrusive P.835 predictor we are aware of. DNSMOS P.835 is made publicly available as an Azure service.", "metadata": {"arxiv_id": "2110.01763v4", "title": "DNSMOS P.835: A Non-Intrusive Perceptual Objective Speech Quality Metric to Evaluate Noise Suppressors", "summary": "Human subjective evaluation is the gold standard to evaluate speech quality optimized for human perception. Perceptual objective metrics serve as a proxy for subjective scores. We have recently developed a non-intrusive speech quality metric called Deep Noise Suppression Mean Opinion Score (DNSMOS) using the scores from ITU-T Rec. P.808 subjective evaluation. The P.808 scores reflect the overall quality of the audio clip. ITU-T Rec. P.835 subjective evaluation framework gives the standalone quality scores of speech and background noise in addition to the overall quality. In this work, we train an objective metric based on P.835 human ratings that outputs 3 scores: i) speech quality (SIG), ii) background noise quality (BAK), and iii) the overall quality (OVRL) of the audio. The developed metric is highly correlated with human ratings, with a Pearson's Correlation Coefficient (PCC)=0.94 for SIG and PCC=0.98 for BAK and OVRL. This is the first non-intrusive P.835 predictor we are aware of. DNSMOS P.835 is made publicly available as an Azure service.", "authors": ["Chandan K A Reddy", "Vishak Gopal", "Ross Cutler"], "published": "2021-10-05T00:42:13Z", "updated": "2022-02-04T06:56:16Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2110.01763v4", "landing_url": "https://arxiv.org/abs/2110.01763v4", "doi": "https://doi.org/10.48550/arXiv.2110.01763"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses a non-intrusive speech quality metric (DNSMOS P.835) and does not involve discrete audio token generation, tokenizers, or token-based modeling, so it does not satisfy the inclusion criteria focused on discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses a non-intrusive speech quality metric (DNSMOS P.835) and does not involve discrete audio token generation, tokenizers, or token-based modeling, so it does not satisfy the inclusion criteria focused on discrete audio tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents a non-intrusive speech quality metric based on human ratings but does not discuss any discretization of audio into token sequences, neural audio codecs, or discrete token design and usage; therefore, it does not fit the inclusion criteria focused on discrete audio token representations nor their impact or use in generative or cross-modal tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents a non-intrusive speech quality metric based on human ratings but does not discuss any discretization of audio into token sequences, neural audio codecs, or discrete token design and usage; therefore, it does not fit the inclusion criteria focused on discrete audio token representations nor their impact or use in generative or cross-modal tasks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "valentinibotinhao2016voicebank", "query_title": "{Investigating {RNN}-based speech enhancement methods for noise-robust Text-to-Speech}", "normalized_title": "investigating rnn based speech enhancement methods for noise robust text to speech", "title": "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech", "abstract": "The quality of text-to-speech (TTS) voices built from noisy speech is compromised. Enhancing the speech data before training has been shown to improve quality but voices built with clean speech are still preferred. In this paper we investigate two different approaches for speech enhancement to train TTS systems. In both approaches we train a recursive neural network (RNN) to map acoustic features extracted from noisy speech to features describing clean speech. The enhanced data is then used to train the TTS acoustic model. In one approach we use the features conventionally employed to train TTS acoustic models, i.e Mel cepstral (MCEP) coefficients, aperiodicity values and fundamental frequency (F0). In the other approach, following conventional speech enhancement methods, we train an RNN using only the MCEP coefficients extracted from the magnitude spectrum. The enhanced MCEP features and the phase extracted from noisy speech are combined to reconstruct the waveform which is then used to extract acoustic features to train the TTS system. We show that the second approach results in larger MCEP distortion but smaller F0 errors. Subjective evaluation shows that synthetic voices trained with data enhanced with this method were rated higher and with similar to scores to voices trained with clean speech.", "source": "semantic_scholar", "source_id": "0764300b1ba8c29e25e748c1df2851feba8ea1b6", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech", "abstract": "The quality of text-to-speech (TTS) voices built from noisy speech is compromised. Enhancing the speech data before training has been shown to improve quality but voices built with clean speech are still preferred. In this paper we investigate two different approaches for speech enhancement to train TTS systems. In both approaches we train a recursive neural network (RNN) to map acoustic features extracted from noisy speech to features describing clean speech. The enhanced data is then used to train the TTS acoustic model. In one approach we use the features conventionally employed to train TTS acoustic models, i.e Mel cepstral (MCEP) coefficients, aperiodicity values and fundamental frequency (F0). In the other approach, following conventional speech enhancement methods, we train an RNN using only the MCEP coefficients extracted from the magnitude spectrum. The enhanced MCEP features and the phase extracted from noisy speech are combined to reconstruct the waveform which is then used to extract acoustic features to train the TTS system. We show that the second approach results in larger MCEP distortion but smaller F0 errors. Subjective evaluation shows that synthetic voices trained with data enhanced with this method were rated higher and with similar to scores to voices trained with clean speech.", "metadata": {"arxiv_id": "semantic_scholar:0764300b1ba8c29e25e748c1df2851feba8ea1b6", "title": "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech", "summary": "The quality of text-to-speech (TTS) voices built from noisy speech is compromised. Enhancing the speech data before training has been shown to improve quality but voices built with clean speech are still preferred. In this paper we investigate two different approaches for speech enhancement to train TTS systems. In both approaches we train a recursive neural network (RNN) to map acoustic features extracted from noisy speech to features describing clean speech. The enhanced data is then used to train the TTS acoustic model. In one approach we use the features conventionally employed to train TTS acoustic models, i.e Mel cepstral (MCEP) coefficients, aperiodicity values and fundamental frequency (F0). In the other approach, following conventional speech enhancement methods, we train an RNN using only the MCEP coefficients extracted from the magnitude spectrum. The enhanced MCEP features and the phase extracted from noisy speech are combined to reconstruct the waveform which is then used to extract acoustic features to train the TTS system. We show that the second approach results in larger MCEP distortion but smaller F0 errors. Subjective evaluation shows that synthetic voices trained with data enhanced with this method were rated higher and with similar to scores to voices trained with clean speech.", "source": "semantic_scholar", "source_id": "0764300b1ba8c29e25e748c1df2851feba8ea1b6", "publication_date": "2016-09-13"}, "round-A_JuniorNano_output": {"reasoning": "The paper discusses RNN-based speech enhancement for TTS using continuous acoustic features (MCEP, F0) and waveform reconstruction, with no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper discusses RNN-based speech enhancement for TTS using continuous acoustic features (MCEP, F0) and waveform reconstruction, with no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on RNN-based speech enhancement methods aimed at improving noisy speech quality for Text-to-Speech systems but does not mention any discrete audio token representation, discretization process, neural audio codec, vector quantization, or the production and use of discrete token sequences; hence it does not meet the inclusion criteria related to discrete audio tokens and their evaluation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on RNN-based speech enhancement methods aimed at improving noisy speech quality for Text-to-Speech systems but does not mention any discrete audio token representation, discretization process, neural audio codec, vector quantization, or the production and use of discrete token sequences; hence it does not meet the inclusion criteria related to discrete audio tokens and their evaluation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "cosentino2020librimix", "query_title": "{{LibriMix}: An Open-Source Dataset for Generalizable Speech Separation}", "normalized_title": "librimix an open source dataset for generalizable speech separation", "title": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation", "abstract": "In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.", "source": "arxiv", "source_id": "2005.11262v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation", "abstract": "In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.", "metadata": {"arxiv_id": "2005.11262v1", "title": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation", "summary": "In recent years, wsj0-2mix has become the reference dataset for single-channel speech separation. Most deep learning-based speech separation models today are benchmarked on it. However, recent studies have shown important performance drops when models trained on wsj0-2mix are evaluated on other, similar datasets. To address this generalization issue, we created LibriMix, an open-source alternative to wsj0-2mix, and to its noisy extension, WHAM!. Based on LibriSpeech, LibriMix consists of two- or three-speaker mixtures combined with ambient noise samples from WHAM!. Using Conv-TasNet, we achieve competitive performance on all LibriMix versions. In order to fairly evaluate across datasets, we introduce a third test set based on VCTK for speech and WHAM! for noise. Our experiments show that the generalization error is smaller for models trained with LibriMix than with WHAM!, in both clean and noisy conditions. Aiming towards evaluation in more realistic, conversation-like scenarios, we also release a sparsely overlapping version of LibriMix's test set.", "authors": ["Joris Cosentino", "Manuel Pariente", "Samuele Cornell", "Antoine Deleforge", "Emmanuel Vincent"], "published": "2020-05-22T16:26:54Z", "updated": "2020-05-22T16:26:54Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2005.11262v1", "landing_url": "https://arxiv.org/abs/2005.11262v1", "doi": "https://doi.org/10.48550/arXiv.2005.11262"}, "round-A_JuniorNano_output": {"reasoning": "The article centers on LibriMix as a dataset for generalizable speech separation and does not discuss discrete audio tokens, tokenizers, or token-based modeling; thus it fails to meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article centers on LibriMix as a dataset for generalizable speech separation and does not discuss discrete audio tokens, tokenizers, or token-based modeling; thus it fails to meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses a dataset for speech separation and evaluates deep learning models on it, but does not mention the use or generation of discrete audio tokens through neural audio codecs or discretization processes, nor does it address token design choices or downstream tasks involving discrete token sequences as defined by the inclusion criteria. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their application.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses a dataset for speech separation and evaluates deep learning models on it, but does not mention the use or generation of discrete audio tokens through neural audio codecs or discretization processes, nor does it address token design choices or downstream tasks involving discrete token sequences as defined by the inclusion criteria. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and their application.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "gulati_conformer", "query_title": "{Conformer: Convolution-augmented Transformer for Speech Recognition}", "normalized_title": "conformer convolution augmented transformer for speech recognition", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "source": "arxiv", "source_id": "2005.08100v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "metadata": {"arxiv_id": "2005.08100v1", "title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "summary": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "authors": ["Anmol Gulati", "James Qin", "Chung-Cheng Chiu", "Niki Parmar", "Yu Zhang", "Jiahui Yu", "Wei Han", "Shibo Wang", "Zhengdong Zhang", "Yonghui Wu", "Ruoming Pang"], "published": "2020-05-16T20:56:25Z", "updated": "2020-05-16T20:56:25Z", "categories": ["eess.AS", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2005.08100v1", "landing_url": "https://arxiv.org/abs/2005.08100v1", "doi": "https://doi.org/10.48550/arXiv.2005.08100"}, "round-A_JuniorNano_output": {"reasoning": "The Conformer paper targets ASR architecture (CNN+Transformer) and benchmark results; it does not discuss discrete audio tokens, tokenizers, or token-level discretization, so it fails to meet the inclusion criteria and does not satisfy the exclusion criteria either in terms of token design.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The Conformer paper targets ASR architecture (CNN+Transformer) and benchmark results; it does not discuss discrete audio tokens, tokenizers, or token-level discretization, so it fails to meet the inclusion criteria and does not satisfy the exclusion criteria either in terms of token design.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article focuses on combining convolutional networks and transformers to improve speech recognition accuracy, without describing the production or use of discrete audio tokens derived from audio discretization processes such as vector quantization or neural audio codecs; thus, it lacks core aspects of the discrete audio tokens topic, including token design, discrete vocabulary, or token-based modeling. Therefore, it does not meet the inclusion criteria focusing on discrete tokens as core representations.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article focuses on combining convolutional networks and transformers to improve speech recognition accuracy, without describing the production or use of discrete audio tokens derived from audio discretization processes such as vector quantization or neural audio codecs; thus, it lacks core aspects of the discrete audio tokens topic, including token design, discrete vocabulary, or token-based modeling. Therefore, it does not meet the inclusion criteria focusing on discrete tokens as core representations.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "zhan2024anygpt", "query_title": "{AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling}", "normalized_title": "anygpt unified multimodal llm with discrete sequence modeling", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/", "source": "arxiv", "source_id": "2402.12226v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/", "metadata": {"arxiv_id": "2402.12226v5", "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "summary": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/", "authors": ["Jun Zhan", "Junqi Dai", "Jiasheng Ye", "Yunhua Zhou", "Dong Zhang", "Zhigeng Liu", "Xin Zhang", "Ruibin Yuan", "Ge Zhang", "Linyang Li", "Hang Yan", "Jie Fu", "Tao Gui", "Tianxiang Sun", "Yu-Gang Jiang", "Xipeng Qiu"], "published": "2024-02-19T15:33:10Z", "updated": "2025-09-08T07:04:17Z", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2402.12226v5", "landing_url": "https://arxiv.org/abs/2402.12226v5", "doi": "https://doi.org/10.48550/arXiv.2402.12226"}, "round-A_JuniorNano_output": {"reasoning": "The work discusses discrete representations in a unified multimodal LLM, but does not explicitly describe discrete audio tokens, tokenizers, or audio-specific token design/experiments; insufficient evidence to classify as 'Discrete Audio Tokens' core research.", "evaluation": 3}, "round-A_JuniorNano_reasoning": "The work discusses discrete representations in a unified multimodal LLM, but does not explicitly describe discrete audio tokens, tokenizers, or audio-specific token design/experiments; insufficient evidence to classify as 'Discrete Audio Tokens' core research.", "round-A_JuniorNano_evaluation": 3, "round-A_JuniorMini_output": {"reasoning": "The paper presents AnyGPT, a multimodal language model processing multiple modalities including speech, text, images, and music using discrete representations. However, the abstract does not detail the generation process, architecture, codebook settings, or any specific discrete tokenization methodology for audio, which is essential for inclusion based on the criteria. Hence, it is unclear if the study meets the core requirement of discrete audio tokens with explicit tokenizer or codec design and evaluation relevant to audio generation or alignment tasks.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents AnyGPT, a multimodal language model processing multiple modalities including speech, text, images, and music using discrete representations. However, the abstract does not detail the generation process, architecture, codebook settings, or any specific discrete tokenization methodology for audio, which is essential for inclusion based on the criteria. Hence, it is unclear if the study meets the core requirement of discrete audio tokens with explicit tokenizer or codec design and evaluation relevant to audio generation or alignment tasks.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers flagged insufficient evidence that AnyGPT provides concrete, audio-specific discrete token design or reproducible tokenizer/codec details: the abstract states the use of “discrete representations” for modalities including speech/music but gives no information about how audio waveforms are discretized (no tokenizer/codec architecture, quantization/codebook settings, bitrates, or evaluation of discrete audio tokens). Per the inclusion criteria, papers must clearly describe the discrete audio token generation process or provide empirical results tied to such tokens; the exclusion criteria rule out works that only mention discrete representations without any token/codebook details. Given the ambiguity and lack of necessary methodological detail in the abstract, this item should not be included. 1-sentence explanation: Better to exclude because the abstract lacks the required, explicit description and evaluation of discrete audio token generation (tokenizer/codec/codebook) needed to meet the inclusion criteria.", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Both junior reviewers flagged insufficient evidence that AnyGPT provides concrete, audio-specific discrete token design or reproducible tokenizer/codec details: the abstract states the use of “discrete representations” for modalities including speech/music but gives no information about how audio waveforms are discretized (no tokenizer/codec architecture, quantization/codebook settings, bitrates, or evaluation of discrete audio tokens). Per the inclusion criteria, papers must clearly describe the discrete audio token generation process or provide empirical results tied to such tokens; the exclusion criteria rule out works that only mention discrete representations without any token/codebook details. Given the ambiguity and lack of necessary methodological detail in the abstract, this item should not be included. 1-sentence explanation: Better to exclude because the abstract lacks the required, explicit description and evaluation of discrete audio token generation (tokenizer/codec/codebook) needed to meet the inclusion criteria.", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "xin2024bigcodec", "query_title": "{Bigcodec: Pushing the limits of low-bitrate neural speech codec}", "normalized_title": "bigcodec pushing the limits of low bitrate neural speech codec", "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.", "source": "arxiv", "source_id": "2409.05377v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.", "metadata": {"arxiv_id": "2409.05377v1", "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec", "summary": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.", "authors": ["Detai Xin", "Xu Tan", "Shinnosuke Takamichi", "Hiroshi Saruwatari"], "published": "2024-09-09T07:18:07Z", "updated": "2024-09-09T07:18:07Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2409.05377v1", "landing_url": "https://arxiv.org/abs/2409.05377v1", "doi": "https://doi.org/10.48550/arXiv.2409.05377"}, "round-A_JuniorNano_output": {"reasoning": "BigCodec uses a neural audio codec with vector quantization to produce discrete tokens and utilizes multi-codebook approaches, aligning with the discrete audio tokens criteria by describing token-level encoding/quantization in an English abstract.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "BigCodec uses a neural audio codec with vector quantization to produce discrete tokens and utilizes multi-codebook approaches, aligning with the discrete audio tokens criteria by describing token-level encoding/quantization in an English abstract.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents BigCodec, a neural speech codec that utilizes low-dimensional vector quantization, indicating the generation of discrete tokens through a neural audio codec mechanism; the work focuses on low-bitrate encoding and provides model details and evaluations, aligning with the inclusion criteria centered on discrete audio tokens produced via neural codecs and their impact on audio quality and representation. It meets the requirement of English abstract and technical details, and discusses the tokenization process via vector quantization, roughly fitting the criteria for inclusion.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper presents BigCodec, a neural speech codec that utilizes low-dimensional vector quantization, indicating the generation of discrete tokens through a neural audio codec mechanism; the work focuses on low-bitrate encoding and provides model details and evaluations, aligning with the inclusion criteria centered on discrete audio tokens produced via neural codecs and their impact on audio quality and representation. It meets the requirement of English abstract and technical details, and discusses the tokenization process via vector quantization, roughly fitting the criteria for inclusion.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "niu2024ndvq", "query_title": "{{NDVQ}: Robust neural audio codec with normal distribution-based vector quantization}", "normalized_title": "ndvq robust neural audio codec with normal distribution based vector quantization", "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization", "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.", "source": "arxiv", "source_id": "2409.12717v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization", "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.", "metadata": {"arxiv_id": "2409.12717v1", "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization", "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.", "authors": ["Zhikang Niu", "Sanyuan Chen", "Long Zhou", "Ziyang Ma", "Xie Chen", "Shujie Liu"], "published": "2024-09-19T12:41:30Z", "updated": "2024-09-19T12:41:30Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2409.12717v1", "landing_url": "https://arxiv.org/abs/2409.12717v1", "doi": "https://doi.org/10.48550/arXiv.2409.12717"}, "round-A_JuniorNano_output": {"reasoning": "The work uses a neural audio codec with normal distribution-based vector quantization, where codebook entries effectively form discrete tokens for audio; it targets downstream tasks like zero-shot TTS and mentions VQ codes for reconstruction, aligning with the discrete audio token criteria and providing English abstract content, so it should be included.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The work uses a neural audio codec with normal distribution-based vector quantization, where codebook entries effectively form discrete tokens for audio; it targets downstream tasks like zero-shot TTS and mentions VQ codes for reconstruction, aligning with the discrete audio token criteria and providing English abstract content, so it should be included.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe a neural audio codec using vector quantization to produce discrete audio tokens from continuous audio waveforms, fitting the inclusion criteria of discrete audio tokens derived from neural audio codecs with detailed methodology and experimental evaluation, while not falling under any exclusion criteria; the study aims at improving token-based audio compression and generation, particularly relevant to downstream tasks like TTS, which aligns well with the thematic focus and evaluation standards specified.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The title and abstract describe a neural audio codec using vector quantization to produce discrete audio tokens from continuous audio waveforms, fitting the inclusion criteria of discrete audio tokens derived from neural audio codecs with detailed methodology and experimental evaluation, while not falling under any exclusion criteria; the study aims at improving token-based audio compression and generation, particularly relevant to downstream tasks like TTS, which aligns well with the thematic focus and evaluation standards specified.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "siuzdak2024snac", "query_title": "{{SNAC}: Multi-scale neural audio codec}", "normalized_title": "snac multi scale neural audio codec", "title": "SNAC: Multi-Scale Neural Audio Codec", "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.", "source": "arxiv", "source_id": "2410.14411v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SNAC: Multi-Scale Neural Audio Codec", "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.", "metadata": {"arxiv_id": "2410.14411v1", "title": "SNAC: Multi-Scale Neural Audio Codec", "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.", "authors": ["Hubert Siuzdak", "Florian Grötschla", "Luca A. Lanzendörfer"], "published": "2024-10-18T12:24:05Z", "updated": "2024-10-18T12:24:05Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2410.14411v1", "landing_url": "https://arxiv.org/abs/2410.14411v1", "doi": "https://doi.org/10.48550/arXiv.2410.14411"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses a neural audio codec with discrete codebooks (RVQ) and a multi-scale quantization scheme, producing discrete token representations across temporal scales, aligning with the discrete audio token criterion.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper uses a neural audio codec with discrete codebooks (RVQ) and a multi-scale quantization scheme, producing discrete token representations across temporal scales, aligning with the discrete audio token criterion.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a neural audio codec employing residual vector quantization with multi-scale quantizers, which produces discrete tokens representing audio at multiple temporal resolutions for compression; it includes technical details, evaluations, and makes code available, aligning well with the inclusion criteria that require generation of discrete audio tokens from neural audio codecs with multi-codebooks and temporal downsampling, and focuses on token design impacts on audio fidelity and compression, while the abstract and title are in English with sufficient methodological information for assessment.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a neural audio codec employing residual vector quantization with multi-scale quantizers, which produces discrete tokens representing audio at multiple temporal resolutions for compression; it includes technical details, evaluations, and makes code available, aligning well with the inclusion criteria that require generation of discrete audio tokens from neural audio codecs with multi-codebooks and temporal downsampling, and focuses on token design impacts on audio fidelity and compression, while the abstract and title are in English with sufficient methodological information for assessment.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "chiu2022bestrq", "query_title": "{Self-supervised learning with random-projection quantizer for speech recognition}", "normalized_title": "self supervised learning with random projection quantizer for speech recognition", "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition", "abstract": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.", "source": "arxiv", "source_id": "2202.01855v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition", "abstract": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.", "metadata": {"arxiv_id": "2202.01855v2", "title": "Self-supervised Learning with Random-projection Quantizer for Speech Recognition", "summary": "We present a simple and effective self-supervised learning approach for speech recognition. The approach learns a model to predict the masked speech signals, in the form of discrete labels generated with a random-projection quantizer. In particular the quantizer projects speech inputs with a randomly initialized matrix, and does a nearest-neighbor lookup in a randomly-initialized codebook. Neither the matrix nor the codebook is updated during self-supervised learning. Since the random-projection quantizer is not trained and is separated from the speech recognition model, the design makes the approach flexible and is compatible with universal speech recognition architecture. On LibriSpeech our approach achieves similar word-error-rates as previous work using self-supervised learning with non-streaming models, and provides lower word-error-rates and latency than wav2vec 2.0 and w2v-BERT with streaming models. On multilingual tasks the approach also provides significant improvement over wav2vec 2.0 and w2v-BERT.", "authors": ["Chung-Cheng Chiu", "James Qin", "Yu Zhang", "Jiahui Yu", "Yonghui Wu"], "published": "2022-02-03T21:29:04Z", "updated": "2022-06-29T21:35:41Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2202.01855v2", "landing_url": "https://arxiv.org/abs/2202.01855v2", "doi": "https://doi.org/10.48550/arXiv.2202.01855"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly generates discrete labels via a (random-projection) quantizer and uses them as targets in SSL for speech recognition, fitting the discrete audio token criterion and reporting downstream performance.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly generates discrete labels via a (random-projection) quantizer and uses them as targets in SSL for speech recognition, fitting the discrete audio token criterion and reporting downstream performance.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents a self-supervised learning approach for speech recognition that utilizes discrete labels generated by a random-projection quantizer, which discretizes continuous speech signals into token sequences; it clearly details the token generation process and evaluates the impact on downstream tasks, aligning well with the inclusion criteria focusing on discrete audio tokens for modeling and generation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents a self-supervised learning approach for speech recognition that utilizes discrete labels generated by a random-projection quantizer, which discretizes continuous speech signals into token sequences; it clearly details the token generation process and evaluates the impact on downstream tasks, aligning well with the inclusion criteria focusing on discrete audio tokens for modeling and generation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "zhang2023googleusm", "query_title": "{Google {USM}: Scaling automatic speech recognition beyond 100 languages}", "normalized_title": "google usm scaling automatic speech recognition beyond 100 languages", "title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.", "source": "arxiv", "source_id": "2303.01037v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "abstract": "We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.", "metadata": {"arxiv_id": "2303.01037v3", "title": "Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages", "summary": "We introduce the Universal Speech Model (USM), a single large model that performs automatic speech recognition (ASR) across 100+ languages. This is achieved by pre-training the encoder of the model on a large unlabeled multilingual dataset of 12 million (M) hours spanning over 300 languages, and fine-tuning on a smaller labeled dataset. We use multilingual pre-training with random-projection quantization and speech-text modality matching to achieve state-of-the-art performance on downstream multilingual ASR and speech-to-text translation tasks. We also demonstrate that despite using a labeled training set 1/7-th the size of that used for the Whisper model, our model exhibits comparable or better performance on both in-domain and out-of-domain speech recognition tasks across many languages.", "authors": ["Yu Zhang", "Wei Han", "James Qin", "Yongqiang Wang", "Ankur Bapna", "Zhehuai Chen", "Nanxin Chen", "Bo Li", "Vera Axelrod", "Gary Wang", "Zhong Meng", "Ke Hu", "Andrew Rosenberg", "Rohit Prabhavalkar", "Daniel S. Park", "Parisa Haghani", "Jason Riesa", "Ginger Perng", "Hagen Soltau", "Trevor Strohman", "Bhuvana Ramabhadran", "Tara Sainath", "Pedro Moreno", "Chung-Cheng Chiu", "Johan Schalkwyk", "Françoise Beaufays", "Yonghui Wu"], "published": "2023-03-02T07:47:18Z", "updated": "2023-09-25T01:20:23Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2303.01037v3", "landing_url": "https://arxiv.org/abs/2303.01037v3", "doi": "https://doi.org/10.48550/arXiv.2303.01037"}, "round-A_JuniorNano_output": {"reasoning": "Although the abstract discusses quantization, the work targets scaling ASR across many languages and does not provide explicit discrete audio token methodology (tokenizer/codec architecture, quantization scheme, vocab/mcodebook, or token-level analyses). Therefore it does not fulfill the discrete audio token inclusion criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "Although the abstract discusses quantization, the work targets scaling ASR across many languages and does not provide explicit discrete audio token methodology (tokenizer/codec architecture, quantization scheme, vocab/mcodebook, or token-level analyses). Therefore it does not fulfill the discrete audio token inclusion criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on a large-scale multilingual ASR model but does not mention the use of discrete audio tokens or tokenization processes involving vector quantization or neural audio codecs; it primarily addresses continuous speech recognition and model pretraining without evidence of discrete token generation or analysis fitting the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on a large-scale multilingual ASR model but does not mention the use of discrete audio tokens or tokenization processes involving vector quantization or neural audio codecs; it primarily addresses continuous speech recognition and model pretraining without evidence of discrete token generation or analysis fitting the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "langman2024spectral", "query_title": "{Spectral codecs: Spectrogram-based audio codecs for high quality speech synthesis}", "normalized_title": "spectral codecs spectrogram based audio codecs for high quality speech synthesis", "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs", "abstract": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.", "source": "arxiv", "source_id": "2406.05298", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs", "abstract": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.", "metadata": {"arxiv_id": "2406.05298", "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs", "summary": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.", "authors": ["Ryan Langman", "Ante Jukić", "Kunal Dhawan", "Nithin Rao Koluguri", "Jason Li"], "published": "2024-06-07T23:47:51Z", "updated": "2025-06-04T16:25:54Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.05298v2", "landing_url": "https://arxiv.org/abs/2406.05298v2", "doi": "https://doi.org/10.48550/arXiv.2406.05298"}, "round-A_JuniorNano_output": {"reasoning": "The abstract presents a discrete quantization-based spectral codec (FSQ) applied to mel-spectrograms and shows its use in improving non-autoregressive TTS, aligning with the discrete audio token criteria and detailing downstream evaluation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract presents a discrete quantization-based spectral codec (FSQ) applied to mel-spectrograms and shows its use in improving non-autoregressive TTS, aligning with the discrete audio token criteria and detailing downstream evaluation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a spectral codec using scalar quantization to compress mel-spectrograms and reconstruct audio, focusing on discrete audio tokens derived from neural audio codecs with detailed method descriptions relevant to token design and TTS tasks, fitting the inclusion criteria of addressing discrete token schemes and their impact on speech synthesis quality, without any exclusion criteria triggered.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a spectral codec using scalar quantization to compress mel-spectrograms and reconstruct audio, focusing on discrete audio tokens derived from neural audio codecs with detailed method descriptions relevant to token design and TTS tasks, fitting the inclusion criteria of addressing discrete token schemes and their impact on speech synthesis quality, without any exclusion criteria triggered.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "gu2024esc", "query_title": "{Esc: Efficient speech coding with cross-scale residual vector quantized transformers}", "normalized_title": "esc efficient speech coding with cross scale residual vector quantized transformers", "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.", "source": "arxiv", "source_id": "2404.19441v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.", "metadata": {"arxiv_id": "2404.19441v3", "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers", "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.", "authors": ["Yuzhe Gu", "Enmao Diao"], "published": "2024-04-30T10:44:33Z", "updated": "2024-10-03T12:23:26Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2404.19441v3", "landing_url": "https://arxiv.org/abs/2404.19441v3", "doi": "https://doi.org/10.48550/arXiv.2404.19441"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe a neural speech codec using cross-scale residual vector quantization, i.e., discrete tokens produced by vector-quantized neural codecs, with explicit attention to tokenization and bitrate-efficient representations, fitting the discrete audio token criteria and not falling into exclusion categories (continuous representations or non-token-focused work).", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The title/abstract describe a neural speech codec using cross-scale residual vector quantization, i.e., discrete tokens produced by vector-quantized neural codecs, with explicit attention to tokenization and bitrate-efficient representations, fitting the discrete audio token criteria and not falling into exclusion categories (continuous representations or non-token-focused work).", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study presents a neural speech codec based on cross-scale residual vector quantization and transformers, focusing on an efficient parameter-efficient speech codec that employs vector quantization techniques—a key method for generating discrete audio tokens. The abstract describes the use of hierarchical quantization and a vector quantized neural codec, which aligns well with the inclusion criteria of discrete token generation and codec architecture, detailed with quantization and hierarchical structure supporting reproducibility. It addresses token design for audio representation, meeting requirements for research contribution type and token generation mechanism. Therefore, this work aligns with the topic of discrete audio tokens, focusing on audio coding and tokenization for speech, with clear English title and abstract and appropriate methodology description.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a neural speech codec based on cross-scale residual vector quantization and transformers, focusing on an efficient parameter-efficient speech codec that employs vector quantization techniques—a key method for generating discrete audio tokens. The abstract describes the use of hierarchical quantization and a vector quantized neural codec, which aligns well with the inclusion criteria of discrete token generation and codec architecture, detailed with quantization and hierarchical structure supporting reproducibility. It addresses token design for audio representation, meeting requirements for research contribution type and token generation mechanism. Therefore, this work aligns with the topic of discrete audio tokens, focusing on audio coding and tokenization for speech, with clear English title and abstract and appropriate methodology description.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "tang2024singomd", "query_title": "{Sing{OMD}: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models}", "normalized_title": "singomd singing oriented multi resolution discrete representation construction from speech models", "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models", "abstract": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.", "source": "semantic_scholar", "source_id": "b7359944a3a8d9a8a6d4acd83e7a507a4da1094a", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models", "abstract": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.", "metadata": {"arxiv_id": "semantic_scholar:b7359944a3a8d9a8a6d4acd83e7a507a4da1094a", "title": "SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models", "summary": "Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.", "source": "semantic_scholar", "source_id": "b7359944a3a8d9a8a6d4acd83e7a507a4da1094a", "publication_date": "2024-06-13"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a discrete audio token framework for singing generation, deriving tokens from speech SSL with multi-resolution discretization, and applies them to singing vocoders/voice synthesis, aligning with the discrete audio token criteria and downstream generation tasks.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item describes a discrete audio token framework for singing generation, deriving tokens from speech SSL with multi-resolution discretization, and applies them to singing vocoders/voice synthesis, aligning with the discrete audio token criteria and downstream generation tasks.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on developing discrete representations (discrete tokens) derived from self-supervised speech models specifically adapted for singing generation. It involves discretization through clustering of multi-resolution features, aligning well with the inclusion criteria of using discrete audio tokens for downstream generation tasks, and details the token generation process sufficiently in the abstract. It also targets generative tasks (singing vocoders, voice synthesis) and discusses representation design choices and evaluations, thereby meeting both inclusion and exclusion criteria well.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on developing discrete representations (discrete tokens) derived from self-supervised speech models specifically adapted for singing generation. It involves discretization through clustering of multi-resolution features, aligning well with the inclusion criteria of using discrete audio tokens for downstream generation tasks, and details the token generation process sufficiently in the abstract. It also targets generative tasks (singing vocoders, voice synthesis) and discusses representation design choices and evaluations, thereby meeting both inclusion and exclusion criteria well.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "shi2024mmm", "query_title": "{{MMM}: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model}", "normalized_title": "mmm multi layer multi residual multi stream discrete speech representation from self supervised learning model", "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model", "abstract": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.", "source": "arxiv", "source_id": "2406.09869v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model", "abstract": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.", "metadata": {"arxiv_id": "2406.09869v1", "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model", "summary": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.", "authors": ["Jiatong Shi", "Xutai Ma", "Hirofumi Inaguma", "Anna Sun", "Shinji Watanabe"], "published": "2024-06-14T09:29:45Z", "updated": "2024-06-14T09:29:45Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.09869v1", "landing_url": "https://arxiv.org/abs/2406.09869v1", "doi": "https://doi.org/10.48550/arXiv.2406.09869"}, "round-A_JuniorNano_output": {"reasoning": "The item presents a method to extract and use discrete speech tokens (multi-layer, multi-stream, with residual vector quantization) from self-supervised models and evaluates them on ASR, resynthesis, and TTS, directly addressing discrete audio tokens and downstream tasks, with English title/abstract and actionable methodological details.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item presents a method to extract and use discrete speech tokens (multi-layer, multi-stream, with residual vector quantization) from self-supervised models and evaluates them on ASR, resynthesis, and TTS, directly addressing discrete audio tokens and downstream tasks, with English title/abstract and actionable methodological details.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a multi-layer, multi-residual, multi-stream discrete token extraction method from a self-supervised learning model for speech discrete representations, explicitly detailing iterative residual vector quantization with K-means and evaluating its effectiveness in downstream tasks like speech recognition and synthesis. This aligns with the inclusion criteria focusing on discrete audio tokens derived from neural or self-supervised models, with clear methodological descriptions and evaluations on downstream generation and understanding tasks. Therefore, it meets the inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a multi-layer, multi-residual, multi-stream discrete token extraction method from a self-supervised learning model for speech discrete representations, explicitly detailing iterative residual vector quantization with K-means and evaluating its effectiveness in downstream tasks like speech recognition and synthesis. This aligns with the inclusion criteria focusing on discrete audio tokens derived from neural or self-supervised models, with clear methodological descriptions and evaluations on downstream generation and understanding tasks. Therefore, it meets the inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "messica2024nast", "query_title": "{Nast: Noise aware speech tokenization for speech language models}", "normalized_title": "nast noise aware speech tokenization for speech language models", "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.", "source": "arxiv", "source_id": "2406.11037v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.", "metadata": {"arxiv_id": "2406.11037v1", "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models", "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.", "authors": ["Shoval Messica", "Yossi Adi"], "published": "2024-06-16T18:20:45Z", "updated": "2024-06-16T18:20:45Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.11037v1", "landing_url": "https://arxiv.org/abs/2406.11037v1", "doi": "https://doi.org/10.48550/arXiv.2406.11037"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on a noise-aware speech tokenization approach and mentions tokens and a tokenization pipeline, but it does not clearly specify/disclose the discrete token generation mechanism (e.g., neural codec, vocabulary size, or self-supervised discrete units) or provide concrete token-level evaluations, making it unclear whether it fully satisfies the discrete audio token criteria.", "evaluation": 3}, "round-A_JuniorNano_reasoning": "The abstract centers on a noise-aware speech tokenization approach and mentions tokens and a tokenization pipeline, but it does not clearly specify/disclose the discrete token generation mechanism (e.g., neural codec, vocabulary size, or self-supervised discrete units) or provide concrete token-level evaluations, making it unclear whether it fully satisfies the discrete audio token criteria.", "round-A_JuniorNano_evaluation": 3, "round-A_JuniorMini_output": {"reasoning": "The study explicitly addresses speech tokenization by proposing a noise-aware speech tokenization method (NAST) used for speech language models, which involves representation of speech signals as sequences of discrete units (tokens). It indicates evaluation of tokenization under noisy conditions and analysis of the approach with respect to robustness and disentanglement properties, which suggests focus on token design and its effect on downstream tasks. The abstract also mentions code and pretrained models are available, supporting reproducibility. The topic fits the inclusion criteria of discrete audio tokens generated by neural models for use in sequence modeling and downstream tasks like speech understanding and generation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study explicitly addresses speech tokenization by proposing a noise-aware speech tokenization method (NAST) used for speech language models, which involves representation of speech signals as sequences of discrete units (tokens). It indicates evaluation of tokenization under noisy conditions and analysis of the approach with respect to robustness and disentanglement properties, which suggests focus on token design and its effect on downstream tasks. The abstract also mentions code and pretrained models are available, supporting reproducibility. The topic fits the inclusion criteria of discrete audio tokens generated by neural models for use in sequence modeling and downstream tasks like speech understanding and generation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": {"reasoning": "I reviewed both junior reviewers: JuniorNano was uncertain because the abstract does not specify concrete token-generation/quantization/codebook details, while JuniorMini judged it clearly in-scope since the work explicitly targets speech tokenization into discrete units for speech language models and provides code/models. The paper's title and abstract claim discrete-unit tokenization (NAST) and evaluate it for spoken language modeling and robustness to noise, and the availability of code/pretrained models increases the likelihood that discrete tokenization details exist in the full paper; however, the abstract itself omits explicit tokenizer/quantization/codebook specifications required by the inclusion checklist. One-sentence explanation for my score: include (4) — the work is directly about discrete speech tokenization for speech LMs and likely meets the topic, but the abstract lacks enough technical detail about the discrete token generation to warrant a maximal certainty.", "evaluation": 4}, "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers: JuniorNano was uncertain because the abstract does not specify concrete token-generation/quantization/codebook details, while JuniorMini judged it clearly in-scope since the work explicitly targets speech tokenization into discrete units for speech language models and provides code/models. The paper's title and abstract claim discrete-unit tokenization (NAST) and evaluate it for spoken language modeling and robustness to noise, and the availability of code/pretrained models increases the likelihood that discrete tokenization details exist in the full paper; however, the abstract itself omits explicit tokenizer/quantization/codebook specifications required by the inclusion checklist. One-sentence explanation for my score: include (4) — the work is directly about discrete speech tokenization for speech LMs and likely meets the topic, but the abstract lacks enough technical detail about the discrete token generation to warrant a maximal certainty.", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "chen-etal-2024-towards-robust", "query_title": "\"Towards Robust Speech Representation Learning for Thousands of Languages\"", "normalized_title": "towards robust speech representation learning for thousands of languages", "title": "Towards Robust Speech Representation Learning for Thousands of Languages", "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.", "source": "arxiv", "source_id": "2407.00837v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards Robust Speech Representation Learning for Thousands of Languages", "abstract": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.", "metadata": {"arxiv_id": "2407.00837v2", "title": "Towards Robust Speech Representation Learning for Thousands of Languages", "summary": "Self-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavlab.org/activities/2024/xeus/.", "authors": ["William Chen", "Wangyou Zhang", "Yifan Peng", "Xinjian Li", "Jinchuan Tian", "Jiatong Shi", "Xuankai Chang", "Soumi Maiti", "Karen Livescu", "Shinji Watanabe"], "published": "2024-06-30T21:40:26Z", "updated": "2024-07-02T17:23:44Z", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.00837v2", "landing_url": "https://arxiv.org/abs/2407.00837v2", "doi": "https://doi.org/10.48550/arXiv.2407.00837"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on self-supervised speech representation learning across many languages and dereverberation, but does not describe discrete audio tokens, tokenizers, or codec-based tokenization, so it fails the token-centric inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised speech representation learning across many languages and dereverberation, but does not describe discrete audio tokens, tokenizers, or codec-based tokenization, so it fails the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses self-supervised learning for speech representation over thousands of languages, focusing on model robustness and coverage; however, there is no mention or indication of discretizing continuous audio waveforms into discrete token sequences or providing details on tokenization, vocabulary, or quantization schemes, which are essential to meet the inclusion criteria focusing on discrete audio tokens. Therefore, although it relates to speech processing, it does not satisfy the core inclusion criterion of using or analyzing discrete audio tokens as representations for modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses self-supervised learning for speech representation over thousands of languages, focusing on model robustness and coverage; however, there is no mention or indication of discretizing continuous audio waveforms into discrete token sequences or providing details on tokenization, vocabulary, or quantization schemes, which are essential to meet the inclusion criteria focusing on discrete audio tokens. Therefore, although it relates to speech processing, it does not satisfy the core inclusion criterion of using or analyzing discrete audio tokens as representations for modeling or generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "zhang2023speechgpt", "query_title": "{Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities}", "normalized_title": "speechgpt empowering large language models with intrinsic cross modal conversational abilities", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "source": "arxiv", "source_id": "2305.11000v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "abstract": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "metadata": {"arxiv_id": "2305.11000v2", "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities", "summary": "Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.", "authors": ["Dong Zhang", "Shimin Li", "Xin Zhang", "Jun Zhan", "Pengyu Wang", "Yaqian Zhou", "Xipeng Qiu"], "published": "2023-05-18T14:23:25Z", "updated": "2023-05-19T14:41:16Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2305.11000v2", "landing_url": "https://arxiv.org/abs/2305.11000v2", "doi": "https://doi.org/10.48550/arXiv.2305.11000"}, "round-A_JuniorNano_output": {"reasoning": "The abstract mentions discrete speech representations but does not provide details on tokenization/codec design or demonstrate discrete audio tokens as the core input for modeling, so it does not clearly meet all inclusion criteria for discrete audio token research.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The abstract mentions discrete speech representations but does not provide details on tokenization/codec design or demonstrate discrete audio tokens as the core input for modeling, so it does not clearly meet all inclusion criteria for discrete audio token research.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The abstract describes SpeechGPT as a multi-modal large language model that uses discrete speech representations and constructs a large-scale cross-modal speech instruction dataset, with a training strategy involving modality adaptation and cross-modal instruction fine-tuning. However, while discrete speech representations are mentioned, the abstract lacks clear and detailed information on the explicit process of generating discrete audio tokens, such as the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or token properties. Hence, it is unclear whether the study satisfies the inclusion criteria regarding the clear definition and use of discrete audio tokens as core representations for modeling and generation, which is essential for inclusion. Therefore, the evaluation reflects uncertainty about inclusion due to insufficient details about the discrete token design and usage.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The abstract describes SpeechGPT as a multi-modal large language model that uses discrete speech representations and constructs a large-scale cross-modal speech instruction dataset, with a training strategy involving modality adaptation and cross-modal instruction fine-tuning. However, while discrete speech representations are mentioned, the abstract lacks clear and detailed information on the explicit process of generating discrete audio tokens, such as the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or token properties. Hence, it is unclear whether the study satisfies the inclusion criteria regarding the clear definition and use of discrete audio tokens as core representations for modeling and generation, which is essential for inclusion. Therefore, the evaluation reflects uncertainty about inclusion due to insufficient details about the discrete token design and usage.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers highlighted that although the abstract mentions “discrete speech representations,” it provides no tokenizer/codec/quantization/codebook/bitrate or reproducibility details to confirm discrete audio tokens are the core representation for modeling/generation, so based on the abstract alone it should be excluded (insufficient evidence).", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Both junior reviewers highlighted that although the abstract mentions “discrete speech representations,” it provides no tokenizer/codec/quantization/codebook/bitrate or reproducibility details to confirm discrete audio tokens are the core representation for modeling/generation, so based on the abstract alone it should be excluded (insufficient evidence).", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "valin2012definition", "query_title": "{RFC 6716: Definition of the Opus audio codec}", "normalized_title": "rfc 6716 definition of the opus audio codec", "title": "Definition of the Opus Audio Codec.", "abstract": "This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances.  It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s.  Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music.", "source": "rfc_editor", "source_id": "RFC6716", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Definition of the Opus Audio Codec.", "abstract": "This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances. It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s. Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music.", "metadata": {"arxiv_id": "rfc_editor:RFC6716", "title": "Definition of the Opus Audio Codec.", "summary": "This document defines the Opus interactive speech and audio codec. Opus is designed to handle a wide range of interactive audio applications, including Voice over IP, videoconferencing, in-game chat, and even live, distributed music performances.  It scales from low bitrate narrowband speech at 6 kbit/s to very high quality stereo music at 510 kbit/s.  Opus uses both Linear Prediction (LP) and the Modified Discrete Cosine Transform (MDCT) to achieve good compression of both speech and music.", "source": "rfc_editor", "source_id": "RFC6716", "year": 2070}, "round-A_JuniorNano_output": null, "round-A_JuniorNano_reasoning": null, "round-A_JuniorNano_evaluation": null, "round-A_JuniorMini_output": null, "round-A_JuniorMini_reasoning": null, "round-A_JuniorMini_evaluation": null, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "discard (published_on_or_after_cutoff:2070-01-01)", "review_skipped": true, "discard_reason": "published_on_or_after_cutoff:2070-01-01"}]}
{"key": "dietz2015overview", "query_title": "{Overview of the {EVS} codec architecture}", "normalized_title": "overview of the evs codec architecture", "title": "Overview of the EVS codec architecture", "abstract": "The recently standardized 3GPP codec for Enhanced Voice Services (EVS) offers new features and improvements for low-delay real-time communication systems. Based on a novel, switched low-delay speech/audio codec, the EVS codec contains various tools for better compression efficiency and higher quality for clean/noisy speech, mixed content and music, including support for wideband, super-wideband and full-band content. The EVS codec operates in a broad range of bitrates, is highly robust against packet loss and provides an AMR-WB interoperable mode for compatibility with existing systems. This paper gives an overview of the underlying architecture as well as the novel technologies in the EVS codec and presents listening test results showing the performance of the new codec in terms of compression and speech/audio quality.", "source": "semantic_scholar", "source_id": "d63c953aa3a7327202d7af9baeda6fd0a646d5a5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Overview of the EVS codec architecture", "abstract": "The recently standardized 3GPP codec for Enhanced Voice Services (EVS) offers new features and improvements for low-delay real-time communication systems. Based on a novel, switched low-delay speech/audio codec, the EVS codec contains various tools for better compression efficiency and higher quality for clean/noisy speech, mixed content and music, including support for wideband, super-wideband and full-band content. The EVS codec operates in a broad range of bitrates, is highly robust against packet loss and provides an AMR-WB interoperable mode for compatibility with existing systems. This paper gives an overview of the underlying architecture as well as the novel technologies in the EVS codec and presents listening test results showing the performance of the new codec in terms of compression and speech/audio quality.", "metadata": {"arxiv_id": "semantic_scholar:d63c953aa3a7327202d7af9baeda6fd0a646d5a5", "title": "Overview of the EVS codec architecture", "summary": "The recently standardized 3GPP codec for Enhanced Voice Services (EVS) offers new features and improvements for low-delay real-time communication systems. Based on a novel, switched low-delay speech/audio codec, the EVS codec contains various tools for better compression efficiency and higher quality for clean/noisy speech, mixed content and music, including support for wideband, super-wideband and full-band content. The EVS codec operates in a broad range of bitrates, is highly robust against packet loss and provides an AMR-WB interoperable mode for compatibility with existing systems. This paper gives an overview of the underlying architecture as well as the novel technologies in the EVS codec and presents listening test results showing the performance of the new codec in terms of compression and speech/audio quality.", "source": "semantic_scholar", "source_id": "d63c953aa3a7327202d7af9baeda6fd0a646d5a5", "publication_date": "2015-04-01"}, "round-A_JuniorNano_output": {"reasoning": "此篇為 EVS 編解碼器的架構與性能概述，未涉及離散音訊 token 的產生、量化、詞彙表或跨模態推理，因此不符合本題的離散音訊 token 主題範疇，應排除。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此篇為 EVS 編解碼器的架構與性能概述，未涉及離散音訊 token 的產生、量化、詞彙表或跨模態推理，因此不符合本題的離散音訊 token 主題範疇，應排除。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses the EVS codec architecture which is a traditional audio codec focused on compression efficiency and quality for real-time communication, but it does not describe any discrete audio tokenization process or neural audio codec techniques that produce discrete token sequences for generative or cross-modal modeling, nor does it focus on token design or discrete representation evaluation; thus, it does not meet the inclusion criteria centered on discrete token generation and usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses the EVS codec architecture which is a traditional audio codec focused on compression efficiency and quality for real-time communication, but it does not describe any discrete audio tokenization process or neural audio codec techniques that produce discrete token sequences for generative or cross-modal modeling, nor does it focus on token design or discrete representation evaluation; thus, it does not meet the inclusion criteria centered on discrete token generation and usage.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "fma_dataset", "query_title": "{{FMA}: A Dataset for Music Analysis}", "normalized_title": "fma a dataset for music analysis", "title": "FMA: A Dataset For Music Analysis", "abstract": "We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma", "source": "arxiv", "source_id": "1612.01840v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FMA: A Dataset For Music Analysis", "abstract": "We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma", "metadata": {"arxiv_id": "1612.01840v3", "title": "FMA: A Dataset For Music Analysis", "summary": "We introduce the Free Music Archive (FMA), an open and easily accessible dataset suitable for evaluating several tasks in MIR, a field concerned with browsing, searching, and organizing large music collections. The community's growing interest in feature and end-to-end learning is however restrained by the limited availability of large audio datasets. The FMA aims to overcome this hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a hierarchical taxonomy of 161 genres. It provides full-length and high-quality audio, pre-computed features, together with track- and user-level metadata, tags, and free-form text such as biographies. We here describe the dataset and how it was created, propose a train/validation/test split and three subsets, discuss some suitable MIR tasks, and evaluate some baselines for genre recognition. Code, data, and usage examples are available at https://github.com/mdeff/fma", "authors": ["Michaël Defferrard", "Kirell Benzi", "Pierre Vandergheynst", "Xavier Bresson"], "published": "2016-12-06T14:58:59Z", "updated": "2017-09-05T18:38:33Z", "categories": ["cs.SD", "cs.IR"], "pdf_url": "https://arxiv.org/pdf/1612.01840v3", "landing_url": "https://arxiv.org/abs/1612.01840v3", "doi": "https://doi.org/10.48550/arXiv.1612.01840"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes the Free Music Archive dataset and its use for MIR tasks, with no discussion of discrete audio tokenization, codecs, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes the Free Music Archive dataset and its use for MIR tasks, with no discussion of discrete audio tokenization, codecs, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The dataset described is a large collection of music tracks with metadata suitable for music information retrieval tasks but does not describe any discrete audio tokenization process, neural audio codec usage, or explicit discrete token design/evaluation as required by the inclusion criteria. It focuses on providing raw audio and features rather than mapping continuous audio to discrete tokens or analyzing such tokens for generation or modeling. Hence, it does not meet the core requirement of studying discrete audio tokens as defined, nor does it provide sufficient methodological details about tokenizers or codecs for evaluation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The dataset described is a large collection of music tracks with metadata suitable for music information retrieval tasks but does not describe any discrete audio tokenization process, neural audio codec usage, or explicit discrete token design/evaluation as required by the inclusion criteria. It focuses on providing raw audio and features rather than mapping continuous audio to discrete tokens or analyzing such tokens for generation or modeling. Hence, it does not meet the core requirement of studying discrete audio tokens as defined, nor does it provide sufficient methodological details about tokenizers or codecs for evaluation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "evans2025stable", "query_title": "{Stable audio open}", "normalized_title": "stable audio open", "title": "Stable Audio Open", "abstract": "Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.", "source": "arxiv", "source_id": "2407.14358v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Stable Audio Open", "abstract": "Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.", "metadata": {"arxiv_id": "2407.14358v2", "title": "Stable Audio Open", "summary": "Open generative models are vitally important for the community, allowing for fine-tunes and serving as baselines when presenting new models. However, most current text-to-audio models are private and not accessible for artists and researchers to build upon. Here we describe the architecture and training process of a new open-weights text-to-audio model trained with Creative Commons data. Our evaluation shows that the model's performance is competitive with the state-of-the-art across various metrics. Notably, the reported FDopenl3 results (measuring the realism of the generations) showcase its potential for high-quality stereo sound synthesis at 44.1kHz.", "authors": ["Zach Evans", "Julian D. Parker", "CJ Carr", "Zack Zukowski", "Josiah Taylor", "Jordi Pons"], "published": "2024-07-19T14:40:23Z", "updated": "2024-07-31T16:22:42Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.14358v2", "landing_url": "https://arxiv.org/abs/2407.14358v2", "doi": "https://doi.org/10.48550/arXiv.2407.14358"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe an open-weights text-to-audio model with CC data, but there is no mention of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe an open-weights text-to-audio model with CC data, but there is no mention of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe an open-weights text-to-audio generative model trained on Creative Commons data with competitive performance, but there is no mention of discretization of audio into token sequences, token generation methods, codecs, quantization, or any design related to discrete audio tokens as required by the inclusion criteria. The abstract focuses on model architecture and performance, without addressing discrete token representation or analysis. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens, nor does it provide sufficient English methodological details to evaluate such tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe an open-weights text-to-audio generative model trained on Creative Commons data with competitive performance, but there is no mention of discretization of audio into token sequences, token generation methods, codecs, quantization, or any design related to discrete audio tokens as required by the inclusion criteria. The abstract focuses on model architecture and performance, without addressing discrete token representation or analysis. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens, nor does it provide sufficient English methodological details to evaluate such tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "van2022comparison", "query_title": "{A comparison of discrete and soft speech units for improved voice conversion}", "normalized_title": "a comparison of discrete and soft speech units for improved voice conversion", "title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion", "abstract": "The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech. Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code available at https://github.com/bshall/soft-vc/.", "source": "arxiv", "source_id": "2111.02392v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion", "abstract": "The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech. Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code available at https://github.com/bshall/soft-vc/.", "metadata": {"arxiv_id": "2111.02392v2", "title": "A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion", "summary": "The goal of voice conversion is to transform source speech into a target voice, keeping the content unchanged. In this paper, we focus on self-supervised representation learning for voice conversion. Specifically, we compare discrete and soft speech units as input features. We find that discrete representations effectively remove speaker information but discard some linguistic content - leading to mispronunciations. As a solution, we propose soft speech units. To learn soft units, we predict a distribution over discrete speech units. By modeling uncertainty, soft units capture more content information, improving the intelligibility and naturalness of converted speech. Samples available at https://ubisoft-laforge.github.io/speech/soft-vc/. Code available at https://github.com/bshall/soft-vc/.", "authors": ["Benjamin van Niekerk", "Marc-André Carbonneau", "Julian Zaïdi", "Mathew Baas", "Hugo Seuté", "Herman Kamper"], "published": "2021-11-03T17:58:03Z", "updated": "2022-06-08T15:06:57Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2111.02392v2", "landing_url": "https://arxiv.org/abs/2111.02392v2", "doi": "https://doi.org/10.1109/ICASSP43922.2022.9746484"}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly centers on discrete speech units as core representations and compares them with soft units in a voice-conversion setting, including a distribution-over-discrete-units approach, fitting the discrete audio token criteria and addressing downstream generation/understanding tasks.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly centers on discrete speech units as core representations and compares them with soft units in a voice-conversion setting, including a distribution-over-discrete-units approach, fitting the discrete audio token criteria and addressing downstream generation/understanding tasks.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper discusses discrete and soft speech units in the context of self-supervised voice conversion, explicitly comparing discrete speech units which are representations learned through discretization methods, meeting the theme of discrete audio tokens derived from self-supervised learning models; although detailed tokenizer architecture or codebook settings are not described in the abstract, it studies discrete token design's effect on downstream voice conversion performance, aligning with the inclusion criteria of token design impact and downstream evaluation. It also provides English title, abstract, and code repository links suggesting reproducibility. Hence, it fits well within the inclusion scope without triggering exclusion criteria.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The paper discusses discrete and soft speech units in the context of self-supervised voice conversion, explicitly comparing discrete speech units which are representations learned through discretization methods, meeting the theme of discrete audio tokens derived from self-supervised learning models; although detailed tokenizer architecture or codebook settings are not described in the abstract, it studies discrete token design's effect on downstream voice conversion performance, aligning with the inclusion criteria of token design impact and downstream evaluation. It also provides English title, abstract, and code repository links suggesting reproducibility. Hence, it fits well within the inclusion scope without triggering exclusion criteria.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "guo2024socodec", "query_title": "{SoCodec: A Semantic-Ordered Multi-Stream Speech Codec For Efficient Language Model Based Text-to-Speech Synthesis}", "normalized_title": "socodec a semantic ordered multi stream speech codec for efficient language model based text to speech synthesis", "title": "SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis", "abstract": "The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.", "source": "arxiv", "source_id": "2409.00933v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis", "abstract": "The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.", "metadata": {"arxiv_id": "2409.00933v1", "title": "SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis", "summary": "The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.", "authors": ["Haohan Guo", "Fenglong Xie", "Kun Xie", "Dongchao Yang", "Dake Guo", "Xixin Wu", "Helen Meng"], "published": "2024-09-02T04:13:20Z", "updated": "2024-09-02T04:13:20Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.00933v1", "landing_url": "https://arxiv.org/abs/2409.00933v1", "doi": "https://doi.org/10.48550/arXiv.2409.00933"}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly introduces a discrete semantic token sequence produced by a neural audio codec with multi-stream and ordered quantization, and applies it to LM-based TTS, aligning with the discrete audio tokens criteria and providing sufficient methodological detail for evaluation.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly introduces a discrete semantic token sequence produced by a neural audio codec with multi-stream and ordered quantization, and applies it to LM-based TTS, aligning with the discrete audio tokens criteria and providing sufficient methodological detail for evaluation.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a neural speech codec (SoCodec) that compresses speech into a multi-stream discrete semantic token sequence, with clear mention of quantization and discrete semantic representations used for efficient language model-based TTS, aligning well with the definition of discrete audio tokens and inclusion criteria; the abstract provides sufficient methodological detail and focuses on the generation of discrete tokens for downstream TTS tasks, with no signs of exclusion criteria being met.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a neural speech codec (SoCodec) that compresses speech into a multi-stream discrete semantic token sequence, with clear mention of quantization and discrete semantic representations used for efficient language model-based TTS, aligning well with the definition of discrete audio tokens and inclusion criteria; the abstract provides sufficient methodological detail and focuses on the generation of discrete tokens for downstream TTS tasks, with no signs of exclusion criteria being met.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "casanova2024lfsc", "query_title": "{Low frame-rate speech codec: a codec designed for fast high-quality speech {LLM} training and inference}", "normalized_title": "low frame rate speech codec a codec designed for fast high quality speech llm training and inference", "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.", "source": "arxiv", "source_id": "2409.12117v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.", "metadata": {"arxiv_id": "2409.12117v1", "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference", "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.", "authors": ["Edresson Casanova", "Ryan Langman", "Paarth Neekhara", "Shehzeen Hussain", "Jason Li", "Subhankar Ghosh", "Ante Jukić", "Sang-gil Lee"], "published": "2024-09-18T16:39:10Z", "updated": "2024-09-18T16:39:10Z", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2409.12117v1", "landing_url": "https://arxiv.org/abs/2409.12117v1", "doi": "https://doi.org/10.48550/arXiv.2409.12117"}, "round-A_JuniorNano_output": {"reasoning": "LFSC uses a neural audio codec with finite scalar quantization to produce discrete tokens for downstream LLM-based speech tasks, explicitly addressing token design and downstream modeling, thus fitting the discrete audio tokens criterion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "LFSC uses a neural audio codec with finite scalar quantization to produce discrete tokens for downstream LLM-based speech tasks, explicitly addressing token design and downstream modeling, thus fitting the discrete audio tokens criterion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a neural audio codec that produces discrete tokens through quantization for faster LLM-based speech processing, meeting the inclusion criteria of defining discrete audio tokens generated by a neural codec with detailed bitrate and frame-rate specifications. It focuses on token design affecting quality and computational cost, aligning with research contributions on discrete token schemes for audio generation and inference, and provides sufficient English information and methodology to support reproducibility. Therefore, it fulfils all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The abstract describes a neural audio codec that produces discrete tokens through quantization for faster LLM-based speech processing, meeting the inclusion criteria of defining discrete audio tokens generated by a neural codec with detailed bitrate and frame-rate specifications. It focuses on token design affecting quality and computational cost, aligning with research contributions on discrete token schemes for audio generation and inference, and provides sufficient English information and methodology to support reproducibility. Therefore, it fulfils all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "ren2024ticodec", "query_title": "{Fewer-token neural speech codec with time-invariant codes}", "normalized_title": "fewer token neural speech codec with time invariant codes", "title": "Fewer-token Neural Speech Codec with Time-invariant Codes", "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance and force it to capture more global information, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model.", "source": "arxiv", "source_id": "2310.00014v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Fewer-token Neural Speech Codec with Time-invariant Codes", "abstract": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance and force it to capture more global information, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model.", "metadata": {"arxiv_id": "2310.00014v2", "title": "Fewer-token Neural Speech Codec with Time-invariant Codes", "summary": "Language model based text-to-speech (TTS) models, like VALL-E, have gained attention for their outstanding in-context learning capability in zero-shot scenarios. Neural speech codec is a critical component of these models, which can convert speech into discrete token representations. However, excessive token sequences from the codec may negatively affect prediction accuracy and restrict the progression of Language model based TTS models. To address this issue, this paper proposes a novel neural speech codec with time-invariant codes named TiCodec. By encoding and quantizing time-invariant information into a separate code, TiCodec can reduce the amount of frame-level information that needs encoding, effectively decreasing the number of tokens as codes of speech. Furthermore, this paper introduces a time-invariant encoding consistency loss to enhance the consistency of time-invariant code within an utterance and force it to capture more global information, which can benefit the zero-shot TTS task. Experimental results demonstrate that TiCodec can not only enhance the quality of reconstruction speech with fewer tokens but also increase the similarity and naturalness, as well as reduce the word error rate of the synthesized speech by the TTS model.", "authors": ["Yong Ren", "Tao Wang", "Jiangyan Yi", "Le Xu", "Jianhua Tao", "Chuyuan Zhang", "Junzuo Zhou"], "published": "2023-09-15T04:32:26Z", "updated": "2024-03-11T02:09:29Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2310.00014v2", "landing_url": "https://arxiv.org/abs/2310.00014v2", "doi": "https://doi.org/10.48550/arXiv.2310.00014"}, "round-A_JuniorNano_output": {"reasoning": "This item presents a neural speech codec that encodes speech into discrete token representations (tokens/codebooks) and uses them in downstream TTS tasks, including a novel time-invariant code and a consistency loss, meeting all criteria for discrete audio tokens and token-based modeling.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "This item presents a neural speech codec that encodes speech into discrete token representations (tokens/codebooks) and uses them in downstream TTS tasks, including a novel time-invariant code and a consistency loss, meeting all criteria for discrete audio tokens and token-based modeling.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a novel neural speech codec that uses time-invariant codes to reduce token sequence length, clearly involving the discrete tokenization of speech via a neural codec, fitting the inclusion criteria regarding discrete audio tokens for modeling and generation in TTS tasks, with detailed token generation and evaluation in English; it neither focuses solely on continuous representations nor on non-core or discriminative tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes a novel neural speech codec that uses time-invariant codes to reduce token sequence length, clearly involving the discrete tokenization of speech via a neural codec, fitting the inclusion criteria regarding discrete audio tokens for modeling and generation in TTS tasks, with detailed token generation and evaluation in English; it neither focuses solely on continuous representations nor on non-core or discriminative tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "parker2024scaling", "query_title": "{Scaling Transformers for Low-Bitrate High-Quality Speech Coding}", "normalized_title": "scaling transformers for low bitrate high quality speech coding", "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "abstract": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "source": "arxiv", "source_id": "2411.19842v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "abstract": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "metadata": {"arxiv_id": "2411.19842v1", "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "summary": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "authors": ["Julian D Parker", "Anton Smirnov", "Jordi Pons", "CJ Carr", "Zack Zukowski", "Zach Evans", "Xubo Liu"], "published": "2024-11-29T16:58:02Z", "updated": "2024-11-29T16:58:02Z", "categories": ["eess.AS", "cs.AI", "cs.LG", "cs.SD", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2411.19842v1", "landing_url": "https://arxiv.org/abs/2411.19842v1", "doi": "https://doi.org/10.48550/arXiv.2411.19842"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly discusses discrete audio tokens produced by neural audio codecs via quantization (FSQ) and token-based processing for speech coding, meeting the Discrete Audio Tokens topic and providing methodological details, so it should be included and does not fit the exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly discusses discrete audio tokens produced by neural audio codecs via quantization (FSQ) and token-based processing for speech coding, meeting the Discrete Audio Tokens topic and providing methodological details, so it should be included and does not fit the exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study discusses a neural audio codec model that scales transformers for low-bitrate speech coding, using tokenization methods involving discrete representation through a finite scalar quantization bottleneck, which aligns with the definition of producing discrete audio tokens from continuous speech waveforms. It provides details on the tokenizer architecture, bitrate, and demonstrates improved speech quality, meeting inclusion criteria of using neural audio codec derived discrete tokens and contributing new designs with empirical evaluation. The abstract is in English and sufficiently describes the quantization and token bottleneck, supporting reproducibility assessment, and the study focuses on token design impact on audio generation quality, thus fitting the discrete audio tokens theme and not meeting any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study discusses a neural audio codec model that scales transformers for low-bitrate speech coding, using tokenization methods involving discrete representation through a finite scalar quantization bottleneck, which aligns with the definition of producing discrete audio tokens from continuous speech waveforms. It provides details on the tokenizer architecture, bitrate, and demonstrates improved speech quality, meeting inclusion criteria of using neural audio codec derived discrete tokens and contributing new designs with empirical evaluation. The abstract is in English and sufficiently describes the quantization and token bottleneck, supporting reproducibility assessment, and the study focuses on token design impact on audio generation quality, thus fitting the discrete audio tokens theme and not meeting any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "bai2024dmel", "query_title": "{{D}mel: Speech tokenization made simple}", "normalized_title": "dmel speech tokenization made simple", "title": "dMel: Speech Tokenization made Simple", "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.", "source": "arxiv", "source_id": "2407.15835", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "dMel: Speech Tokenization made Simple", "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.", "metadata": {"arxiv_id": "2407.15835", "title": "dMel: Speech Tokenization made Simple", "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.", "authors": ["Richard He Bai", "Tatiana Likhomanenko", "Ruixiang Zhang", "Zijin Gu", "Zakaria Aldeneh", "Navdeep Jaitly"], "published": "2024-07-22T17:51:53Z", "updated": "2025-05-21T16:55:34Z", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.15835v3", "landing_url": "https://arxiv.org/abs/2407.15835v3", "doi": "https://doi.org/10.48550/arXiv.2407.15835"}, "round-A_JuniorNano_output": {"reasoning": "The paper introduces dMel, a discrete speech token representation by discretizing mel-filterbank channels, uses an LM-style transformer for token-based modeling, and demonstrates results on downstream TTS and ASR tasks, aligning with the discrete audio tokens criteria and providing token design and evaluation details, with no indication it focuses solely on continuous representations or non-token-based tasks.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper introduces dMel, a discrete speech token representation by discretizing mel-filterbank channels, uses an LM-style transformer for token-based modeling, and demonstrates results on downstream TTS and ASR tasks, aligning with the discrete audio tokens criteria and providing token design and evaluation details, with no indication it focuses solely on continuous representations or non-token-based tasks.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins to create discrete tokens, aiming to enable language modeling techniques on discrete speech tokens. It focuses on token design, compression, and its applications to TTS and ASR, meeting the inclusion criteria of discrete audio tokens, the process of tokenization, and analysis of downstream tasks. The abstract provides sufficient English technical details about the tokenization method and experiments, aligning with the inclusion criteria and not triggering exclusion conditions.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins to create discrete tokens, aiming to enable language modeling techniques on discrete speech tokens. It focuses on token design, compression, and its applications to TTS and ASR, meeting the inclusion criteria of discrete audio tokens, the process of tokenization, and analysis of downstream tasks. The abstract provides sufficient English technical details about the tokenization method and experiments, aligning with the inclusion criteria and not triggering exclusion conditions.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "ai2024apcodec", "query_title": "{APCodec: A neural audio codec with parallel amplitude and phase spectrum encoding and decoding}", "normalized_title": "apcodec a neural audio codec with parallel amplitude and phase spectrum encoding and decoding", "title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding", "abstract": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.", "source": "arxiv", "source_id": "2402.10533v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding", "abstract": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.", "metadata": {"arxiv_id": "2402.10533v2", "title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding", "summary": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.", "authors": ["Yang Ai", "Xiao-Hang Jiang", "Ye-Xin Lu", "Hui-Peng Du", "Zhen-Hua Ling"], "published": "2024-02-16T09:38:16Z", "updated": "2024-09-24T01:14:07Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2402.10533v2", "landing_url": "https://arxiv.org/abs/2402.10533v2", "doi": "https://doi.org/10.48550/arXiv.2402.10533"}, "round-A_JuniorNano_output": {"reasoning": "APCodec introduces a neural audio codec that uses residual vector quantization to produce a discrete latent code, i.e., discrete tokens, for amplitude/phase spectra; this aligns with the core idea of discrete audio tokens and provides English description of the tokenization/quantization process, triggering inclusion while not violating the exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "APCodec introduces a neural audio codec that uses residual vector quantization to produce a discrete latent code, i.e., discrete tokens, for amplitude/phase spectra; this aligns with the core idea of discrete audio tokens and provides English description of the tokenization/quantization process, triggering inclusion while not violating the exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a neural audio codec using residual vector quantization (a form of discretization) to compress amplitude and phase spectra into continuous latent codes subsequently quantized into discrete tokens. This aligns with the definition of \"Discrete Audio Tokens\" through neural audio codec-produced discrete vocabularies. The details on codec architecture, quantization method, bitrate, and experimental evaluations suggest sufficient information for reproducibility and systemic discussion, meeting inclusion criteria. The study does not focus on continuous audio representations alone, nor is it limited to traditional compression without neural-based discrete tokenization. Therefore, it fits the scope for inclusion.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper introduces a neural audio codec using residual vector quantization (a form of discretization) to compress amplitude and phase spectra into continuous latent codes subsequently quantized into discrete tokens. This aligns with the definition of \"Discrete Audio Tokens\" through neural audio codec-produced discrete vocabularies. The details on codec architecture, quantization method, bitrate, and experimental evaluations suggest sufficient information for reproducibility and systemic discussion, meeting inclusion criteria. The study does not focus on continuous audio representations alone, nor is it limited to traditional compression without neural-based discrete tokenization. Therefore, it fits the scope for inclusion.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "kolbaek2017multitalker", "query_title": "{{M}ultitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks}", "normalized_title": "multitalker speech separation with utterance level permutation invariant training of deep recurrent neural networks", "title": "Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks", "abstract": "In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "source": "semantic_scholar", "source_id": "256ad591c6fd5269fc6f88b9715bf379f210f53d", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks", "abstract": "In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "metadata": {"arxiv_id": "semantic_scholar:256ad591c6fd5269fc6f88b9715bf379f210f53d", "title": "Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks", "summary": "In this paper, we propose the utterance-level permutation invariant training (uPIT) technique. uPIT is a practically applicable, end-to-end, deep-learning-based solution for speaker independent multitalker speech separation. Specifically, uPIT extends the recently proposed permutation invariant training (PIT) technique with an utterance-level cost function, hence eliminating the need for solving an additional permutation problem during inference, which is otherwise required by frame-level PIT. We achieve this using recurrent neural networks (RNNs) that, during training, minimize the utterance-level separation error, hence forcing separated frames belonging to the same speaker to be aligned to the same output stream. In practice, this allows RNNs, trained with uPIT, to separate multitalker mixed speech without any prior knowledge of signal duration, number of speakers, speaker identity, or gender. We evaluated uPIT on the WSJ0 and Danish two- and three-talker mixed-speech separation tasks and found that uPIT outperforms techniques based on nonnegative matrix factorization and computational auditory scene analysis, and compares favorably with deep clustering, and the deep attractor network. Furthermore, we found that models trained with uPIT generalize well to unseen speakers and languages. Finally, we found that a single model, trained with uPIT, can handle both two-speaker, and three-speaker speech mixtures.", "source": "semantic_scholar", "source_id": "256ad591c6fd5269fc6f88b9715bf379f210f53d", "publication_date": "2017-03-18"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on utterance-level permutation invariant training for multitalker speech separation and does not discuss discrete audio tokens, tokenization, or neural audio codecs, thus not meeting the inclusion criteria for discrete audio token representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on utterance-level permutation invariant training for multitalker speech separation and does not discuss discrete audio tokens, tokenization, or neural audio codecs, thus not meeting the inclusion criteria for discrete audio token representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on multitalker speech separation using deep recurrent neural networks with permutation invariant training; however, it does not discuss discretization of continuous audio waveforms into discrete token sequences, neural audio codecs, vector quantization, or the design and evaluation of discrete audio tokens as defined in the inclusion criteria. Therefore, it does not meet the required focus on discrete audio tokens and related methodologies.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on multitalker speech separation using deep recurrent neural networks with permutation invariant training; however, it does not discuss discretization of continuous audio waveforms into discrete token sequences, neural audio codecs, vector quantization, or the design and evaluation of discrete audio tokens as defined in the inclusion criteria. Therefore, it does not meet the required focus on discrete audio tokens and related methodologies.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "piczak2015esc", "query_title": "{ESC: Dataset for environmental sound classification}", "normalized_title": "esc dataset for environmental sound classification", "title": "ESC: Dataset for Environmental Sound Classification", "abstract": "One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.", "source": "semantic_scholar", "source_id": "99e6f700d374e34c8376f1f43af994b278924f28", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESC: Dataset for Environmental Sound Classification", "abstract": "One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.", "metadata": {"arxiv_id": "semantic_scholar:99e6f700d374e34c8376f1f43af994b278924f28", "title": "ESC: Dataset for Environmental Sound Classification", "summary": "One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.", "source": "semantic_scholar", "source_id": "99e6f700d374e34c8376f1f43af994b278924f28", "publication_date": "2015-10-13"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on an environmental sound dataset with MFCC/zero-crossing features and baseline classifiers, and contains no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on an environmental sound dataset with MFCC/zero-crossing features and baseline classifiers, and contains no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a dataset for environmental sound classification and evaluates classifiers on this dataset. However, it does not discuss or present any discrete audio tokenization methods, neural audio codecs, or quantization strategies that transform continuous audio into discrete token sequences, nor does it focus on token design or downstream generative or multi-modal tasks involving discrete tokens. Its emphasis is on dataset creation and classification tasks, which falls under continuous audio representation and discriminative tasks, making it outside the scope of the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a dataset for environmental sound classification and evaluates classifiers on this dataset. However, it does not discuss or present any discrete audio tokenization methods, neural audio codecs, or quantization strategies that transform continuous audio into discrete token sequences, nor does it focus on token design or downstream generative or multi-modal tasks involving discrete tokens. Its emphasis is on dataset creation and classification tasks, which falls under continuous audio representation and discriminative tasks, making it outside the scope of the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "tzanetakis2002musical", "query_title": "{Musical genre classification of audio signals}", "normalized_title": "musical genre classification of audio signals", "title": "Musical genre classification of audio signals", "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.", "source": "semantic_scholar", "source_id": "7ab881283270e427b05c6e9469562ff39dd6282a", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Musical genre classification of audio signals", "abstract": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.", "metadata": {"arxiv_id": "semantic_scholar:7ab881283270e427b05c6e9469562ff39dd6282a", "title": "Musical genre classification of audio signals", "summary": "Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.", "source": "semantic_scholar", "source_id": "7ab881283270e427b05c6e9469562ff39dd6282a", "publication_date": "2002-11-07"}, "round-A_JuniorNano_output": {"reasoning": "The item addresses automatic musical genre classification using feature sets and classifiers, with no discussion of discrete audio tokens, tokenization, codecs, or token-based downstream tasks, thus it does not meet the token-focused inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item addresses automatic musical genre classification using feature sets and classifiers, with no discussion of discrete audio tokens, tokenization, codecs, or token-based downstream tasks, thus it does not meet the token-focused inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on automatic musical genre classification based on audio signal features, without describing any discretization process of continuous audio waveforms into discrete token sequences nor elaborating on neural audio codecs, vector quantization, or any kind of token vocabulary/indexing mechanism. It mainly addresses classification tasks without proposing or analyzing discrete audio tokens or their applications to generative or multimodal modeling. Therefore, it does not meet the inclusion criteria that require the presence and detailed description of discrete audio tokens and related design or evaluation aspects.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on automatic musical genre classification based on audio signal features, without describing any discretization process of continuous audio waveforms into discrete token sequences nor elaborating on neural audio codecs, vector quantization, or any kind of token vocabulary/indexing mechanism. It mainly addresses classification tasks without proposing or analyzing discrete audio tokens or their applications to generative or multimodal modeling. Therefore, it does not meet the inclusion criteria that require the presence and detailed description of discrete audio tokens and related design or evaluation aspects.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "li2023mert", "query_title": "{{MERT}: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}", "normalized_title": "mert acoustic music understanding model with large scale self supervised training", "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training", "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.", "source": "arxiv", "source_id": "2306.00107v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training", "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.", "metadata": {"arxiv_id": "2306.00107v5", "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training", "summary": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.", "authors": ["Yizhi Li", "Ruibin Yuan", "Ge Zhang", "Yinghao Ma", "Xingran Chen", "Hanzhi Yin", "Chenghao Xiao", "Chenghua Lin", "Anton Ragni", "Emmanouil Benetos", "Norbert Gyenge", "Roger Dannenberg", "Ruibo Liu", "Wenhu Chen", "Gus Xia", "Yemin Shi", "Wenhao Huang", "Zili Wang", "Yike Guo", "Jie Fu"], "published": "2023-05-31T18:27:43Z", "updated": "2024-12-27T12:28:34Z", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2306.00107v5", "landing_url": "https://arxiv.org/abs/2306.00107v5", "doi": "https://doi.org/10.48550/arXiv.2306.00107"}, "round-A_JuniorNano_output": {"reasoning": "The abstract indicates use of RVQ-VAE (a neural audio codec producing discrete tokens) and MLM-style pretraining for music understanding, which aligns with discrete audio token design and downstream tasks, so include.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract indicates use of RVQ-VAE (a neural audio codec producing discrete tokens) and MLM-style pretraining for music understanding, which aligns with discrete audio token design and downstream tasks, so include.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper proposes the MERT model for acoustic music understanding with large-scale self-supervised training and uses teacher models including RVQ-VAE (a vector quantization variational autoencoder) and Constant-Q Transform to generate pseudo labels in masked language modeling style, indicating use of discrete token representations derived from neural audio encoders for music audio; however, the abstract lacks explicit detailed descriptions of the tokenizer architecture, quantization specifics, vocabulary/codebook configurations, or tokenization process needed to fully confirm reproducibility and clear connection to discrete audio tokens as per the inclusion criteria. Nevertheless, the use of RVQ-VAE suggests a discrete token approach aligned with the topic. Thus, while it strongly relates to the discrete audio tokens theme, the insufficient explicit description of token generation and codebook settings leaves some uncertainty on eligibility.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The paper proposes the MERT model for acoustic music understanding with large-scale self-supervised training and uses teacher models including RVQ-VAE (a vector quantization variational autoencoder) and Constant-Q Transform to generate pseudo labels in masked language modeling style, indicating use of discrete token representations derived from neural audio encoders for music audio; however, the abstract lacks explicit detailed descriptions of the tokenizer architecture, quantization specifics, vocabulary/codebook configurations, or tokenization process needed to fully confirm reproducibility and clear connection to discrete audio tokens as per the inclusion criteria. Nevertheless, the use of RVQ-VAE suggests a discrete token approach aligned with the topic. Thus, while it strongly relates to the discrete audio tokens theme, the insufficient explicit description of token generation and codebook settings leaves some uncertainty on eligibility.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Junior reviewers split: one (JuniorNano) recommended inclusion noting the use of RVQ-VAE which implies discrete tokenization; the other (JuniorMini) expressed uncertainty because the abstract lacks explicit tokenizer/codebook/vocabulary details required for full reproducibility. The paper explicitly cites a Residual Vector Quantisation VAE (RVQ-VAE) as an acoustic teacher (a neural audio codec/vector-quantization mechanism) which satisfies the ‘token source’ inclusion criterion that the work uses neural audio codec–derived discrete tokens for downstream sequence modeling; while the abstract does not list codebook/vocabulary/bitrate details, the presence of RVQ-VAE and MLM-style pseudo-labeling strongly aligns with the Discrete Audio Tokens topic and therefore merits inclusion at this screening stage. One-sentence explanation for score: I give a 4 because the abstract clearly indicates the use of a neural vector-quantized codec (RVQ-VAE) to produce discrete tokens—meeting core inclusion requirements—even though tokenizer/config details are not provided in the abstract.", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Junior reviewers split: one (JuniorNano) recommended inclusion noting the use of RVQ-VAE which implies discrete tokenization; the other (JuniorMini) expressed uncertainty because the abstract lacks explicit tokenizer/codebook/vocabulary details required for full reproducibility. The paper explicitly cites a Residual Vector Quantisation VAE (RVQ-VAE) as an acoustic teacher (a neural audio codec/vector-quantization mechanism) which satisfies the ‘token source’ inclusion criterion that the work uses neural audio codec–derived discrete tokens for downstream sequence modeling; while the abstract does not list codebook/vocabulary/bitrate details, the presence of RVQ-VAE and MLM-style pseudo-labeling strongly aligns with the Discrete Audio Tokens topic and therefore merits inclusion at this screening stage. One-sentence explanation for score: I give a 4 because the abstract clearly indicates the use of a neural vector-quantized codec (RVQ-VAE) to produce discrete tokens—meeting core inclusion requirements—even though tokenizer/config details are not provided in the abstract.", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "kong2020panns", "query_title": "{Panns: Large-scale pretrained audio neural networks for audio pattern recognition}", "normalized_title": "panns large scale pretrained audio neural networks for audio pattern recognition", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "abstract": "Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.", "source": "arxiv", "source_id": "1912.10211v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "abstract": "Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.", "metadata": {"arxiv_id": "1912.10211v5", "title": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition", "summary": "Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset_tagging_cnn.", "authors": ["Qiuqiang Kong", "Yin Cao", "Turab Iqbal", "Yuxuan Wang", "Wenwu Wang", "Mark D. Plumbley"], "published": "2019-12-21T06:53:14Z", "updated": "2020-08-23T11:35:41Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1912.10211v5", "landing_url": "https://arxiv.org/abs/1912.10211v5", "doi": "https://doi.org/10.48550/arXiv.1912.10211"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents large-scale pretrained audio neural networks for audio pattern recognition and experiments on log-mel and waveform inputs, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the token-centric inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper presents large-scale pretrained audio neural networks for audio pattern recognition and experiments on log-mel and waveform inputs, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the token-centric inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes pretrained audio neural networks for audio pattern recognition using continuous audio features like log-mel spectrogram and waveforms, but does not describe or use any discrete audio tokens obtained via discretization or neural audio codecs; it focuses on continuous feature modeling for classification tasks without addressing discrete tokenization or token design which are essential to the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper proposes pretrained audio neural networks for audio pattern recognition using continuous audio features like log-mel spectrogram and waveforms, but does not describe or use any discrete audio tokens obtained via discretization or neural audio codecs; it focuses on continuous feature modeling for classification tasks without addressing discrete tokenization or token design which are essential to the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wang2022usb", "query_title": "{Usb: A unified semi-supervised learning benchmark for classification}", "normalized_title": "usb a unified semi supervised learning benchmark for classification", "title": "USB: A Unified Semi-supervised Learning Benchmark for Classification", "abstract": "Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.", "source": "arxiv", "source_id": "2208.07204v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "USB: A Unified Semi-supervised Learning Benchmark for Classification", "abstract": "Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.", "metadata": {"arxiv_id": "2208.07204v2", "title": "USB: A Unified Semi-supervised Learning Benchmark for Classification", "summary": "Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.", "authors": ["Yidong Wang", "Hao Chen", "Yue Fan", "Wang Sun", "Ran Tao", "Wenxin Hou", "Renjie Wang", "Linyi Yang", "Zhi Zhou", "Lan-Zhe Guo", "Heli Qi", "Zhen Wu", "Yu-Feng Li", "Satoshi Nakamura", "Wei Ye", "Marios Savvides", "Bhiksha Raj", "Takahiro Shinozaki", "Bernt Schiele", "Jindong Wang", "Xing Xie", "Yue Zhang"], "published": "2022-08-12T15:45:48Z", "updated": "2022-10-14T01:49:43Z", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf_url": "https://arxiv.org/pdf/2208.07204v2", "landing_url": "https://arxiv.org/abs/2208.07204v2", "doi": "https://doi.org/10.48550/arXiv.2208.07204"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses a unified semi-supervised learning benchmark across CV/NLP/Audio and evaluation protocols; it does not address discrete audio tokens, tokenization methods, or neural audio codecs as core contributions, so it does not meet the inclusion criteria for discrete audio token studies.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses a unified semi-supervised learning benchmark across CV/NLP/Audio and evaluation protocols; it does not address discrete audio tokens, tokenization methods, or neural audio codecs as core contributions, so it does not meet the inclusion criteria for discrete audio token studies.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a Unified Semi-supervised Learning Benchmark across multiple domains including audio, but the focus is on SSL evaluation protocols rather than the generation, design, or use of discrete audio tokens from neural audio codecs or self-supervised models as defined in the inclusion criteria. The abstract does not mention discretization, tokenization, codec architecture, or the analysis of discrete token design for audio tasks, hence it does not meet the inclusion requirements for discrete audio token research, nor does it fall into any exclusion criteria for lack of English content or relevance to audio token methods, but overall it does not align with the core topic of discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper proposes a Unified Semi-supervised Learning Benchmark across multiple domains including audio, but the focus is on SSL evaluation protocols rather than the generation, design, or use of discrete audio tokens from neural audio codecs or self-supervised models as defined in the inclusion criteria. The abstract does not mention discretization, tokenization, codec architecture, or the analysis of discrete token design for audio tasks, hence it does not meet the inclusion requirements for discrete audio token research, nor does it fall into any exclusion criteria for lack of English content or relevance to audio token methods, but overall it does not align with the core topic of discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mousavi2024dasb", "query_title": "{DASB--Discrete Audio and Speech Benchmark}", "normalized_title": "dasb discrete audio and speech benchmark", "title": "DASB - Discrete Audio and Speech Benchmark", "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.", "source": "arxiv", "source_id": "2406.14294v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "DASB - Discrete Audio and Speech Benchmark", "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.", "metadata": {"arxiv_id": "2406.14294v2", "title": "DASB - Discrete Audio and Speech Benchmark", "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.", "authors": ["Pooneh Mousavi", "Luca Della Libera", "Jarod Duret", "Artem Ploujnikov", "Cem Subakan", "Mirco Ravanelli"], "published": "2024-06-20T13:23:27Z", "updated": "2024-06-21T17:07:17Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.14294v2", "landing_url": "https://arxiv.org/abs/2406.14294v2", "doi": "https://doi.org/10.48550/arXiv.2406.14294"}, "round-A_JuniorNano_output": {"reasoning": "The DASB work centers on discrete audio tokens generated by tokenizers/encoders, analyzes their design choices and evaluates them on a broad set of downstream tasks (speech recognition, speaker/emotion/keyword/intent, TTS, enhancement, separation), and provides English content suitable for reproducibility assessment, meeting all inclusion criteria without triggering any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The DASB work centers on discrete audio tokens generated by tokenizers/encoders, analyzes their design choices and evaluates them on a broad set of downstream tasks (speech recognition, speaker/emotion/keyword/intent, TTS, enhancement, separation), and provides English content suitable for reproducibility assessment, meeting all inclusion criteria without triggering any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study presents a comprehensive benchmark for discrete audio tokens which are derived from neural audio encoders, including semantic and compression tokens; it evaluates their performance across various discriminative and generative tasks relevant to audio understanding and generation, which aligns well with the inclusion criteria focusing on discrete token design and evaluation, with English title and abstract provided and technical content allowing assessment of discrete token mechanisms.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a comprehensive benchmark for discrete audio tokens which are derived from neural audio encoders, including semantic and compression tokens; it evaluates their performance across various discriminative and generative tasks relevant to audio understanding and generation, which aligns well with the inclusion criteria focusing on discrete token design and evaluation, with English title and abstract provided and technical content allowing assessment of discrete token mechanisms.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2023lmvc", "query_title": "{Lm-vc: Zero-shot voice conversion via speech generation based on language models}", "normalized_title": "lm vc zero shot voice conversion via speech generation based on language models", "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models", "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.", "source": "arxiv", "source_id": "2306.10521v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models", "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.", "metadata": {"arxiv_id": "2306.10521v2", "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models", "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.", "authors": ["Zhichao Wang", "Yuanzhe Chen", "Lei Xie", "Qiao Tian", "Yuping Wang"], "published": "2023-06-18T10:59:06Z", "updated": "2023-08-21T02:21:06Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2306.10521v2", "landing_url": "https://arxiv.org/abs/2306.10521v2", "doi": "https://doi.org/10.48550/arXiv.2306.10521"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly uses discrete audio tokens by tokenizing speech into semantic and acoustic tokens (e.g., HuBERT, SoundStream) and builds a token-based LM-driven voice-conversion framework, addressing token design and reconstruction stages, which aligns with the discrete audio token literature and downstream generation tasks.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly uses discrete audio tokens by tokenizing speech into semantic and acoustic tokens (e.g., HuBERT, SoundStream) and builds a token-based LM-driven voice-conversion framework, addressing token design and reconstruction stages, which aligns with the discrete audio token literature and downstream generation tasks.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper explores zero-shot voice conversion leveraging language models that process discrete semantic and acoustic tokens extracted using HuBERT and SoundStream; it clearly involves discrete audio tokenization through neural audio encoders and focuses on generation tasks, meeting all inclusion criteria, with no exclusion criteria triggered.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper explores zero-shot voice conversion leveraging language models that process discrete semantic and acoustic tokens extracted using HuBERT and SoundStream; it clearly involves discrete audio tokenization through neural audio encoders and focuses on generation tasks, meeting all inclusion criteria, with no exclusion criteria triggered.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "maimon2023speaking", "query_title": "{Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units}", "normalized_title": "speaking style conversion in the waveform domain using discrete self supervised units", "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units", "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "source": "arxiv", "source_id": "2212.09730v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units", "abstract": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "metadata": {"arxiv_id": "2212.09730v2", "title": "Speaking Style Conversion in the Waveform Domain Using Discrete Self-Supervised Units", "summary": "We introduce DISSC, a novel, lightweight method that converts the rhythm, pitch contour and timbre of a recording to a target speaker in a textless manner. Unlike DISSC, most voice conversion (VC) methods focus primarily on timbre, and ignore people's unique speaking style (prosody). The proposed approach uses a pretrained, self-supervised model for encoding speech to discrete units, which makes it simple, effective, and fast to train. All conversion modules are only trained on reconstruction like tasks, thus suitable for any-to-many VC with no paired data. We introduce a suite of quantitative and qualitative evaluation metrics for this setup, and empirically demonstrate that DISSC significantly outperforms the evaluated baselines. Code and samples are available at https://pages.cs.huji.ac.il/adiyoss-lab/dissc/.", "authors": ["Gallil Maimon", "Yossi Adi"], "published": "2022-12-19T18:53:04Z", "updated": "2023-10-18T19:23:27Z", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2212.09730v2", "landing_url": "https://arxiv.org/abs/2212.09730v2", "doi": "https://doi.org/10.48550/arXiv.2212.09730"}, "round-A_JuniorNano_output": {"reasoning": "The work centers on converting audio using discrete self-supervised units (discrete tokens) for voice conversion, addressing token generation/representation and downstream modeling, and is presented in English with methodological details relevant to discrete token design.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work centers on converting audio using discrete self-supervised units (discrete tokens) for voice conversion, addressing token generation/representation and downstream modeling, and is presented in English with methodological details relevant to discrete token design.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents DISSC, which uses a pretrained self-supervised model to convert speech by encoding it into discrete units, implying a tokenization process from continuous audio to discrete units. The discrete units are used as core representations for voice conversion, aligning with the inclusion criteria describing the use of self-supervised discrete tokens for audio modeling tasks. The abstract provides sufficient methodological details about the discrete units and their application. Therefore, the study meets inclusion criteria related to discrete token generation and application, and there is no indication that it falls under any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents DISSC, which uses a pretrained self-supervised model to convert speech by encoding it into discrete units, implying a tokenization process from continuous audio to discrete units. The discrete units are used as core representations for voice conversion, aligning with the inclusion criteria describing the use of self-supervised discrete tokens for audio modeling tasks. The abstract provides sufficient methodological details about the discrete units and their application. Therefore, the study meets inclusion criteria related to discrete token generation and application, and there is no indication that it falls under any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wang2024streamvoice", "query_title": "{StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion}", "normalized_title": "streamvoice streamable context aware language modeling for real time zero shot voice conversion", "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion", "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model's forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice's streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.", "source": "arxiv", "source_id": "2401.11053v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion", "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model's forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice's streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.", "metadata": {"arxiv_id": "2401.11053v5", "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion", "summary": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model's forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice's streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.", "authors": ["Zhichao Wang", "Yuanzhe Chen", "Xinsheng Wang", "Lei Xie", "Yuping Wang"], "published": "2024-01-19T23:05:05Z", "updated": "2024-07-19T02:22:44Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2401.11053v5", "landing_url": "https://arxiv.org/abs/2401.11053v5", "doi": "https://doi.org/10.48550/arXiv.2401.11053"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on streaming LM-based real-time zero-shot voice conversion and continuous acoustic representations, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the discrete-audio-token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on streaming LM-based real-time zero-shot voice conversion and continuous acoustic representations, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the discrete-audio-token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a streaming LM-based zero-shot voice conversion model focused on converting source speech semantics to acoustic features in real time. However, the abstract does not mention the use or extraction of discrete audio tokens through discretization, neural audio codecs, vector quantization, or any discrete token vocabulary that can be modeled by sequence models; instead, it focuses on language modeling of continuous acoustic features. Thus, it does not meet the inclusion criteria centered on discrete audio tokens as core representations for generating or modeling audio.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a streaming LM-based zero-shot voice conversion model focused on converting source speech semantics to acoustic features in real time. However, the abstract does not mention the use or extraction of discrete audio tokens through discretization, neural audio codecs, vector quantization, or any discrete token vocabulary that can be modeled by sequence models; instead, it focuses on language modeling of continuous acoustic features. Thus, it does not meet the inclusion criteria centered on discrete audio tokens as core representations for generating or modeling audio.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "shi2024espnet", "query_title": "{{ESPnet-Codec}: Comprehensive training and evaluation of neural codecs for audio, music, and speech}", "normalized_title": "espnet codec comprehensive training and evaluation of neural codecs for audio music and speech", "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech", "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.", "source": "arxiv", "source_id": "2409.15897v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech", "abstract": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.", "metadata": {"arxiv_id": "2409.15897v2", "title": "ESPnet-Codec: Comprehensive Training and Evaluation of Neural Codecs for Audio, Music, and Speech", "summary": "Neural codecs have become crucial to recent speech and audio generation research. In addition to signal compression capabilities, discrete codecs have also been found to enhance downstream training efficiency and compatibility with autoregressive language models. However, as extensive downstream applications are investigated, challenges have arisen in ensuring fair comparisons across diverse applications. To address these issues, we present a new open-source platform ESPnet-Codec, which is built on ESPnet and focuses on neural codec training and evaluation. ESPnet-Codec offers various recipes in audio, music, and speech for training and evaluation using several widely adopted codec models. Together with ESPnet-Codec, we present VERSA, a standalone evaluation toolkit, which provides a comprehensive evaluation of codec performance over 20 audio evaluation metrics. Notably, we demonstrate that ESPnet-Codec can be integrated into six ESPnet tasks, supporting diverse applications.", "authors": ["Jiatong Shi", "Jinchuan Tian", "Yihan Wu", "Jee-weon Jung", "Jia Qi Yip", "Yoshiki Masuyama", "William Chen", "Yuning Wu", "Yuxun Tang", "Massa Baali", "Dareen Alharhi", "Dong Zhang", "Ruifan Deng", "Tejes Srivastava", "Haibin Wu", "Alexander H. Liu", "Bhiksha Raj", "Qin Jin", "Ruihua Song", "Shinji Watanabe"], "published": "2024-09-24T09:16:11Z", "updated": "2025-02-24T18:34:41Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2409.15897v2", "landing_url": "https://arxiv.org/abs/2409.15897v2", "doi": "https://doi.org/10.48550/arXiv.2409.15897"}, "round-A_JuniorNano_output": {"reasoning": "While the abstract discusses neural/discrete codecs, it does not explicitly describe discrete audio token generation, tokenizers, or token-based downstream tasks, so it does not clearly meet the core inclusion criteria focused on discrete audio tokens.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "While the abstract discusses neural/discrete codecs, it does not explicitly describe discrete audio token generation, tokenizers, or token-based downstream tasks, so it does not clearly meet the core inclusion criteria focused on discrete audio tokens.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The study focuses on neural audio codecs that perform discretization of audio signals into token sequences, supported by comprehensive training and evaluation on various audio types, which aligns with the inclusion criteria for discrete audio tokens and discusses codec architectures and evaluations that support reproducibility and downstream tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on neural audio codecs that perform discretization of audio signals into token sequences, supported by comprehensive training and evaluation on various audio types, which aligns with the inclusion criteria for discrete audio tokens and discusses codec architectures and evaluations that support reproducibility and downstream tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": {"reasoning": "Junior reviewers disagreed (one recommended exclusion due to lack of explicit tokenization detail; the other recommended inclusion citing codec discretization and downstream compatibility). The abstract explicitly discusses “discrete codecs,” their value for downstream training and compatibility with autoregressive language models, and an open-source platform (ESPnet-Codec) plus an evaluation toolkit (VERSA) for widely adopted codec models across audio/music/speech — all of which align well with the “Discrete Audio Tokens” topic. I score 4 (better to include) because the work clearly targets neural discrete codecs and their downstream use (matching the inclusion topic) although the abstract omits fine-grained tokenizer/codebook settings so the final inclusion should verify the full paper/repo contains the required tokenization details.", "evaluation": 4}, "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended exclusion due to lack of explicit tokenization detail; the other recommended inclusion citing codec discretization and downstream compatibility). The abstract explicitly discusses “discrete codecs,” their value for downstream training and compatibility with autoregressive language models, and an open-source platform (ESPnet-Codec) plus an evaluation toolkit (VERSA) for widely adopted codec models across audio/music/speech — all of which align well with the “Discrete Audio Tokens” topic. I score 4 (better to include) because the work clearly targets neural discrete codecs and their downstream use (matching the inclusion topic) although the abstract omits fine-grained tokenizer/codebook settings so the final inclusion should verify the full paper/repo contains the required tokenization details.", "round-B_SeniorLead_evaluation": 4, "final_verdict": "include (senior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "vashishth2024stab", "query_title": "{{STAB}: Speech Tokenizer Assessment Benchmark}", "normalized_title": "stab speech tokenizer assessment benchmark", "title": "STAB: Speech Tokenizer Assessment Benchmark", "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.", "source": "arxiv", "source_id": "2409.02384v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "STAB: Speech Tokenizer Assessment Benchmark", "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.", "metadata": {"arxiv_id": "2409.02384v1", "title": "STAB: Speech Tokenizer Assessment Benchmark", "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.", "authors": ["Shikhar Vashishth", "Harman Singh", "Shikhar Bharadwaj", "Sriram Ganapathy", "Chulayuth Asawaroengchai", "Kartik Audhkhasi", "Andrew Rosenberg", "Ankur Bapna", "Bhuvana Ramabhadran"], "published": "2024-09-04T02:20:59Z", "updated": "2024-09-04T02:20:59Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.02384v1", "landing_url": "https://arxiv.org/abs/2409.02384v1", "doi": "https://doi.org/10.48550/arXiv.2409.02384"}, "round-A_JuniorNano_output": {"reasoning": "The article presents STAB, a benchmark framework for evaluating discrete speech tokenizers (token generation, vocabularies,/codebooks) and their impact on downstream tasks, aligning with the inclusion criteria for discrete audio tokens and excluding the exclusion criteria (it's not just a continuous-representation/compression-focused or non-token-centric study).", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The article presents STAB, a benchmark framework for evaluating discrete speech tokenizers (token generation, vocabularies,/codebooks) and their impact on downstream tasks, aligning with the inclusion criteria for discrete audio tokens and excluding the exclusion criteria (it's not just a continuous-representation/compression-focused or non-token-centric study).", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study presents STAB, a comprehensive evaluation framework for speech tokenizers that convert continuous speech into discrete token sequences, aligning well with the discrete audio token topic. It discusses detailed properties of tokenizers and their correlations with downstream tasks, fitting the inclusion criteria regarding token design choices, evaluation, and English clarity. There is no indication that the study focuses only on continuous representations or non-discrete tokens, nor is it limited to discriminatory tasks without analysis of discrete tokens. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents STAB, a comprehensive evaluation framework for speech tokenizers that convert continuous speech into discrete token sequences, aligning well with the discrete audio token topic. It discusses detailed properties of tokenizers and their correlations with downstream tasks, fitting the inclusion criteria regarding token design choices, evaluation, and English clarity. There is no indication that the study focuses only on continuous representations or non-discrete tokens, nor is it limited to discriminatory tasks without analysis of discrete tokens. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "zen2019libritts", "query_title": "{Libri{TTS}: A Corpus Derived from LibriSpeech for Text-to-Speech}", "normalized_title": "libritts a corpus derived from librispeech for text to speech", "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech", "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL.", "source": "semantic_scholar", "source_id": "2789b6c84ba1422746246685001accba5563e7c1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech", "abstract": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL.", "metadata": {"arxiv_id": "semantic_scholar:2789b6c84ba1422746246685001accba5563e7c1", "title": "LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech", "summary": "This paper introduces a new speech corpus called \"LibriTTS\" designed for text-to-speech use. It is derived from the original audio and text materials of the LibriSpeech corpus, which has been used for training and evaluating automatic speech recognition systems. The new corpus inherits desired properties of the LibriSpeech corpus while addressing a number of issues which make LibriSpeech less than ideal for text-to-speech work. The released corpus consists of 585 hours of speech data at 24kHz sampling rate from 2,456 speakers and the corresponding texts. Experimental results show that neural end-to-end TTS models trained from the LibriTTS corpus achieved above 4.0 in mean opinion scores in naturalness in five out of six evaluation speakers. The corpus is freely available for download from this http URL.", "source": "semantic_scholar", "source_id": "2789b6c84ba1422746246685001accba5563e7c1", "publication_date": "2019-04-05"}, "round-A_JuniorNano_output": {"reasoning": "This item is about a LibriSpeech-derived text-to-speech corpus and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the inclusion criteria for discrete audio tokens.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is about a LibriSpeech-derived text-to-speech corpus and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the inclusion criteria for discrete audio tokens.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces LibriTTS, a speech corpus derived for text-to-speech tasks, but it does not describe any process of discretizing audio waveforms into discrete token sequences using neural audio codecs or self-supervised models, nor does it focus on discrete audio tokens or their design, evaluation, or use in downstream tasks as defined in the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces LibriTTS, a speech corpus derived for text-to-speech tasks, but it does not describe any process of discretizing audio waveforms into discrete token sequences using neural audio codecs or self-supervised models, nor does it focus on discrete audio tokens or their design, evaluation, or use in downstream tasks as defined in the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "7178964", "query_title": "{Librispeech: An ASR corpus based on public domain audio books}", "normalized_title": "librispeech an asr corpus based on public domain audio books", "title": "Librispeech: An ASR corpus based on public domain audio books", "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.", "source": "semantic_scholar", "source_id": "34038d9424ce602d7ac917a4e582d977725d4393", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Librispeech: An ASR corpus based on public domain audio books", "abstract": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.", "metadata": {"arxiv_id": "semantic_scholar:34038d9424ce602d7ac917a4e582d977725d4393", "title": "Librispeech: An ASR corpus based on public domain audio books", "summary": "This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.", "source": "semantic_scholar", "source_id": "34038d9424ce602d7ac917a4e582d977725d4393", "publication_date": "2015-04-19"}, "round-A_JuniorNano_output": {"reasoning": "The item describes LibriSpeech as a corpus for ASR without addressing discrete audio tokens or discretization/tokenization of audio; it concerns continuous audio data rather than token-based representations, thus meeting an Exclusion criterion.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes LibriSpeech as a corpus for ASR without addressing discrete audio tokens or discretization/tokenization of audio; it concerns continuous audio data rather than token-based representations, thus meeting an Exclusion criterion.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a speech corpus derived from audiobooks, focusing on training and evaluating automatic speech recognition (ASR) systems without describing any discrete audio token generation methods, neural audio codecs, or quantization approaches, thus it does not meet the inclusion criteria of using discrete audio tokens as core representations for downstream tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a speech corpus derived from audiobooks, focusing on training and evaluating automatic speech recognition (ASR) systems without describing any discrete audio token generation methods, neural audio codecs, or quantization approaches, thus it does not meet the inclusion criteria of using discrete audio tokens as core representations for downstream tasks.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "lam2023efficient", "query_title": "{Efficient neural music generation}", "normalized_title": "efficient neural music generation", "title": "Efficient Neural Music Generation", "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.", "source": "arxiv", "source_id": "2305.15719v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Efficient Neural Music Generation", "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.", "metadata": {"arxiv_id": "2305.15719v1", "title": "Efficient Neural Music Generation", "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.", "authors": ["Max W. Y. Lam", "Qiao Tian", "Tang Li", "Zongyu Yin", "Siyuan Feng", "Ming Tu", "Yuliang Ji", "Rui Xia", "Mingbo Ma", "Xuchen Song", "Jitong Chen", "Yuping Wang", "Yuxuan Wang"], "published": "2023-05-25T05:02:35Z", "updated": "2023-05-25T05:02:35Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.15719v1", "landing_url": "https://arxiv.org/abs/2305.15719v1", "doi": "https://doi.org/10.48550/arXiv.2305.15719"}, "round-A_JuniorNano_output": {"reasoning": "Although the abstract discusses using 'semantic tokens' and a token-conditioned diffusion process with an audio VAE-GAN, it does not provide explicit details about a discrete tokenization/codec (tokenizer, codebook, vocabulary) used as the core representation, leaving its alignment with the 'Discrete Audio Tokens' inclusion criteria ambiguous.", "evaluation": 3}, "round-A_JuniorNano_reasoning": "Although the abstract discusses using 'semantic tokens' and a token-conditioned diffusion process with an audio VAE-GAN, it does not provide explicit details about a discrete tokenization/codec (tokenizer, codebook, vocabulary) used as the core representation, leaving its alignment with the 'Discrete Audio Tokens' inclusion criteria ambiguous.", "round-A_JuniorNano_evaluation": 3, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on efficient neural music generation using a novel LM-guided diffusion model that builds upon an existing hierarchical LM (MusicLM). However, the abstract does not clearly state the generation or usage of discrete audio tokens via neural audio codecs or vector quantization, nor does it describe discrete token vocabularies, quantization schemes, or tokenization details. The emphasis is on the efficiency and quality of waveform generation through diffusion modeling, without explicit mention of discrete audio token representations which are essential per the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on efficient neural music generation using a novel LM-guided diffusion model that builds upon an existing hierarchical LM (MusicLM). However, the abstract does not clearly state the generation or usage of discrete audio tokens via neural audio codecs or vector quantization, nor does it describe discrete token vocabularies, quantization schemes, or tokenization details. The emphasis is on the efficiency and quality of waveform generation through diffusion modeling, without explicit mention of discrete audio token representations which are essential per the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers flagged that the abstract references 'semantic tokens' and builds on MusicLM but does not provide explicit tokenizer/codec/codebook/quantization details required by the \"Discrete Audio Tokens\" inclusion criteria; while the mention of semantic tokens suggests discrete-token usage, the abstract is ambiguous and lacks the methodological details needed to confirm it meets the topic requirements. I therefore rate this as 3 (Not sure) because the paper likely uses discrete semantic tokens but the abstract does not supply the explicit tokenization/codec specifications required to deterministically include it under the discrete audio tokens topic.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "Both junior reviewers flagged that the abstract references 'semantic tokens' and builds on MusicLM but does not provide explicit tokenizer/codec/codebook/quantization details required by the \"Discrete Audio Tokens\" inclusion criteria; while the mention of semantic tokens suggests discrete-token usage, the abstract is ambiguous and lacks the methodological details needed to confirm it meets the topic requirements. I therefore rate this as 3 (Not sure) because the paper likely uses discrete semantic tokens but the abstract does not supply the explicit tokenization/codec specifications required to deterministically include it under the discrete audio tokens topic.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "rouard2024audio", "query_title": "{Audio conditioning for music generation via discrete bottleneck features}", "normalized_title": "audio conditioning for music generation via discrete bottleneck features", "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features", "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on https://musicgenstyle.github.io in order to show the quality of our model.", "source": "arxiv", "source_id": "2407.12563v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features", "abstract": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on https://musicgenstyle.github.io in order to show the quality of our model.", "metadata": {"arxiv_id": "2407.12563v2", "title": "Audio Conditioning for Music Generation via Discrete Bottleneck Features", "summary": "While most music generation models use textual or parametric conditioning (e.g. tempo, harmony, musical genre), we propose to condition a language model based music generation system with audio input. Our exploration involves two distinct strategies. The first strategy, termed textual inversion, leverages a pre-trained text-to-music model to map audio input to corresponding \"pseudowords\" in the textual embedding space. For the second model we train a music language model from scratch jointly with a text conditioner and a quantized audio feature extractor. At inference time, we can mix textual and audio conditioning and balance them thanks to a novel double classifier free guidance method. We conduct automatic and human studies that validates our approach. We will release the code and we provide music samples on https://musicgenstyle.github.io in order to show the quality of our model.", "authors": ["Simon Rouard", "Yossi Adi", "Jade Copet", "Axel Roebel", "Alexandre Défossez"], "published": "2024-07-17T13:47:17Z", "updated": "2024-07-30T09:44:09Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.12563v2", "landing_url": "https://arxiv.org/abs/2407.12563v2", "doi": "https://doi.org/10.48550/arXiv.2407.12563"}, "round-A_JuniorNano_output": {"reasoning": "The work uses discrete audio tokens via a quantized audio feature extractor for conditioning a music generation language model, directly addressing discrete token representations and providing implementation details, thus meeting inclusion and not triggering exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work uses discrete audio tokens via a quantized audio feature extractor for conditioning a music generation language model, directly addressing discrete token representations and providing implementation details, thus meeting inclusion and not triggering exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes conditioning a language model for music generation using audio input, involving a quantized audio feature extractor and discrete bottleneck features, which aligns with the discrete audio token concept described in the inclusion criteria as it involves discretization of audio into tokens for generative modeling. The abstract suggests the use of a quantized audio feature extractor (implying a codec or vector quantization mechanism) used jointly with a text conditioner in a language model, and validates its approach through studies. Although detailed tokenizer or codec architecture is not explicitly described in the abstract, the mention of quantized features and discrete bottleneck features strongly suggests the generation of discrete tokens from audio for the modeling task. The system addresses music generation (a relevant downstream task) using discrete audio tokens and provides empirical evaluation including human studies, fulfilling several key criteria for inclusion. The information is in English and provides enough context for evaluation. Therefore, the paper matches the inclusion criteria and does not violate any exclusion criteria based on the abstract and title provided.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study proposes conditioning a language model for music generation using audio input, involving a quantized audio feature extractor and discrete bottleneck features, which aligns with the discrete audio token concept described in the inclusion criteria as it involves discretization of audio into tokens for generative modeling. The abstract suggests the use of a quantized audio feature extractor (implying a codec or vector quantization mechanism) used jointly with a text conditioner in a language model, and validates its approach through studies. Although detailed tokenizer or codec architecture is not explicitly described in the abstract, the mention of quantized features and discrete bottleneck features strongly suggests the generation of discrete tokens from audio for the modeling task. The system addresses music generation (a relevant downstream task) using discrete audio tokens and provides empirical evaluation including human studies, fulfilling several key criteria for inclusion. The information is in English and provides enough context for evaluation. Therefore, the paper matches the inclusion criteria and does not violate any exclusion criteria based on the abstract and title provided.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "chen2024musicldm", "query_title": "{Musicldm: Enhancing novelty in text-to-music generation using beat-synchronous mixup strategies}", "normalized_title": "musicldm enhancing novelty in text to music generation using beat synchronous mixup strategies", "title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies", "abstract": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.", "source": "arxiv", "source_id": "2308.01546v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies", "abstract": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.", "metadata": {"arxiv_id": "2308.01546v1", "title": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies", "summary": "Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.", "authors": ["Ke Chen", "Yusong Wu", "Haohe Liu", "Marianna Nezhurina", "Taylor Berg-Kirkpatrick", "Shlomo Dubnov"], "published": "2023-08-03T05:35:37Z", "updated": "2023-08-03T05:35:37Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.01546v1", "landing_url": "https://arxiv.org/abs/2308.01546v1", "doi": "https://doi.org/10.48550/arXiv.2308.01546"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on diffusion-based text-to-music generation and beat-synchronous mixup data augmentation, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper centers on diffusion-based text-to-music generation and beat-synchronous mixup data augmentation, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes MusicLDM, a text-to-music generation model based on diffusion models and data augmentation strategies, but it does not mention any discrete audio tokenization process such as vector quantization, neural audio codec-based tokenization, or semantic unit discretization. The focus seems to be on generating music from raw audio data through diffusion rather than on representing audio as discrete token sequences for modeling, and no details about token vocabularies, quantization, or discrete token design are provided, which does not meet the inclusion criteria requiring clear discrete token generation and usage. Therefore, this work does not satisfy the core requirement of dealing with discrete audio tokens as defined in the criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes MusicLDM, a text-to-music generation model based on diffusion models and data augmentation strategies, but it does not mention any discrete audio tokenization process such as vector quantization, neural audio codec-based tokenization, or semantic unit discretization. The focus seems to be on generating music from raw audio data through diffusion rather than on representing audio as discrete token sequences for modeling, and no details about token vocabularies, quantization, or discrete token design are provided, which does not meet the inclusion criteria requiring clear discrete token generation and usage. Therefore, this work does not satisfy the core requirement of dealing with discrete audio tokens as defined in the criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yang2024generative", "query_title": "{Generative de-quantization for neural speech codec via latent diffusion}", "normalized_title": "generative de quantization for neural speech codec via latent diffusion", "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion", "abstract": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.", "source": "arxiv", "source_id": "2311.08330v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion", "abstract": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.", "metadata": {"arxiv_id": "2311.08330v2", "title": "Generative De-Quantization for Neural Speech Codec via Latent Diffusion", "summary": "In low-bitrate speech coding, end-to-end speech coding networks aim to learn compact yet expressive features and a powerful decoder in a single network. A challenging problem as such results in unwelcome complexity increase and inferior speech quality. In this paper, we propose to separate the representation learning and information reconstruction tasks. We leverage an end-to-end codec for learning low-dimensional discrete tokens and employ a latent diffusion model to de-quantize coded features into a high-dimensional continuous space, relieving the decoder's burden of de-quantizing and upsampling. To mitigate the issue of over-smooth generation, we introduce midway-infilling with less noise reduction and stronger conditioning. In ablation studies, we investigate the hyperparameters for midway-infilling and latent diffusion space with different dimensions. Subjective listening tests show that our model outperforms the state-of-the-art at two low bitrates, 1.5 and 3 kbps. Codes and samples of this work are available on our webpage.", "authors": ["Haici Yang", "Inseon Jang", "Minje Kim"], "published": "2023-11-14T17:19:40Z", "updated": "2023-11-15T15:23:03Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2311.08330v2", "landing_url": "https://arxiv.org/abs/2311.08330v2", "doi": "https://doi.org/10.48550/arXiv.2311.08330"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly centers on discrete audio tokens produced by a neural codec and uses a latent diffusion model to de-quantize those tokens into a high-dimensional continuous space, addressing token design, quantization, and downstream generation quality—clearly matching the Discrete Audio Tokens criteria with English abstract and method details.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly centers on discrete audio tokens produced by a neural codec and uses a latent diffusion model to de-quantize those tokens into a high-dimensional continuous space, addressing token design, quantization, and downstream generation quality—clearly matching the Discrete Audio Tokens criteria with English abstract and method details.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on low-bitrate speech coding using an end-to-end codec that learns low-dimensional discrete tokens, likely via vector quantization, which aligns with the theme of discrete audio tokens as a core representation. The usage of discrete tokens derived from neural codecs to facilitate speech generation and reconstruction supports its relevance. However, although the model uses discrete tokens for coding, the emphasis seems more on de-quantization via latent diffusion rather than introducing or analyzing token design choices or their impact on downstream tasks, which is a partial fit to inclusion requirements. The paper is in English with details on token generation and codec design, satisfying the evaluability criteria, thus overall it is better to include this study for its contributions to discrete tokens in speech coding and generation.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study focuses on low-bitrate speech coding using an end-to-end codec that learns low-dimensional discrete tokens, likely via vector quantization, which aligns with the theme of discrete audio tokens as a core representation. The usage of discrete tokens derived from neural codecs to facilitate speech generation and reconstruction supports its relevance. However, although the model uses discrete tokens for coding, the emphasis seems more on de-quantization via latent diffusion rather than introducing or analyzing token design choices or their impact on downstream tasks, which is a partial fit to inclusion requirements. The paper is in English with details on token generation and codec design, satisfying the evaluability criteria, thus overall it is better to include this study for its contributions to discrete tokens in speech coding and generation.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "li2024single", "query_title": "{Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation}", "normalized_title": "single codec single codebook speech codec towards high performance speech generation", "title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation", "abstract": "The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.", "source": "arxiv", "source_id": "2406.07422v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation", "abstract": "The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.", "metadata": {"arxiv_id": "2406.07422v1", "title": "Single-Codec: Single-Codebook Speech Codec towards High-Performance Speech Generation", "summary": "The multi-codebook speech codec enables the application of large language models (LLM) in TTS but bottlenecks efficiency and robustness due to multi-sequence prediction. To avoid this obstacle, we propose Single-Codec, a single-codebook single-sequence codec, which employs a disentangled VQ-VAE to decouple speech into a time-invariant embedding and a phonetically-rich discrete sequence. Furthermore, the encoder is enhanced with 1) contextual modeling with a BLSTM module to exploit the temporal information, 2) a hybrid sampling module to alleviate distortion from upsampling and downsampling, and 3) a resampling module to encourage discrete units to carry more phonetic information. Compared with multi-codebook codecs, e.g., EnCodec and TiCodec, Single-Codec demonstrates higher reconstruction quality with a lower bandwidth of only 304bps. The effectiveness of Single-Code is further validated by LLM-TTS experiments, showing improved naturalness and intelligibility.", "authors": ["Hanzhao Li", "Liumeng Xue", "Haohan Guo", "Xinfa Zhu", "Yuanjun Lv", "Lei Xie", "Yunlin Chen", "Hao Yin", "Zhifei Li"], "published": "2024-06-11T16:22:57Z", "updated": "2024-06-11T16:22:57Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.07422v1", "landing_url": "https://arxiv.org/abs/2406.07422v1", "doi": "https://doi.org/10.48550/arXiv.2406.07422"}, "round-A_JuniorNano_output": {"reasoning": "This work explicitly constructs a discrete token sequence via a disentangled VQ-VAE (single-codebook) and uses it for speech generation with LLM-TTS, addressing token design, bitrate, and downstream applications, thus satisfying the discrete audio token criteria and not triggering any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "This work explicitly constructs a discrete token sequence via a disentangled VQ-VAE (single-codebook) and uses it for speech generation with LLM-TTS, addressing token design, bitrate, and downstream applications, thus satisfying the discrete audio token criteria and not triggering any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study describes a single-codebook neural audio codec that produces discrete tokens by vector quantization, detailing the codec architecture (disentangled VQ-VAE) and factors like bandwidth, phonetically rich discrete sequences, and improvements over multi-codebook codecs. It applies these discrete tokens in TTS with empirical evaluation, aligning well with the inclusion criteria on discrete audio tokens derived from neural codecs, their design, and downstream application and evaluation. The abstract is in English and sufficiently describes token generation for reviewer assessment, with a clear focus on discrete token representation and utilization in speech generation. There is no evidence of exclusion criteria such as continuous-only representation or lack of token details. Therefore, it meets all inclusion and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study describes a single-codebook neural audio codec that produces discrete tokens by vector quantization, detailing the codec architecture (disentangled VQ-VAE) and factors like bandwidth, phonetically rich discrete sequences, and improvements over multi-codebook codecs. It applies these discrete tokens in TTS with empirical evaluation, aligning well with the inclusion criteria on discrete audio tokens derived from neural codecs, their design, and downstream application and evaluation. The abstract is in English and sufficiently describes token generation for reviewer assessment, with a clear focus on discrete token representation and utilization in speech generation. There is no evidence of exclusion criteria such as continuous-only representation or lack of token details. Therefore, it meets all inclusion and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "huang2023repcodec", "query_title": "\"{R}ep{C}odec: A Speech Representation Codec for Speech Tokenization\"", "normalized_title": "repcodec a speech representation codec for speech tokenization", "title": "RepCodec: A Speech Representation Codec for Speech Tokenization", "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.", "source": "semantic_scholar", "source_id": "8b6c00246a0ae34f097aa64af7d9cb35b2b43a30", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "RepCodec: A Speech Representation Codec for Speech Tokenization", "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.", "metadata": {"arxiv_id": "semantic_scholar:8b6c00246a0ae34f097aa64af7d9cb35b2b43a30", "title": "RepCodec: A Speech Representation Codec for Speech Tokenization", "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.", "source": "semantic_scholar", "source_id": "8b6c00246a0ae34f097aa64af7d9cb35b2b43a30", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "The study introduces RepCodec, a vector-quantized codec that converts speech waveforms into discrete semantic tokens using a codebook, and reports downstream task performance, directly addressing discrete audio tokens and tokenization concepts.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The study introduces RepCodec, a vector-quantized codec that converts speech waveforms into discrete semantic tokens using a codebook, and reports downstream task performance, directly addressing discrete audio tokens and tokenization concepts.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper clearly focuses on discrete speech tokenization by proposing RepCodec that transforms continuous speech waveforms into semantic tokens via vector quantization and neural speech encoders; it addresses token design choices affecting information retention and downstream tasks like speech understanding and generation, with clear English abstract and methodology details, thus satisfying all inclusion criteria without any exclusion triggers.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper clearly focuses on discrete speech tokenization by proposing RepCodec that transforms continuous speech waveforms into semantic tokens via vector quantization and neural speech encoders; it addresses token design choices affecting information retention and downstream tasks like speech understanding and generation, with clear English abstract and methodology details, thus satisfying all inclusion criteria without any exclusion triggers.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "siuzdak2023vocos", "query_title": "{Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}", "normalized_title": "vocos closing the gap between time domain and fourier based neural vocoders for high quality audio synthesis", "title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis", "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced at https://github.com/gemelo-ai/vocos.", "source": "arxiv", "source_id": "2306.00814v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis", "abstract": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced at https://github.com/gemelo-ai/vocos.", "metadata": {"arxiv_id": "2306.00814v3", "title": "Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis", "summary": "Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks (GANs) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in reduntant and computionally-intensive upsampling operations. Fourier-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates Fourier spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced at https://github.com/gemelo-ai/vocos.", "authors": ["Hubert Siuzdak"], "published": "2023-06-01T15:40:32Z", "updated": "2024-05-29T14:21:47Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2306.00814v3", "landing_url": "https://arxiv.org/abs/2306.00814v3", "doi": "https://doi.org/10.48550/arXiv.2306.00814"}, "round-A_JuniorNano_output": {"reasoning": "The article centers on continuous Fourier-based spectral vocoding and time-domain vs Fourier-based approaches, with no discussion of discrete audio tokens, tokenizers, or codebooks, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article centers on continuous Fourier-based spectral vocoding and time-domain vs Fourier-based approaches, with no discussion of discrete audio tokens, tokenizers, or codebooks, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on neural vocoders generating audio spectral coefficients but does not mention the discrete tokenization process or the use of discrete audio tokens as defined in the inclusion criteria, nor does it describe the design or evaluation of discrete audio token sequences or tokenizers. Therefore, it does not meet the core topic requirements related to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on neural vocoders generating audio spectral coefficients but does not mention the discrete tokenization process or the use of discrete audio tokens as defined in the inclusion criteria, nor does it describe the design or evaluation of discrete audio token sequences or tokenizers. Therefore, it does not meet the core topic requirements related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kang2024libriheavy", "query_title": "{Libriheavy: A 50,000 hours ASR corpus with punctuation casing and context}", "normalized_title": "libriheavy a 50 000 hours asr corpus with punctuation casing and context", "title": "Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context", "abstract": "In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting of 50,000 hours of read English speech derived from LibriVox. To the best of our knowledge, Libriheavy is the largest freely-available corpus of speech with supervisions. Different from other open-sourced datasets that only provide normalized transcriptions, Libriheavy contains richer information such as punctuation, casing and text context, which brings more flexibility for system building. Specifically, we propose a general and efficient pipeline to locate, align and segment the audios in previously published Librilight to its corresponding texts. The same as Librilight, Libriheavy also has three training subsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We also extract the dev and test evaluation sets from the aligned audios and guarantee there is no overlapping speakers and books in training sets. Baseline systems are built on the popular CTC-Attention and transducer models. Additionally, we open-source our dataset creatation pipeline which can also be used to other audio alignment tasks.", "source": "arxiv", "source_id": "2309.08105v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context", "abstract": "In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting of 50,000 hours of read English speech derived from LibriVox. To the best of our knowledge, Libriheavy is the largest freely-available corpus of speech with supervisions. Different from other open-sourced datasets that only provide normalized transcriptions, Libriheavy contains richer information such as punctuation, casing and text context, which brings more flexibility for system building. Specifically, we propose a general and efficient pipeline to locate, align and segment the audios in previously published Librilight to its corresponding texts. The same as Librilight, Libriheavy also has three training subsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We also extract the dev and test evaluation sets from the aligned audios and guarantee there is no overlapping speakers and books in training sets. Baseline systems are built on the popular CTC-Attention and transducer models. Additionally, we open-source our dataset creatation pipeline which can also be used to other audio alignment tasks.", "metadata": {"arxiv_id": "2309.08105v2", "title": "Libriheavy: a 50,000 hours ASR corpus with punctuation casing and context", "summary": "In this paper, we introduce Libriheavy, a large-scale ASR corpus consisting of 50,000 hours of read English speech derived from LibriVox. To the best of our knowledge, Libriheavy is the largest freely-available corpus of speech with supervisions. Different from other open-sourced datasets that only provide normalized transcriptions, Libriheavy contains richer information such as punctuation, casing and text context, which brings more flexibility for system building. Specifically, we propose a general and efficient pipeline to locate, align and segment the audios in previously published Librilight to its corresponding texts. The same as Librilight, Libriheavy also has three training subsets small, medium, large of the sizes 500h, 5000h, 50000h respectively. We also extract the dev and test evaluation sets from the aligned audios and guarantee there is no overlapping speakers and books in training sets. Baseline systems are built on the popular CTC-Attention and transducer models. Additionally, we open-source our dataset creatation pipeline which can also be used to other audio alignment tasks.", "authors": ["Wei Kang", "Xiaoyu Yang", "Zengwei Yao", "Fangjun Kuang", "Yifan Yang", "Liyong Guo", "Long Lin", "Daniel Povey"], "published": "2023-09-15T01:59:21Z", "updated": "2024-01-15T01:58:51Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2309.08105v2", "landing_url": "https://arxiv.org/abs/2309.08105v2", "doi": "https://doi.org/10.48550/arXiv.2309.08105"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a large ASR corpus with punctuation/casing/context and an alignment pipeline, but provides no discussion of discrete audio tokens, tokenization, or token-based modeling; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a large ASR corpus with punctuation/casing/context and an alignment pipeline, but provides no discussion of discrete audio tokens, tokenization, or token-based modeling; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a large-scale ASR corpus with rich transcription features but does not describe any process of discretizing continuous audio waveforms into discrete audio tokens, nor does it mention neural audio codecs, vector quantization, or any tokenization mechanisms central to discrete audio tokens. Hence, it does not meet the core inclusion criteria focused on discrete audio tokens and their application.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a large-scale ASR corpus with rich transcription features but does not describe any process of discretizing continuous audio waveforms into discrete audio tokens, nor does it mention neural audio codecs, vector quantization, or any tokenization mechanisms central to discrete audio tokens. Hence, it does not meet the core inclusion criteria focused on discrete audio tokens and their application.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "maimon2024suite", "query_title": "{Salmon: A Suite for Acoustic Language Model Evaluation}", "normalized_title": "salmon a suite for acoustic language model evaluation", "title": "Salmon: A Suite for Acoustic Language Model Evaluation", "abstract": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. We make the code and data publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "source": "arxiv", "source_id": "2409.07437v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Salmon: A Suite for Acoustic Language Model Evaluation", "abstract": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. We make the code and data publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "metadata": {"arxiv_id": "2409.07437v3", "title": "Salmon: A Suite for Acoustic Language Model Evaluation", "summary": "Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. We make the code and data publicly available at https://pages.cs.huji.ac.il/adiyoss-lab/salmon/ .", "authors": ["Gallil Maimon", "Amit Roth", "Yossi Adi"], "published": "2024-09-11T17:34:52Z", "updated": "2025-01-15T06:39:57Z", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.07437v3", "landing_url": "https://arxiv.org/abs/2409.07437v3", "doi": "https://doi.org/10.48550/arXiv.2409.07437"}, "round-A_JuniorNano_output": {"reasoning": "The Salmon SALMon paper presents an acoustic evaluation suite for language models and does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the inclusion criteria focused on discrete audio tokens.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The Salmon SALMon paper presents an acoustic evaluation suite for language models and does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the inclusion criteria focused on discrete audio tokens.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on evaluating speech language models in terms of their acoustic understanding, such as background noise, emotion, and speaker identity, but it does not discuss or involve discretizing continuous audio waveforms into discrete token sequences, nor does it detail any neural audio codec or tokenization process relevant to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on evaluating speech language models in terms of their acoustic understanding, such as background noise, emotion, and speaker identity, but it does not discuss or involve discretizing continuous audio waveforms into discrete token sequences, nor does it detail any neural audio codec or tokenization process relevant to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "LMCodec", "query_title": "\"Towards Codec-{LM} Co-design for Neural Codec Language Models\"", "normalized_title": "towards codec lm co design for neural codec language models", "title": "Towards Codec-LM Co-design for Neural Codec Language Models", "abstract": "Neural codec language models (or codec LMs ) are emerging as a powerful framework for audio generation tasks like text-to-speech (TTS). These models leverage advancements in language modeling and residual vector quantization (RVQ)-based audio codecs, which compress audios into discrete codes for LMs to process. Despite the close interdependence of codecs and LMs in these systems, research on codecs and LMs has largely remained siloed. In this work, we propose three techniques for better codec-LM co-design: (i) a frame-wise codec encoder that improves both LM log-likelihood and end-to-end TTS metrics, (ii) LM codebook level dropout , a method to efficiently navigate a portion of the codec-LM design space by training a single LM, and (iii) increased codec frame duration , which we show can accelerate inference while maintaining end-to-end performance. Our experiments demonstrate that combining all three co-design techniques results in doubled inference speed, and improvements in intelligibility, audio quality, and speaker con-trol in TTS relative to a siloed baseline.", "source": "semantic_scholar", "source_id": "9ae0d46500c36c58533ad0b456f517d0b4da17b4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Towards Codec-LM Co-design for Neural Codec Language Models", "abstract": "Neural codec language models (or codec LMs ) are emerging as a powerful framework for audio generation tasks like text-to-speech (TTS). These models leverage advancements in language modeling and residual vector quantization (RVQ)-based audio codecs, which compress audios into discrete codes for LMs to process. Despite the close interdependence of codecs and LMs in these systems, research on codecs and LMs has largely remained siloed. In this work, we propose three techniques for better codec-LM co-design: (i) a frame-wise codec encoder that improves both LM log-likelihood and end-to-end TTS metrics, (ii) LM codebook level dropout , a method to efficiently navigate a portion of the codec-LM design space by training a single LM, and (iii) increased codec frame duration , which we show can accelerate inference while maintaining end-to-end performance. Our experiments demonstrate that combining all three co-design techniques results in doubled inference speed, and improvements in intelligibility, audio quality, and speaker con-trol in TTS relative to a siloed baseline.", "metadata": {"arxiv_id": "semantic_scholar:9ae0d46500c36c58533ad0b456f517d0b4da17b4", "title": "Towards Codec-LM Co-design for Neural Codec Language Models", "summary": "Neural codec language models (or codec LMs ) are emerging as a powerful framework for audio generation tasks like text-to-speech (TTS). These models leverage advancements in language modeling and residual vector quantization (RVQ)-based audio codecs, which compress audios into discrete codes for LMs to process. Despite the close interdependence of codecs and LMs in these systems, research on codecs and LMs has largely remained siloed. In this work, we propose three techniques for better codec-LM co-design: (i) a frame-wise codec encoder that improves both LM log-likelihood and end-to-end TTS metrics, (ii) LM codebook level dropout , a method to efficiently navigate a portion of the codec-LM design space by training a single LM, and (iii) increased codec frame duration , which we show can accelerate inference while maintaining end-to-end performance. Our experiments demonstrate that combining all three co-design techniques results in doubled inference speed, and improvements in intelligibility, audio quality, and speaker con-trol in TTS relative to a siloed baseline.", "source": "semantic_scholar", "source_id": "9ae0d46500c36c58533ad0b456f517d0b4da17b4", "year": 2025}, "round-A_JuniorNano_output": {"reasoning": "The paper uses discrete audio tokens produced by neural audio codecs (RVQ-based) and studies codec–LM co-design with token-level design choices, impacts on LM log-likelihood/TTS metrics, and downstream generation tasks, aligning with the Discrete Audio Tokens topic and providing English abstract/details.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper uses discrete audio tokens produced by neural audio codecs (RVQ-based) and studies codec–LM co-design with token-level design choices, impacts on LM log-likelihood/TTS metrics, and downstream generation tasks, aligning with the Discrete Audio Tokens topic and providing English abstract/details.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study clearly focuses on neural codec language models that leverage residual vector quantization-based audio codecs to compress audio into discrete codes, which aligns with the inclusion criteria regarding the use of neural audio codecs to produce discrete token sequences for audio generation tasks; it also discusses design considerations impacting intelligibility, audio quality, controllability, and computational cost in TTS, fulfilling the systemic issue attentions noted. The abstract details the codec and tokenization scheme sufficiently for evaluation, and the task is related to audio generation using discrete token sequences, matching the inclusion criteria. There are no signs of exclusion criteria such as purely continuous audio representations, traditional codecs without neural or discrete token focus, or absence of clarity about tokenization.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study clearly focuses on neural codec language models that leverage residual vector quantization-based audio codecs to compress audio into discrete codes, which aligns with the inclusion criteria regarding the use of neural audio codecs to produce discrete token sequences for audio generation tasks; it also discusses design considerations impacting intelligibility, audio quality, controllability, and computational cost in TTS, fulfilling the systemic issue attentions noted. The abstract details the codec and tokenization scheme sufficiently for evaluation, and the task is related to audio generation using discrete token sequences, matching the inclusion criteria. There are no signs of exclusion criteria such as purely continuous audio representations, traditional codecs without neural or discrete token focus, or absence of clarity about tokenization.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "bie2024learning", "query_title": "{Learning source disentanglement in neural audio codec}", "normalized_title": "learning source disentanglement in neural audio codec", "title": "Learning Source Disentanglement in Neural Audio Codec", "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.", "source": "arxiv", "source_id": "2409.11228v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Learning Source Disentanglement in Neural Audio Codec", "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.", "metadata": {"arxiv_id": "2409.11228v2", "title": "Learning Source Disentanglement in Neural Audio Codec", "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.", "authors": ["Xiaoyu Bie", "Xubo Liu", "Gaël Richard"], "published": "2024-09-17T14:21:02Z", "updated": "2025-02-11T10:35:04Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.11228v2", "landing_url": "https://arxiv.org/abs/2409.11228v2", "doi": "https://doi.org/10.48550/arXiv.2409.11228"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly uses neural audio coding with multiple codebooks to produce discrete representations for different sound sources, enablingable disentanglement and potential controllable generation, which aligns with discrete audio token design/usage criteria and supports downstream tasks; it also provides English content and methodology sufficient to judge token discretization, satisfying inclusion and not triggering exclusions.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly uses neural audio coding with multiple codebooks to produce discrete representations for different sound sources, enablingable disentanglement and potential controllable generation, which aligns with discrete audio token design/usage criteria and supports downstream tasks; it also provides English content and methodology sufficient to judge token discretization, satisfying inclusion and not triggering exclusions.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study introduces a neural audio codec that discretizes continuous audio signals into discrete tokens organized in multi-domain codebooks, explicitly addressing token design choices and their effect on audio generation control and interpretability, fulfilling the inclusion criteria about discrete tokens from neural audio codecs and providing sufficient English abstract and method descriptions for evaluation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study introduces a neural audio codec that discretizes continuous audio signals into discrete tokens organized in multi-domain codebooks, explicitly addressing token design choices and their effect on audio generation control and interpretability, fulfilling the inclusion criteria about discrete tokens from neural audio codecs and providing sufficient English abstract and method descriptions for evaluation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "liu2024semanticodec", "query_title": "{Semanticodec: An ultra low bitrate semantic audio codec for general sound}", "normalized_title": "semanticodec an ultra low bitrate semantic audio codec for general sound", "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.", "source": "arxiv", "source_id": "2405.00233v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound", "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.", "metadata": {"arxiv_id": "2405.00233v2", "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound", "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.", "authors": ["Haohe Liu", "Xuenan Xu", "Yi Yuan", "Mengyue Wu", "Wenwu Wang", "Mark D. Plumbley"], "published": "2024-04-30T22:51:36Z", "updated": "2024-11-28T12:31:04Z", "categories": ["cs.SD", "cs.AI", "cs.MM", "eess.AS", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2405.00233v2", "landing_url": "https://arxiv.org/abs/2405.00233v2", "doi": "https://doi.org/10.1109/JSTSP.2024.3506286"}, "round-A_JuniorNano_output": {"reasoning": "The abstract presents a neural audio codec that generates discrete tokens (semantic tokens via AudioMAE + k-means) with token rates and a diffusion-based decoder, addressing general sound, speech, and music—matching the discrete audio token criteria and excluding non-discrete approaches.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract presents a neural audio codec that generates discrete tokens (semantic tokens via AudioMAE + k-means) with token rates and a diffusion-based decoder, addressing general sound, speech, and music—matching the discrete audio token criteria and excluding non-discrete approaches.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study introduces SemantiCodec, a novel audio codec leveraging a dual-encoder architecture involving a semantic encoder based on a self-supervised Audio Masked Autoencoder and k-means clustering to discretize audio into tokens; it explicitly discusses token rates and discrete token representations for general audio types, aligning with the defined criteria for discrete audio tokens, including tokenizer architecture and bitrate details, and aims at audio reconstruction and semantic information retention, excluding traditional continuous audio representation methods or mere discriminative tasks, meeting all inclusion criteria with detailed English abstract and method information.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study introduces SemantiCodec, a novel audio codec leveraging a dual-encoder architecture involving a semantic encoder based on a self-supervised Audio Masked Autoencoder and k-means clustering to discretize audio into tokens; it explicitly discusses token rates and discrete token representations for general audio types, aligning with the defined criteria for discrete audio tokens, including tokenizer architecture and bitrate details, and aims at audio reconstruction and semantic information retention, excluding traditional continuous audio representation methods or mere discriminative tasks, meeting all inclusion criteria with detailed English abstract and method information.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "jiang22_interspeech", "query_title": "{Cross-Scale Vector Quantization for Scalable Neural Speech Coding}", "normalized_title": "cross scale vector quantization for scalable neural speech coding", "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding", "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.", "source": "arxiv", "source_id": "2207.03067v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding", "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.", "metadata": {"arxiv_id": "2207.03067v1", "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding", "summary": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.", "authors": ["Xue Jiang", "Xiulian Peng", "Huaying Xue", "Yuan Zhang", "Yan Lu"], "published": "2022-07-07T03:23:25Z", "updated": "2022-07-07T03:23:25Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2207.03067v1", "landing_url": "https://arxiv.org/abs/2207.03067v1", "doi": "https://doi.org/10.48550/arXiv.2207.03067"}, "round-A_JuniorNano_output": {"reasoning": "The paper discusses a cross-scale vector quantization scheme in neural audio coding, implying discrete token production via codebooks (VQ) and stepwise refinement, which aligns with the discrete audio tokens criterion, though the abstract emphasizes scalability rather than token design specifics.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper discusses a cross-scale vector quantization scheme in neural audio coding, implying discrete token production via codebooks (VQ) and stepwise refinement, which aligns with the discrete audio tokens criterion, though the abstract emphasizes scalability rather than token design specifics.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a cross-scale scalable vector quantization (CSVQ) method aiming at bitrate scalability for neural audio codecs, involving vector quantization techniques to discretize audio signals progressively to obtain discrete representations. However, the abstract lacks explicit details on producing discrete token sequences mapped to a finite vocabulary for use with sequence models like Transformers or LLMs, nor does it discuss downstream tasks such as audio generation or cross-modal modeling. It mainly targets bitrate scalable compression, not deeper token design choices or systematic evaluation in generative or alignment frameworks. Hence, it partially addresses neural audio encoding but misses core criteria for discrete audio tokens as defined in inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper introduces a cross-scale scalable vector quantization (CSVQ) method aiming at bitrate scalability for neural audio codecs, involving vector quantization techniques to discretize audio signals progressively to obtain discrete representations. However, the abstract lacks explicit details on producing discrete token sequences mapped to a finite vocabulary for use with sequence models like Transformers or LLMs, nor does it discuss downstream tasks such as audio generation or cross-modal modeling. It mainly targets bitrate scalable compression, not deeper token design choices or systematic evaluation in generative or alignment frameworks. Hence, it partially addresses neural audio encoding but misses core criteria for discrete audio tokens as defined in inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": {"reasoning": "Integrated reasoning: Both junior reviewers noted the paper employs vector quantization (CSVQ) for a scalable neural audio codec—so it likely produces discrete codebook-based representations—but the abstract lacks explicit details required by our inclusion criteria (e.g., tokenizer/codebook/vocabulary settings, time downsampling, or how discrete token sequences are produced and used with sequence models). Moreover, the paper’s stated focus is bitrate scalability and compression performance rather than on designing or evaluating discrete audio tokens for downstream generative/alignment/multimodal modeling. JuniorNano leaned toward inclusion based on the use of VQ, while JuniorMini highlighted the absence of tokenization and downstream usage details; I agree with the latter assessment that the abstract does not meet the ‘‘Discrete Audio Tokens’’ scope. Summary (1-sentence explanation): Although the method uses vector quantization, the abstract lacks explicit tokenization/codebook and downstream token usage details and is centered on codec bitrate scalability, so it should be excluded.", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Integrated reasoning: Both junior reviewers noted the paper employs vector quantization (CSVQ) for a scalable neural audio codec—so it likely produces discrete codebook-based representations—but the abstract lacks explicit details required by our inclusion criteria (e.g., tokenizer/codebook/vocabulary settings, time downsampling, or how discrete token sequences are produced and used with sequence models). Moreover, the paper’s stated focus is bitrate scalability and compression performance rather than on designing or evaluating discrete audio tokens for downstream generative/alignment/multimodal modeling. JuniorNano leaned toward inclusion based on the use of VQ, while JuniorMini highlighted the absence of tokenization and downstream usage details; I agree with the latter assessment that the abstract does not meet the ‘‘Discrete Audio Tokens’’ scope. Summary (1-sentence explanation): Although the method uses vector quantization, the abstract lacks explicit tokenization/codebook and downstream token usage details and is centered on codec bitrate scalability, so it should be excluded.", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "yang2024qwen2", "query_title": "{Qwen2 technical report}", "normalized_title": "qwen2 technical report", "title": "Qwen2 Technical Report", "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.", "source": "arxiv", "source_id": "2407.10671v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Qwen2 Technical Report", "abstract": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning. The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach. To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.", "metadata": {"arxiv_id": "2407.10671v4", "title": "Qwen2 Technical Report", "summary": "This report introduces the Qwen2 series, the latest addition to our large language models and large multimodal models. We release a comprehensive suite of foundational and instruction-tuned language models, encompassing a parameter range from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts model. Qwen2 surpasses most prior open-weight models, including its predecessor Qwen1.5, and exhibits competitive performance relative to proprietary models across diverse benchmarks on language understanding, generation, multilingual proficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on MMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base language model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1 on MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2 demonstrates robust multilingual capabilities, proficient in approximately 30 languages, spanning English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and global reach.\n  To foster community innovation and accessibility, we have made the Qwen2 model weights openly available on Hugging Face and ModelScope, and the supplementary materials including example code on GitHub. These platforms also include resources for quantization, fine-tuning, and deployment, facilitating a wide range of applications and research endeavors.", "authors": ["An Yang", "Baosong Yang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chang Zhou", "Chengpeng Li", "Chengyuan Li", "Dayiheng Liu", "Fei Huang", "Guanting Dong", "Haoran Wei", "Huan Lin", "Jialong Tang", "Jialin Wang", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Ma", "Jianxin Yang", "Jin Xu", "Jingren Zhou", "Jinze Bai", "Jinzheng He", "Junyang Lin", "Kai Dang", "Keming Lu", "Keqin Chen", "Kexin Yang", "Mei Li", "Mingfeng Xue", "Na Ni", "Pei Zhang", "Peng Wang", "Ru Peng", "Rui Men", "Ruize Gao", "Runji Lin", "Shijie Wang", "Shuai Bai", "Sinan Tan", "Tianhang Zhu", "Tianhao Li", "Tianyu Liu", "Wenbin Ge", "Xiaodong Deng", "Xiaohuan Zhou", "Xingzhang Ren", "Xinyu Zhang", "Xipin Wei", "Xuancheng Ren", "Xuejing Liu", "Yang Fan", "Yang Yao", "Yichang Zhang", "Yu Wan", "Yunfei Chu", "Yuqiong Liu", "Zeyu Cui", "Zhenru Zhang", "Zhifang Guo", "Zhihao Fan"], "published": "2024-07-15T12:35:42Z", "updated": "2024-09-10T13:25:53Z", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2407.10671v4", "landing_url": "https://arxiv.org/abs/2407.10671v4", "doi": "https://doi.org/10.48550/arXiv.2407.10671"}, "round-A_JuniorNano_output": {"reasoning": "The item is a Qwen2 technical report focusing on large language models and benchmarks, with no discussion of discrete audio tokens, neural audio codecs, or tokenization strategies for audio; thus it fails the inclusion criteria and is not relevant to the topic.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a Qwen2 technical report focusing on large language models and benchmarks, with no discussion of discrete audio tokens, neural audio codecs, or tokenization strategies for audio; thus it fails the inclusion criteria and is not relevant to the topic.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe a technical report on large language models and multimodal models but do not mention any discrete audio tokens, neural audio codecs, discretization of continuous audio waveforms, or related audio tokenization mechanisms; therefore, it does not meet the inclusion criteria focused on discrete audio token research nor provide relevant technical details for assessment.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe a technical report on large language models and multimodal models but do not mention any discrete audio tokens, neural audio codecs, discretization of continuous audio waveforms, or related audio tokenization mechanisms; therefore, it does not meet the inclusion criteria focused on discrete audio token research nor provide relevant technical details for assessment.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "huang2022mulan", "query_title": "{MuLan: A Joint Embedding of Music Audio and Natural Language}", "normalized_title": "mulan a joint embedding of music audio and natural language", "title": "MuLan: A Joint Embedding of Music Audio and Natural Language", "abstract": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.", "source": "arxiv", "source_id": "2208.12415v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MuLan: A Joint Embedding of Music Audio and Natural Language", "abstract": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.", "metadata": {"arxiv_id": "2208.12415v1", "title": "MuLan: A Joint Embedding of Music Audio and Natural Language", "summary": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.", "authors": ["Qingqing Huang", "Aren Jansen", "Joonseok Lee", "Ravi Ganti", "Judith Yue Li", "Daniel P. W. Ellis"], "published": "2022-08-26T03:13:21Z", "updated": "2022-08-26T03:13:21Z", "categories": ["eess.AS", "cs.CL", "cs.SD", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2208.12415v1", "landing_url": "https://arxiv.org/abs/2208.12415v1", "doi": "https://doi.org/10.48550/arXiv.2208.12415"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on a joint audio-text embedding model (MuLan) for linking music audio with natural language, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper centers on a joint audio-text embedding model (MuLan) for linking music audio with natural language, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes MuLan as a joint audio-text embedding model linking music audio to natural language descriptions, mainly focusing on cross-modal retrieval and language understanding rather than discretizing audio waveforms into token sequences for sequence modeling like Transformers or LLMs; it does not specify using neural audio codecs, vector quantization, or discrete token representations as defined by the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The abstract describes MuLan as a joint audio-text embedding model linking music audio to natural language descriptions, mainly focusing on cross-modal retrieval and language understanding rather than discretizing audio waveforms into token sequences for sequence modeling like Transformers or LLMs; it does not specify using neural audio codecs, vector quantization, or discrete token representations as defined by the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "turetzky_last_2024", "query_title": "{{LAST}: {Language} {Model} {Aware} {Speech} {Tokenization}}", "normalized_title": "last language model aware speech tokenization", "title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "source": "arxiv", "source_id": "2409.03701v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "metadata": {"arxiv_id": "2409.03701v2", "title": "LAST: Language Model Aware Speech Tokenization", "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "authors": ["Arnon Turetzky", "Yossi Adi"], "published": "2024-09-05T16:57:39Z", "updated": "2024-09-10T14:45:15Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.03701v2", "landing_url": "https://arxiv.org/abs/2409.03701v2", "doi": "https://doi.org/10.48550/arXiv.2409.03701"}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on discrete speech tokens via a tokenizer/codec for LM-based speech processing and reports downstream evaluation, meeting the inclusion criteria and not falling under any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work focuses on discrete speech tokens via a tokenizer/codec for LM-based speech processing and reports downstream evaluation, meeting the inclusion criteria and not falling under any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a novel approach to speech tokenization that involves discrete speech representations aligned with LMs, aiming to create discrete tokens from continuous speech for improved modeling; it describes the tokenization process, design choices (vocabulary size, feature space), and evaluates impact on speech modeling tasks, aligning well with the discrete audio tokens definition and evaluation criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on a novel approach to speech tokenization that involves discrete speech representations aligned with LMs, aiming to create discrete tokens from continuous speech for improved modeling; it describes the tokenization process, design choices (vocabulary size, feature space), and evaluates impact on speech modeling tasks, aligning well with the discrete audio tokens definition and evaluation criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}, {"title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "metadata": {"arxiv_id": "2409.03701v2", "title": "LAST: Language Model Aware Speech Tokenization", "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "authors": ["Arnon Turetzky", "Yossi Adi"], "published": "2024-09-05T16:57:39Z", "updated": "2024-09-10T14:45:15Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.03701v2", "landing_url": "https://arxiv.org/abs/2409.03701v2", "doi": "https://doi.org/10.48550/arXiv.2409.03701"}, "round-A_JuniorNano_output": {"reasoning": "The title and abstract clearly focus on discrete speech tokens/tokenization and their use in downstream speech-LM tasks, covering token design and evaluation of token-level representations, aligning with the inclusion criteria and not triggering any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The title and abstract clearly focus on discrete speech tokens/tokenization and their use in downstream speech-LM tasks, covering token design and evaluation of token-level representations, aligning with the inclusion criteria and not triggering any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a novel speech tokenizer leveraging textual language model objectives to generate discrete speech representations suitable for spoken language modeling and speech-to-text tasks. It clearly involves mapping continuous speech into discrete token sequences aligned with language models, fitting within the theme of discrete audio tokens produced via neural methods. The abstract suggests design choices impacting token vocabulary and integration with language models, aligned with systematic analysis and evaluation, which meets inclusion criteria without conflicting with exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a novel speech tokenizer leveraging textual language model objectives to generate discrete speech representations suitable for spoken language modeling and speech-to-text tasks. It clearly involves mapping continuous speech into discrete token sequences aligned with language models, fitting within the theme of discrete audio tokens produced via neural methods. The abstract suggests design choices impacting token vocabulary and integration with language models, aligned with systematic analysis and evaluation, which meets inclusion criteria without conflicting with exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "nguyen2024spiritlminterleavedspokenwritten", "query_title": "\"{S}pi{R}it-{LM}: Interleaved Spoken and Written Language Model\"", "normalized_title": "spirit lm interleaved spoken and written language model", "title": "SpiRit-LM: Interleaved Spoken and Written Language Model", "abstract": "Abstract We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2", "source": "semantic_scholar", "source_id": "7547e30ba98a0217f07a6bb9fc393902bbc89269", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SpiRit-LM: Interleaved Spoken and Written Language Model", "abstract": "Abstract We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2", "metadata": {"arxiv_id": "semantic_scholar:7547e30ba98a0217f07a6bb9fc393902bbc89269", "title": "SpiRit-LM: Interleaved Spoken and Written Language Model", "summary": "Abstract We introduce SpiRit-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a 7B pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single stream of tokens, and trained with a word-level interleaving method using a small automatically curated speech-text parallel corpus. SpiRit-LM comes in two versions: a Base version that uses speech phonetic units (HuBERT) and an Expressive version that models expressivity using pitch and style units in addition to the phonetic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SpiRit-LM can learn new tasks in a few-shot fashion across modalities (i.e., ASR, TTS, Speech Classification). We make available model weights and inference code.1,2", "source": "semantic_scholar", "source_id": "7547e30ba98a0217f07a6bb9fc393902bbc89269", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "Describes a multimodal language model that uses discrete audio tokens (HuBERT phonetic units and pitch/style units) interleaved with text tokens for cross-modal tasks, satisfying the discrete audio token criterion and providing an English abstract; thus it should be included.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Describes a multimodal language model that uses discrete audio tokens (HuBERT phonetic units and pitch/style units) interleaved with text tokens for cross-modal tasks, satisfying the discrete audio token criterion and providing an English abstract; thus it should be included.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper proposes SpiRit-LM, a multimodal language model mixing text and speech, utilizing speech phonetic units (HuBERT) and style/pitch units as discrete tokens, which aligns with the inclusion criterion of using discrete audio tokens produced by self-supervised models for cross-modal modeling. The abstract mentions token-level interleaving and speech units derived from learned models, confirming the presence of discrete audio tokens representing speech. Although detailed tokenizer/codec specifics are not fully provided in the abstract, the model weights and inference code availability suggest reproducibility. The study applies these tokens in generation and understanding tasks (ASR, TTS, classification), meeting downstream usage requirements. Therefore, the study fits the inclusion criteria and does not fall under any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper proposes SpiRit-LM, a multimodal language model mixing text and speech, utilizing speech phonetic units (HuBERT) and style/pitch units as discrete tokens, which aligns with the inclusion criterion of using discrete audio tokens produced by self-supervised models for cross-modal modeling. The abstract mentions token-level interleaving and speech units derived from learned models, confirming the presence of discrete audio tokens representing speech. Although detailed tokenizer/codec specifics are not fully provided in the abstract, the model weights and inference code availability suggest reproducibility. The study applies these tokens in generation and understanding tasks (ASR, TTS, classification), meeting downstream usage requirements. Therefore, the study fits the inclusion criteria and does not fall under any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "nguyen2020zeroresourcespeechbenchmark", "query_title": "{The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling}", "normalized_title": "the zero resource speech benchmark 2021 metrics and baselines for unsupervised spoken language modeling", "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling", "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.", "source": "arxiv", "source_id": "2011.11588v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling", "abstract": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.", "metadata": {"arxiv_id": "2011.11588v2", "title": "The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling", "summary": "We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels, along with the Zero Resource Speech Benchmark 2021: a suite of 4 black-box, zero-shot metrics probing for the quality of the learned models at 4 linguistic levels: phonetics, lexicon, syntax and semantics. We present the results and analyses of a composite baseline made of the concatenation of three unsupervised systems: self-supervised contrastive representation learning (CPC), clustering (k-means) and language modeling (LSTM or BERT). The language models learn on the basis of the pseudo-text derived from clustering the learned representations. This simple pipeline shows better than chance performance on all four metrics, demonstrating the feasibility of spoken language modeling from raw speech. It also yields worse performance compared to text-based 'topline' systems trained on the same data, delineating the space to be explored by more sophisticated end-to-end models.", "authors": ["Tu Anh Nguyen", "Maureen de Seyssel", "Patricia Rozé", "Morgane Rivière", "Evgeny Kharitonov", "Alexei Baevski", "Ewan Dunbar", "Emmanuel Dupoux"], "published": "2020-11-23T18:01:37Z", "updated": "2020-12-01T15:53:57Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2011.11588v2", "landing_url": "https://arxiv.org/abs/2011.11588v2", "doi": "https://doi.org/10.48550/arXiv.2011.11588"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses discrete tokens produced by clustering (k-means) on learned representations to form a pseudo-text used for downstream language modeling and evaluation across phonetics, lexicon, syntax, and semantics, which directly aligns with the discrete audio token core representation and downstream modeling criteria; it provides an English abstract and methodological details sufficient for assessment.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper uses discrete tokens produced by clustering (k-means) on learned representations to form a pseudo-text used for downstream language modeling and evaluation across phonetics, lexicon, syntax, and semantics, which directly aligns with the discrete audio token core representation and downstream modeling criteria; it provides an English abstract and methodological details sufficient for assessment.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on unsupervised spoken language modeling from raw audio, employing clustering to produce pseudo-text tokens used in language modeling, satisfying key aspects of discretizing continuous audio into token sequences for modeling, which aligns with the discrete audio tokens inclusion criteria.", "evaluation": 4}, "round-A_JuniorMini_reasoning": "The study focuses on unsupervised spoken language modeling from raw audio, employing clustering to produce pseudo-text tokens used in language modeling, satisfying key aspects of discretizing continuous audio into token sequences for modeling, which aligns with the discrete audio tokens inclusion criteria.", "round-A_JuniorMini_evaluation": 4, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "manilow2019cutting", "query_title": "{Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity}", "normalized_title": "cutting music source separation some slakh a dataset to study the impact of training data quality and quantity", "title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity", "abstract": "Music source separation performance has greatly improved in recent years with the advent of approaches based on deep learning. Such methods typically require large amounts of labelled training data, which in the case of music consist of mixtures and corresponding instrument stems. However, stems are unavailable for most commercial music, and only limited datasets have so far been released to the public. It can thus be difficult to draw conclusions when comparing various source separation methods, as the difference in performance may stem as much from better data augmentation techniques or training tricks to alleviate the limited availability of training data, as from intrinsically better model architectures and objective functions. In this paper, we present the synthesized Lakh dataset (Slakh) as a new tool for music source separation research. Slakh consists of high-quality renderings of instrumental mixtures and corresponding stems generated from the Lakh MIDI dataset (LMD) using professional-grade sample-based virtual instruments. A first version, Slakh2100, focuses on 2100 songs, resulting in 145 hours of mixtures. While not fully comparable because it is purely instrumental, this dataset contains an order of magnitude more data than MUSDB18, the {\\it de facto} standard dataset in the field. We show that Slakh can be used to effectively augment existing datasets for musical instrument separation, while opening the door to a wide array of data-intensive music signal analysis tasks.", "source": "arxiv", "source_id": "1909.08494v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity", "abstract": "Music source separation performance has greatly improved in recent years with the advent of approaches based on deep learning. Such methods typically require large amounts of labelled training data, which in the case of music consist of mixtures and corresponding instrument stems. However, stems are unavailable for most commercial music, and only limited datasets have so far been released to the public. It can thus be difficult to draw conclusions when comparing various source separation methods, as the difference in performance may stem as much from better data augmentation techniques or training tricks to alleviate the limited availability of training data, as from intrinsically better model architectures and objective functions. In this paper, we present the synthesized Lakh dataset (Slakh) as a new tool for music source separation research. Slakh consists of high-quality renderings of instrumental mixtures and corresponding stems generated from the Lakh MIDI dataset (LMD) using professional-grade sample-based virtual instruments. A first version, Slakh2100, focuses on 2100 songs, resulting in 145 hours of mixtures. While not fully comparable because it is purely instrumental, this dataset contains an order of magnitude more data than MUSDB18, the {\\it de facto} standard dataset in the field. We show that Slakh can be used to effectively augment existing datasets for musical instrument separation, while opening the door to a wide array of data-intensive music signal analysis tasks.", "metadata": {"arxiv_id": "1909.08494v1", "title": "Cutting Music Source Separation Some Slakh: A Dataset to Study the Impact of Training Data Quality and Quantity", "summary": "Music source separation performance has greatly improved in recent years with the advent of approaches based on deep learning. Such methods typically require large amounts of labelled training data, which in the case of music consist of mixtures and corresponding instrument stems. However, stems are unavailable for most commercial music, and only limited datasets have so far been released to the public. It can thus be difficult to draw conclusions when comparing various source separation methods, as the difference in performance may stem as much from better data augmentation techniques or training tricks to alleviate the limited availability of training data, as from intrinsically better model architectures and objective functions. In this paper, we present the synthesized Lakh dataset (Slakh) as a new tool for music source separation research. Slakh consists of high-quality renderings of instrumental mixtures and corresponding stems generated from the Lakh MIDI dataset (LMD) using professional-grade sample-based virtual instruments. A first version, Slakh2100, focuses on 2100 songs, resulting in 145 hours of mixtures. While not fully comparable because it is purely instrumental, this dataset contains an order of magnitude more data than MUSDB18, the {\\it de facto} standard dataset in the field. We show that Slakh can be used to effectively augment existing datasets for musical instrument separation, while opening the door to a wide array of data-intensive music signal analysis tasks.", "authors": ["Ethan Manilow", "Gordon Wichern", "Prem Seetharaman", "Jonathan Le Roux"], "published": "2019-09-18T15:14:27Z", "updated": "2019-09-18T15:14:27Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1909.08494v1", "landing_url": "https://arxiv.org/abs/1909.08494v1", "doi": "https://doi.org/10.48550/arXiv.1909.08494"}, "round-A_JuniorNano_output": {"reasoning": "This work focuses on synthetic datasets for music source separation and data augmentation; it does not address discrete audio tokens, tokenizers, or token-based modeling, so it should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This work focuses on synthetic datasets for music source separation and data augmentation; it does not address discrete audio tokens, tokenizers, or token-based modeling, so it should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses a dataset for music source separation aimed at improving training data quality and quantity; however, it does not mention any form of discretization of audio waveforms into token sequences, neural audio codecs, quantization methods, or token design/evaluation as required by the inclusion criteria. The focus is on continuous audio mixtures and stems rather than discrete audio tokens or token-based modeling, thus it does not meet the inclusion criteria nor address the core concerns of discrete token representations for audio.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses a dataset for music source separation aimed at improving training data quality and quantity; however, it does not mention any form of discretization of audio waveforms into token sequences, neural audio codecs, quantization methods, or token design/evaluation as required by the inclusion criteria. The focus is on continuous audio mixtures and stems rather than discrete audio tokens or token-based modeling, thus it does not meet the inclusion criteria nor address the core concerns of discrete token representations for audio.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "arnault2020urban", "query_title": "{Urban Sound Classification: striving towards a fair comparison}", "normalized_title": "urban sound classification striving towards a fair comparison", "title": "Urban Sound Classification : striving towards a fair comparison", "abstract": "Urban sound classification has been achieving remarkable progress and is still an active research area in audio pattern recognition. In particular, it allows to monitor the noise pollution, which becomes a growing concern for large cities. The contribution of this paper is two-fold. First, we present our DCASE 2020 task 5 winning solution which aims at helping the monitoring of urban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse / fine classification on validation set. Moreover, it reaches accuracies of 89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not easy to find a fair comparison and to reproduce the performance of existing models. Sometimes authors copy-pasting the results of the original papers which is not helping reproducibility. As a result, we provide a fair comparison by using the same input representation, metrics and optimizer to assess performances. We preserve data augmentation used by the original papers. We hope this framework could help evaluate new architectures in this field. For better reproducibility, the code is available on our GitHub repository.", "source": "arxiv", "source_id": "2010.11805v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Urban Sound Classification : striving towards a fair comparison", "abstract": "Urban sound classification has been achieving remarkable progress and is still an active research area in audio pattern recognition. In particular, it allows to monitor the noise pollution, which becomes a growing concern for large cities. The contribution of this paper is two-fold. First, we present our DCASE 2020 task 5 winning solution which aims at helping the monitoring of urban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse / fine classification on validation set. Moreover, it reaches accuracies of 89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not easy to find a fair comparison and to reproduce the performance of existing models. Sometimes authors copy-pasting the results of the original papers which is not helping reproducibility. As a result, we provide a fair comparison by using the same input representation, metrics and optimizer to assess performances. We preserve data augmentation used by the original papers. We hope this framework could help evaluate new architectures in this field. For better reproducibility, the code is available on our GitHub repository.", "metadata": {"arxiv_id": "2010.11805v1", "title": "Urban Sound Classification : striving towards a fair comparison", "summary": "Urban sound classification has been achieving remarkable progress and is still an active research area in audio pattern recognition. In particular, it allows to monitor the noise pollution, which becomes a growing concern for large cities. The contribution of this paper is two-fold. First, we present our DCASE 2020 task 5 winning solution which aims at helping the monitoring of urban noise pollution. It achieves a macro-AUPRC of 0.82 / 0.62 for the coarse / fine classification on validation set. Moreover, it reaches accuracies of 89.7% and 85.41% respectively on ESC-50 and US8k datasets. Second, it is not easy to find a fair comparison and to reproduce the performance of existing models. Sometimes authors copy-pasting the results of the original papers which is not helping reproducibility. As a result, we provide a fair comparison by using the same input representation, metrics and optimizer to assess performances. We preserve data augmentation used by the original papers. We hope this framework could help evaluate new architectures in this field. For better reproducibility, the code is available on our GitHub repository.", "authors": ["Augustin Arnault", "Baptiste Hanssens", "Nicolas Riche"], "published": "2020-10-22T15:37:39Z", "updated": "2020-10-22T15:37:39Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2010.11805v1", "landing_url": "https://arxiv.org/abs/2010.11805v1", "doi": "https://doi.org/10.48550/arXiv.2010.11805"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on fair model comparison for urban sound classification and reporting performance on ESC-50/US8k; it does not mention discrete audio tokens, tokenizers, neural codecs, or any analysis of token design, so it does not satisfy the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on fair model comparison for urban sound classification and reporting performance on ESC-50/US8k; it does not mention discrete audio tokens, tokenizers, neural codecs, or any analysis of token design, so it does not satisfy the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on urban sound classification, which is primarily a discriminative task aiming at noise pollution monitoring through audio event classification. It does not describe any approach involving the creation or use of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it emphasize token design, discretization, or downstream generative or cross-modal tasks. The description lacks discussion on tokenization or discrete token sequences, making it irrelevant to the defined inclusion criteria for discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on urban sound classification, which is primarily a discriminative task aiming at noise pollution monitoring through audio event classification. It does not describe any approach involving the creation or use of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it emphasize token design, discretization, or downstream generative or cross-modal tasks. The description lacks discussion on tokenization or discrete token sequences, making it irrelevant to the defined inclusion criteria for discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wisdom2021s", "query_title": "{What’s all the fuss about free universal sound separation data?}", "normalized_title": "whats all the fuss about free universal sound separation data", "title": "What’s all the Fuss about Free Universal Sound Separation Data?", "abstract": "We introduce the Free Universal Sound Separation (FUSS) dataset, a new corpus for experiments in separating mixtures of an unknown number of sounds from an open domain of sound types. The dataset consists of 23 hours of single-source audio data drawn from 357 classes, which are used to create mixtures of one to four sources. To simulate reverberation, an acoustic room simulator is used to generate impulse responses of box-shaped rooms with frequency-dependent reflective walls. Additional open-source data augmentation tools are also provided to produce new mixtures with different combinations of sources and room simulations. Finally, we introduce an open-source baseline separation model, based on an improved time-domain convolutional network (TDCN++), that can separate a variable number of sources in a mixture. This model achieves 9.8 dB of scale-invariant signal-to-noise ratio improvement (SI-SNRi) on mixtures with two to four sources, while reconstructing single-source inputs with 35.8 dB absolute SI-SNR. We hope this dataset will lower the barrier to new research and allow for fast iteration and application of novel techniques from other machine learning domains to the sound separation challenge.", "source": "semantic_scholar", "source_id": "1932eb0e2ede027909ef42bfa065d9983824e0a9", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "What’s all the Fuss about Free Universal Sound Separation Data?", "abstract": "We introduce the Free Universal Sound Separation (FUSS) dataset, a new corpus for experiments in separating mixtures of an unknown number of sounds from an open domain of sound types. The dataset consists of 23 hours of single-source audio data drawn from 357 classes, which are used to create mixtures of one to four sources. To simulate reverberation, an acoustic room simulator is used to generate impulse responses of box-shaped rooms with frequency-dependent reflective walls. Additional open-source data augmentation tools are also provided to produce new mixtures with different combinations of sources and room simulations. Finally, we introduce an open-source baseline separation model, based on an improved time-domain convolutional network (TDCN++), that can separate a variable number of sources in a mixture. This model achieves 9.8 dB of scale-invariant signal-to-noise ratio improvement (SI-SNRi) on mixtures with two to four sources, while reconstructing single-source inputs with 35.8 dB absolute SI-SNR. We hope this dataset will lower the barrier to new research and allow for fast iteration and application of novel techniques from other machine learning domains to the sound separation challenge.", "metadata": {"arxiv_id": "semantic_scholar:1932eb0e2ede027909ef42bfa065d9983824e0a9", "title": "What’s all the Fuss about Free Universal Sound Separation Data?", "summary": "We introduce the Free Universal Sound Separation (FUSS) dataset, a new corpus for experiments in separating mixtures of an unknown number of sounds from an open domain of sound types. The dataset consists of 23 hours of single-source audio data drawn from 357 classes, which are used to create mixtures of one to four sources. To simulate reverberation, an acoustic room simulator is used to generate impulse responses of box-shaped rooms with frequency-dependent reflective walls. Additional open-source data augmentation tools are also provided to produce new mixtures with different combinations of sources and room simulations. Finally, we introduce an open-source baseline separation model, based on an improved time-domain convolutional network (TDCN++), that can separate a variable number of sources in a mixture. This model achieves 9.8 dB of scale-invariant signal-to-noise ratio improvement (SI-SNRi) on mixtures with two to four sources, while reconstructing single-source inputs with 35.8 dB absolute SI-SNR. We hope this dataset will lower the barrier to new research and allow for fast iteration and application of novel techniques from other machine learning domains to the sound separation challenge.", "source": "semantic_scholar", "source_id": "1932eb0e2ede027909ef42bfa065d9983824e0a9", "publication_date": "2020-11-02"}, "round-A_JuniorNano_output": {"reasoning": "This paper introduces a dataset and a baseline model for sound separation, but it does not discuss discrete audio tokens, tokenization, or token-based representations required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper introduces a dataset and a baseline model for sound separation, but it does not discuss discrete audio tokens, tokenization, or token-based representations required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a dataset and baseline model for universal sound separation but does not describe any discrete audio token generation or usage involving discretization of audio signals into token sequences; hence, it does not meet the inclusion criteria focused on discrete token representations for audio modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a dataset and baseline model for universal sound separation but does not describe any discrete audio token generation or usage involving discretization of audio signals into token sequences; hence, it does not meet the inclusion criteria focused on discrete token representations for audio modeling or generation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "hershey2016deep", "query_title": "{Deep clustering: Discriminative embeddings for segmentation and separation}", "normalized_title": "deep clustering discriminative embeddings for segmentation and separation", "title": "Deep clustering: Discriminative embeddings for segmentation and separation", "abstract": "We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be \"decoded\" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.", "source": "arxiv", "source_id": "1508.04306v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "abstract": "We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be \"decoded\" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.", "metadata": {"arxiv_id": "1508.04306v1", "title": "Deep clustering: Discriminative embeddings for segmentation and separation", "summary": "We address the problem of acoustic source separation in a deep learning framework we call \"deep clustering.\" Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a class-independent way. In contrast, spectral clustering approaches are flexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise affinity matrix, in a class-independent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be \"decoded\" by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on two-speaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.", "authors": ["John R. Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "published": "2015-08-18T13:12:34Z", "updated": "2015-08-18T13:12:34Z", "categories": ["cs.NE", "cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1508.04306v1", "landing_url": "https://arxiv.org/abs/1508.04306v1", "doi": "https://doi.org/10.48550/arXiv.1508.04306"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on deep clustering for speech separation using continuous spectrogram embeddings and does not introduce discrete audio tokens, tokenization schemes, or a token-based downstream modeling framework, thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on deep clustering for speech separation using continuous spectrogram embeddings and does not introduce discrete audio tokens, tokenization schemes, or a token-based downstream modeling framework, thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on acoustic source separation using deep embeddings, without describing the generation or use of discrete audio tokens, neural audio codecs, or vector quantization to produce token sequences; thus, it does not meet the inclusion criteria centered on discretization and token-based audio modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on acoustic source separation using deep embeddings, without describing the generation or use of discrete audio tokens, neural audio codecs, or vector quantization to produce token sequences; thus, it does not meet the inclusion criteria centered on discretization and token-based audio modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "liu2024revisiting", "query_title": "{Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective}", "normalized_title": "revisiting self supervised learning of speech representation from a mutual information perspective", "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective", "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.", "source": "arxiv", "source_id": "2401.08833v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective", "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.", "metadata": {"arxiv_id": "2401.08833v1", "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective", "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.", "authors": ["Alexander H. Liu", "Sung-Lin Yeh", "James Glass"], "published": "2024-01-16T21:13:22Z", "updated": "2024-01-16T21:13:22Z", "categories": ["eess.AS", "cs.CL", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2401.08833v1", "landing_url": "https://arxiv.org/abs/2401.08833v1", "doi": "https://doi.org/10.48550/arXiv.2401.08833"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses self-supervised speech representations and mutual information but does not describe discrete audio tokens or tokenization, thus it fails the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses self-supervised speech representations and mutual information but does not describe discrete audio tokens or tokenization, thus it fails the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on self-supervised learning of speech representations analyzed from a mutual information perspective, but does not explicitly mention the discretization of continuous audio waveforms into discrete tokens or the use of neural audio codecs/vector quantization to generate discrete token sequences. The abstract lacks clear details on the presence or design of discrete audio tokens, tokenizers, codebooks, or quantization processes that would allow reproducibility or evaluation as discrete tokens. Therefore, it does not meet the inclusion criteria which require focus on discrete audio tokens or detailed token generation mechanisms.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning of speech representations analyzed from a mutual information perspective, but does not explicitly mention the discretization of continuous audio waveforms into discrete tokens or the use of neural audio codecs/vector quantization to generate discrete token sequences. The abstract lacks clear details on the presence or design of discrete audio tokens, tokenizers, codebooks, or quantization processes that would allow reproducibility or evaluation as discrete tokens. Therefore, it does not meet the inclusion criteria which require focus on discrete audio tokens or detailed token generation mechanisms.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "chung2025kad", "query_title": "{KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation}", "normalized_title": "kad no more fad an effective and efficient evaluation metric for audio generation", "title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation", "abstract": "Although being widely adopted for evaluating generated audio signals, the Fréchet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD captures nuanced differences between real and generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.", "source": "arxiv", "source_id": "2502.15602v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation", "abstract": "Although being widely adopted for evaluating generated audio signals, the Fréchet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD captures nuanced differences between real and generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.", "metadata": {"arxiv_id": "2502.15602v2", "title": "KAD: No More FAD! An Effective and Efficient Evaluation Metric for Audio Generation", "summary": "Although being widely adopted for evaluating generated audio signals, the Fréchet Audio Distance (FAD) suffers from significant limitations, including reliance on Gaussian assumptions, sensitivity to sample size, and high computational complexity. As an alternative, we introduce the Kernel Audio Distance (KAD), a novel, distribution-free, unbiased, and computationally efficient metric based on Maximum Mean Discrepancy (MMD). Through analysis and empirical validation, we demonstrate KAD's advantages: (1) faster convergence with smaller sample sizes, enabling reliable evaluation with limited data; (2) lower computational cost, with scalable GPU acceleration; and (3) stronger alignment with human perceptual judgments. By leveraging advanced embeddings and characteristic kernels, KAD captures nuanced differences between real and generated audio. Open-sourced in the kadtk toolkit, KAD provides an efficient, reliable, and perceptually aligned benchmark for evaluating generative audio models.", "authors": ["Yoonjin Chung", "Pilsun Eu", "Junwon Lee", "Keunwoo Choi", "Juhan Nam", "Ben Sangbae Chon"], "published": "2025-02-21T17:19:15Z", "updated": "2025-03-09T06:46:13Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2502.15602v2", "landing_url": "https://arxiv.org/abs/2502.15602v2", "doi": "https://doi.org/10.48550/arXiv.2502.15602"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses a new audio evaluation metric (KAD) and its properties, not discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item discusses a new audio evaluation metric (KAD) and its properties, not discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on proposing a new evaluation metric (KAD) for audio generation that compares real and generated audio distributions. However, it does not discuss or involve discretization of continuous audio waveforms into discrete tokens (neural audio codecs, vector quantization, or discrete token sequences) as defined in the inclusion criteria. The work is centered on evaluation metrics rather than discrete audio token representations or their design, generation, or applications in downstream tasks. Therefore, it does not fulfill the core inclusion criteria related to discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on proposing a new evaluation metric (KAD) for audio generation that compares real and generated audio distributions. However, it does not discuss or involve discretization of continuous audio waveforms into discrete tokens (neural audio codecs, vector quantization, or discrete token sequences) as defined in the inclusion criteria. The work is centered on evaluation metrics rather than discrete audio token representations or their design, generation, or applications in downstream tasks. Therefore, it does not fulfill the core inclusion criteria related to discrete audio tokens and should be excluded.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "fadtk", "query_title": "{Adapting frechet audio distance for generative music evaluation}", "normalized_title": "adapting frechet audio distance for generative music evaluation", "title": "Adapting Frechet Audio Distance for Generative Music Evaluation", "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.", "source": "arxiv", "source_id": "2311.01616v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Adapting Frechet Audio Distance for Generative Music Evaluation", "abstract": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.", "metadata": {"arxiv_id": "2311.01616v2", "title": "Adapting Frechet Audio Distance for Generative Music Evaluation", "summary": "The growing popularity of generative music models underlines the need for perceptually relevant, objective music quality metrics. The Frechet Audio Distance (FAD) is commonly used for this purpose even though its correlation with perceptual quality is understudied. We show that FAD performance may be hampered by sample size bias, poor choice of audio embeddings, or the use of biased or low-quality reference sets. We propose reducing sample size bias by extrapolating scores towards an infinite sample size. Through comparisons with MusicCaps labels and a listening test we identify audio embeddings and music reference sets that yield FAD scores well-correlated with acoustic and musical quality. Our results suggest that per-song FAD can be useful to identify outlier samples and predict perceptual quality for a range of music sets and generative models. Finally, we release a toolkit that allows adapting FAD for generative music evaluation.", "authors": ["Azalea Gui", "Hannes Gamper", "Sebastian Braun", "Dimitra Emmanouilidou"], "published": "2023-11-02T21:58:55Z", "updated": "2024-03-05T22:14:14Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2311.01616v2", "landing_url": "https://arxiv.org/abs/2311.01616v2", "doi": "https://doi.org/10.48550/arXiv.2311.01616"}, "round-A_JuniorNano_output": {"reasoning": "The paper evaluates Frechet Audio Distance (FAD) and audio embeddings for generative music quality, addressing biases and correlations, but it does not define or analyze discrete audio tokens or tokenization schemes, nor use tokens as core input/output; thus it does not meet the discrete audio token inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper evaluates Frechet Audio Distance (FAD) and audio embeddings for generative music quality, addressing biases and correlations, but it does not define or analyze discrete audio tokens or tokenization schemes, nor use tokens as core input/output; thus it does not meet the discrete audio token inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on evaluating generative music quality using Frechet Audio Distance, which is a continuous audio metric, without discussing or involving discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it describe any discrete token generation process or vocabulary settings crucial for the inclusion criteria centered on \"Discrete Audio Tokens.\" This indicates the work is primarily on continuous audio representation evaluation, not on discrete token-based modeling or analysis.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on evaluating generative music quality using Frechet Audio Distance, which is a continuous audio metric, without discussing or involving discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it describe any discrete token generation process or vocabulary settings crucial for the inclusion criteria centered on \"Discrete Audio Tokens.\" This indicates the work is primarily on continuous audio representation evaluation, not on discrete token-based modeling or analysis.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "koutini2021passt", "query_title": "{Efficient Training of Audio Transformers with Patchout}", "normalized_title": "efficient training of audio transformers with patchout", "title": "Efficient Training of Audio Transformers with Patchout", "abstract": "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST", "source": "arxiv", "source_id": "2110.05069v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Efficient Training of Audio Transformers with Patchout", "abstract": "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST", "metadata": {"arxiv_id": "2110.05069v3", "title": "Efficient Training of Audio Transformers with Patchout", "summary": "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST", "authors": ["Khaled Koutini", "Jan Schlüter", "Hamid Eghbal-zadeh", "Gerhard Widmer"], "published": "2021-10-11T08:07:50Z", "updated": "2022-03-29T12:25:02Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2110.05069v3", "landing_url": "https://arxiv.org/abs/2110.05069v3", "doi": "https://doi.org/10.21437/Interspeech.2022-227"}, "round-A_JuniorNano_output": {"reasoning": "This study targets Efficient Training of Audio Transformers with Patchout and discusses training speed/accuracy on audio spectrograms; it does not describe discrete audio tokens, tokenizers, or token-based representations, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This study targets Efficient Training of Audio Transformers with Patchout and discusses training speed/accuracy on audio spectrograms; it does not describe discrete audio tokens, tokenizers, or token-based representations, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on optimizing transformers for audio spectrograms and achieves state-of-the-art performance, but it does not describe discretizing continuous audio waveforms into discrete tokens or provide details on tokenization or codecs as required by the inclusion criteria. It also does not discuss using discrete audio tokens as core representations for generation, modeling, or alignment, so it does not meet the necessary criteria for inclusion.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on optimizing transformers for audio spectrograms and achieves state-of-the-art performance, but it does not describe discretizing continuous audio waveforms into discrete tokens or provide details on tokenization or codecs as required by the inclusion criteria. It also does not discuss using discrete audio tokens as core representations for generation, modeling, or alignment, so it does not meet the necessary criteria for inclusion.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "san2023discrete", "query_title": "{From discrete tokens to high-fidelity audio using multi-band diffusion}", "normalized_title": "from discrete tokens to high fidelity audio using multi band diffusion", "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion", "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.", "source": "arxiv", "source_id": "2308.02560v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion", "abstract": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.", "metadata": {"arxiv_id": "2308.02560v2", "title": "From Discrete Tokens to High-Fidelity Audio Using Multi-Band Diffusion", "summary": "Deep generative models can generate high-fidelity audio conditioned on various types of representations (e.g., mel-spectrograms, Mel-frequency Cepstral Coefficients (MFCC)). Recently, such models have been used to synthesize audio waveforms conditioned on highly compressed representations. Although such methods produce impressive results, they are prone to generate audible artifacts when the conditioning is flawed or imperfect. An alternative modeling approach is to use diffusion models. However, these have mainly been used as speech vocoders (i.e., conditioned on mel-spectrograms) or generating relatively low sampling rate signals. In this work, we propose a high-fidelity multi-band diffusion-based framework that generates any type of audio modality (e.g., speech, music, environmental sounds) from low-bitrate discrete representations. At equal bit rate, the proposed approach outperforms state-of-the-art generative techniques in terms of perceptual quality. Training and, evaluation code, along with audio samples, are available on the facebookresearch/audiocraft Github page.", "authors": ["Robin San Roman", "Yossi Adi", "Antoine Deleforge", "Romain Serizel", "Gabriel Synnaeve", "Alexandre Défossez"], "published": "2023-08-02T22:14:29Z", "updated": "2023-11-08T10:04:00Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.02560v2", "landing_url": "https://arxiv.org/abs/2308.02560v2", "doi": "https://doi.org/10.48550/arXiv.2308.02560"}, "round-A_JuniorNano_output": {"reasoning": "Contains explicit use of discrete audio tokens via a neural codec/self-supervised units and demonstrates token-based generation across speech/music/environmental audio with a diffusion model, aligning with the discrete-token inclusion criteria and lacking exclusion indicators.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Contains explicit use of discrete audio tokens via a neural codec/self-supervised units and demonstrates token-based generation across speech/music/environmental audio with a diffusion model, aligning with the discrete-token inclusion criteria and lacking exclusion indicators.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a multi-band diffusion-based framework generating audio from low-bitrate discrete representations, which aligns with the definition of discrete audio tokens as compressed, tokenized sequences derived from continuous audio. However, the abstract lacks explicit details about the tokenization procedure, such as codec architecture, vector quantization, vocabulary/codebook configuration, or sampling specifics necessary for confidently determining whether the discrete tokens originated from a neural audio codec or self-supervised representation. Because the inclusion criteria require clear descriptions supporting reproducibility and token design evaluation, and these details are not explicitly provided in the abstract, it is uncertain if the study fully meets the inclusion criteria.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study proposes a multi-band diffusion-based framework generating audio from low-bitrate discrete representations, which aligns with the definition of discrete audio tokens as compressed, tokenized sequences derived from continuous audio. However, the abstract lacks explicit details about the tokenization procedure, such as codec architecture, vector quantization, vocabulary/codebook configuration, or sampling specifics necessary for confidently determining whether the discrete tokens originated from a neural audio codec or self-supervised representation. Because the inclusion criteria require clear descriptions supporting reproducibility and token design evaluation, and these details are not explicitly provided in the abstract, it is uncertain if the study fully meets the inclusion criteria.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers raised relevant points: JuniorNano flags that the paper explicitly uses discrete audio tokens (token-based generation across speech/music/environmental audio) and aligns with the inclusion criteria; JuniorMini correctly notes the abstract omits low-level tokenizer/codebook details (codec architecture, quantization, bitrate specifics), making reproducibility uncertain from the abstract alone. The paper title and abstract, however, explicitly state generation from “low-bitrate discrete representations” and even point to facebookresearch/audiocraft GitHub with training/evaluation code and audio samples, which strongly indicates that discrete-token generation (likely via a neural codec or discrete units) is central and that implementation/details are available in the repository; this satisfies the requirement that the work uses discrete token sequences as the core representation for audio generation and that details are (likely) provided. Therefore, despite the abstract’s brevity on tokenizer internals, the explicit framing around discrete tokens, multimodal token-based generation, and public code/materials lead me to include this paper. 1-sentence explanation: The title/abstract explicitly state generation from low-bitrate discrete tokens across audio modalities and point to public code, so it meets the discrete-audio-token inclusion criteria.", "evaluation": 5}, "round-B_SeniorLead_reasoning": "Both junior reviewers raised relevant points: JuniorNano flags that the paper explicitly uses discrete audio tokens (token-based generation across speech/music/environmental audio) and aligns with the inclusion criteria; JuniorMini correctly notes the abstract omits low-level tokenizer/codebook details (codec architecture, quantization, bitrate specifics), making reproducibility uncertain from the abstract alone. The paper title and abstract, however, explicitly state generation from “low-bitrate discrete representations” and even point to facebookresearch/audiocraft GitHub with training/evaluation code and audio samples, which strongly indicates that discrete-token generation (likely via a neural codec or discrete units) is central and that implementation/details are available in the repository; this satisfies the requirement that the work uses discrete token sequences as the core representation for audio generation and that details are (likely) provided. Therefore, despite the abstract’s brevity on tokenizer internals, the explicit framing around discrete tokens, multimodal token-based generation, and public code/materials lead me to include this paper. 1-sentence explanation: The title/abstract explicitly state generation from low-bitrate discrete tokens across audio modalities and point to public code, so it meets the discrete-audio-token inclusion criteria.", "round-B_SeniorLead_evaluation": 5, "final_verdict": "include (senior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "yang2023diffsound", "query_title": "{Diffsound: Discrete diffusion model for text-to-sound generation}", "normalized_title": "diffsound discrete diffusion model for text to sound generation", "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation", "abstract": "Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.", "source": "arxiv", "source_id": "2207.09983v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation", "abstract": "Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.", "metadata": {"arxiv_id": "2207.09983v2", "title": "Diffsound: Discrete Diffusion Model for Text-to-sound Generation", "summary": "Generating sound effects that humans want is an important topic. However, there are few studies in this area for sound generation. In this study, we investigate generating sound conditioned on a text prompt and propose a novel text-to-sound generation framework that consists of a text encoder, a Vector Quantized Variational Autoencoder (VQ-VAE), a decoder, and a vocoder. The framework first uses the decoder to transfer the text features extracted from the text encoder to a mel-spectrogram with the help of VQ-VAE, and then the vocoder is used to transform the generated mel-spectrogram into a waveform. We found that the decoder significantly influences the generation performance. Thus, we focus on designing a good decoder in this study. We begin with the traditional autoregressive decoder, which has been proved as a state-of-the-art method in previous sound generation works. However, the AR decoder always predicts the mel-spectrogram tokens one by one in order, which introduces the unidirectional bias and accumulation of errors problems. Moreover, with the AR decoder, the sound generation time increases linearly with the sound duration. To overcome the shortcomings introduced by AR decoders, we propose a non-autoregressive decoder based on the discrete diffusion model, named Diffsound. Specifically, the Diffsound predicts all of the mel-spectrogram tokens in one step and then refines the predicted tokens in the next step, so the best-predicted results can be obtained after several steps. Our experiments show that our proposed Diffsound not only produces better text-to-sound generation results when compared with the AR decoder but also has a faster generation speed, e.g., MOS: 3.56 \\textit{v.s} 2.786, and the generation speed is five times faster than the AR decoder.", "authors": ["Dongchao Yang", "Jianwei Yu", "Helin Wang", "Wen Wang", "Chao Weng", "Yuexian Zou", "Dong Yu"], "published": "2022-07-20T15:41:47Z", "updated": "2023-04-28T07:45:43Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2207.09983v2", "landing_url": "https://arxiv.org/abs/2207.09983v2", "doi": "https://doi.org/10.48550/arXiv.2207.09983"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses a discrete audio tokenization via VQ-VAE and a discrete diffusion model for text-to-sound generation, addressing token design and downstream generation tasks, which aligns with the discrete audio token criteria and inclusion requirements.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper uses a discrete audio tokenization via VQ-VAE and a discrete diffusion model for text-to-sound generation, addressing token design and downstream generation tasks, which aligns with the discrete audio token criteria and inclusion requirements.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study employs a Vector Quantized Variational Autoencoder (VQ-VAE), a neural audio codec that generates discrete tokens representing audio (mel-spectrogram tokens), which are then used for modeling in a text-to-sound generation task, aligning well with the inclusion criteria focused on discrete audio token representation and downstream generative modeling; the abstract provides adequate methodological detail in English to assess tokenization and codec design, and the research contributes a novel non-autoregressive discrete diffusion decoder improving performance and speed, indicating a core use of discrete token sequences for audio generation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study employs a Vector Quantized Variational Autoencoder (VQ-VAE), a neural audio codec that generates discrete tokens representing audio (mel-spectrogram tokens), which are then used for modeling in a text-to-sound generation task, aligning well with the inclusion criteria focused on discrete audio token representation and downstream generative modeling; the abstract provides adequate methodological detail in English to assess tokenization and codec design, and the research contributes a novel non-autoregressive discrete diffusion decoder improving performance and speed, indicating a core use of discrete token sequences for audio generation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wu2023large", "query_title": "{Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation}", "normalized_title": "large scale contrastive language audio pretraining with feature fusion and keyword to caption augmentation", "title": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation", "abstract": "Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.", "source": "arxiv", "source_id": "2211.06687v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation", "abstract": "Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.", "metadata": {"arxiv_id": "2211.06687v4", "title": "Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation", "summary": "Contrastive learning has shown remarkable success in the field of multimodal representation learning. In this paper, we propose a pipeline of contrastive language-audio pretraining to develop an audio representation by combining audio data with natural language descriptions. To accomplish this target, we first release LAION-Audio-630K, a large collection of 633,526 audio-text pairs from different data sources. Second, we construct a contrastive language-audio pretraining model by considering different audio encoders and text encoders. We incorporate the feature fusion mechanism and keyword-to-caption augmentation into the model design to further enable the model to process audio inputs of variable lengths and enhance the performance. Third, we perform comprehensive experiments to evaluate our model across three tasks: text-to-audio retrieval, zero-shot audio classification, and supervised audio classification. The results demonstrate that our model achieves superior performance in text-to-audio retrieval task. In audio classification tasks, the model achieves state-of-the-art performance in the zero-shot setting and is able to obtain performance comparable to models' results in the non-zero-shot setting. LAION-Audio-630K and the proposed model are both available to the public.", "authors": ["Yusong Wu", "Ke Chen", "Tianyu Zhang", "Yuchen Hui", "Marianna Nezhurina", "Taylor Berg-Kirkpatrick", "Shlomo Dubnov"], "published": "2022-11-12T15:25:20Z", "updated": "2024-03-21T21:35:04Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2211.06687v4", "landing_url": "https://arxiv.org/abs/2211.06687v4", "doi": "https://doi.org/10.48550/arXiv.2211.06687"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on contrastive language-audio pretraining using audio-text pairs and does not discuss discrete audio tokens or a tokenization/codec design, therefore it does not meet the discrete audio tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper centers on contrastive language-audio pretraining using audio-text pairs and does not discuss discrete audio tokens or a tokenization/codec design, therefore it does not meet the discrete audio tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on contrastive language-audio pretraining for audio representation and does not mention the discretization of continuous audio waveforms into discrete token sequences or provide any details about tokenizers, codecs, or quantization methods that produce discrete audio tokens. It emphasizes audio encoders and feature fusion but lacks information on discrete token generation or usage, therefore it does not meet the inclusion criteria centered on discrete audio token methodologies.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on contrastive language-audio pretraining for audio representation and does not mention the discretization of continuous audio waveforms into discrete token sequences or provide any details about tokenizers, codecs, or quantization methods that produce discrete audio tokens. It emphasizes audio encoders and feature fusion but lacks information on discrete token generation or usage, therefore it does not meet the inclusion criteria centered on discrete audio token methodologies.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "huang2023make", "query_title": "{Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models}", "normalized_title": "make an audio text to audio generation with prompt enhanced diffusion models", "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models", "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io", "source": "arxiv", "source_id": "2301.12661v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models", "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io", "metadata": {"arxiv_id": "2301.12661v1", "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models", "summary": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io", "authors": ["Rongjie Huang", "Jiawei Huang", "Dongchao Yang", "Yi Ren", "Luping Liu", "Mingze Li", "Zhenhui Ye", "Jinglin Liu", "Xiang Yin", "Zhou Zhao"], "published": "2023-01-30T04:44:34Z", "updated": "2023-01-30T04:44:34Z", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2301.12661v1", "landing_url": "https://arxiv.org/abs/2301.12661v1", "doi": "https://doi.org/10.48550/arXiv.2301.12661"}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on text-to-audio generation using diffusion models and spectrogram-based representations, with no discussion of discrete audio tokens, tokenizers, or codebooks; it targets continuous representations rather than discrete token modeling, so it aligns with exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work focuses on text-to-audio generation using diffusion models and spectrogram-based representations, with no discussion of discrete audio tokens, tokenizers, or codebooks; it targets continuous representations rather than discrete token modeling, so it aligns with exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a text-to-audio generation model leveraging prompt-enhanced diffusion and spectrogram autoencoders to predict self-supervised audio representations, but it does not explicitly describe the generation or use of discrete audio tokens derived from neural audio codecs or self-supervised quantized units, nor detailed tokenization, codebook, or quantization schemes that meet the discrete audio token criteria defined; therefore, it does not clearly meet the inclusion criteria centered on discrete token representations for audio modeling.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper proposes a text-to-audio generation model leveraging prompt-enhanced diffusion and spectrogram autoencoders to predict self-supervised audio representations, but it does not explicitly describe the generation or use of discrete audio tokens derived from neural audio codecs or self-supervised quantized units, nor detailed tokenization, codebook, or quantization schemes that meet the discrete audio token criteria defined; therefore, it does not clearly meet the inclusion criteria centered on discrete token representations for audio modeling.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "audiocaps", "query_title": "{AudioCaps: Generating Captions for Audios in The Wild}", "normalized_title": "audiocaps generating captions for audios in the wild", "title": "AudioCaps: Generating Captions for Audios in The Wild", "abstract": "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.", "source": "semantic_scholar", "source_id": "c4798919e74411d87f7745840e45b8bcf61128ff", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioCaps: Generating Captions for Audios in The Wild", "abstract": "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.", "metadata": {"arxiv_id": "semantic_scholar:c4798919e74411d87f7745840e45b8bcf61128ff", "title": "AudioCaps: Generating Captions for Audios in The Wild", "summary": "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention.", "source": "semantic_scholar", "source_id": "c4798919e74411d87f7745840e45b8bcf61128ff", "publication_date": "2019-06-01"}, "round-A_JuniorNano_output": {"reasoning": "The item concerns audio captioning and dataset creation, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, and thus does not meet the inclusion criteria focused on discrete audio token design/use.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item concerns audio captioning and dataset creation, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, and thus does not meet the inclusion criteria focused on discrete audio token design/use.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on generating natural language captions for audio clips without mentioning any discrete audio tokenization, neural audio codecs, or discrete token representations, nor does it describe token-based modeling; hence, it does not meet the inclusion criteria centered on discrete audio tokens and their design or usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on generating natural language captions for audio clips without mentioning any discrete audio tokenization, neural audio codecs, or discrete token representations, nor does it describe token-based modeling; hence, it does not meet the inclusion criteria centered on discrete audio tokens and their design or usage.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "macs", "query_title": "{Diversity and bias in audio captioning datasets}", "normalized_title": "diversity and bias in audio captioning datasets", "title": "Diversity and Bias in Audio Captioning Datasets", "abstract": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.", "source": "semantic_scholar", "source_id": "2ee853d36dced58204983c1bb108c14d3310cd2c", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Diversity and Bias in Audio Captioning Datasets", "abstract": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.", "metadata": {"arxiv_id": "semantic_scholar:2ee853d36dced58204983c1bb108c14d3310cd2c", "title": "Diversity and Bias in Audio Captioning Datasets", "summary": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016.", "source": "semantic_scholar", "source_id": "2ee853d36dced58204983c1bb108c14d3310cd2c", "year": 2021}, "round-A_JuniorNano_output": {"reasoning": "This paper addresses diversity and bias in audio captioning datasets and does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper addresses diversity and bias in audio captioning datasets and does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract focus on diversity and bias in audio captioning datasets but do not mention discrete audio tokens, neural audio codecs, or any discretization processes to produce token sequences for modeling, generation, or cross-modal reasoning as required by the inclusion criteria. Therefore, this work does not meet the core criteria of addressing discrete audio tokens or related systematic issues.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract focus on diversity and bias in audio captioning datasets but do not mention discrete audio tokens, neural audio codecs, or any discretization processes to produce token sequences for modeling, generation, or cross-modal reasoning as required by the inclusion criteria. Therefore, this work does not meet the core criteria of addressing discrete audio tokens or related systematic issues.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "liu2023audioldm", "query_title": "{{A}udio{LDM}: Text-to-Audio Generation with Latent Diffusion Models}", "normalized_title": "audioldm text to audio generation with latent diffusion models", "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models", "abstract": "Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.", "source": "semantic_scholar", "source_id": "fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models", "abstract": "Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.", "metadata": {"arxiv_id": "semantic_scholar:fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b", "title": "AudioLDM: Text-to-Audio Generation with Latent Diffusion Models", "summary": "Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.", "source": "semantic_scholar", "source_id": "fa0f3d8aa20e8987dbc7a516d5399cfa3dc97b1b", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on text-to-audio generation using latent diffusion in a continuous audio latent space and CLAP embeddings; it does not introduce or evaluate discrete audio tokens, tokenizers, or codebooks required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on text-to-audio generation using latent diffusion in a continuous audio latent space and CLAP embeddings; it does not introduce or evaluate discrete audio tokens, tokenizers, or codebooks required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes AudioLDM, a text-to-audio generation system based on latent diffusion models utilizing continuous audio representations from pretrained contrastive language-audio models, but it lacks explicit mention or evidence of discrete audio tokenization, vector quantization or tokenizer design to convert continuous audio into discrete token sequences required by the inclusion criteria; thus, it appears focused on continuous latent audio representations rather than discrete token sequences as core methodology.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper proposes AudioLDM, a text-to-audio generation system based on latent diffusion models utilizing continuous audio representations from pretrained contrastive language-audio models, but it lacks explicit mention or evidence of discrete audio tokenization, vector quantization or tokenizer design to convert continuous audio into discrete token sequences required by the inclusion criteria; thus, it appears focused on continuous latent audio representations rather than discrete token sequences as core methodology.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "liu2024audioldm2", "query_title": "{Audio{LDM} 2: Learning holistic audio generation with self-supervised pretraining}", "normalized_title": "audioldm 2 learning holistic audio generation with self supervised pretraining", "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining", "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches.", "source": "semantic_scholar", "source_id": "33de773be1733347a01cb07a5bb1b6cdfa956a47", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining", "abstract": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches.", "metadata": {"arxiv_id": "semantic_scholar:33de773be1733347a01cb07a5bb1b6cdfa956a47", "title": "AudioLDM 2: Learning Holistic Audio Generation With Self-Supervised Pretraining", "summary": "Although audio generation shares commonalities across different types of audio, such as speech, music, and sound effects, designing models for each type requires careful consideration of specific objectives and biases that can significantly differ from those of other types. To bring us closer to a unified perspective of audio generation, this paper proposes a holistic framework that utilizes the same learning method for speech, music, and sound effect generation. Our framework utilizes a general representation of audio, called “language of audio” (LOA). Any audio can be translated into LOA based on AudioMAE, a self-supervised pre-trained representation learning model. In the generation process, we translate other modalities into LOA by using a GPT-2 model, and we perform self-supervised audio generation learning with a latent diffusion model conditioned on the LOA of audio in our training set. The proposed framework naturally brings advantages such as reusable self-supervised pretrained latent diffusion models. Experiments on the major benchmarks of text-to-audio, text-to-music, and text-to-speech with three AudioLDM 2 variants demonstrate competitive performance of the AudioLDM 2 framework against previous approaches.", "source": "semantic_scholar", "source_id": "33de773be1733347a01cb07a5bb1b6cdfa956a47", "publication_date": "2023-08-10"}, "round-A_JuniorNano_output": {"reasoning": "Although AudioLDM 2 proposes a unified framework for audio generation using a language of audio and self-supervised pretraining, it does not introduce discrete audio tokens, tokenizers, or codebooks as core representations, so it fails the Discrete Audio Tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Although AudioLDM 2 proposes a unified framework for audio generation using a language of audio and self-supervised pretraining, it does not introduce discrete audio tokens, tokenizers, or codebooks as core representations, so it fails the Discrete Audio Tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study AudioLDM 2 focuses on self-supervised audio generation using a framework that translates audio into a general representation called 'language of audio' (LOA) via AudioMAE, a self-supervised pre-trained model. However, the abstract does not explicitly describe any discrete tokenization process or use of discrete audio tokens (e.g., neural audio codec, vector quantization, or explicit token vocabulary) as defined in the inclusion criteria. The methodology seems to rely on latent diffusion and self-supervised representations but lacks clear detail on discrete token extraction, design, or usage which are core requirements. Hence, it does not meet the inclusion criteria related to discrete audio tokens and token-based modeling for generation or cross-modal tasks, nor does it clarify tokenization details necessary for reproducibility and evaluation.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study AudioLDM 2 focuses on self-supervised audio generation using a framework that translates audio into a general representation called 'language of audio' (LOA) via AudioMAE, a self-supervised pre-trained model. However, the abstract does not explicitly describe any discrete tokenization process or use of discrete audio tokens (e.g., neural audio codec, vector quantization, or explicit token vocabulary) as defined in the inclusion criteria. The methodology seems to rely on latent diffusion and self-supervised representations but lacks clear detail on discrete token extraction, design, or usage which are core requirements. Hence, it does not meet the inclusion criteria related to discrete audio tokens and token-based modeling for generation or cross-modal tasks, nor does it clarify tokenization details necessary for reproducibility and evaluation.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "wang2024v2a", "query_title": "{V2a-mapper: A lightweight solution for vision-to-audio generation by connecting foundation models}", "normalized_title": "v2a mapper a lightweight solution for vision to audio generation by connecting foundation models", "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models", "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.", "source": "arxiv", "source_id": "2308.09300v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models", "abstract": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.", "metadata": {"arxiv_id": "2308.09300v4", "title": "V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models", "summary": "Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose a simple yet effective mapper mechanism (V2A-Mapper) to bridge the domain gap by translating the visual input between CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained audio generative FM AudioLDM is adopted to produce high-fidelity and visually-aligned sound. Compared to previous approaches, our method only requires a quick training of the V2A-Mapper. We further analyze and conduct extensive experiments on the choice of the V2A-Mapper and show that a generative mapper is better at fidelity and variability (FD) while a regression mapper is slightly better at relevance (CS). Both objective and subjective evaluation on two V2A datasets demonstrate the superiority of our proposed method compared to current state-of-the-art approaches - trained with 86% fewer parameters but achieving 53% and 19% improvement in FD and CS, respectively.", "authors": ["Heng Wang", "Jianbo Ma", "Santiago Pascual", "Richard Cartwright", "Weidong Cai"], "published": "2023-08-18T04:49:38Z", "updated": "2023-12-14T00:15:14Z", "categories": ["cs.CV", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.09300v4", "landing_url": "https://arxiv.org/abs/2308.09300v4", "doi": "https://doi.org/10.48550/arXiv.2308.09300"}, "round-A_JuniorNano_output": {"reasoning": "Abstract describes V2A generation via mapping between CLIP/CLAP embeddings and AudioLDM; there is no discussion of discrete audio tokenization or token design, so it does not meet the discrete-token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Abstract describes V2A generation via mapping between CLIP/CLAP embeddings and AudioLDM; there is no discussion of discrete audio tokenization or token design, so it does not meet the discrete-token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper proposes a method for generating audio from visual inputs leveraging foundation models, but it does not explicitly mention the use or design of discrete audio tokens or a neural audio codec that produces discretized token sequences, which is critical according to the inclusion criteria. The focus is on bridging latent spaces and generating audio but lacks clear details on tokenization, quantization, or discrete token vocabulary, therefore it does not meet the core inclusion requirements.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper proposes a method for generating audio from visual inputs leveraging foundation models, but it does not explicitly mention the use or design of discrete audio tokens or a neural audio codec that produces discretized token sequences, which is critical according to the inclusion criteria. The focus is on bridging latent spaces and generating audio but lacks clear details on tokenization, quantization, or discrete token vocabulary, therefore it does not meet the core inclusion requirements.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "pascual2024masked", "query_title": "{Masked generative video-to-audio transformers with enhanced synchronicity}", "normalized_title": "masked generative video to audio transformers with enhanced synchronicity", "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .", "source": "arxiv", "source_id": "2407.10387v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "abstract": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .", "metadata": {"arxiv_id": "2407.10387v1", "title": "Masked Generative Video-to-Audio Transformers with Enhanced Synchronicity", "summary": "Video-to-audio (V2A) generation leverages visual-only video features to render plausible sounds that match the scene. Importantly, the generated sound onsets should match the visual actions that are aligned with them, otherwise unnatural synchronization artifacts arise. Recent works have explored the progression of conditioning sound generators on still images and then video features, focusing on quality and semantic matching while ignoring synchronization, or by sacrificing some amount of quality to focus on improving synchronization only. In this work, we propose a V2A generative model, named MaskVAT, that interconnects a full-band high-quality general audio codec with a sequence-to-sequence masked generative model. This combination allows modeling both high audio quality, semantic matching, and temporal synchronicity at the same time. Our results show that, by combining a high-quality codec with the proper pre-trained audio-visual features and a sequence-to-sequence parallel structure, we are able to yield highly synchronized results on one hand, whilst being competitive with the state of the art of non-codec generative audio models. Sample videos and generated audios are available at https://maskvat.github.io .", "authors": ["Santiago Pascual", "Chunghsin Yeh", "Ioannis Tsiamas", "Joan Serrà"], "published": "2024-07-15T01:49:59Z", "updated": "2024-07-15T01:49:59Z", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.10387v1", "landing_url": "https://arxiv.org/abs/2407.10387v1", "doi": "https://doi.org/10.48550/arXiv.2407.10387"}, "round-A_JuniorNano_output": {"reasoning": "This abstract describes MaskVAT, a video-to-audio model using a neural audio codec for high quality and synchronization, but it does not explicitly describe discrete audio tokens or a tokenization/lexicon design, so it does not meet the token-centric inclusion criteria.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "This abstract describes MaskVAT, a video-to-audio model using a neural audio codec for high quality and synchronization, but it does not explicitly describe discrete audio tokens or a tokenization/lexicon design, so it does not meet the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The study proposes a video-to-audio generative model using a high-quality audio codec integrated with a masked generative model, aiming for high audio quality and synchronicity. However, the abstract lacks specific description of discrete audio token generation details, e.g., tokenizer/codec architecture, quantization method, vocabulary/codebook settings, or clear evidence of modeling discrete token sequences as core representations. While it involves a neural audio codec, there is insufficient information on discrete token sequences and their use in modeling or generation, making it unclear if it fully meets the inclusion criteria focused on discrete audio tokens and their systematic evaluation. Therefore, inclusion is uncertain.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study proposes a video-to-audio generative model using a high-quality audio codec integrated with a masked generative model, aiming for high audio quality and synchronicity. However, the abstract lacks specific description of discrete audio token generation details, e.g., tokenizer/codec architecture, quantization method, vocabulary/codebook settings, or clear evidence of modeling discrete token sequences as core representations. While it involves a neural audio codec, there is insufficient information on discrete token sequences and their use in modeling or generation, making it unclear if it fully meets the inclusion criteria focused on discrete audio tokens and their systematic evaluation. Therefore, inclusion is uncertain.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers flagged missing evidence for discrete audio tokens: JuniorNano marked exclusion (2) noting no explicit discrete token/tokenizer details, and JuniorMini was uncertain (3) due to insufficient description of tokenization or codebook/quantization settings. The abstract only states integration of a “high-quality audio codec” with a masked generative model but gives no English-method details about discrete token generation (e.g., vector quantization, codebook/vocabulary, bitrate, time downsampling) or that discrete token sequences are the core modeling units—so it does not meet the collection’s required evidence for discrete audio tokens.", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Both junior reviewers flagged missing evidence for discrete audio tokens: JuniorNano marked exclusion (2) noting no explicit discrete token/tokenizer details, and JuniorMini was uncertain (3) due to insufficient description of tokenization or codebook/quantization settings. The abstract only states integration of a “high-quality audio codec” with a masked generative model but gives no English-method details about discrete token generation (e.g., vector quantization, codebook/vocabulary, bitrate, time downsampling) or that discrete token sequences are the core modeling units—so it does not meet the collection’s required evidence for discrete audio tokens.", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "zhang2024foleycrafter", "query_title": "{Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds}", "normalized_title": "foleycrafter bring silent videos to life with lifelike and synchronized sounds", "title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds", "abstract": "We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. Models and codes are available at https://github.com/open-mmlab/FoleyCrafter.", "source": "arxiv", "source_id": "2407.01494v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds", "abstract": "We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. Models and codes are available at https://github.com/open-mmlab/FoleyCrafter.", "metadata": {"arxiv_id": "2407.01494v1", "title": "FoleyCrafter: Bring Silent Videos to Life with Lifelike and Synchronized Sounds", "summary": "We study Neural Foley, the automatic generation of high-quality sound effects synchronizing with videos, enabling an immersive audio-visual experience. Despite its wide range of applications, existing approaches encounter limitations when it comes to simultaneously synthesizing high-quality and video-aligned (i.e.,, semantic relevant and temporal synchronized) sounds. To overcome these limitations, we propose FoleyCrafter, a novel framework that leverages a pre-trained text-to-audio model to ensure high-quality audio generation. FoleyCrafter comprises two key components: the semantic adapter for semantic alignment and the temporal controller for precise audio-video synchronization. The semantic adapter utilizes parallel cross-attention layers to condition audio generation on video features, producing realistic sound effects that are semantically relevant to the visual content. Meanwhile, the temporal controller incorporates an onset detector and a timestampbased adapter to achieve precise audio-video alignment. One notable advantage of FoleyCrafter is its compatibility with text prompts, enabling the use of text descriptions to achieve controllable and diverse video-to-audio generation according to user intents. We conduct extensive quantitative and qualitative experiments on standard benchmarks to verify the effectiveness of FoleyCrafter. Models and codes are available at https://github.com/open-mmlab/FoleyCrafter.", "authors": ["Yiming Zhang", "Yicheng Gu", "Yanhong Zeng", "Zhening Xing", "Yuancheng Wang", "Zhizheng Wu", "Kai Chen"], "published": "2024-07-01T17:35:56Z", "updated": "2024-07-01T17:35:56Z", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.01494v1", "landing_url": "https://arxiv.org/abs/2407.01494v1", "doi": "https://doi.org/10.48550/arXiv.2407.01494"}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes FoleyCrafter, a video-to-audio generation framework using a pre-trained text-to-audio model with semantic adapters and a temporal controller; it does not discuss generating or evaluating discrete audio tokens, tokenizers, or quantized representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes FoleyCrafter, a video-to-audio generation framework using a pre-trained text-to-audio model with semantic adapters and a temporal controller; it does not discuss generating or evaluating discrete audio tokens, tokenizers, or quantized representations, so it does not meet the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes FoleyCrafter, a neural method for generating synchronized sound effects for silent videos, leveraging a pre-trained text-to-audio model to produce high-quality audio with semantic and temporal alignment. However, there is no explicit mention that the approach involves discretizing continuous audio waveforms into discrete tokens or neural audio codecs producing discrete unit sequences, which is central to the inclusion criteria. Furthermore, the abstract lacks details about tokenization, codebooks, or vector quantization schemes, and focuses primarily on audio generation quality and synchronization rather than token design or evaluation. Therefore, it does not clearly meet the inclusion criteria related to discrete audio tokens and their systematic study, nor is it excluded categorically due to lack of information or presence of continuous audio methods without discrete tokens. Hence, the evaluation is uncertain.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The abstract describes FoleyCrafter, a neural method for generating synchronized sound effects for silent videos, leveraging a pre-trained text-to-audio model to produce high-quality audio with semantic and temporal alignment. However, there is no explicit mention that the approach involves discretizing continuous audio waveforms into discrete tokens or neural audio codecs producing discrete unit sequences, which is central to the inclusion criteria. Furthermore, the abstract lacks details about tokenization, codebooks, or vector quantization schemes, and focuses primarily on audio generation quality and synchronization rather than token design or evaluation. Therefore, it does not clearly meet the inclusion criteria related to discrete audio tokens and their systematic study, nor is it excluded categorically due to lack of information or presence of continuous audio methods without discrete tokens. Hence, the evaluation is uncertain.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers flagged the absence of any discrete-token details: the abstract describes a video-to-audio generation system (FoleyCrafter) built on a pre-trained text-to-audio model with semantic and temporal modules but contains no mention of discretization, neural audio codecs, vector quantization, codebooks, tokenizers, or discrete token sequences—therefore it fails the inclusion requirement that discrete audio tokens be the core representation and meets the exclusion criterion of continuous audio modeling without discrete tokens. Junior A gave 1 (exclude) and Junior B was uncertain (3); I concur with Junior A. One-sentence explanation: Because the paper provides no evidence of producing or using discrete audio tokens (tokenizers/codebooks/quantization) in the abstract, it should be excluded (score = 1).", "evaluation": 1}, "round-B_SeniorLead_reasoning": "Both junior reviewers flagged the absence of any discrete-token details: the abstract describes a video-to-audio generation system (FoleyCrafter) built on a pre-trained text-to-audio model with semantic and temporal modules but contains no mention of discretization, neural audio codecs, vector quantization, codebooks, tokenizers, or discrete token sequences—therefore it fails the inclusion requirement that discrete audio tokens be the core representation and meets the exclusion criterion of continuous audio modeling without discrete tokens. Junior A gave 1 (exclude) and Junior B was uncertain (3); I concur with Junior A. One-sentence explanation: Because the paper provides no evidence of producing or using discrete audio tokens (tokenizers/codebooks/quantization) in the abstract, it should be excluded (score = 1).", "round-B_SeniorLead_evaluation": 1, "final_verdict": "exclude (senior:1)", "review_skipped": false, "discard_reason": "exclude (senior:1)"}]}
{"key": "dong2023clipsonic", "query_title": "{Clipsonic: Text-to-audio synthesis with unlabeled videos and pretrained language-vision models}", "normalized_title": "clipsonic text to audio synthesis with unlabeled videos and pretrained language vision models", "title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models", "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.", "source": "arxiv", "source_id": "2306.09635v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models", "abstract": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.", "metadata": {"arxiv_id": "2306.09635v2", "title": "CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models", "summary": "Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pretrained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.", "authors": ["Hao-Wen Dong", "Xiaoyu Liu", "Jordi Pons", "Gautam Bhattacharya", "Santiago Pascual", "Joan Serrà", "Taylor Berg-Kirkpatrick", "Julian McAuley"], "published": "2023-06-16T05:42:01Z", "updated": "2023-07-23T07:53:42Z", "categories": ["cs.SD", "cs.LG", "cs.MM", "eess.AS", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2306.09635v2", "landing_url": "https://arxiv.org/abs/2306.09635v2", "doi": "https://doi.org/10.48550/arXiv.2306.09635"}, "round-A_JuniorNano_output": {"reasoning": "The paper targets text-to-audio synthesis from unlabeled videos using CLIP and diffusion models, but it does not discuss discrete audio tokens, tokenization, or neural audio codecs, thus failing to meet the Core 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper targets text-to-audio synthesis from unlabeled videos using CLIP and diffusion models, but it does not discuss discrete audio tokens, tokenization, or neural audio codecs, thus failing to meet the Core 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on text-to-audio synthesis using unlabeled videos and pretrained language-vision models, but it does not explicitly describe the use or design of discrete audio tokens derived through neural audio codecs or self-supervised models; it primarily addresses audio generation from continuous audio rather than discrete token sequences, and lacks details on discretization processes or token vocabularies required by the inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on text-to-audio synthesis using unlabeled videos and pretrained language-vision models, but it does not explicitly describe the use or design of discrete audio tokens derived through neural audio codecs or self-supervised models; it primarily addresses audio generation from continuous audio rather than discrete token sequences, and lacks details on discretization processes or token vocabularies required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "jeong2024read", "query_title": "{Read, watch and scream! sound generation from text and video}", "normalized_title": "read watch and scream sound generation from text and video", "title": "Read, Watch and Scream! Sound Generation from Text and Video", "abstract": "Despite the impressive progress of multimodal generative models, video-to-audio generation still suffers from limited performance and limits the flexibility to prioritize sound synthesis for specific objects within the scene. Conversely, text-to-audio generation methods generate high-quality audio but pose challenges in ensuring comprehensive scene depiction and time-varying control. To tackle these challenges, we propose a novel video-and-text-to-audio generation method, called \\ours, where video serves as a conditional control for a text-to-audio generation model. Especially, our method estimates the structural information of sound (namely, energy) from the video while receiving key content cues from a user prompt. We employ a well-performing text-to-audio model to consolidate the video control, which is much more efficient for training multimodal diffusion models with massive triplet-paired (audio-video-text) data. In addition, by separating the generative components of audio, it becomes a more flexible system that allows users to freely adjust the energy, surrounding environment, and primary sound source according to their preferences. Experimental results demonstrate that our method shows superiority in terms of quality, controllability, and training efficiency. Code and demo are available at https://naver-ai.github.io/rewas.", "source": "arxiv", "source_id": "2407.05551v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Read, Watch and Scream! Sound Generation from Text and Video", "abstract": "Despite the impressive progress of multimodal generative models, video-to-audio generation still suffers from limited performance and limits the flexibility to prioritize sound synthesis for specific objects within the scene. Conversely, text-to-audio generation methods generate high-quality audio but pose challenges in ensuring comprehensive scene depiction and time-varying control. To tackle these challenges, we propose a novel video-and-text-to-audio generation method, called \\ours, where video serves as a conditional control for a text-to-audio generation model. Especially, our method estimates the structural information of sound (namely, energy) from the video while receiving key content cues from a user prompt. We employ a well-performing text-to-audio model to consolidate the video control, which is much more efficient for training multimodal diffusion models with massive triplet-paired (audio-video-text) data. In addition, by separating the generative components of audio, it becomes a more flexible system that allows users to freely adjust the energy, surrounding environment, and primary sound source according to their preferences. Experimental results demonstrate that our method shows superiority in terms of quality, controllability, and training efficiency. Code and demo are available at https://naver-ai.github.io/rewas.", "metadata": {"arxiv_id": "2407.05551v2", "title": "Read, Watch and Scream! Sound Generation from Text and Video", "summary": "Despite the impressive progress of multimodal generative models, video-to-audio generation still suffers from limited performance and limits the flexibility to prioritize sound synthesis for specific objects within the scene. Conversely, text-to-audio generation methods generate high-quality audio but pose challenges in ensuring comprehensive scene depiction and time-varying control. To tackle these challenges, we propose a novel video-and-text-to-audio generation method, called \\ours, where video serves as a conditional control for a text-to-audio generation model. Especially, our method estimates the structural information of sound (namely, energy) from the video while receiving key content cues from a user prompt. We employ a well-performing text-to-audio model to consolidate the video control, which is much more efficient for training multimodal diffusion models with massive triplet-paired (audio-video-text) data. In addition, by separating the generative components of audio, it becomes a more flexible system that allows users to freely adjust the energy, surrounding environment, and primary sound source according to their preferences. Experimental results demonstrate that our method shows superiority in terms of quality, controllability, and training efficiency. Code and demo are available at https://naver-ai.github.io/rewas.", "authors": ["Yujin Jeong", "Yunji Kim", "Sanghyuk Chun", "Jiyoung Lee"], "published": "2024-07-08T01:59:17Z", "updated": "2024-12-26T15:23:36Z", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.05551v2", "landing_url": "https://arxiv.org/abs/2407.05551v2", "doi": "https://doi.org/10.48550/arXiv.2407.05551"}, "round-A_JuniorNano_output": {"reasoning": "The abstract addresses video-to-audio and text-to-audio generation with video as a control signal, but there is no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not satisfy the inclusion criteria for discrete audio tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract addresses video-to-audio and text-to-audio generation with video as a control signal, but there is no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not satisfy the inclusion criteria for discrete audio tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study proposes a multimodal audio generation method combining video and text inputs to produce sound but does not explicitly describe the use of discrete audio tokens generated via neural audio codec or clustering mechanisms as required by the inclusion criteria, nor does it detail any tokenization or codebook structures. Therefore, it does not meet the core criteria of discrete audio token representation essential for inclusion.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study proposes a multimodal audio generation method combining video and text inputs to produce sound but does not explicitly describe the use of discrete audio tokens generated via neural audio codec or clustering mechanisms as required by the inclusion criteria, nor does it detail any tokenization or codebook structures. Therefore, it does not meet the core criteria of discrete audio token representation essential for inclusion.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "chen2024video", "query_title": "{Video-guided foley sound generation with multimodal controls}", "normalized_title": "video guided foley sound generation with multimodal controls", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "source": "arxiv", "source_id": "2411.17698v4", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "metadata": {"arxiv_id": "2411.17698v4", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "summary": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "authors": ["Ziyang Chen", "Prem Seetharaman", "Bryan Russell", "Oriol Nieto", "David Bourgin", "Andrew Owens", "Justin Salamon"], "published": "2024-11-26T18:59:58Z", "updated": "2025-03-17T17:44:37Z", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.17698v4", "landing_url": "https://arxiv.org/abs/2411.17698v4", "doi": "https://doi.org/10.48550/arXiv.2411.17698"}, "round-A_JuniorNano_output": {"reasoning": "The abstract covers video-guided sound generation with multimodal conditioning but does not discuss discrete audio tokens, tokenizers, or neural codec-based token representations; thus it does not meet the ‘Discrete Audio Tokens’ criterion and should be excluded under the stated exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract covers video-guided sound generation with multimodal conditioning but does not discuss discrete audio tokens, tokenizers, or neural codec-based token representations; thus it does not meet the ‘Discrete Audio Tokens’ criterion and should be excluded under the stated exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a model for video-guided Foley sound generation using multimodal inputs, focusing on generating synchronized high-quality sounds. However, there is no explicit mention of the use of discrete audio tokens, such as neural audio codecs or vector quantization processes that produce token sequences, nor details about tokenization, codebooks, or discrete representation design, which are required to meet the inclusion criteria. The study seems to focus on continuous audio generation conditioned on multimodal inputs rather than discrete token-based audio modeling, thus failing the core inclusion requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes a model for video-guided Foley sound generation using multimodal inputs, focusing on generating synchronized high-quality sounds. However, there is no explicit mention of the use of discrete audio tokens, such as neural audio codecs or vector quantization processes that produce token sequences, nor details about tokenization, codebooks, or discrete representation design, which are required to meet the inclusion criteria. The study seems to focus on continuous audio generation conditioned on multimodal inputs rather than discrete token-based audio modeling, thus failing the core inclusion requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "metadata": {"arxiv_id": "2411.17698v4", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "summary": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "authors": ["Ziyang Chen", "Prem Seetharaman", "Bryan Russell", "Oriol Nieto", "David Bourgin", "Andrew Owens", "Justin Salamon"], "published": "2024-11-26T18:59:58Z", "updated": "2025-03-17T17:44:37Z", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.17698v4", "landing_url": "https://arxiv.org/abs/2411.17698v4", "doi": "https://doi.org/10.48550/arXiv.2411.17698"}, "round-A_JuniorNano_output": {"reasoning": "The item describes video-guided sound generation with multimodal controls, but there is no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the discrete-audio-token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes video-guided sound generation with multimodal controls, but there is no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the discrete-audio-token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on video-guided sound generation using multimodal controls, particularly generating synchronized high-quality sounds conditioned on video, text, and audio inputs. However, the abstract lacks any mention or indication of discrete audio tokenization methods, such as the use of neural audio codecs or vector quantization producing discrete token sequences; nor does it specify any quantization or tokenizer design details, vocabulary, codebook settings, or discrete unit representations. The methodology seems to be based on direct sound generation rather than using discrete token representations as core modeling units. Therefore, it does not meet the inclusion criteria requiring clear evidence of discrete audio token generation and modeling, and falls under the exclusion criteria of continuous audio modeling without discrete token use.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on video-guided sound generation using multimodal controls, particularly generating synchronized high-quality sounds conditioned on video, text, and audio inputs. However, the abstract lacks any mention or indication of discrete audio tokenization methods, such as the use of neural audio codecs or vector quantization producing discrete token sequences; nor does it specify any quantization or tokenizer design details, vocabulary, codebook settings, or discrete unit representations. The methodology seems to be based on direct sound generation rather than using discrete token representations as core modeling units. Therefore, it does not meet the inclusion criteria requiring clear evidence of discrete audio token generation and modeling, and falls under the exclusion criteria of continuous audio modeling without discrete token use.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "saito2024soundctm", "query_title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "normalized_title": "soundctm uniting score based and consistency models for text to sound generation", "title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "abstract": "This repository is the official implementation of \"SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation\"", "source": "github", "source_id": "sony/soundctm", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "abstract": "This repository is the official implementation of \"SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation\"", "metadata": {"arxiv_id": "github:sony/soundctm", "title": "{Sound{CTM}: Uniting Score-based and Consistency Models for Text-to-Sound Generation}", "summary": "This repository is the official implementation of \"SoundCTM: Uniting Score-based and Consistency Models for Text-to-Sound Generation\"", "source": "github", "source_id": "sony/soundctm", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe text-to-sound generation using score-based and consistency models, but there is no mention of discrete audio tokens, tokenizers, codebooks, or token-based representations, so it fails the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe text-to-sound generation using score-based and consistency models, but there is no mention of discrete audio tokens, tokenizers, codebooks, or token-based representations, so it fails the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The provided title and abstract lack sufficient detailed information regarding the discrete audio token generation process, such as tokenizer design, codec architecture, quantization method, or codebook settings, which are essential to determine if the study fits within the discrete audio tokens topic. Furthermore, it does not clarify whether a neural audio codec or vector quantization is employed or how the tokens are utilized in subsequent modeling. Therefore, based on the abstract alone, it is not possible to verify that the study meets the inclusion criteria nor excludes it definitively.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The provided title and abstract lack sufficient detailed information regarding the discrete audio token generation process, such as tokenizer design, codec architecture, quantization method, or codebook settings, which are essential to determine if the study fits within the discrete audio tokens topic. Furthermore, it does not clarify whether a neural audio codec or vector quantization is employed or how the tokens are utilized in subsequent modeling. Therefore, based on the abstract alone, it is not possible to verify that the study meets the inclusion criteria nor excludes it definitively.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "The title/abstract only state a text-to-sound generation implementation (SoundCTM) using score-based and consistency models and provide no information about any discrete audio tokenization, tokenizer/codec architecture, vector quantization, codebooks, or token-based representations required by the “Discrete Audio Tokens” inclusion criteria. JuniorNano recommended exclusion for the same reason and JuniorMini flagged uncertainty due to lack of sufficient methodological detail; after reviewing both opinions and the item, there is still no evidence in the title/abstract that discrete tokens are produced or used, nor any English-method details to verify such, so the item fails the inclusion requirements and matches the exclusion criteria for continuous audio modeling without discrete tokenization. Explanation: No mention or evidence of discrete audio tokens, tokenizers, or codebooks in the provided title/abstract, so it should be excluded.", "evaluation": 1}, "round-B_SeniorLead_reasoning": "The title/abstract only state a text-to-sound generation implementation (SoundCTM) using score-based and consistency models and provide no information about any discrete audio tokenization, tokenizer/codec architecture, vector quantization, codebooks, or token-based representations required by the “Discrete Audio Tokens” inclusion criteria. JuniorNano recommended exclusion for the same reason and JuniorMini flagged uncertainty due to lack of sufficient methodological detail; after reviewing both opinions and the item, there is still no evidence in the title/abstract that discrete tokens are produced or used, nor any English-method details to verify such, so the item fails the inclusion requirements and matches the exclusion criteria for continuous audio modeling without discrete tokenization. Explanation: No mention or evidence of discrete audio tokens, tokenizers, or codebooks in the provided title/abstract, so it should be excluded.", "round-B_SeniorLead_evaluation": 1, "final_verdict": "exclude (senior:1)", "review_skipped": false, "discard_reason": "exclude (senior:1)"}]}
{"key": "wang2025frieren", "query_title": "{Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching}", "normalized_title": "frieren efficient video to audio generation network with rectified flow matching", "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "abstract": "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io.", "source": "arxiv", "source_id": "2406.00320v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "abstract": "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io.", "metadata": {"arxiv_id": "2406.00320v4", "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "summary": "Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io.", "authors": ["Yongqi Wang", "Wenxiang Guo", "Rongjie Huang", "Jiawei Huang", "Zehan Wang", "Fuming You", "Ruiqi Li", "Zhou Zhao"], "published": "2024-06-01T06:40:22Z", "updated": "2025-01-04T18:12:07Z", "categories": ["cs.SD", "cs.CV", "cs.MM", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.00320v4", "landing_url": "https://arxiv.org/abs/2406.00320v4", "doi": "https://doi.org/10.48550/arXiv.2406.00320"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on continuous spectrogram latent generation and flow-based sampling for video-to-audio, with no discussion of discretized audio tokens or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on continuous spectrogram latent generation and flow-based sampling for video-to-audio, with no discussion of discretized audio tokens or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses a video-to-audio generation model called Frieren that generates audio from silent video using rectified flow matching, focusing on audio quality and temporal alignment. However, the abstract does not mention discrete audio tokens, neural audio codecs with quantization, or any discretization process converting continuous audio into token sequences, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria related to discrete audio token representation or their modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses a video-to-audio generation model called Frieren that generates audio from silent video using rectified flow matching, focusing on audio quality and temporal alignment. However, the abstract does not mention discrete audio tokens, neural audio codecs with quantization, or any discretization process converting continuous audio into token sequences, which is a core requirement for inclusion. Therefore, it does not meet the inclusion criteria related to discrete audio token representation or their modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "luo2023diff", "query_title": "{Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models}", "normalized_title": "diff foley synchronized video to audio synthesis with latent diffusion models", "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models", "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/", "source": "arxiv", "source_id": "2306.17203v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models", "abstract": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/", "metadata": {"arxiv_id": "2306.17203v1", "title": "Diff-Foley: Synchronized Video-to-Audio Synthesis with Latent Diffusion Models", "summary": "The Video-to-Audio (V2A) model has recently gained attention for its practical application in generating audio directly from silent videos, particularly in video/film production. However, previous methods in V2A have limited generation quality in terms of temporal synchronization and audio-visual relevance. We present Diff-Foley, a synchronized Video-to-Audio synthesis method with a latent diffusion model (LDM) that generates high-quality audio with improved synchronization and audio-visual relevance. We adopt contrastive audio-visual pretraining (CAVP) to learn more temporally and semantically aligned features, then train an LDM with CAVP-aligned visual features on spectrogram latent space. The CAVP-aligned features enable LDM to capture the subtler audio-visual correlation via a cross-attention module. We further significantly improve sample quality with `double guidance'. Diff-Foley achieves state-of-the-art V2A performance on current large scale V2A dataset. Furthermore, we demonstrate Diff-Foley practical applicability and generalization capabilities via downstream finetuning. Project Page: see https://diff-foley.github.io/", "authors": ["Simian Luo", "Chuanhao Yan", "Chenxu Hu", "Hang Zhao"], "published": "2023-06-29T12:39:58Z", "updated": "2023-06-29T12:39:58Z", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2306.17203v1", "landing_url": "https://arxiv.org/abs/2306.17203v1", "doi": "https://doi.org/10.48550/arXiv.2306.17203"}, "round-A_JuniorNano_output": {"reasoning": "This work focuses on synchronized video-to-audio synthesis using latent diffusion models with continuous spectrogram latent space and does not introduce or analyze discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This work focuses on synchronized video-to-audio synthesis using latent diffusion models with continuous spectrogram latent space and does not introduce or analyze discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on Video-to-Audio synthesis using latent diffusion models to generate audio from silent videos, emphasizing temporal synchronization and audio-visual relevance. However, the abstract does not indicate the use or design of discrete audio tokens obtained via discretization processes, neural audio codecs, or self-supervised models to represent audio as token sequences, which is a core inclusion criterion. Therefore, it does not meet the criteria related to discrete audio token representation and is more about continuous audio generation conditioned on video features.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on Video-to-Audio synthesis using latent diffusion models to generate audio from silent videos, emphasizing temporal synchronization and audio-visual relevance. However, the abstract does not indicate the use or design of discrete audio tokens obtained via discretization processes, neural audio codecs, or self-supervised models to represent audio as token sequences, which is a core inclusion criterion. Therefore, it does not meet the criteria related to discrete audio token representation and is more about continuous audio generation conditioned on video features.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "sheffer2023hear", "query_title": "{I hear your true colors: Image guided audio generation}", "normalized_title": "i hear your true colors image guided audio generation", "title": "I Hear Your True Colors: Image Guided Audio Generation", "abstract": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.", "source": "arxiv", "source_id": "2211.03089v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "I Hear Your True Colors: Image Guided Audio Generation", "abstract": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.", "metadata": {"arxiv_id": "2211.03089v2", "title": "I Hear Your True Colors: Image Guided Audio Generation", "summary": "We propose Im2Wav, an image guided open-domain audio generation system. Given an input image or a sequence of images, Im2Wav generates a semantically relevant sound. Im2Wav is based on two Transformer language models, that operate over a hierarchical discrete audio representation obtained from a VQ-VAE based model. We first produce a low-level audio representation using a language model. Then, we upsample the audio tokens using an additional language model to generate a high-fidelity audio sample. We use the rich semantics of a pre-trained CLIP (Contrastive Language-Image Pre-training) embedding as a visual representation to condition the language model. In addition, to steer the generation process towards the conditioning image, we apply the classifier-free guidance method. Results suggest that Im2Wav significantly outperforms the evaluated baselines in both fidelity and relevance evaluation metrics. Additionally, we provide an ablation study to better assess the impact of each of the method components on overall performance. Lastly, to better evaluate image-to-audio models, we propose an out-of-domain image dataset, denoted as ImageHear. ImageHear can be used as a benchmark for evaluating future image-to-audio models. Samples and code can be found inside the manuscript.", "authors": ["Roy Sheffer", "Yossi Adi"], "published": "2022-11-06T11:48:20Z", "updated": "2023-02-27T11:15:40Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2211.03089v2", "landing_url": "https://arxiv.org/abs/2211.03089v2", "doi": "https://doi.org/10.48550/arXiv.2211.03089"}, "round-A_JuniorNano_output": {"reasoning": "The paper explicitly uses discrete audio tokens produced by a neural codec (VQ-VAE with multi-codebooks) and a hierarchical token representation, conditions generation on image embeddings (CLIP), and reports downstream audio generation results, aligning with the 'Discrete Audio Tokens' inclusion criteria and not fitting the exclusions (continuous-only or non-token-based work).", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete audio tokens produced by a neural codec (VQ-VAE with multi-codebooks) and a hierarchical token representation, conditions generation on image embeddings (CLIP), and reports downstream audio generation results, aligning with the 'Discrete Audio Tokens' inclusion criteria and not fitting the exclusions (continuous-only or non-token-based work).", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes Im2Wav, which uses discrete audio representations obtained from a VQ-VAE based model and two Transformer language models to generate audio conditioned on images. The discrete audio tokens are hierarchical and low-level, and the paper details the discrete tokenization process and modeling, fulfilling the inclusion criteria regarding the use of neural audio codec generated discrete tokens for audio generation and cross-modal conditioning. The abstract is in English and describes the method and evaluation clearly. There is no indication that the study uses continuous audio representations only or excludes discrete tokens, nor does it focus on discriminative tasks or traditional non-neural codecs. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes Im2Wav, which uses discrete audio representations obtained from a VQ-VAE based model and two Transformer language models to generate audio conditioned on images. The discrete audio tokens are hierarchical and low-level, and the paper details the discrete tokenization process and modeling, fulfilling the inclusion criteria regarding the use of neural audio codec generated discrete tokens for audio generation and cross-modal conditioning. The abstract is in English and describes the method and evaluation clearly. There is no indication that the study uses continuous audio representations only or excludes discrete tokens, nor does it focus on discriminative tasks or traditional non-neural codecs. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "raffel2020exploring", "query_title": "{Exploring the limits of transfer learning with a unified text-to-text transformer}", "normalized_title": "exploring the limits of transfer learning with a unified text to text transformer", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.", "source": "arxiv", "source_id": "1910.10683v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.", "metadata": {"arxiv_id": "1910.10683v4", "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "summary": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.", "authors": ["Colin Raffel", "Noam Shazeer", "Adam Roberts", "Katherine Lee", "Sharan Narang", "Michael Matena", "Yanqi Zhou", "Wei Li", "Peter J. Liu"], "published": "2019-10-23T17:37:36Z", "updated": "2023-09-19T15:14:48Z", "categories": ["cs.LG", "cs.CL", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1910.10683v4", "landing_url": "https://arxiv.org/abs/1910.10683v4", "doi": "https://doi.org/10.48550/arXiv.1910.10683"}, "round-A_JuniorNano_output": {"reasoning": "This item targets NLP transfer learning with a unified text-to-text framework and contains no discrete audio token generation or analysis, so it does not meet the inclusion criteria and clearly falls outside the exclusion criteria scope for discrete audio tokens.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item targets NLP transfer learning with a unified text-to-text framework and contains no discrete audio token generation or analysis, so it does not meet the inclusion criteria and clearly falls outside the exclusion criteria scope for discrete audio tokens.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on transfer learning in NLP using a text-to-text transformer framework, without any mention of discrete audio tokens, neural audio codecs, or discretization of continuous audio signals into token sequences; thus, it does not meet the inclusion criteria related to discrete audio token representations or their applications in audio generation, understanding, or multimodal reasoning.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on transfer learning in NLP using a text-to-text transformer framework, without any mention of discrete audio tokens, neural audio codecs, or discretization of continuous audio signals into token sequences; thus, it does not meet the inclusion criteria related to discrete audio token representations or their applications in audio generation, understanding, or multimodal reasoning.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "peng2024survey", "query_title": "{A survey on speech large language models}", "normalized_title": "a survey on speech large language models", "title": "A Survey on Speech Large Language Models for Understanding", "abstract": "Speech understanding is essential for interpreting the diverse forms of information embedded in spoken language, including linguistic, paralinguistic, and non-linguistic cues that are vital for effective human-computer interaction. The rapid advancement of large language models (LLMs) has catalyzed the emergence of Speech Large Language Models (Speech LLMs), which marks a transformative shift toward general-purpose speech understanding systems. To further clarify and systematically delineate task objectives, in this paper, we formally define the concept of speech understanding and introduce a structured taxonomy encompassing its informational, functional, and format dimensions. Within this scope of definition, we present a comprehensive review of current Speech LLMs, analyzing their architectures through a three-stage abstraction: Modality Feature Extraction, Modality Information Fusion, and LLM Inference. In addition, we examine training strategies, discuss representative datasets, and review evaluation methodologies adopted in the field. Based on empirical analyses and experimental evidence, we identify two key challenges currently facing Speech LLMs: instruction sensitivity and degradation in semantic reasoning and propose concrete directions for addressing these issues. Through this systematic and detailed survey, we aim to offer a foundational reference for researchers and practitioners working toward more robust, generalizable, and human-aligned Speech LLMs.", "source": "arxiv", "source_id": "2410.18908", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "A Survey on Speech Large Language Models for Understanding", "abstract": "Speech understanding is essential for interpreting the diverse forms of information embedded in spoken language, including linguistic, paralinguistic, and non-linguistic cues that are vital for effective human-computer interaction. The rapid advancement of large language models (LLMs) has catalyzed the emergence of Speech Large Language Models (Speech LLMs), which marks a transformative shift toward general-purpose speech understanding systems. To further clarify and systematically delineate task objectives, in this paper, we formally define the concept of speech understanding and introduce a structured taxonomy encompassing its informational, functional, and format dimensions. Within this scope of definition, we present a comprehensive review of current Speech LLMs, analyzing their architectures through a three-stage abstraction: Modality Feature Extraction, Modality Information Fusion, and LLM Inference. In addition, we examine training strategies, discuss representative datasets, and review evaluation methodologies adopted in the field. Based on empirical analyses and experimental evidence, we identify two key challenges currently facing Speech LLMs: instruction sensitivity and degradation in semantic reasoning and propose concrete directions for addressing these issues. Through this systematic and detailed survey, we aim to offer a foundational reference for researchers and practitioners working toward more robust, generalizable, and human-aligned Speech LLMs.", "metadata": {"arxiv_id": "2410.18908", "title": "A Survey on Speech Large Language Models for Understanding", "summary": "Speech understanding is essential for interpreting the diverse forms of information embedded in spoken language, including linguistic, paralinguistic, and non-linguistic cues that are vital for effective human-computer interaction. The rapid advancement of large language models (LLMs) has catalyzed the emergence of Speech Large Language Models (Speech LLMs), which marks a transformative shift toward general-purpose speech understanding systems. To further clarify and systematically delineate task objectives, in this paper, we formally define the concept of speech understanding and introduce a structured taxonomy encompassing its informational, functional, and format dimensions. Within this scope of definition, we present a comprehensive review of current Speech LLMs, analyzing their architectures through a three-stage abstraction: Modality Feature Extraction, Modality Information Fusion, and LLM Inference. In addition, we examine training strategies, discuss representative datasets, and review evaluation methodologies adopted in the field. Based on empirical analyses and experimental evidence, we identify two key challenges currently facing Speech LLMs: instruction sensitivity and degradation in semantic reasoning and propose concrete directions for addressing these issues. Through this systematic and detailed survey, we aim to offer a foundational reference for researchers and practitioners working toward more robust, generalizable, and human-aligned Speech LLMs.", "authors": ["Jing Peng", "Yucheng Wang", "Bohan Li", "Yiwei Guo", "Hankun Wang", "Yangui Fang", "Yu Xi", "Haoyu Li", "Xu Li", "Ke Zhang", "Shuai Wang", "Kai Yu"], "published": "2024-10-24T16:59:28Z", "updated": "2025-11-26T09:57:19Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2410.18908v6", "landing_url": "https://arxiv.org/abs/2410.18908v6", "doi": "https://doi.org/10.1109/JSTSP.2025.3640535"}, "round-A_JuniorNano_output": {"reasoning": "This item is a general survey of Speech LLMs and speech understanding, with no focus on discrete audio tokens, tokenization/codec details, or token-based downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is a general survey of Speech LLMs and speech understanding, with no focus on discrete audio tokens, tokenization/codec details, or token-based downstream tasks, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper is a comprehensive survey on Speech Large Language Models focused on speech understanding, discussing architectures, training strategies, datasets, and evaluation methods. However, it does not focus on 'Discrete Audio Tokens' as defined in the inclusion criteria — i.e., mapping continuous audio waveforms into discrete token sequences for sequence modeling. There is no mention of discrete tokenization processes such as neural audio codec, vector quantization, or discrete token design affecting tasks like generation or alignment. Hence, it does not meet the core topic and method requirements for inclusion and falls under exclusion criteria concerning no discrete token use as core focus.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper is a comprehensive survey on Speech Large Language Models focused on speech understanding, discussing architectures, training strategies, datasets, and evaluation methods. However, it does not focus on 'Discrete Audio Tokens' as defined in the inclusion criteria — i.e., mapping continuous audio waveforms into discrete token sequences for sequence modeling. There is no mention of discrete tokenization processes such as neural audio codec, vector quantization, or discrete token design affecting tasks like generation or alignment. Hence, it does not meet the core topic and method requirements for inclusion and falls under exclusion criteria concerning no discrete token use as core focus.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "cui2024recent", "query_title": "{Recent advances in speech language models: A survey}", "normalized_title": "recent advances in speech language models a survey", "title": "Recent Advances in Speech Language Models: A Survey", "abstract": "Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "source": "arxiv", "source_id": "2410.03751v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Recent Advances in Speech Language Models: A Survey", "abstract": "Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "metadata": {"arxiv_id": "2410.03751v4", "title": "Recent Advances in Speech Language Models: A Survey", "summary": "Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey", "authors": ["Wenqian Cui", "Dianzhi Yu", "Xiaoqi Jiao", "Ziqiao Meng", "Guangyan Zhang", "Qichao Wang", "Yiwen Guo", "Irwin King"], "published": "2024-10-01T21:48:12Z", "updated": "2025-08-07T13:29:43Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2410.03751v4", "landing_url": "https://arxiv.org/abs/2410.03751v4", "doi": "https://doi.org/10.48550/arXiv.2410.03751"}, "round-A_JuniorNano_output": {"reasoning": "此項目聚焦於 Speech Language Models 的概述與評估，未描述離散音訊 token 的產生、編碼/碼本設計或對下游任務的離散 token 影響，因此不符合「Discrete Audio Tokens」主題的包含條件，且符合排除條件。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此項目聚焦於 Speech Language Models 的概述與評估，未描述離散音訊 token 的產生、編碼/碼本設計或對下游任務的離散 token 影響，因此不符合「Discrete Audio Tokens」主題的包含條件，且符合排除條件。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses Speech Language Models (SpeechLMs) focusing on end-to-end speech generation models rather than discrete audio tokenization methods. It mainly surveys architectures, training methods, and capabilities of SpeechLMs without detailing the production or utilization of discrete token sequences derived from discretization of audio waveforms, neural audio codecs, or quantization methods. There is no indication that the paper addresses the design, evaluation, or systematic analysis of discrete audio tokens as defined in the inclusion criteria, nor does it focus on token-based modeling, generation, or alignment. Therefore, it does not meet the core criteria regarding discrete audio tokens and token-based methods, leading to exclusion based on the topic and methodology requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses Speech Language Models (SpeechLMs) focusing on end-to-end speech generation models rather than discrete audio tokenization methods. It mainly surveys architectures, training methods, and capabilities of SpeechLMs without detailing the production or utilization of discrete token sequences derived from discretization of audio waveforms, neural audio codecs, or quantization methods. There is no indication that the paper addresses the design, evaluation, or systematic analysis of discrete audio tokens as defined in the inclusion criteria, nor does it focus on token-based modeling, generation, or alignment. Therefore, it does not meet the core criteria regarding discrete audio tokens and token-based methods, leading to exclusion based on the topic and methodology requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ji2024wavchat", "query_title": "{Wavchat: A survey of spoken dialogue models}", "normalized_title": "wavchat a survey of spoken dialogue models", "title": "WavChat: A Survey of Spoken Dialogue Models", "abstract": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.", "source": "arxiv", "source_id": "2411.13577v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WavChat: A Survey of Spoken Dialogue Models", "abstract": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.", "metadata": {"arxiv_id": "2411.13577v2", "title": "WavChat: A Survey of Spoken Dialogue Models", "summary": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.", "authors": ["Shengpeng Ji", "Yifu Chen", "Minghui Fang", "Jialong Zuo", "Jingyu Lu", "Hanting Wang", "Ziyue Jiang", "Long Zhou", "Shujie Liu", "Xize Cheng", "Xiaoda Yang", "Zehan Wang", "Qian Yang", "Jian Li", "Yidi Jiang", "Jingzhen He", "Yunfei Chu", "Jin Xu", "Zhou Zhao"], "published": "2024-11-15T04:16:45Z", "updated": "2024-11-26T09:20:48Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.MM", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2411.13577v2", "landing_url": "https://arxiv.org/abs/2411.13577v2", "doi": "https://doi.org/10.48550/arXiv.2411.13577"}, "round-A_JuniorNano_output": {"reasoning": "The item is a survey of spoken dialogue models and does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a survey of spoken dialogue models and does not discuss discrete audio tokens, tokenization, or token-based representations, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article provides a comprehensive survey of spoken dialogue models but does not explicitly discuss or include the generation or modeling of discrete audio tokens as defined by the inclusion criteria. There is no mention of discrete tokenization processes, neural audio codecs, vector quantization, or any discrete token-based framework, which are core to the inclusion. The survey focuses more broadly on spoken dialogue model architectures and datasets rather than on discrete audio token design, evaluation, or their impact on downstream tasks. Therefore, it does not meet the specific inclusion criteria of focusing on discrete audio tokens as representations for audio modeling, generation, or cross-modal reasoning.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article provides a comprehensive survey of spoken dialogue models but does not explicitly discuss or include the generation or modeling of discrete audio tokens as defined by the inclusion criteria. There is no mention of discrete tokenization processes, neural audio codecs, vector quantization, or any discrete token-based framework, which are core to the inclusion. The survey focuses more broadly on spoken dialogue model architectures and datasets rather than on discrete audio token design, evaluation, or their impact on downstream tasks. Therefore, it does not meet the specific inclusion criteria of focusing on discrete audio tokens as representations for audio modeling, generation, or cross-modal reasoning.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "latif2023sparks", "query_title": "{Sparks of large audio models: A survey and outlook}", "normalized_title": "sparks of large audio models a survey and outlook", "title": "Sparks of Large Audio Models: A Survey and Outlook", "abstract": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.", "source": "arxiv", "source_id": "2308.12792v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Sparks of Large Audio Models: A Survey and Outlook", "abstract": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.", "metadata": {"arxiv_id": "2308.12792v3", "title": "Sparks of Large Audio Models: A Survey and Outlook", "summary": "This survey paper provides a comprehensive overview of the recent advancements and challenges in applying large language models to the field of audio signal processing. Audio processing, with its diverse signal representations and a wide range of sources--from human voices to musical instruments and environmental sounds--poses challenges distinct from those found in traditional Natural Language Processing scenarios. Nevertheless, \\textit{Large Audio Models}, epitomized by transformer-based architectures, have shown marked efficacy in this sphere. By leveraging massive amount of data, these models have demonstrated prowess in a variety of audio tasks, spanning from Automatic Speech Recognition and Text-To-Speech to Music Generation, among others. Notably, recently these Foundational Audio Models, like SeamlessM4T, have started showing abilities to act as universal translators, supporting multiple speech tasks for up to 100 languages without any reliance on separate task-specific systems. This paper presents an in-depth analysis of state-of-the-art methodologies regarding \\textit{Foundational Large Audio Models}, their performance benchmarks, and their applicability to real-world scenarios. We also highlight current limitations and provide insights into potential future research directions in the realm of \\textit{Large Audio Models} with the intent to spark further discussion, thereby fostering innovation in the next generation of audio-processing systems. Furthermore, to cope with the rapid development in this area, we will consistently update the relevant repository with relevant recent articles and their open-source implementations at https://github.com/EmulationAI/awesome-large-audio-models.", "authors": ["Siddique Latif", "Moazzam Shoukat", "Fahad Shamshad", "Muhammad Usama", "Yi Ren", "Heriberto Cuayáhuitl", "Wenwu Wang", "Xulong Zhang", "Roberto Togneri", "Erik Cambria", "Björn W. Schuller"], "published": "2023-08-24T13:47:16Z", "updated": "2023-09-22T01:44:46Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2308.12792v3", "landing_url": "https://arxiv.org/abs/2308.12792v3", "doi": "https://doi.org/10.48550/arXiv.2308.12792"}, "round-A_JuniorNano_output": {"reasoning": "Abstract discusses large audio models and cross-task capabilities but does not address discrete audio tokens, tokenizer/codec design, discretization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Abstract discusses large audio models and cross-task capabilities but does not address discrete audio tokens, tokenizer/codec design, discretization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper is a comprehensive survey of large audio models applied to various audio tasks, focusing on architectures like transformers and foundational models such as SeamlessM4T. However, the abstract does not describe or emphasize the use of discrete audio tokens derived from neural audio codecs or self-supervised discrete units, nor does it detail tokenization, quantization, codebook settings, or related systemic evaluations tied to discrete audio tokens. It mainly discusses large models' applicability and challenges in audio processing broadly, rather than focusing on the 'Discrete Audio Tokens' theme as defined by detailed token discretization and token-based modeling. Therefore, it does not satisfy the inclusion criteria around discrete token design, representation, and comprehensive token-centric analysis essential for inclusion.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper is a comprehensive survey of large audio models applied to various audio tasks, focusing on architectures like transformers and foundational models such as SeamlessM4T. However, the abstract does not describe or emphasize the use of discrete audio tokens derived from neural audio codecs or self-supervised discrete units, nor does it detail tokenization, quantization, codebook settings, or related systemic evaluations tied to discrete audio tokens. It mainly discusses large models' applicability and challenges in audio processing broadly, rather than focusing on the 'Discrete Audio Tokens' theme as defined by detailed token discretization and token-based modeling. Therefore, it does not satisfy the inclusion criteria around discrete token design, representation, and comprehensive token-centric analysis essential for inclusion.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "dunbar2021zero", "query_title": "{The Zero Resource Speech Challenge 2021: Spoken Language Modelling}", "normalized_title": "the zero resource speech challenge 2021 spoken language modelling", "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.", "source": "arxiv", "source_id": "2104.14700v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "abstract": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.", "metadata": {"arxiv_id": "2104.14700v2", "title": "The Zero Resource Speech Challenge 2021: Spoken language modelling", "summary": "We present the Zero Resource Speech Challenge 2021, which asks participants to learn a language model directly from audio, without any text or labels. The challenge is based on the Libri-light dataset, which provides up to 60k hours of audio from English audio books without any associated text. We provide a pipeline baseline system consisting on an encoder based on contrastive predictive coding (CPC), a quantizer ($k$-means) and a standard language model (BERT or LSTM). The metrics evaluate the learned representations at the acoustic (ABX discrimination), lexical (spot-the-word), syntactic (acceptability judgment) and semantic levels (similarity judgment). We present an overview of the eight submitted systems from four groups and discuss the main results.", "authors": ["Ewan Dunbar", "Mathieu Bernard", "Nicolas Hamilakis", "Tu Anh Nguyen", "Maureen de Seyssel", "Patricia Rozé", "Morgane Rivière", "Eugene Kharitonov", "Emmanuel Dupoux"], "published": "2021-04-29T23:53:37Z", "updated": "2021-08-09T19:09:58Z", "categories": ["cs.CL", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2104.14700v2", "landing_url": "https://arxiv.org/abs/2104.14700v2", "doi": "https://doi.org/10.48550/arXiv.2104.14700"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses using a neural encoder with a discretization (quantizer/k-means) to produce discrete audio tokens that are then used for downstream language modelling, matching the discrete audio tokens focus and token-based downstream tasks in the criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item discusses using a neural encoder with a discretization (quantizer/k-means) to produce discrete audio tokens that are then used for downstream language modelling, matching the discrete audio tokens focus and token-based downstream tasks in the criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study discusses the Zero Resource Speech Challenge 2021, which involves learning language models directly from audio without text or labels, and uses a pipeline including an encoder based on contrastive predictive coding (CPC) and a quantizer (k-means) to obtain discrete representations; thus, it clearly involves discretization of continuous audio into discrete tokens for subsequent modeling, aligning with the inclusion criteria focused on discrete audio tokens produced via neural encoders or self-supervised learning models and used for modeling and analysis. The abstract provides enough detail on methods to verify the approach, and the use of discrete tokens is central to their modeling, meeting the criteria for inclusion.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study discusses the Zero Resource Speech Challenge 2021, which involves learning language models directly from audio without text or labels, and uses a pipeline including an encoder based on contrastive predictive coding (CPC) and a quantizer (k-means) to obtain discrete representations; thus, it clearly involves discretization of continuous audio into discrete tokens for subsequent modeling, aligning with the inclusion criteria focused on discrete audio tokens produced via neural encoders or self-supervised learning models and used for modeling and analysis. The abstract provides enough detail on methods to verify the approach, and the use of discrete tokens is central to their modeling, meeting the criteria for inclusion.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "lin2024alignslm", "query_title": "{Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback}", "normalized_title": "align slm textless spoken language models with reinforcement learning from ai feedback", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.", "source": "arxiv", "source_id": "2411.01834v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.", "metadata": {"arxiv_id": "2411.01834v2", "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback", "summary": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.", "authors": ["Guan-Ting Lin", "Prashanth Gurunath Shivakumar", "Aditya Gourav", "Yile Gu", "Ankur Gandhe", "Hung-yi Lee", "Ivan Bulyko"], "published": "2024-11-04T06:07:53Z", "updated": "2025-05-27T16:17:52Z", "categories": ["cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.01834v2", "landing_url": "https://arxiv.org/abs/2411.01834v2", "doi": "https://doi.org/10.48550/arXiv.2411.01834"}, "round-A_JuniorNano_output": {"reasoning": "The paper centers on textless spoken language models and RL-based alignment, not on discrete audio tokens or tokenizers, so it does not meet the discrete audio token criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper centers on textless spoken language models and RL-based alignment, not on discrete audio tokens or tokenizers, so it does not meet the discrete audio token criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on improving textless spoken language models (SLMs) using reinforcement learning and AI feedback to enhance semantic coherence. However, there is no explicit mention of discretizing continuous audio waveforms into discrete audio tokens, nor are there details on tokenizers, neural audio codec architectures, vector quantization, or codebooks that are central to the discrete audio tokens theme. Therefore, it does not meet the inclusion criteria centered on discrete token representation as a core method or analysis object, making it unsuitable for inclusion.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on improving textless spoken language models (SLMs) using reinforcement learning and AI feedback to enhance semantic coherence. However, there is no explicit mention of discretizing continuous audio waveforms into discrete audio tokens, nor are there details on tokenizers, neural audio codec architectures, vector quantization, or codebooks that are central to the discrete audio tokens theme. Therefore, it does not meet the inclusion criteria centered on discrete token representation as a core method or analysis object, making it unsuitable for inclusion.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "radford2023robust", "query_title": "{Robust speech recognition via large-scale weak supervision}", "normalized_title": "robust speech recognition via large scale weak supervision", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "source": "arxiv", "source_id": "2212.04356v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Robust Speech Recognition via Large-Scale Weak Supervision", "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "metadata": {"arxiv_id": "2212.04356v1", "title": "Robust Speech Recognition via Large-Scale Weak Supervision", "summary": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.", "authors": ["Alec Radford", "Jong Wook Kim", "Tao Xu", "Greg Brockman", "Christine McLeavey", "Ilya Sutskever"], "published": "2022-12-06T18:46:04Z", "updated": "2022-12-06T18:46:04Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2212.04356v1", "landing_url": "https://arxiv.org/abs/2212.04356v1", "doi": "https://doi.org/10.48550/arXiv.2212.04356"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on robust ASR with large-scale weak supervision and does not address discrete audio tokens, tokenizers, or neural codec-based token generation as required by the Discrete Audio Tokens criteria; thus it should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on robust ASR with large-scale weak supervision and does not address discrete audio tokens, tokenizers, or neural codec-based token generation as required by the Discrete Audio Tokens criteria; thus it should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on large-scale speech recognition using models trained on extensive audio-transcript pairs, emphasizing robustness and zero-shot generalization. However, the abstract does not mention any discretization process that converts continuous audio waveforms into discrete tokens, nor does it describe neural audio codecs, vector quantization, or any discrete token-based approach central to the study, which is a key inclusion criterion. The work appears to be about continuous speech recognition models rather than discrete audio token representations or their effects on downstream tasks. Thus, it does not meet the core inclusion criteria and aligns more with continuous audio modeling, suggesting exclusion from the topic focused on discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on large-scale speech recognition using models trained on extensive audio-transcript pairs, emphasizing robustness and zero-shot generalization. However, the abstract does not mention any discretization process that converts continuous audio waveforms into discrete tokens, nor does it describe neural audio codecs, vector quantization, or any discrete token-based approach central to the study, which is a key inclusion criterion. The work appears to be about continuous speech recognition models rather than discrete audio token representations or their effects on downstream tasks. Thus, it does not meet the core inclusion criteria and aligns more with continuous audio modeling, suggesting exclusion from the topic focused on discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mostafazadeh2016corpus", "query_title": "\"A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories\"", "normalized_title": "a corpus and cloze evaluation for deeper understanding of commonsense stories", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "source": "semantic_scholar", "source_id": "85b68477a6e031d88b963833e15a4b4fc6855264", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "abstract": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "metadata": {"arxiv_id": "semantic_scholar:85b68477a6e031d88b963833e15a4b4fc6855264", "title": "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories", "summary": "Representation and learning of commonsense knowledge is one of the foundational problems in the quest to enable deep language understanding. This issue is particularly challenging for understanding casual and correlational relationships between events. While this topic has received a lot of interest in the NLP community, research has been hindered by the lack of a proper evaluation framework. This paper attempts to address this problem with a new framework for evaluating story understanding and script learning: the 'Story Cloze Test'. This test requires a system to choose the correct ending to a four-sentence story. We created a new corpus of ~50k five-sentence commonsense stories, ROCStories, to enable this evaluation. This corpus is unique in two ways: (1) it captures a rich set of causal and temporal commonsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding.", "source": "semantic_scholar", "source_id": "85b68477a6e031d88b963833e15a4b4fc6855264", "publication_date": "2016-04-06"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on story understanding and a story-end selection task (ROCStories) in NLP, with no discussion of discrete audio tokens, tokenization, or audio codecs, so it does not meet the inclusion criteria tied to Discrete Audio Tokens and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on story understanding and a story-end selection task (ROCStories) in NLP, with no discussion of discrete audio tokens, tokenization, or audio codecs, so it does not meet the inclusion criteria tied to Discrete Audio Tokens and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on commonsense story understanding and introduces a story cloze test and corpus for evaluating language understanding; it does not discuss or involve discrete audio tokens, neural audio encoders, or any discretization of continuous audio signals as per the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on commonsense story understanding and introduces a story cloze test and corpus for evaluating language understanding; it does not discuss or involve discrete audio tokens, neural audio encoders, or any discretization of continuous audio signals as per the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yang2024uniaudio", "query_title": "{Uni{A}udio 1.5: Large Language Model-Driven Audio Codec is A Few-Shot Audio Task Learner}", "normalized_title": "uniaudio 1 5 large language model driven audio codec is a few shot audio task learner", "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner", "abstract": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.", "source": "semantic_scholar", "source_id": "33672a1ef7bb4722902a53c77e30ec92b338a94c", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner", "abstract": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.", "metadata": {"arxiv_id": "semantic_scholar:33672a1ef7bb4722902a53c77e30ec92b338a94c", "title": "UniAudio 1.5: Large Language Model-driven Audio Codec is A Few-shot Audio Task Learner", "summary": "The Large Language models (LLMs) have demonstrated supreme capabilities in text understanding and generation, but cannot be directly applied to cross-modal tasks without fine-tuning. This paper proposes a cross-modal in-context learning approach, empowering the frozen LLMs to achieve multiple audio tasks in a few-shot style without any parameter update. Specifically, we propose a novel and LLMs-driven audio codec model, LLM-Codec, to transfer the audio modality into the textual space, \\textit{i.e.} representing audio tokens with words or sub-words in the vocabulary of LLMs, while keeping high audio reconstruction quality. The key idea is to reduce the modality heterogeneity between text and audio by compressing the audio modality into a well-trained LLMs token space. Thus, the audio representation can be viewed as a new \\textit{foreign language}, and LLMs can learn the new \\textit{foreign language} with several demonstrations. In experiments, we investigate the performance of the proposed approach across multiple audio understanding and generation tasks, \\textit{e.g.} speech emotion classification, audio classification, text-to-speech generation, speech enhancement, etc. The experimental results demonstrate that the LLMs equipped with the proposed LLM-Codec, named as UniAudio 1.5, prompted by only a few examples, can achieve the expected functions in simple scenarios. It validates the feasibility and effectiveness of the proposed cross-modal in-context learning approach. To facilitate research on few-shot audio task learning and multi-modal LLMs, we have open-sourced the LLM-Codec model.", "source": "semantic_scholar", "source_id": "33672a1ef7bb4722902a53c77e30ec92b338a94c", "publication_date": "2024-06-14"}, "round-A_JuniorNano_output": {"reasoning": "The item proposes discrete audio tokens by encoding audio into the LLM’s token vocabulary (audio↔text as a ‘foreign language’), enabling downstream tasks via few-shot prompting, and discusses multiple generation/understanding tasks; it provides an English title/abstract and appears to focus on discrete token design and evaluation, aligning with the inclusion criteria and excluding none of the stated exclusions.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item proposes discrete audio tokens by encoding audio into the LLM’s token vocabulary (audio↔text as a ‘foreign language’), enabling downstream tasks via few-shot prompting, and discusses multiple generation/understanding tasks; it provides an English title/abstract and appears to focus on discrete token design and evaluation, aligning with the inclusion criteria and excluding none of the stated exclusions.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a novel neural audio codec model (LLM-Codec) that converts audio signals into discrete tokens in the vocabulary of large language models, enabling few-shot audio task learning via discrete audio token sequences; it describes the codec architecture, token representation, and downstream applications including TTS, speech enhancement, and cross-modal tasks, fulfilling the inclusion criteria on discrete token generation, use for multiple audio tasks, and clear English abstract and method description, with no exclusion criteria met.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a novel neural audio codec model (LLM-Codec) that converts audio signals into discrete tokens in the vocabulary of large language models, enabling few-shot audio task learning via discrete audio token sequences; it describes the codec architecture, token representation, and downstream applications including TTS, speech enhancement, and cross-modal tasks, fulfilling the inclusion criteria on discrete token generation, use for multiple audio tasks, and clear English abstract and method description, with no exclusion criteria met.", "round-A_JuniorMini_evaluation": 5, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "park2024long", "query_title": "{Long-Form Speech Generation with Spoken Language Models}", "normalized_title": "long form speech generation with spoken language models", "title": "Long-Form Speech Generation with Spoken Language Models", "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.", "source": "arxiv", "source_id": "2412.18603v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Long-Form Speech Generation with Spoken Language Models", "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.", "metadata": {"arxiv_id": "2412.18603v2", "title": "Long-Form Speech Generation with Spoken Language Models", "summary": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.", "authors": ["Se Jin Park", "Julian Salazar", "Aren Jansen", "Keisuke Kinoshita", "Yong Man Ro", "RJ Skerry-Ryan"], "published": "2024-12-24T18:56:46Z", "updated": "2025-07-10T17:52:43Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2412.18603v2", "landing_url": "https://arxiv.org/abs/2412.18603v2", "doi": "https://doi.org/10.48550/arXiv.2412.18603"}, "round-A_JuniorNano_output": {"reasoning": "This item discusses long-form speech generation with spoken language models but does not mention discrete audio tokens, tokenizers, or any token-based representation; it therefore does not meet the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses long-form speech generation with spoken language models but does not mention discrete audio tokens, tokenizers, or any token-based representation; it therefore does not meet the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents SpeechSSM, a spoken language model generating long-form speech without text intermediates, focusing on overcoming coherence and memory issues in speech generation. However, the abstract does not explicitly describe the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor the tokenization details such as quantization methods, vocabulary size, or codebook settings, which are necessary to confirm it fits the 'Discrete Audio Tokens' topic. Thus, there's insufficient evidence that the study centers on discrete token representations or their systematic design and evaluation as per inclusion criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents SpeechSSM, a spoken language model generating long-form speech without text intermediates, focusing on overcoming coherence and memory issues in speech generation. However, the abstract does not explicitly describe the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor the tokenization details such as quantization methods, vocabulary size, or codebook settings, which are necessary to confirm it fits the 'Discrete Audio Tokens' topic. Thus, there's insufficient evidence that the study centers on discrete token representations or their systematic design and evaluation as per inclusion criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "dhariwal2020jukebox", "query_title": "{Jukebox: A generative model for music}", "normalized_title": "jukebox a generative model for music", "title": "Jukebox: A Generative Model for Music", "abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox", "source": "arxiv", "source_id": "2005.00341v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Jukebox: A Generative Model for Music", "abstract": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox", "metadata": {"arxiv_id": "2005.00341v1", "title": "Jukebox: A Generative Model for Music", "summary": "We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox", "authors": ["Prafulla Dhariwal", "Heewoo Jun", "Christine Payne", "Jong Wook Kim", "Alec Radford", "Ilya Sutskever"], "published": "2020-04-30T09:02:45Z", "updated": "2020-04-30T09:02:45Z", "categories": ["eess.AS", "cs.LG", "cs.SD", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2005.00341v1", "landing_url": "https://arxiv.org/abs/2005.00341v1", "doi": "https://doi.org/10.48550/arXiv.2005.00341"}, "round-A_JuniorNano_output": {"reasoning": "The Jukebox paper presents a generative model for music in the raw audio domain that uses a neural audio codec (multi-scale VQ-VAE) to map waveforms to discrete codes (tokens) and then models those codes with autoregressive Transformers; this directly embodies discrete audio tokens and token-based modeling, with English abstract and implementation details, and it targets music generation (not just compression or discrimination). It therefore satisfies inclusion criteria and does not fit any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The Jukebox paper presents a generative model for music in the raw audio domain that uses a neural audio codec (multi-scale VQ-VAE) to map waveforms to discrete codes (tokens) and then models those codes with autoregressive Transformers; this directly embodies discrete audio tokens and token-based modeling, with English abstract and implementation details, and it targets music generation (not just compression or discrimination). It therefore satisfies inclusion criteria and does not fit any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper introduces Jukebox, which uses a multi-scale VQ-VAE (a type of neural audio codec with vector quantization) to discretize raw audio into discrete codes that are modeled by autoregressive Transformers. This approach directly fits the inclusion criteria of generating discrete audio tokens via neural audio encoding with vector quantization and using these tokens for downstream generative tasks (music with singing). The abstract also suggests the release of model weights and code, supporting reproducibility and evaluation. Additionally, the paper is in English with clear details about the tokenizer/codec architecture and usage, fulfilling the English evaluability criteria. Hence, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper introduces Jukebox, which uses a multi-scale VQ-VAE (a type of neural audio codec with vector quantization) to discretize raw audio into discrete codes that are modeled by autoregressive Transformers. This approach directly fits the inclusion criteria of generating discrete audio tokens via neural audio encoding with vector quantization and using these tokens for downstream generative tasks (music with singing). The abstract also suggests the release of model weights and code, supporting reproducibility and evaluation. Additionally, the paper is in English with clear details about the tokenizer/codec architecture and usage, fulfilling the English evaluability criteria. Hence, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "ziv2024masked", "query_title": "{Masked Audio Generation using a Single Non-Autoregressive Transformer}", "normalized_title": "masked audio generation using a single non autoregressive transformer", "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer", "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.", "source": "arxiv", "source_id": "2401.04577v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Masked Audio Generation using a Single Non-Autoregressive Transformer", "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.", "metadata": {"arxiv_id": "2401.04577v2", "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer", "summary": "We introduce MAGNeT, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, MAGNeT is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from MAGNeT, which will be then used for later decoding steps. Lastly, we explore a hybrid version of MAGNeT, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner while the rest of the sequence is being decoded in parallel. We demonstrate the efficiency of MAGNeT for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (x7 faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising MAGNeT, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality. Samples are available on our demo page https://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.", "authors": ["Alon Ziv", "Itai Gat", "Gael Le Lan", "Tal Remez", "Felix Kreuk", "Alexandre Défossez", "Jade Copet", "Gabriel Synnaeve", "Yossi Adi"], "published": "2024-01-09T14:29:39Z", "updated": "2024-03-05T09:12:35Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2401.04577v2", "landing_url": "https://arxiv.org/abs/2401.04577v2", "doi": "https://doi.org/10.48550/arXiv.2401.04577"}, "round-A_JuniorNano_output": {"reasoning": "Based on the title/abstract, the study uses discrete audio tokens and token-based modeling (audio tokens, masking of token spans) within a single non-autoregressive transformer for text-to-audio/music generation, aligning with the discrete-token criteria and not falling under pure continuous-signal work.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "Based on the title/abstract, the study uses discrete audio tokens and token-based modeling (audio tokens, masking of token spans) within a single non-autoregressive transformer for text-to-audio/music generation, aligning with the discrete-token criteria and not falling under pure continuous-signal work.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study introduces MAGNeT, a masked generative audio sequence modeling method working directly on audio tokens with a transformer architecture. However, the abstract does not explicitly describe the process of discretizing continuous audio waveforms into discrete tokens via neural audio codecs or self-supervised models, nor does it clarify details on tokenizer architecture, vector quantization, codebook design, or token properties. Without such explicit evidence of discrete audio token generation and characterization, it is unclear whether the study fully meets the inclusion criteria centered on discrete audio tokens and their systematic evaluation.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The study introduces MAGNeT, a masked generative audio sequence modeling method working directly on audio tokens with a transformer architecture. However, the abstract does not explicitly describe the process of discretizing continuous audio waveforms into discrete tokens via neural audio codecs or self-supervised models, nor does it clarify details on tokenizer architecture, vector quantization, codebook design, or token properties. Without such explicit evidence of discrete audio token generation and characterization, it is unclear whether the study fully meets the inclusion criteria centered on discrete audio tokens and their systematic evaluation.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "The abstract clearly works with \"audio tokens\" and a token-based masked generative transformer for text-to-audio/music, and one junior reviewer opted to include (5) recognizing token-based modeling; however, the other junior reviewer flagged that the abstract does not explicitly describe how continuous audio is discretized (e.g., neural codec/tokenizer architecture, vector quantization, codebook/vocabulary settings, bitrate or downsampling) as required by the inclusion criteria. Because the inclusion rules require explicit, assessable description of discrete-token generation/codec details and the abstract lacks those specifics, I cannot confidently confirm it meets all inclusion requirements from the provided text. Brief explanation for score: mentions audio tokens but lacks tokenizer/codec/tokenization details required to verify it fits the \"Discrete Audio Tokens\" scope.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "The abstract clearly works with \"audio tokens\" and a token-based masked generative transformer for text-to-audio/music, and one junior reviewer opted to include (5) recognizing token-based modeling; however, the other junior reviewer flagged that the abstract does not explicitly describe how continuous audio is discretized (e.g., neural codec/tokenizer architecture, vector quantization, codebook/vocabulary settings, bitrate or downsampling) as required by the inclusion criteria. Because the inclusion rules require explicit, assessable description of discrete-token generation/codec details and the abstract lacks those specifics, I cannot confidently confirm it meets all inclusion requirements from the provided text. Brief explanation for score: mentions audio tokens but lacks tokenizer/codec/tokenization details required to verify it fits the \"Discrete Audio Tokens\" scope.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "garcia2023vampnet", "query_title": "{VampNet: Music Generation via Masked Acoustic Token Modeling}", "normalized_title": "vampnet music generation via masked acoustic token modeling", "title": "VampNet: Music Generation via Masked Acoustic Token Modeling", "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.", "source": "arxiv", "source_id": "2307.04686v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "VampNet: Music Generation via Masked Acoustic Token Modeling", "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.", "metadata": {"arxiv_id": "2307.04686v2", "title": "VampNet: Music Generation via Masked Acoustic Token Modeling", "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.", "authors": ["Hugo Flores Garcia", "Prem Seetharaman", "Rithesh Kumar", "Bryan Pardo"], "published": "2023-07-10T16:42:03Z", "updated": "2023-07-12T17:06:41Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2307.04686v2", "landing_url": "https://arxiv.org/abs/2307.04686v2", "doi": "https://doi.org/10.48550/arXiv.2307.04686"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on masked acoustic token modeling for music generation, implying discrete audio tokens and token-based downstream tasks, which aligns with the inclusion criteria and does not clearly trigger any exclusion criteria.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract centers on masked acoustic token modeling for music generation, implying discrete audio tokens and token-based downstream tasks, which aligns with the inclusion criteria and does not clearly trigger any exclusion criteria.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper introduces VampNet, focusing on masked acoustic token modeling for music generation. However, the abstract does not explicitly describe the discrete token generation process, such as the tokenizer architecture, quantization methods, vocabulary or codebook settings, which are critical for identifying the study as within discrete audio token frameworks. Without such details on the nature of tokens—whether they stem from neural audio codecs or self-supervised acoustic units—it is difficult to confirm that the tokens used satisfy the inclusion criteria of discrete token design and reproducibility. Hence, it lacks sufficient information for clear inclusion under the stated criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper introduces VampNet, focusing on masked acoustic token modeling for music generation. However, the abstract does not explicitly describe the discrete token generation process, such as the tokenizer architecture, quantization methods, vocabulary or codebook settings, which are critical for identifying the study as within discrete audio token frameworks. Without such details on the nature of tokens—whether they stem from neural audio codecs or self-supervised acoustic units—it is difficult to confirm that the tokens used satisfy the inclusion criteria of discrete token design and reproducibility. Hence, it lacks sufficient information for clear inclusion under the stated criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": {"reasoning": "Both junior reviewers raised relevant points: JuniorNano judged the work likely relevant because it centers on masked acoustic token modeling for music generation, while JuniorMini cautioned that the abstract lacks explicit details about how discrete tokens are produced (tokenizer/codec, quantization, codebook, bitrate, etc.). According to the inclusion criteria, the paper must clearly describe the discrete token generation process in English to confirm it uses discrete audio tokens; the provided title and abstract do not supply those necessary tokenizer/codec details, so we cannot confirm it meets the required criteria. Therefore I recommend excluding this item. One-sentence explanation: The abstract refers to \"acoustic token modeling\" but does not describe the tokenizer/codec or quantization/codebook details required to verify it uses discrete audio tokens, so it fails the inclusion requirement.", "evaluation": 2}, "round-B_SeniorLead_reasoning": "Both junior reviewers raised relevant points: JuniorNano judged the work likely relevant because it centers on masked acoustic token modeling for music generation, while JuniorMini cautioned that the abstract lacks explicit details about how discrete tokens are produced (tokenizer/codec, quantization, codebook, bitrate, etc.). According to the inclusion criteria, the paper must clearly describe the discrete token generation process in English to confirm it uses discrete audio tokens; the provided title and abstract do not supply those necessary tokenizer/codec details, so we cannot confirm it meets the required criteria. Therefore I recommend excluding this item. One-sentence explanation: The abstract refers to \"acoustic token modeling\" but does not describe the tokenizer/codec or quantization/codebook details required to verify it uses discrete audio tokens, so it fails the inclusion requirement.", "round-B_SeniorLead_evaluation": 2, "final_verdict": "exclude (senior:2)", "review_skipped": false, "discard_reason": "exclude (senior:2)"}]}
{"key": "gu2021efficiently", "query_title": "{Efficiently Modeling Long Sequences with Structured State Spaces}", "normalized_title": "efficiently modeling long sequences with structured state spaces", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.", "source": "arxiv", "source_id": "2111.00396v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Efficiently Modeling Long Sequences with Structured State Spaces", "abstract": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.", "metadata": {"arxiv_id": "2111.00396v3", "title": "Efficiently Modeling Long Sequences with Structured State Spaces", "summary": "A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.", "authors": ["Albert Gu", "Karan Goel", "Christopher Ré"], "published": "2021-10-31T03:32:18Z", "updated": "2022-08-05T17:54:38Z", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2111.00396v3", "landing_url": "https://arxiv.org/abs/2111.00396v3", "doi": "https://doi.org/10.48550/arXiv.2111.00396"}, "round-A_JuniorNano_output": {"reasoning": "This item targets long-sequence modeling with structured state spaces (SSM/S4) and does not discuss discrete audio token generation, tokenizers, codebooks, or any discrete audio token design, thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item targets long-sequence modeling with structured state spaces (SSM/S4) and does not discuss discrete audio token generation, tokenizers, codebooks, or any discrete audio token design, thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper addresses sequence modeling techniques for long sequences using structured state space models but does not focus on discrete audio tokens or the discretization of continuous audio waveforms into token sequences. It also lacks any mention of neural audio codecs, quantization, or any audio token generation mechanisms relevant to the inclusion criteria. Therefore, it does not satisfy the inclusion criteria surrounding discrete audio tokens or audio-specific processing, and it falls under the exclusion criteria of focusing on continuous sequence modeling without discrete token usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper addresses sequence modeling techniques for long sequences using structured state space models but does not focus on discrete audio tokens or the discretization of continuous audio waveforms into token sequences. It also lacks any mention of neural audio codecs, quantization, or any audio token generation mechanisms relevant to the inclusion criteria. Therefore, it does not satisfy the inclusion criteria surrounding discrete audio tokens or audio-specific processing, and it falls under the exclusion criteria of focusing on continuous sequence modeling without discrete token usage.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "elmakies2025unsupervised", "query_title": "{Unsupervised Speech Segmentation: A General Approach Using Speech Language Models}", "normalized_title": "unsupervised speech segmentation a general approach using speech language models", "title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models", "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation, which builds on previously researched approaches, e.g., Speaker Diarization, while being applicable to an inclusive set of acoustic-semantic distinctions, paving a path towards a general Unsupervised Speech Segmentation approach. Unlike traditional speech and audio segmentation, which mainly focuses on spectral changes in the input signal, e.g., phone segmentation, our approach tries to segment the spoken utterance into chunks with differing acoustic-semantic styles, focusing on acoustic-semantic information that does not translate well into text, e.g., emotion or speaker. While most Speech Segmentation tasks only handle one style change, e.g., emotion diarization, our approach tries to handle multiple acoustic-semantic style changes. Leveraging recent advances in Speech Language Models (SLMs), we propose a simple unsupervised method to segment a given speech utterance. We empirically demonstrate the effectiveness of the proposed approach by considering several setups. Results suggest that the proposed method is superior to the evaluated baselines on boundary detection, segment purity, and over-segmentation. Code is available at https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.", "source": "arxiv", "source_id": "2501.03711v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models", "abstract": "In this paper, we introduce an unsupervised approach for Speech Segmentation, which builds on previously researched approaches, e.g., Speaker Diarization, while being applicable to an inclusive set of acoustic-semantic distinctions, paving a path towards a general Unsupervised Speech Segmentation approach. Unlike traditional speech and audio segmentation, which mainly focuses on spectral changes in the input signal, e.g., phone segmentation, our approach tries to segment the spoken utterance into chunks with differing acoustic-semantic styles, focusing on acoustic-semantic information that does not translate well into text, e.g., emotion or speaker. While most Speech Segmentation tasks only handle one style change, e.g., emotion diarization, our approach tries to handle multiple acoustic-semantic style changes. Leveraging recent advances in Speech Language Models (SLMs), we propose a simple unsupervised method to segment a given speech utterance. We empirically demonstrate the effectiveness of the proposed approach by considering several setups. Results suggest that the proposed method is superior to the evaluated baselines on boundary detection, segment purity, and over-segmentation. Code is available at https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.", "metadata": {"arxiv_id": "2501.03711v1", "title": "Unsupervised Speech Segmentation: A General Approach Using Speech Language Models", "summary": "In this paper, we introduce an unsupervised approach for Speech Segmentation, which builds on previously researched approaches, e.g., Speaker Diarization, while being applicable to an inclusive set of acoustic-semantic distinctions, paving a path towards a general Unsupervised Speech Segmentation approach. Unlike traditional speech and audio segmentation, which mainly focuses on spectral changes in the input signal, e.g., phone segmentation, our approach tries to segment the spoken utterance into chunks with differing acoustic-semantic styles, focusing on acoustic-semantic information that does not translate well into text, e.g., emotion or speaker. While most Speech Segmentation tasks only handle one style change, e.g., emotion diarization, our approach tries to handle multiple acoustic-semantic style changes. Leveraging recent advances in Speech Language Models (SLMs), we propose a simple unsupervised method to segment a given speech utterance. We empirically demonstrate the effectiveness of the proposed approach by considering several setups. Results suggest that the proposed method is superior to the evaluated baselines on boundary detection, segment purity, and over-segmentation. Code is available at https://github.com/avishaiElmakies/unsupervised_speech_segmentation_using_slm.", "authors": ["Avishai Elmakies", "Omri Abend", "Yossi Adi"], "published": "2025-01-07T11:32:13Z", "updated": "2025-01-07T11:32:13Z", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2501.03711v1", "landing_url": "https://arxiv.org/abs/2501.03711v1", "doi": "https://doi.org/10.48550/arXiv.2501.03711"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on unsupervised speech segmentation using speech language models and acoustic-semantic style changes, but provides no discussion of discrete audio tokens, tokenization, or vocabularies, so it does not meet the 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on unsupervised speech segmentation using speech language models and acoustic-semantic style changes, but provides no discussion of discrete audio tokens, tokenization, or vocabularies, so it does not meet the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents an unsupervised speech segmentation method using Speech Language Models focusing on acoustic-semantic style changes, but it does not describe any process of discretizing continuous audio waveforms into discrete token sequences or specify details of tokenizer/codec architectures, quantization, or codebooks. The abstract lacks mention of generating or utilizing discrete audio tokens as defined by the inclusion criteria, and thus it does not meet the core requirement regarding discrete token representation for audio modeling, generation, or alignment.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents an unsupervised speech segmentation method using Speech Language Models focusing on acoustic-semantic style changes, but it does not describe any process of discretizing continuous audio waveforms into discrete token sequences or specify details of tokenizer/codec architectures, quantization, or codebooks. The abstract lacks mention of generating or utilizing discrete audio tokens as defined by the inclusion criteria, and thus it does not meet the core requirement regarding discrete token representation for audio modeling, generation, or alignment.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "salmonn", "query_title": "{{SALMONN}: Towards Generic Hearing Abilities for Large Language Models}", "normalized_title": "salmonn towards generic hearing abilities for large language models", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "source": "arxiv", "source_id": "2310.13289v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "abstract": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "metadata": {"arxiv_id": "2310.13289v2", "title": "SALMONN: Towards Generic Hearing Abilities for Large Language Models", "summary": "Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.", "authors": ["Changli Tang", "Wenyi Yu", "Guangzhi Sun", "Xianzhao Chen", "Tian Tan", "Wei Li", "Lu Lu", "Zejun Ma", "Chao Zhang"], "published": "2023-10-20T05:41:57Z", "updated": "2024-04-08T06:12:52Z", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2310.13289v2", "landing_url": "https://arxiv.org/abs/2310.13289v2", "doi": "https://doi.org/10.48550/arXiv.2310.13289"}, "round-A_JuniorNano_output": {"reasoning": "SALMONN focuses on integrating pre-trained LLMs with speech/audio encoders to process continuous audio; it does not specify or rely on discrete audio tokens, tokenizers, or codebooks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "SALMONN focuses on integrating pre-trained LLMs with speech/audio encoders to process continuous audio; it does not specify or rely on discrete audio tokens, tokenizers, or codebooks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper SALMONN discusses integrating a pre-trained large language model with speech and audio encoders to handle various audio tasks, but the abstract does not explicitly mention the use or design of discrete audio tokens, neural audio codecs, or any tokenization process that discretizes continuous audio signals into finite token vocabularies, which is essential according to the inclusion criteria. Without clear evidence of discrete token usage or detailed tokenizer architecture, it fails to meet the core inclusion requirements and appears to focus on multimodal learning with continuous audio features and LLM integration rather than discrete audio token modeling.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper SALMONN discusses integrating a pre-trained large language model with speech and audio encoders to handle various audio tasks, but the abstract does not explicitly mention the use or design of discrete audio tokens, neural audio codecs, or any tokenization process that discretizes continuous audio signals into finite token vocabularies, which is essential according to the inclusion criteria. Without clear evidence of discrete token usage or detailed tokenizer architecture, it fails to meet the core inclusion requirements and appears to focus on multimodal learning with continuous audio features and LLM integration rather than discrete audio token modeling.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "qwen_audio", "query_title": "{Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models}", "normalized_title": "qwen audio advancing universal audio understanding via unified large scale audio language models", "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models", "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.", "source": "arxiv", "source_id": "2311.07919v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models", "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.", "metadata": {"arxiv_id": "2311.07919v2", "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models", "summary": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.", "authors": ["Yunfei Chu", "Jin Xu", "Xiaohuan Zhou", "Qian Yang", "Shiliang Zhang", "Zhijie Yan", "Chang Zhou", "Jingren Zhou"], "published": "2023-11-14T05:34:50Z", "updated": "2023-12-21T10:20:42Z", "categories": ["eess.AS", "cs.CL", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2311.07919v2", "landing_url": "https://arxiv.org/abs/2311.07919v2", "doi": "https://doi.org/10.48550/arXiv.2311.07919"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on scaling audio-language models for universal audio understanding and does not discuss discrete audio tokens, tokenizers, or discretization pipelines, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on scaling audio-language models for universal audio understanding and does not discuss discrete audio tokens, tokenizers, or discretization pipelines, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents Qwen-Audio, a large-scale audio-language model for various audio types and tasks, focusing on multi-task training to improve performance without task-specific fine-tuning; however, the abstract does not mention any discrete audio tokenization process or the design of discrete token vocabularies/codebooks, nor does it describe neural audio codecs or quantization methods that produce discrete tokens, which are core to the inclusion criteria. Without explicit evidence of discrete audio token usage or design, it does not meet the inclusion criteria focused on discrete audio tokens.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents Qwen-Audio, a large-scale audio-language model for various audio types and tasks, focusing on multi-task training to improve performance without task-specific fine-tuning; however, the abstract does not mention any discrete audio tokenization process or the design of discrete token vocabularies/codebooks, nor does it describe neural audio codecs or quantization methods that produce discrete tokens, which are core to the inclusion criteria. Without explicit evidence of discrete audio token usage or design, it does not meet the inclusion criteria focused on discrete audio tokens.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "maimon2025slamming", "query_title": "{Slamming: Training a Speech Language Model on One GPU in a Day}", "normalized_title": "slamming training a speech language model on one gpu in a day", "title": "Slamming: Training a Speech Language Model on One GPU in a Day", "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "source": "arxiv", "source_id": "2502.15814v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Slamming: Training a Speech Language Model on One GPU in a Day", "abstract": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "metadata": {"arxiv_id": "2502.15814v2", "title": "Slamming: Training a Speech Language Model on One GPU in a Day", "summary": "We introduce Slam, a recipe for training high-quality Speech Language Models (SLMs) on a single academic GPU in 24 hours. We do so through empirical analysis of model initialisation and architecture, synthetic training data, preference optimisation with synthetic data and tweaking all other components. We empirically demonstrate that this training recipe also scales well with more compute getting results on par with leading SLMs in a fraction of the compute cost. We hope these insights will make SLM training and research more accessible. In the context of SLM scaling laws, our results far outperform predicted compute optimal performance, giving an optimistic view to SLM feasibility. See code, data, models, samples at - https://pages.cs.huji.ac.il/adiyoss-lab/slamming .", "authors": ["Gallil Maimon", "Avishai Elmakies", "Yossi Adi"], "published": "2025-02-19T17:21:15Z", "updated": "2025-05-22T16:55:59Z", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2502.15814v2", "landing_url": "https://arxiv.org/abs/2502.15814v2", "doi": "https://doi.org/10.48550/arXiv.2502.15814"}, "round-A_JuniorNano_output": {"reasoning": "The item does not address discrete audio tokens or tokenization and centers on rapid training of Speech Language Models on a single GPU, so it fails the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item does not address discrete audio tokens or tokenization and centers on rapid training of Speech Language Models on a single GPU, so it fails the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes training Speech Language Models efficiently but does not mention any form of discretization of audio into token sequences, neural audio codecs, vector quantization, or discrete audio tokens, which are core inclusion requirements; it focuses on training methods and scaling rather than discrete audio token design or usage, thus failing to meet the specific criteria for inclusion.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes training Speech Language Models efficiently but does not mention any form of discretization of audio into token sequences, neural audio codecs, vector quantization, or discrete audio tokens, which are core inclusion requirements; it focuses on training methods and scaling rather than discrete audio token design or usage, thus failing to meet the specific criteria for inclusion.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "shi2021aishell", "query_title": "{{AISHELL-3}: A Multi-Speaker Mandarin {TTS} Corpus}", "normalized_title": "aishell 3 a multi speaker mandarin tts corpus", "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus", "abstract": "In this paper, we present AISHELL-3 , a large-scale multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-To-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spanning across 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Moreover, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We also present some data processing strategies and techniques which match with the characteristics of the presented corpus and conduct experiments on multiple speech-synthesis systems to assess the quality of the generated speech samples, showing promising results. The corpus is available online at openslr.org/93/ under Apache v2.0 license.", "source": "semantic_scholar", "source_id": "a6387132862d1270b1a0b7ec8352b1773ec2e990", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus", "abstract": "In this paper, we present AISHELL-3 , a large-scale multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-To-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spanning across 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Moreover, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We also present some data processing strategies and techniques which match with the characteristics of the presented corpus and conduct experiments on multiple speech-synthesis systems to assess the quality of the generated speech samples, showing promising results. The corpus is available online at openslr.org/93/ under Apache v2.0 license.", "metadata": {"arxiv_id": "semantic_scholar:a6387132862d1270b1a0b7ec8352b1773ec2e990", "title": "AISHELL-3: A Multi-Speaker Mandarin TTS Corpus", "summary": "In this paper, we present AISHELL-3 , a large-scale multi-speaker Mandarin speech corpus which could be used to train multi-speaker Text-To-Speech (TTS) systems. The corpus contains roughly 85 hours of emotion-neutral recordings spanning across 218 native Chinese mandarin speakers. Their auxiliary attributes such as gender, age group and native accents are explicitly marked and provided in the corpus. Moreover, transcripts in Chinese character-level and pinyin-level are provided along with the recordings. We also present some data processing strategies and techniques which match with the characteristics of the presented corpus and conduct experiments on multiple speech-synthesis systems to assess the quality of the generated speech samples, showing promising results. The corpus is available online at openslr.org/93/ under Apache v2.0 license.", "source": "semantic_scholar", "source_id": "a6387132862d1270b1a0b7ec8352b1773ec2e990", "publication_date": "2021-08-30"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a Mandarin multi-speaker TTS corpus with transcripts, but provides no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a Mandarin multi-speaker TTS corpus with transcripts, but provides no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The presented AISHELL-3 corpus paper focuses on providing a large-scale multi-speaker Mandarin speech corpus for TTS systems, describing its content, attributes, and some data processing strategies; however, it does not detail any discrete audio token generation process or tokenizer/codec architecture, nor does it discuss token quantization or vocabulary/codebook settings required to confirm its alignment with the discrete audio tokens theme.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The presented AISHELL-3 corpus paper focuses on providing a large-scale multi-speaker Mandarin speech corpus for TTS systems, describing its content, attributes, and some data processing strategies; however, it does not detail any discrete audio token generation process or tokenizer/codec architecture, nor does it discuss token quantization or vocabulary/codebook settings required to confirm its alignment with the discrete audio tokens theme.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "shi2021highland", "query_title": "{Highland {P}uebla {N}ahuatl speech translation corpus for endangered language documentation}", "normalized_title": "highland puebla nahuatl speech translation corpus for endangered language documentation", "title": "Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation", "abstract": "Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR > MT) pipeline when translating endangered language documentation materials.", "source": "semantic_scholar", "source_id": "59f3e3cad309eb4965d67773d68bc2f91b2e376f", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation", "abstract": "Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR > MT) pipeline when translating endangered language documentation materials.", "metadata": {"arxiv_id": "semantic_scholar:59f3e3cad309eb4965d67773d68bc2f91b2e376f", "title": "Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation", "summary": "Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR > MT) pipeline when translating endangered language documentation materials.", "source": "semantic_scholar", "source_id": "59f3e3cad309eb4965d67773d68bc2f91b2e376f", "year": 2021}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on a Nahuatl speech translation corpus and ST models but does not discuss discrete audio tokens, tokenization, or neural discretization, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on a Nahuatl speech translation corpus and ST models but does not discuss discrete audio tokens, tokenization, or neural discretization, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a speech translation corpus for an endangered language and applies end-to-end speech translation models outperforming cascaded pipelines; however, it does not describe the generation or use of discrete audio tokens, neural audio codecs, or vector quantization processes central to the inclusion criteria, nor does it provide detailed information about discrete token vocabularies or tokenizer architectures, which are essential for reproducibility and evaluation in this context.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents a speech translation corpus for an endangered language and applies end-to-end speech translation models outperforming cascaded pipelines; however, it does not describe the generation or use of discrete audio tokens, neural audio codecs, or vector quantization processes central to the inclusion criteria, nor does it provide detailed information about discrete token vocabularies or tokenizer architectures, which are essential for reproducibility and evaluation in this context.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "shi2021leveraging", "query_title": "{Leveraging End-to-End {ASR} for Endangered Language Documentation: An Empirical Study on {Y}ol{\\'o}xochitl {M}ixtec}", "normalized_title": "leveraging end to end asr for endangered language documentation an empirical study on yol oxochitl mixtec", "title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolóxochitl Mixtec", "abstract": "“Transcription bottlenecks”, created by a shortage of effective human transcribers (i.e., transcriber shortage), are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.", "source": "semantic_scholar", "source_id": "8c4d1e81c277f71cd9e3c9a0af356203c7948dca", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolóxochitl Mixtec", "abstract": "“Transcription bottlenecks”, created by a shortage of effective human transcribers (i.e., transcriber shortage), are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.", "metadata": {"arxiv_id": "semantic_scholar:8c4d1e81c277f71cd9e3c9a0af356203c7948dca", "title": "Leveraging End-to-End ASR for Endangered Language Documentation: An Empirical Study on Yolóxochitl Mixtec", "summary": "“Transcription bottlenecks”, created by a shortage of effective human transcribers (i.e., transcriber shortage), are one of the main challenges to endangered language (EL) documentation. Automatic speech recognition (ASR) has been suggested as a tool to overcome such bottlenecks. Following this suggestion, we investigated the effectiveness for EL documentation of end-to-end ASR, which unlike Hidden Markov Model ASR systems, eschews linguistic resources but is instead more dependent on large-data settings. We open source a Yoloxóchitl Mixtec EL corpus. First, we review our method in building an end-to-end ASR system in a way that would be reproducible by the ASR community. We then propose a novice transcription correction task and demonstrate how ASR systems and novice transcribers can work together to improve EL documentation. We believe this combinatory methodology would mitigate the transcription bottleneck and transcriber shortage that hinders EL documentation.", "source": "semantic_scholar", "source_id": "8c4d1e81c277f71cd9e3c9a0af356203c7948dca", "year": 2021}, "round-A_JuniorNano_output": {"reasoning": "Does not discuss discrete audio tokens, tokenization, or token design; only end-to-end ASR for endangered language documentation, so fails inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Does not discuss discrete audio tokens, tokenization, or token design; only end-to-end ASR for endangered language documentation, so fails inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on using end-to-end ASR systems for endangered language documentation and transcription correction, but there is no indication that it involves discrete audio tokens derived through discretization, neural audio codecs, or self-supervised tokenization; thus, it does not meet the inclusion criteria centered on discrete token sequences for audio modeling and generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on using end-to-end ASR systems for endangered language documentation and transcription correction, but there is no indication that it involves discrete audio tokens derived through discretization, neural audio codecs, or self-supervised tokenization; thus, it does not meet the inclusion criteria centered on discrete token sequences for audio modeling and generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "huang2021multi", "query_title": "{Multi-singer: Fast multi-singer singing voice vocoder with a large-scale corpus}", "normalized_title": "multi singer fast multi singer singing voice vocoder with a large scale corpus", "title": "Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus", "abstract": "High-fidelity multi-singer singing voice synthesis is challenging for neural vocoder due to the singing voice data shortage, limited singer generalization, and large computational cost. Existing open corpora could not meet requirements for high-fidelity singing voice synthesis because of the scale and quality weaknesses. Previous vocoders have difficulty in multi-singer modeling, and a distinct degradation emerges when conducting unseen singer singing voice generation. To accelerate singing voice researches in the community, we release a large-scale, multi-singer Chinese singing voice dataset OpenSinger. To tackle the difficulty in unseen singer modeling, we propose Multi-Singer, a fast multi-singer vocoder with generative adversarial networks. Specifically, 1) Multi-Singer uses a multi-band generator to speed up both training and inference procedure. 2) to capture and rebuild singer identity from the acoustic feature (i.e., mel-spectrogram), Multi-Singer adopts a singer conditional discriminator and conditional adversarial training objective. 3) to supervise the reconstruction of singer identity in the spectrum envelopes in frequency domain, we propose an auxiliary singer perceptual loss. The joint training approach effectively works in GANs for multi-singer voices modeling. Experimental results verify the effectiveness of OpenSinger and show that Multi-Singer improves unseen singer singing voices modeling in both speed and quality over previous methods. The further experiment proves that combined with FastSpeech 2 as the acoustic model, Multi-Singer achieves strong robustness in the multi-singer singing voice synthesis pipeline. Samples are available at https://Multi-Singer.github.io/", "source": "arxiv", "source_id": "2112.10358v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus", "abstract": "High-fidelity multi-singer singing voice synthesis is challenging for neural vocoder due to the singing voice data shortage, limited singer generalization, and large computational cost. Existing open corpora could not meet requirements for high-fidelity singing voice synthesis because of the scale and quality weaknesses. Previous vocoders have difficulty in multi-singer modeling, and a distinct degradation emerges when conducting unseen singer singing voice generation. To accelerate singing voice researches in the community, we release a large-scale, multi-singer Chinese singing voice dataset OpenSinger. To tackle the difficulty in unseen singer modeling, we propose Multi-Singer, a fast multi-singer vocoder with generative adversarial networks. Specifically, 1) Multi-Singer uses a multi-band generator to speed up both training and inference procedure. 2) to capture and rebuild singer identity from the acoustic feature (i.e., mel-spectrogram), Multi-Singer adopts a singer conditional discriminator and conditional adversarial training objective. 3) to supervise the reconstruction of singer identity in the spectrum envelopes in frequency domain, we propose an auxiliary singer perceptual loss. The joint training approach effectively works in GANs for multi-singer voices modeling. Experimental results verify the effectiveness of OpenSinger and show that Multi-Singer improves unseen singer singing voices modeling in both speed and quality over previous methods. The further experiment proves that combined with FastSpeech 2 as the acoustic model, Multi-Singer achieves strong robustness in the multi-singer singing voice synthesis pipeline. Samples are available at https://Multi-Singer.github.io/", "metadata": {"arxiv_id": "2112.10358v1", "title": "Multi-Singer: Fast Multi-Singer Singing Voice Vocoder With A Large-Scale Corpus", "summary": "High-fidelity multi-singer singing voice synthesis is challenging for neural vocoder due to the singing voice data shortage, limited singer generalization, and large computational cost. Existing open corpora could not meet requirements for high-fidelity singing voice synthesis because of the scale and quality weaknesses. Previous vocoders have difficulty in multi-singer modeling, and a distinct degradation emerges when conducting unseen singer singing voice generation. To accelerate singing voice researches in the community, we release a large-scale, multi-singer Chinese singing voice dataset OpenSinger. To tackle the difficulty in unseen singer modeling, we propose Multi-Singer, a fast multi-singer vocoder with generative adversarial networks. Specifically, 1) Multi-Singer uses a multi-band generator to speed up both training and inference procedure. 2) to capture and rebuild singer identity from the acoustic feature (i.e., mel-spectrogram), Multi-Singer adopts a singer conditional discriminator and conditional adversarial training objective. 3) to supervise the reconstruction of singer identity in the spectrum envelopes in frequency domain, we propose an auxiliary singer perceptual loss. The joint training approach effectively works in GANs for multi-singer voices modeling. Experimental results verify the effectiveness of OpenSinger and show that Multi-Singer improves unseen singer singing voices modeling in both speed and quality over previous methods. The further experiment proves that combined with FastSpeech 2 as the acoustic model, Multi-Singer achieves strong robustness in the multi-singer singing voice synthesis pipeline. Samples are available at https://Multi-Singer.github.io/", "authors": ["Rongjie Huang", "Feiyang Chen", "Yi Ren", "Jinglin Liu", "Chenye Cui", "Zhou Zhao"], "published": "2021-12-20T06:41:27Z", "updated": "2021-12-20T06:41:27Z", "categories": ["eess.AS", "cs.MM", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2112.10358v1", "landing_url": "https://arxiv.org/abs/2112.10358v1", "doi": "https://doi.org/10.48550/arXiv.2112.10358"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on a fast multi-singer vocoder and a large singing voice dataset, with no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on a fast multi-singer vocoder and a large singing voice dataset, with no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a multi-singer singing voice vocoder using generative adversarial networks for high-fidelity singing voice synthesis, but it does not describe the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or related discretization mechanisms; it mainly deals with continuous acoustic features (mel-spectrograms) and GAN modeling rather than discrete token sequences for subsequent modeling, generation, or multimodal tasks. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens, nor does it provide sufficient detail on tokenizer or codebook design relevant for inclusion.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on a multi-singer singing voice vocoder using generative adversarial networks for high-fidelity singing voice synthesis, but it does not describe the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or related discretization mechanisms; it mainly deals with continuous acoustic features (mel-spectrograms) and GAN modeling rather than discrete token sequences for subsequent modeling, generation, or multimodal tasks. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens, nor does it provide sufficient detail on tokenizer or codebook design relevant for inclusion.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "dai2023singstyle111", "query_title": "{Singstyle111: A multilingual singing dataset with style transfer}", "normalized_title": "singstyle111 a multilingual singing dataset with style transfer", "title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer", "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.", "source": "semantic_scholar", "source_id": "0cab8f24dec845c4b8e64e97dd4a92a1b6338ea1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer", "abstract": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.", "metadata": {"arxiv_id": "semantic_scholar:0cab8f24dec845c4b8e64e97dd4a92a1b6338ea1", "title": "SingStyle111: A Multilingual Singing Dataset With Style Transfer", "summary": "There has been a persistent lack of publicly accessible data in singing voice research, particularly concerning the diversity of languages and performance styles. In this paper, we introduce SingStyle111, a large studio-quality singing dataset with multiple languages and different singing styles, and present singing style transfer examples. The dataset features 111 songs performed by eight professional singers, spanning 12.8 hours and covering English, Chinese, and Italian. SingStyle111 incorporates different singing styles, such as bel canto opera, Chinese folk singing, pop, jazz, and children. Specifically, 80 songs include at least two distinct singing styles performed by the same singer. All recordings were conducted in professional studios, yielding clean, dry vocal tracks in mono format with a 44.1 kHz sample rate. We have segmented the singing voices into phrases, providing lyrics, performance MIDI, and scores with phoneme-level alignment. We also extracted acoustic features such as Mel-Spectrogram, F0 contour, and loudness curves. This dataset applies to various MIR tasks such as Singing Voice Synthesis, Singing Voice Conversion, Singing Transcription, Score Following, and Lyrics Detection. It is also designed for Singing Style Transfer, including both performance and voice timbre style. We make the dataset freely available for research purposes. Examples and download information can be found at https://shuqid.net/singstyle111.", "source": "semantic_scholar", "source_id": "0cab8f24dec845c4b8e64e97dd4a92a1b6338ea1", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "The item is a dataset paper (singing dataset with multilingual styles) and does not present discrete audio tokenization/codec design or token-based modeling as its core contribution, so it does not meet the inclusion criteria focused on discrete audio tokens.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a dataset paper (singing dataset with multilingual styles) and does not present discrete audio tokenization/codec design or token-based modeling as its core contribution, so it does not meet the inclusion criteria focused on discrete audio tokens.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a multilingual singing dataset with style transfer but does not mention any discrete audio token generation, neural audio codec, vector quantization, or discrete token sequences used for modeling, generation, or alignment; thus, it does not meet the core inclusion criteria related to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a multilingual singing dataset with style transfer but does not mention any discrete audio token generation, neural audio codec, vector quantization, or discrete token sequences used for modeling, generation, or alignment; thus, it does not meet the core inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "zhang2022m4singer", "query_title": "{M4singer: A multi-style, multi-singer and musical score provided mandarin singing corpus}", "normalized_title": "m4singer a multi style multi singer and musical score provided mandarin singing corpus", "title": "M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus", "abstract": "Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.", "source": "semantic_scholar", "source_id": "9441b35773ec51c19eb5c028a46705a3f3f732d5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus", "abstract": "Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.", "metadata": {"arxiv_id": "semantic_scholar:9441b35773ec51c19eb5c028a46705a3f3f732d5", "title": "M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus", "summary": "Music transcription is the process of transforming recorded sound of musical performances into symbolic representations such as sheet music or MIDI files. Extensive research and development have been carried out in the field of music transcription and technology. This comprehensive review paper surveys the diverse methodologies, techniques, and advancements that have shaped the landscape of music transcription. The paper outlines the significance of music transcription in preserving, analyzing, and disseminating musical compositions across various genres and cultures. It also provides a historical perspective by tracing the evolution of music transcription from traditional manual methods to modern automated approaches. It also highlights the challenges in transcription posed by complex singing techniques, variations in instrumentation, ambiguity in pitch, tempo changes, rhythm, and dynamics. The review also categorizes four different types of transcription techniques, frame-level, note-level, stream-level, and notation-level, discussing their strengths and limitations. It also encompasses the various research domains of music transcription from general melody extraction to vocal melody, note-level monophonic to polyphonic vocal transcription, single-instrument to multi-instrument transcription, and multi-pitch estimation. The survey further covers a broad spectrum of music transcription applications in music production and creation. It also reviews state-of-the-art open-source as well as commercial music transcription tools for pitch estimation, onset and offset detection, general melody detection, and vocal melody detection. In addition, it also encompasses the currently available python libraries that can be used for music transcription. Furthermore, the review highlights the various open-source benchmark datasets for different areas of music transcription. It also provides a wide range of references supporting the historical context, theoretical frameworks, and foundational concepts to help readers understand the background of music transcription and the context of our paper.", "source": "semantic_scholar", "source_id": "9441b35773ec51c19eb5c028a46705a3f3f732d5", "year": 2022}, "round-A_JuniorNano_output": {"reasoning": "The item is a broad survey of music transcription methods and datasets; it does not center on discrete audio tokens, tokenization methods, or downstream tasks driven by discrete tokens, thus failing the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is a broad survey of music transcription methods and datasets; it does not center on discrete audio tokens, tokenization methods, or downstream tasks driven by discrete tokens, thus failing the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The provided article is a comprehensive survey on music transcription techniques, focusing on transforming recorded musical performances into symbolic representations like sheet music or MIDI, without any mention or discussion of discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes. It does not describe any discrete token generation method, related token design choices, or applications involving sequence modeling with discrete tokens. Thus, it clearly does not meet the inclusion criteria focusing on discrete audio tokens for audio generation, understanding, or alignment.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The provided article is a comprehensive survey on music transcription techniques, focusing on transforming recorded musical performances into symbolic representations like sheet music or MIDI, without any mention or discussion of discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes. It does not describe any discrete token generation method, related token design choices, or applications involving sequence modeling with discrete tokens. Thus, it clearly does not meet the inclusion criteria focusing on discrete audio tokens for audio generation, understanding, or alignment.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ogawa2021tohoku", "query_title": "{Tohoku {K}iritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs}", "normalized_title": "tohoku kiritan singing database a singing database for statistical parametric singing synthesis using japanese pop songs", "title": "Tohoku Kiritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs", "abstract": ": We have built a singing database that can be used for research purposes. Since recent songs are protected by copyright law, researchers typically use songs that can be used without copyright. With changes to the copyright law in Japan in 2019, we can now release a singing database consisting of songs protected by the law under several restrictions. Our database mainly consists of Japanese pop songs by a professional singer. We collected a total of 50 songs with around 57 minutes of vocals recorded in a studio. After recording, we labeled the phoneme boundaries and converted the songs into the MusicXML format required for the study of statistical parametric singing synthesis. Statistical analysis of the database was then carried out. First, we counted the number of phonemes to clarify their distribution. Second, we performed acoustical analysis on the distribution of pitch, the interval between notes, and duration. Results showed that although the information is biased, the amount of singing is suﬃcient in light of the ﬁndings of a prior study on singing synthesis. The corpus is freely available at our website, https://zunko.jp/kiridev/login.php [1].", "source": "semantic_scholar", "source_id": "f6781f89263f0e30070bfad392de76681653379c", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Tohoku Kiritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs", "abstract": ": We have built a singing database that can be used for research purposes. Since recent songs are protected by copyright law, researchers typically use songs that can be used without copyright. With changes to the copyright law in Japan in 2019, we can now release a singing database consisting of songs protected by the law under several restrictions. Our database mainly consists of Japanese pop songs by a professional singer. We collected a total of 50 songs with around 57 minutes of vocals recorded in a studio. After recording, we labeled the phoneme boundaries and converted the songs into the MusicXML format required for the study of statistical parametric singing synthesis. Statistical analysis of the database was then carried out. First, we counted the number of phonemes to clarify their distribution. Second, we performed acoustical analysis on the distribution of pitch, the interval between notes, and duration. Results showed that although the information is biased, the amount of singing is suﬃcient in light of the ﬁndings of a prior study on singing synthesis. The corpus is freely available at our website, https://zunko.jp/kiridev/login.php [1].", "metadata": {"arxiv_id": "semantic_scholar:f6781f89263f0e30070bfad392de76681653379c", "title": "Tohoku Kiritan singing database: A singing database for statistical parametric singing synthesis using Japanese pop songs", "summary": ": We have built a singing database that can be used for research purposes. Since recent songs are protected by copyright law, researchers typically use songs that can be used without copyright. With changes to the copyright law in Japan in 2019, we can now release a singing database consisting of songs protected by the law under several restrictions. Our database mainly consists of Japanese pop songs by a professional singer. We collected a total of 50 songs with around 57 minutes of vocals recorded in a studio. After recording, we labeled the phoneme boundaries and converted the songs into the MusicXML format required for the study of statistical parametric singing synthesis. Statistical analysis of the database was then carried out. First, we counted the number of phonemes to clarify their distribution. Second, we performed acoustical analysis on the distribution of pitch, the interval between notes, and duration. Results showed that although the information is biased, the amount of singing is suﬃcient in light of the ﬁndings of a prior study on singing synthesis. The corpus is freely available at our website, https://zunko.jp/kiridev/login.php [1].", "source": "semantic_scholar", "source_id": "f6781f89263f0e30070bfad392de76681653379c", "publication_date": "2021-05-01"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a singing database with phoneme labeling and MusicXML for singing synthesis, but it does not address discrete audio tokens, tokenizers/codebooks, or token-based modeling as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a singing database with phoneme labeling and MusicXML for singing synthesis, but it does not address discrete audio tokens, tokenizers/codebooks, or token-based modeling as required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents a singing database with annotated phoneme boundaries and MusicXML conversion for statistical parametric singing synthesis, but it does not describe any discrete audio tokenization method such as vector quantization or neural audio codec processing to produce discrete token sequences from audio waveforms; hence, it fails to meet the inclusion criteria relating to discrete audio token generation and analysis.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents a singing database with annotated phoneme boundaries and MusicXML conversion for statistical parametric singing synthesis, but it does not describe any discrete audio tokenization method such as vector quantization or neural audio codec processing to produce discrete token sequences from audio waveforms; hence, it fails to meet the inclusion criteria relating to discrete audio token generation and analysis.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wang2022opencpop", "query_title": "{Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis}", "normalized_title": "opencpop a high quality open source chinese popular song corpus for singing voice synthesis", "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis", "abstract": "This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.", "source": "arxiv", "source_id": "2201.07429v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis", "abstract": "This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.", "metadata": {"arxiv_id": "2201.07429v2", "title": "Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis", "summary": "This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voice synthesis (SVS). The corpus consists of 100 popular Mandarin songs performed by a female professional singer. Audio files are recorded with studio quality at a sampling rate of 44,100 Hz and the corresponding lyrics and musical scores are provided. All singing recordings have been phonetically annotated with phoneme boundaries and syllable (note) boundaries. To demonstrate the reliability of the released data and to provide a baseline for future research, we built baseline deep neural network-based SVS models and evaluated them with both objective metrics and subjective mean opinion score (MOS) measure. Experimental results show that the best SVS model trained on our database achieves 3.70 MOS, indicating the reliability of the provided corpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can be found on the project homepage.", "authors": ["Yu Wang", "Xinsheng Wang", "Pengcheng Zhu", "Jie Wu", "Hanzhao Li", "Heyang Xue", "Yongmao Zhang", "Lei Xie", "Mengxiao Bi"], "published": "2022-01-19T06:12:47Z", "updated": "2022-01-20T02:08:47Z", "categories": ["cs.SD", "cs.DB", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2201.07429v2", "landing_url": "https://arxiv.org/abs/2201.07429v2", "doi": "https://doi.org/10.48550/arXiv.2201.07429"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes a Mandarin singing corpus for SVS and baseline models, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling crucial to the criteria; thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper describes a Mandarin singing corpus for SVS and baseline models, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling crucial to the criteria; thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a Mandarin singing corpus with phonetic annotations but does not describe any discrete tokenization process or neural audio codec to produce discrete audio tokens, nor does it focus on discrete audio tokens as core representations for modeling or synthesis; hence, it fails to meet the inclusion criteria related to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a Mandarin singing corpus with phonetic annotations but does not describe any discrete tokenization process or neural audio codec to produce discrete audio tokens, nor does it focus on discrete audio tokens as core representations for modeling or synthesis; hence, it fails to meet the inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "shi2024singing", "query_title": "{Singing Voice Data Scaling-up: An Introduction to {ACE-Opencpop and ACE-KiSing}}", "normalized_title": "singing voice data scaling up an introduction to ace opencpop and ace kising", "title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing", "abstract": "In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (\\url{https://github.com/espnet/espnet})", "source": "arxiv", "source_id": "2401.17619v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing", "abstract": "In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (\\url{https://github.com/espnet/espnet})", "metadata": {"arxiv_id": "2401.17619v3", "title": "Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing", "summary": "In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (\\url{https://github.com/espnet/espnet})", "authors": ["Jiatong Shi", "Yueqian Lin", "Xinyi Bai", "Keyi Zhang", "Yuning Wu", "Yuxun Tang", "Yifeng Yu", "Qin Jin", "Shinji Watanabe"], "published": "2024-01-31T06:17:51Z", "updated": "2024-06-12T20:55:59Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2401.17619v3", "landing_url": "https://arxiv.org/abs/2401.17619v3", "doi": "https://doi.org/10.48550/arXiv.2401.17619"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on data augmentation and dataset construction for singing voice synthesis and does not address discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on data augmentation and dataset construction for singing voice synthesis and does not address discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents new singing voice datasets and improvements in singing voice synthesis but does not describe the use or generation of discrete audio tokens as defined (e.g., via discretization, neural audio codecs, vector quantization) nor does it discuss token vocabularies or tokenization methods necessary to confirm relevance to the 'Discrete Audio Tokens' topic. The abstract focuses on data augmentation and dataset creation rather than discrete token design, modeling, or use for downstream tasks, thus failing key inclusion criteria while not violating exclusion criteria explicitly. Hence, it does not meet the main inclusion requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents new singing voice datasets and improvements in singing voice synthesis but does not describe the use or generation of discrete audio tokens as defined (e.g., via discretization, neural audio codecs, vector quantization) nor does it discuss token vocabularies or tokenization methods necessary to confirm relevance to the 'Discrete Audio Tokens' topic. The abstract focuses on data augmentation and dataset creation rather than discrete token design, modeling, or use for downstream tasks, thus failing key inclusion criteria while not violating exclusion criteria explicitly. Hence, it does not meet the main inclusion requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "koguchi2020pjs", "query_title": "{{PJS}: Phoneme-balanced Japanese singing-voice corpus}", "normalized_title": "pjs phoneme balanced japanese singing voice corpus", "title": "PJS: phoneme-balanced Japanese singing voice corpus", "abstract": "This paper presents a free Japanese singing voice corpus that can be used for highly applicable and reproducible singing voice synthesis research. A singing voice corpus helps develop singing voice synthesis, but existing corpora have two critical problems: data imbalance (singing voice corpora do not guarantee phoneme balance, unlike speaking-voice corpora) and copyright issues (cannot legally share data). As a way to avoid these problems, we constructed a PJS (phoneme-balanced Japanese singing voice) corpus that guarantees phoneme balance and is licensed with CC BY-SA 4.0, and we composed melodies using a phoneme-balanced speaking-voice corpus. This paper describes how we built the corpus.", "source": "arxiv", "source_id": "2006.02959v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "PJS: phoneme-balanced Japanese singing voice corpus", "abstract": "This paper presents a free Japanese singing voice corpus that can be used for highly applicable and reproducible singing voice synthesis research. A singing voice corpus helps develop singing voice synthesis, but existing corpora have two critical problems: data imbalance (singing voice corpora do not guarantee phoneme balance, unlike speaking-voice corpora) and copyright issues (cannot legally share data). As a way to avoid these problems, we constructed a PJS (phoneme-balanced Japanese singing voice) corpus that guarantees phoneme balance and is licensed with CC BY-SA 4.0, and we composed melodies using a phoneme-balanced speaking-voice corpus. This paper describes how we built the corpus.", "metadata": {"arxiv_id": "2006.02959v1", "title": "PJS: phoneme-balanced Japanese singing voice corpus", "summary": "This paper presents a free Japanese singing voice corpus that can be used for highly applicable and reproducible singing voice synthesis research. A singing voice corpus helps develop singing voice synthesis, but existing corpora have two critical problems: data imbalance (singing voice corpora do not guarantee phoneme balance, unlike speaking-voice corpora) and copyright issues (cannot legally share data). As a way to avoid these problems, we constructed a PJS (phoneme-balanced Japanese singing voice) corpus that guarantees phoneme balance and is licensed with CC BY-SA 4.0, and we composed melodies using a phoneme-balanced speaking-voice corpus. This paper describes how we built the corpus.", "authors": ["Junya Koguchi", "Shinnosuke Takamichi"], "published": "2020-06-04T15:41:00Z", "updated": "2020-06-04T15:41:00Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2006.02959v1", "landing_url": "https://arxiv.org/abs/2006.02959v1", "doi": "https://doi.org/10.48550/arXiv.2006.02959"}, "round-A_JuniorNano_output": {"reasoning": "This item centers on a phoneme-balanced Japanese singing voice corpus and its licensing, description of corpus construction, and usage for singing voice synthesis; it does not discuss discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item centers on a phoneme-balanced Japanese singing voice corpus and its licensing, description of corpus construction, and usage for singing voice synthesis; it does not discuss discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces a phoneme-balanced Japanese singing voice corpus aimed at singing voice synthesis research, but it does not describe the generation or use of discrete audio tokens, neural audio codecs, or any tokenization process of continuous audio waveforms into finite vocabularies; thus, it does not meet the inclusion criteria related to discrete token representation and methodology.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces a phoneme-balanced Japanese singing voice corpus aimed at singing voice synthesis research, but it does not describe the generation or use of discrete audio tokens, neural audio codecs, or any tokenization process of continuous audio waveforms into finite vocabularies; thus, it does not meet the inclusion criteria related to discrete token representation and methodology.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "takamichi2020jsut", "query_title": "{{JSUT and JVS}: Free Japanese voice corpora for accelerating speech synthesis research}", "normalized_title": "jsut and jvs free japanese voice corpora for accelerating speech synthesis research", "title": "JSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research", "abstract": ": In this paper, we develop two corpora for speech synthesis research. Thanks to improvements in machine learning techniques, including deep learning, speech synthesis is becoming a machine learning task. To accelerate speech synthesis research, we aim at developing Japanese voice corpora reasonably accessible from not only academic institutions but also commercial companies. In this paper, we construct the JSUT and JVS corpora. They are designed mainly for text-to-speech synthesis and voice conversion, respectively. The JSUT corpus contains 10 hours of reading-style speech uttered by a single speaker, and the JVS corpus contains 30 hours containing three styles of speech uttered by 100 speakers. This paper describes how we designed the corpora and summarizes the speciﬁcations. The corpora are available at our project pages.", "source": "semantic_scholar", "source_id": "aa9b455bb5b16f49a3cfdb3239303e250c41f10f", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "JSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research", "abstract": ": In this paper, we develop two corpora for speech synthesis research. Thanks to improvements in machine learning techniques, including deep learning, speech synthesis is becoming a machine learning task. To accelerate speech synthesis research, we aim at developing Japanese voice corpora reasonably accessible from not only academic institutions but also commercial companies. In this paper, we construct the JSUT and JVS corpora. They are designed mainly for text-to-speech synthesis and voice conversion, respectively. The JSUT corpus contains 10 hours of reading-style speech uttered by a single speaker, and the JVS corpus contains 30 hours containing three styles of speech uttered by 100 speakers. This paper describes how we designed the corpora and summarizes the speciﬁcations. The corpora are available at our project pages.", "metadata": {"arxiv_id": "semantic_scholar:aa9b455bb5b16f49a3cfdb3239303e250c41f10f", "title": "JSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research", "summary": ": In this paper, we develop two corpora for speech synthesis research. Thanks to improvements in machine learning techniques, including deep learning, speech synthesis is becoming a machine learning task. To accelerate speech synthesis research, we aim at developing Japanese voice corpora reasonably accessible from not only academic institutions but also commercial companies. In this paper, we construct the JSUT and JVS corpora. They are designed mainly for text-to-speech synthesis and voice conversion, respectively. The JSUT corpus contains 10 hours of reading-style speech uttered by a single speaker, and the JVS corpus contains 30 hours containing three styles of speech uttered by 100 speakers. This paper describes how we designed the corpora and summarizes the speciﬁcations. The corpora are available at our project pages.", "source": "semantic_scholar", "source_id": "aa9b455bb5b16f49a3cfdb3239303e250c41f10f", "publication_date": "2020-09-01"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes two free Japanese voice corpora for TTS/voice conversion, with no discussion of discrete audio tokens, tokenization methods, or token-based modeling, thus not meeting the token-centric inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper describes two free Japanese voice corpora for TTS/voice conversion, with no discussion of discrete audio tokens, tokenization methods, or token-based modeling, thus not meeting the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on constructing Japanese voice corpora for speech synthesis, but does not describe any discretization of continuous audio waveforms into discrete tokens nor mention neural audio codecs or discrete token generation methods; therefore it does not meet the core inclusion criteria related to discrete audio tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on constructing Japanese voice corpora for speech synthesis, but does not describe any discretization of continuous audio waveforms into discrete tokens nor mention neural audio codecs or discrete token generation methods; therefore it does not meet the core inclusion criteria related to discrete audio tokens.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "amith_yoloxochitl_mixtec", "query_title": "{Audio corpus of Yoloxóchitl Mixtec with accompanying time-coded transcriptions in ELAN}", "normalized_title": "audio corpus of yoloxochitl mixtec with accompanying time coded transcriptions in elan", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found", "review_count": 0, "reviews": []}
{"key": "amith_audio_corpus_sierra", "query_title": "{Audio corpus of Sierra Nororiental and Sierra Norte de Puebla Nahuat(l) with accompanying time-code transcriptions in ELAN}", "normalized_title": "audio corpus of sierra nororiental and sierra norte de puebla nahuat l with accompanying time code transcriptions in elan", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found", "review_count": 0, "reviews": []}
{"key": "amith_totonac", "query_title": "{Audio corpus of Totonac recordings from northern Puebla and adjacent areas of Veracruz}", "normalized_title": "audio corpus of totonac recordings from northern puebla and adjacent areas of veracruz", "title": null, "abstract": null, "source": "missing", "source_id": null, "match_status": "missing", "missing_reason": "not_found", "review_count": 0, "reviews": []}
{"key": "kuhn2014daps", "query_title": "{{DAPS}: Intelligent delay-aware packet scheduling for multipath transport}", "normalized_title": "daps intelligent delay aware packet scheduling for multipath transport", "title": "DAPS: Intelligent delay-aware packet scheduling for multipath transport", "abstract": "International audience", "source": "semantic_scholar", "source_id": "b9f0e3ffa0d102ffd9aca268b1a7e024b89fc66a", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "DAPS: Intelligent delay-aware packet scheduling for multipath transport", "abstract": "International audience", "metadata": {"arxiv_id": "semantic_scholar:b9f0e3ffa0d102ffd9aca268b1a7e024b89fc66a", "title": "DAPS: Intelligent delay-aware packet scheduling for multipath transport", "summary": "International audience", "source": "semantic_scholar", "source_id": "b9f0e3ffa0d102ffd9aca268b1a7e024b89fc66a", "publication_date": "2014-06-10"}, "round-A_JuniorNano_output": {"reasoning": "The item targets networking packet scheduling and lacks any discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it fails the inclusion criteria and aligns with exclusion categories for non-discrete audio token research.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item targets networking packet scheduling and lacks any discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it fails the inclusion criteria and aligns with exclusion categories for non-discrete audio token research.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title 'DAPS: Intelligent delay-aware packet scheduling for multipath transport' and the abstract 'International audience' do not mention any aspect of discrete audio tokens, neural audio codecs, self-supervised audio tokenization, or related audio token generation and modeling; thus, it clearly does not fit the inclusion criteria and likely falls into exclusion based on topic irrelevancy and lack of information.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title 'DAPS: Intelligent delay-aware packet scheduling for multipath transport' and the abstract 'International audience' do not mention any aspect of discrete audio tokens, neural audio codecs, self-supervised audio tokenization, or related audio token generation and modeling; thus, it clearly does not fit the inclusion criteria and likely falls into exclusion based on topic irrelevancy and lack of information.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yamagishi2019cstr", "query_title": "{{CSTR VCTK} Corpus: English multi-speaker corpus for {CSTR} voice cloning toolkit (version 0.92)}", "normalized_title": "cstr vctk corpus english multi speaker corpus for cstr voice cloning toolkit version 0 92", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "abstract": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "metadata": {"arxiv_id": "semantic_scholar:df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "title": "CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92)", "summary": "This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents. Each speaker reads out about 400 sentences, which were selected from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. The newspaper texts were taken from Herald Glasgow, with permission from Herald &amp; Times Group. Each speaker has a different set of the newspaper texts selected based a greedy algorithm that increases the contextual and phonetic coverage. The details of the text selection algorithms are described in the following paper: C. Veaux, J. Yamagishi and S. King, \"The voice bank corpus: Design, collection and data analysis of a large regional accent speech database,\" https://doi.org/10.1109/ICSDA.2013.6709856 The rainbow passage and elicitation paragraph are the same for all speakers. The rainbow passage can be found at International Dialects of English Archive: (http://web.ku.edu/~idea/readings/rainbow.htm). The elicitation paragraph is identical to the one used for the speech accent archive (http://accent.gmu.edu). The details of the the speech accent archive can be found at http://www.ualberta.ca/~aacl2009/PDFs/WeinbergerKunath2009AACL.pdf All speech data was recorded using an identical recording setup: an omni-directional microphone (DPA 4035) and a small diaphragm condenser microphone with very wide bandwidth (Sennheiser MKH 800), 96kHz sampling frequency at 24 bits and in a hemi-anechoic chamber of the University of Edinburgh. (However, two speakers, p280 and p315 had technical issues of the audio recordings using MKH 800). All recordings were converted into 16 bits, were downsampled to 48 kHz, and were manually end-pointed. This corpus was originally aimed for HMM-based text-to-speech synthesis systems, especially for speaker-adaptive HMM-based speech synthesis that uses average voice models trained on multiple speakers and speaker adaptation technologies. This corpus is also suitable for DNN-based multi-speaker text-to-speech synthesis systems and neural waveform modeling. The dataset was was referenced in the Google DeepMind work on WaveNet: https://arxiv.org/pdf/1609.03499.pdf . Please note while text files containing transcripts of the speech are provided for 109 of the 110 recordings, in the '/txt' folder, the 'p315' text was lost due to a hard disk error.", "source": "semantic_scholar", "source_id": "df40f83f5420f73b22562f3554e1e3bd4a5c1ef5", "publication_date": "2019-11-13"}, "round-A_JuniorNano_output": {"reasoning": "The item describes a multi-speaker English speech corpus for TTS (no discussion of discrete audio tokens or tokenization), so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes a multi-speaker English speech corpus for TTS (no discussion of discrete audio tokens or tokenization), so it does not meet the Discrete Audio Tokens criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study describes a multi-speaker speech corpus aimed at TTS synthesis, including data collection and recording details, but does not mention the creation or use of discrete audio tokens, tokenization methods, neural audio codecs, or vector quantization processes required by the inclusion criteria. It primarily focuses on continuous speech data collection rather than discrete token-based audio representation relevant for modeling and generation, and lacks any discrete token design, evaluation, or application as per the criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study describes a multi-speaker speech corpus aimed at TTS synthesis, including data collection and recording details, but does not mention the creation or use of discrete audio tokens, tokenization methods, neural audio codecs, or vector quantization processes required by the inclusion criteria. It primarily focuses on continuous speech data collection rather than discrete token-based audio representation relevant for modeling and generation, and lacks any discrete token design, evaluation, or application as per the criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "yan2023espnet", "query_title": "{{ESPnet-ST-v2}: Multipurpose Spoken Language Translation Toolkit}", "normalized_title": "espnet st v2 multipurpose spoken language translation toolkit", "title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.", "source": "arxiv", "source_id": "2304.04596v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "abstract": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.", "metadata": {"arxiv_id": "2304.04596v3", "title": "ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit", "summary": "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.", "authors": ["Brian Yan", "Jiatong Shi", "Yun Tang", "Hirofumi Inaguma", "Yifan Peng", "Siddharth Dalmia", "Peter Polák", "Patrick Fernandes", "Dan Berrebbi", "Tomoki Hayashi", "Xiaohui Zhang", "Zhaoheng Ni", "Moto Hira", "Soumi Maiti", "Juan Pino", "Shinji Watanabe"], "published": "2023-04-10T14:05:22Z", "updated": "2023-07-06T20:07:49Z", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2304.04596v3", "landing_url": "https://arxiv.org/abs/2304.04596v3", "doi": "https://doi.org/10.48550/arXiv.2304.04596"}, "round-A_JuniorNano_output": {"reasoning": "The item describes ESPnet-ST-v2, a toolkit for offline, simultaneous, and offline speech-to-speech translation with architectures and benchmarks, but it has no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes ESPnet-ST-v2, a toolkit for offline, simultaneous, and offline speech-to-speech translation with architectures and benchmarks, but it has no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents ESPnet-ST-v2, a spoken language translation toolkit that focuses on speech-to-text and speech-to-speech translation, including various architectures but does not explicitly describe or focus on generating discrete audio tokens via neural audio codecs or quantization techniques as required by the inclusion criteria; no detailed information on tokenization, codebook settings, or discrete token generation is provided in the abstract, and the main theme is on translation rather than token design or discrete audio token evaluation.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents ESPnet-ST-v2, a spoken language translation toolkit that focuses on speech-to-text and speech-to-speech translation, including various architectures but does not explicitly describe or focus on generating discrete audio tokens via neural audio codecs or quantization techniques as required by the inclusion criteria; no detailed information on tokenization, codebook settings, or discrete token generation is provided in the abstract, and the main theme is on translation rather than token design or discrete audio token evaluation.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "vincent2006bss", "query_title": "{Performance measurement in blind audio source separation}", "normalized_title": "performance measurement in blind audio source separation", "title": "Performance measurement in blind audio source separation", "abstract": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels", "source": "semantic_scholar", "source_id": "29de8281b8cbc764d605a20d00b818eba6d47da1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Performance measurement in blind audio source separation", "abstract": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels", "metadata": {"arxiv_id": "semantic_scholar:29de8281b8cbc764d605a20d00b818eba6d47da1", "title": "Performance measurement in blind audio source separation", "summary": "In this paper, we discuss the evaluation of blind audio source separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying filters. In each case, we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise, and algorithmic artifacts. Then, we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difficulty levels", "source": "semantic_scholar", "source_id": "29de8281b8cbc764d605a20d00b818eba6d47da1", "publication_date": "2006-07-01"}, "round-A_JuniorNano_output": {"reasoning": "This item is about evaluation metrics for blind audio source separation and does not involve discrete audio tokens, tokenizer/codec design, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is about evaluation metrics for blind audio source separation and does not involve discrete audio tokens, tokenizer/codec design, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on evaluating performance measures for blind audio source separation algorithms and does not discuss the discretization of continuous audio signals into discrete tokens, nor does it mention neural audio codecs, vector quantization, or any discrete token design or usage. Therefore, it does not meet the inclusion criteria related to \"Discrete Audio Tokens\" and instead relates to continuous audio signal processing without discrete token representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on evaluating performance measures for blind audio source separation algorithms and does not discuss the discretization of continuous audio signals into discrete tokens, nor does it mention neural audio codecs, vector quantization, or any discrete token design or usage. Therefore, it does not meet the inclusion criteria related to \"Discrete Audio Tokens\" and instead relates to continuous audio signal processing without discrete token representation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "allal2024SmolLM", "query_title": "{SmolLM - blazingly fast and remarkably powerful}", "normalized_title": "smollm blazingly fast and remarkably powerful", "title": "{SmolLM - blazingly fast and remarkably powerful}", "abstract": "Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance.", "source": "github", "source_id": "huggingface/smollm", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{SmolLM - blazingly fast and remarkably powerful}", "abstract": "Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance.", "metadata": {"arxiv_id": "github:huggingface/smollm", "title": "{SmolLM - blazingly fast and remarkably powerful}", "summary": "Welcome to Smol Models, a family of efficient and lightweight AI models from Hugging Face. Our mission is to create fully open powerful yet compact models, for text and vision, that can run effectively on-device while maintaining strong performance.", "source": "github", "source_id": "huggingface/smollm", "year": 2024}, "round-A_JuniorNano_output": {"reasoning": "The item describes general on-device AI models (SmolLM/Smol Models) with no reference to discrete audio tokens, tokenization, or related downstream tasks, so it does not meet inclusion and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes general on-device AI models (SmolLM/Smol Models) with no reference to discrete audio tokens, tokenization, or related downstream tasks, so it does not meet inclusion and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article discusses efficient and lightweight AI models for text and vision but does not mention discretization of audio waveforms into token sequences, neural audio codecs, or any related audio token mechanisms. It does not meet the inclusion criteria related to discrete audio tokens and focuses on general AI models rather than audio-specific tokenization or modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article discusses efficient and lightweight AI models for text and vision but does not mention discretization of audio waveforms into token sequences, neural audio codecs, or any related audio token mechanisms. It does not meet the inclusion criteria related to discrete audio tokens and focuses on general AI models rather than audio-specific tokenization or modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "rouard2022hybrid", "query_title": "{Hybrid transformers for music source separation}", "normalized_title": "hybrid transformers for music source separation", "title": "Hybrid Transformers for Music Source Separation", "abstract": "A natural question arising in Music Source Separation (MSS) is whether long range contextual information is useful, or whether local acoustic features are sufficient. In other fields, attention based Transformers have shown their ability to integrate information over long sequences. In this work, we introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral bi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a cross-domain Transformer Encoder, using self-attention within one domain, and cross-attention across domains. While it performs poorly when trained only on MUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by 0.45 dB of SDR when using 800 extra training songs. Using sparse attention kernels to extend its receptive field, and per source fine-tuning, we achieve state-of-the-art results on MUSDB with extra training data, with 9.20 dB of SDR.", "source": "arxiv", "source_id": "2211.08553v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Hybrid Transformers for Music Source Separation", "abstract": "A natural question arising in Music Source Separation (MSS) is whether long range contextual information is useful, or whether local acoustic features are sufficient. In other fields, attention based Transformers have shown their ability to integrate information over long sequences. In this work, we introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral bi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a cross-domain Transformer Encoder, using self-attention within one domain, and cross-attention across domains. While it performs poorly when trained only on MUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by 0.45 dB of SDR when using 800 extra training songs. Using sparse attention kernels to extend its receptive field, and per source fine-tuning, we achieve state-of-the-art results on MUSDB with extra training data, with 9.20 dB of SDR.", "metadata": {"arxiv_id": "2211.08553v1", "title": "Hybrid Transformers for Music Source Separation", "summary": "A natural question arising in Music Source Separation (MSS) is whether long range contextual information is useful, or whether local acoustic features are sufficient. In other fields, attention based Transformers have shown their ability to integrate information over long sequences. In this work, we introduce Hybrid Transformer Demucs (HT Demucs), an hybrid temporal/spectral bi-U-Net based on Hybrid Demucs, where the innermost layers are replaced by a cross-domain Transformer Encoder, using self-attention within one domain, and cross-attention across domains. While it performs poorly when trained only on MUSDB, we show that it outperforms Hybrid Demucs (trained on the same data) by 0.45 dB of SDR when using 800 extra training songs. Using sparse attention kernels to extend its receptive field, and per source fine-tuning, we achieve state-of-the-art results on MUSDB with extra training data, with 9.20 dB of SDR.", "authors": ["Simon Rouard", "Francisco Massa", "Alexandre Défossez"], "published": "2022-11-15T22:48:16Z", "updated": "2022-11-15T22:48:16Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2211.08553v1", "landing_url": "https://arxiv.org/abs/2211.08553v1", "doi": "https://doi.org/10.48550/arXiv.2211.08553"}, "round-A_JuniorNano_output": {"reasoning": "The abstract centers on Transformer-based music source separation in continuous-domain representations, with no discussion of discrete audio tokens, tokenizers, or neural codecs, so it does not meet the token-centric inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract centers on Transformer-based music source separation in continuous-domain representations, with no discussion of discrete audio tokens, tokenizers, or neural codecs, so it does not meet the token-centric inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on a hybrid transformer model for music source separation but does not mention any form of discretization to convert continuous audio waveforms into discrete tokens, nor does it describe any neural audio codec or self-supervised discrete unit extraction methods; therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on continuous audio feature modeling instead.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on a hybrid transformer model for music source separation but does not mention any form of discretization to convert continuous audio waveforms into discrete tokens, nor does it describe any neural audio codec or self-supervised discrete unit extraction methods; therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on continuous audio feature modeling instead.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "LuoY2019conv-tasnet", "query_title": "{Conv-tasnet: Surpassing ideal time--frequency magnitude masking for speech separation}", "normalized_title": "conv tasnet surpassing ideal time frequency magnitude masking for speech separation", "title": "Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation", "abstract": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.", "source": "arxiv", "source_id": "1809.07454v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation", "abstract": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.", "metadata": {"arxiv_id": "1809.07454v3", "title": "Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation", "summary": "Single-channel, speaker-independent speech separation methods have recently seen great progress. However, the accuracy, latency, and computational cost of such methods remain insufficient. The majority of the previous methods have formulated the separation problem through the time-frequency representation of the mixed signal, which has several drawbacks, including the decoupling of the phase and magnitude of the signal, the suboptimality of time-frequency representation for speech separation, and the long latency in calculating the spectrograms. To address these shortcomings, we propose a fully-convolutional time-domain audio separation network (Conv-TasNet), a deep learning framework for end-to-end time-domain speech separation. Conv-TasNet uses a linear encoder to generate a representation of the speech waveform optimized for separating individual speakers. Speaker separation is achieved by applying a set of weighting functions (masks) to the encoder output. The modified encoder representations are then inverted back to the waveforms using a linear decoder. The masks are found using a temporal convolutional network (TCN) consisting of stacked 1-D dilated convolutional blocks, which allows the network to model the long-term dependencies of the speech signal while maintaining a small model size. The proposed Conv-TasNet system significantly outperforms previous time-frequency masking methods in separating two- and three-speaker mixtures. Additionally, Conv-TasNet surpasses several ideal time-frequency magnitude masks in two-speaker speech separation as evaluated by both objective distortion measures and subjective quality assessment by human listeners. Finally, Conv-TasNet has a significantly smaller model size and a shorter minimum latency, making it a suitable solution for both offline and real-time speech separation applications.", "authors": ["Yi Luo", "Nima Mesgarani"], "published": "2018-09-20T02:38:05Z", "updated": "2019-05-15T07:40:44Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1809.07454v3", "landing_url": "https://arxiv.org/abs/1809.07454v3", "doi": "https://doi.org/10.1109/TASLP.2019.2915167"}, "round-A_JuniorNano_output": {"reasoning": "The paper proposes Conv-TasNet for time-domain speech separation using a learned encoder/decoder and temporal masks, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria and aligns with the exclusion criteria for token-based discrete representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper proposes Conv-TasNet for time-domain speech separation using a learned encoder/decoder and temporal masks, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria and aligns with the exclusion criteria for token-based discrete representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on end-to-end time-domain speech separation using Conv-TasNet, which is about improving speech separation and latency rather than generating discrete audio tokens mapped to finite vocabularies or involving neural audio codec discretization processes; there is no indication of discrete token representation, tokenizer design, or token-based modeling as defined in the inclusion criteria, nor details about vector quantization or discrete token generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on end-to-end time-domain speech separation using Conv-TasNet, which is about improving speech separation and latency rather than generating discrete audio tokens mapped to finite vocabularies or involving neural audio codec discretization processes; there is no indication of discrete token representation, tokenizer design, or token-based modeling as defined in the inclusion criteria, nor details about vector quantization or discrete token generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "Saijo2024_TFLoco", "query_title": "{TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement}", "normalized_title": "tf locoformer transformer with local modeling by convolution for speech separation and enhancement", "title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement", "abstract": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.", "source": "arxiv", "source_id": "2408.03440v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement", "abstract": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.", "metadata": {"arxiv_id": "2408.03440v1", "title": "TF-Locoformer: Transformer with Local Modeling by Convolution for Speech Separation and Enhancement", "summary": "Time-frequency (TF) domain dual-path models achieve high-fidelity speech separation. While some previous state-of-the-art (SoTA) models rely on RNNs, this reliance means they lack the parallelizability, scalability, and versatility of Transformer blocks. Given the wide-ranging success of pure Transformer-based architectures in other fields, in this work we focus on removing the RNN from TF-domain dual-path models, while maintaining SoTA performance. This work presents TF-Locoformer, a Transformer-based model with LOcal-modeling by COnvolution. The model uses feed-forward networks (FFNs) with convolution layers, instead of linear layers, to capture local information, letting the self-attention focus on capturing global patterns. We place two such FFNs before and after self-attention to enhance the local-modeling capability. We also introduce a novel normalization for TF-domain dual-path models. Experiments on separation and enhancement datasets show that the proposed model meets or exceeds SoTA in multiple benchmarks with an RNN-free architecture.", "authors": ["Kohei Saijo", "Gordon Wichern", "François G. Germain", "Zexu Pan", "Jonathan Le Roux"], "published": "2024-08-06T20:30:14Z", "updated": "2024-08-06T20:30:14Z", "categories": ["eess.AS", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2408.03440v1", "landing_url": "https://arxiv.org/abs/2408.03440v1", "doi": "https://doi.org/10.48550/arXiv.2408.03440"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on Transformer-based speech separation without discussing discrete audio tokens, tokenization, or neural audio codecs, thus it does not meet the ‘Discrete Audio Tokens’ criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on Transformer-based speech separation without discussing discrete audio tokens, tokenization, or neural audio codecs, thus it does not meet the ‘Discrete Audio Tokens’ criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on speech separation and enhancement using a Transformer-based model with local convolutional modeling, but it does not mention any form of discretization mapping continuous audio waveforms into token sequences, neural audio codec usage, vector quantization, or any discrete audio tokens as defined. It instead focuses on continuous TF-domain dual-path models and Transformer architectures without using discrete tokens as core representations, failing to meet the core inclusion criteria related to discrete audio tokenization.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on speech separation and enhancement using a Transformer-based model with local convolutional modeling, but it does not mention any form of discretization mapping continuous audio waveforms into token sequences, neural audio codec usage, vector quantization, or any discrete audio tokens as defined. It instead focuses on continuous TF-domain dual-path models and Transformer architectures without using discrete tokens as core representations, failing to meet the core inclusion criteria related to discrete audio tokenization.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "Kavalerov2019UniversalSS", "query_title": "{Universal Sound Separation}", "normalized_title": "universal sound separation", "title": "Universal Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.", "source": "arxiv", "source_id": "1905.03330v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Universal Sound Separation", "abstract": "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.", "metadata": {"arxiv_id": "1905.03330v2", "title": "Universal Sound Separation", "summary": "Recent deep learning approaches have achieved impressive performance on speech enhancement and separation tasks. However, these approaches have not been investigated for separating mixtures of arbitrary sounds of different types, a task we refer to as universal sound separation, and it is unknown how performance on speech tasks carries over to non-speech tasks. To study this question, we develop a dataset of mixtures containing arbitrary sounds, and use it to investigate the space of mask-based separation architectures, varying both the overall network architecture and the framewise analysis-synthesis basis for signal transformations. These network architectures include convolutional long short-term memory networks and time-dilated convolution stacks inspired by the recent success of time-domain enhancement networks like ConvTasNet. For the latter architecture, we also propose novel modifications that further improve separation performance. In terms of the framewise analysis-synthesis basis, we explore both a short-time Fourier transform (STFT) and a learnable basis, as used in ConvTasNet. For both of these bases, we also examine the effect of window size. In particular, for STFTs, we find that longer windows (25-50 ms) work best for speech/non-speech separation, while shorter windows (2.5 ms) work best for arbitrary sounds. For learnable bases, shorter windows (2.5 ms) work best on all tasks. Surprisingly, for universal sound separation, STFTs outperform learnable bases. Our best methods produce an improvement in scale-invariant signal-to-distortion ratio of over 13 dB for speech/non-speech separation and close to 10 dB for universal sound separation.", "authors": ["Ilya Kavalerov", "Scott Wisdom", "Hakan Erdogan", "Brian Patton", "Kevin Wilson", "Jonathan Le Roux", "John R. Hershey"], "published": "2019-05-08T20:48:49Z", "updated": "2019-08-02T20:44:41Z", "categories": ["cs.SD", "cs.LG", "eess.AS", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/1905.03330v2", "landing_url": "https://arxiv.org/abs/1905.03330v2", "doi": "https://doi.org/10.48550/arXiv.1905.03330"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on universal sound separation using continuous audio representations (STFT and learnable bases) and various network architectures, with no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet inclusion criteria and aligns with the exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on universal sound separation using continuous audio representations (STFT and learnable bases) and various network architectures, with no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet inclusion criteria and aligns with the exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on universal sound separation using deep learning architectures but does not describe generating or using discrete audio tokens derived from discretization or neural codecs, nor does it analyze or design token-based audio representations; thus, it does not meet the inclusion criteria centered on discrete audio tokens nor does it provide relevant tokenization details.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on universal sound separation using deep learning architectures but does not describe generating or using discrete audio tokens derived from discretization or neural codecs, nor does it analyze or design token-based audio representations; thus, it does not meet the inclusion criteria centered on discrete audio tokens nor does it provide relevant tokenization details.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "espnet", "query_title": "{{ESPnet}: End-to-End Speech Processing Toolkit}", "normalized_title": "espnet end to end speech processing toolkit", "title": "ESPnet: End-to-End Speech Processing Toolkit", "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.", "source": "arxiv", "source_id": "1804.00015v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ESPnet: End-to-End Speech Processing Toolkit", "abstract": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.", "metadata": {"arxiv_id": "1804.00015v1", "title": "ESPnet: End-to-End Speech Processing Toolkit", "summary": "This paper introduces a new open source platform for end-to-end speech processing named ESPnet. ESPnet mainly focuses on end-to-end automatic speech recognition (ASR), and adopts widely-used dynamic neural network toolkits, Chainer and PyTorch, as a main deep learning engine. ESPnet also follows the Kaldi ASR toolkit style for data processing, feature extraction/format, and recipes to provide a complete setup for speech recognition and other speech processing experiments. This paper explains a major architecture of this software platform, several important functionalities, which differentiate ESPnet from other open source ASR toolkits, and experimental results with major ASR benchmarks.", "authors": ["Shinji Watanabe", "Takaaki Hori", "Shigeki Karita", "Tomoki Hayashi", "Jiro Nishitoba", "Yuya Unno", "Nelson Enrique Yalta Soplin", "Jahn Heymann", "Matthew Wiesner", "Nanxin Chen", "Adithya Renduchintala", "Tsubasa Ochiai"], "published": "2018-03-30T18:09:39Z", "updated": "2018-03-30T18:09:39Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1804.00015v1", "landing_url": "https://arxiv.org/abs/1804.00015v1", "doi": "https://doi.org/10.48550/arXiv.1804.00015"}, "round-A_JuniorNano_output": {"reasoning": "The item describes ESPnet as an end-to-end ASR toolkit with no discussion of discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the Discrete Audio Tokens inclusion criteria and aligns with the exclusion criteria focusing on non-discretized representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes ESPnet as an end-to-end ASR toolkit with no discussion of discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the Discrete Audio Tokens inclusion criteria and aligns with the exclusion criteria focusing on non-discretized representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper introduces ESPnet, an end-to-end speech processing toolkit focused on automatic speech recognition, but it does not mention any process of discretizing continuous audio into discrete token sequences or describe tokenizers/codec architectures with vector quantization or other discrete token generation methods. The focus is on speech recognition pipelines rather than discrete audio tokens as defined by the inclusion criteria. Therefore, it does not meet the criteria related to discrete token representation or analysis.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper introduces ESPnet, an end-to-end speech processing toolkit focused on automatic speech recognition, but it does not mention any process of discretizing continuous audio into discrete token sequences or describe tokenizers/codec architectures with vector quantization or other discrete token generation methods. The focus is on speech recognition pipelines rather than discrete audio tokens as defined by the inclusion criteria. Therefore, it does not meet the criteria related to discrete token representation or analysis.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mars6", "query_title": "{MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model}", "normalized_title": "mars6 a small and robust hierarchical codec text to speech model", "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model", "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/", "source": "arxiv", "source_id": "2501.05787v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model", "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/", "metadata": {"arxiv_id": "2501.05787v1", "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model", "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/", "authors": ["Matthew Baas", "Pieter Scholtz", "Arnav Mehta", "Elliott Dyson", "Akshat Prakash", "Herman Kamper"], "published": "2025-01-10T08:41:42Z", "updated": "2025-01-10T08:41:42Z", "categories": ["eess.AS", "cs.CL"], "pdf_url": "https://arxiv.org/pdf/2501.05787v1", "landing_url": "https://arxiv.org/abs/2501.05787v1", "doi": "https://doi.org/10.48550/arXiv.2501.05787"}, "round-A_JuniorNano_output": {"reasoning": "The paper describes a codec-based TTS model (MARS6) that refers to programmable ‘speech tokens’ processed by a hierarchical decoder, implying discrete token representations as the core modelling units, which aligns with the discrete audio token focus and downstream TTS generation tasks.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper describes a codec-based TTS model (MARS6) that refers to programmable ‘speech tokens’ processed by a hierarchical decoder, implying discrete token representations as the core modelling units, which aligns with the discrete audio token focus and downstream TTS generation tasks.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a codec-based text-to-speech model (MARS6) that leverages hierarchical token processing for efficient and expressive TTS. It describes the use of a neural audio codec structure to discretize speech into tokens processed by a transformer model, aligning with the inclusion criteria of using discrete audio tokens from neural codec-based discretization. The abstract mentions a hierarchical decoder setup and token processing at 12 Hz, indicating a discrete token sequence representation. Although the abstract does not provide exhaustive details about the tokenizer/codec architecture or explicit quantization schemes, it clearly emphasizes token-based modeling and aligns with the TTS generation use-case, thus meeting the core thematic and evaluability inclusion requirements. Therefore, the study should be included, as it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on a codec-based text-to-speech model (MARS6) that leverages hierarchical token processing for efficient and expressive TTS. It describes the use of a neural audio codec structure to discretize speech into tokens processed by a transformer model, aligning with the inclusion criteria of using discrete audio tokens from neural codec-based discretization. The abstract mentions a hierarchical decoder setup and token processing at 12 Hz, indicating a discrete token sequence representation. Although the abstract does not provide exhaustive details about the tokenizer/codec architecture or explicit quantization schemes, it clearly emphasizes token-based modeling and aligns with the TTS generation use-case, thus meeting the core thematic and evaluability inclusion requirements. Therefore, the study should be included, as it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "simplespeech", "query_title": "{SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models}", "normalized_title": "simplespeech towards simple and efficient text to speech with scalar latent transformer diffusion models", "title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models", "abstract": "In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space. Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability. Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement. Demos are released.", "source": "arxiv", "source_id": "2406.02328v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models", "abstract": "In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space. Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability. Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement. Demos are released.", "metadata": {"arxiv_id": "2406.02328v3", "title": "SimpleSpeech: Towards Simple and Efficient Text-to-Speech with Scalar Latent Transformer Diffusion Models", "summary": "In this study, we propose a simple and efficient Non-Autoregressive (NAR) text-to-speech (TTS) system based on diffusion, named SimpleSpeech. Its simpleness shows in three aspects: (1) It can be trained on the speech-only dataset, without any alignment information; (2) It directly takes plain text as input and generates speech through an NAR way; (3) It tries to model speech in a finite and compact latent space, which alleviates the modeling difficulty of diffusion. More specifically, we propose a novel speech codec model (SQ-Codec) with scalar quantization, SQ-Codec effectively maps the complex speech signal into a finite and compact latent space, named scalar latent space. Benefits from SQ-Codec, we apply a novel transformer diffusion model in the scalar latent space of SQ-Codec. We train SimpleSpeech on 4k hours of a speech-only dataset, it shows natural prosody and voice cloning ability. Compared with previous large-scale TTS models, it presents significant speech quality and generation speed improvement. Demos are released.", "authors": ["Dongchao Yang", "Dingdong Wang", "Haohan Guo", "Xueyuan Chen", "Xixin Wu", "Helen Meng"], "published": "2024-06-04T13:58:28Z", "updated": "2024-06-14T16:04:48Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.02328v3", "landing_url": "https://arxiv.org/abs/2406.02328v3", "doi": "https://doi.org/10.48550/arXiv.2406.02328"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses a neural audio codec with scalar quantization to map speech to a finite discrete latent space and conducts diffusion-based TTS in that discrete latent space, aligning with the discrete audio tokens criteria and providing English title/abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper uses a neural audio codec with scalar quantization to map speech to a finite discrete latent space and conducts diffusion-based TTS in that discrete latent space, aligning with the discrete audio tokens criteria and providing English title/abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The paper describes a novel speech codec model, SQ-Codec, with scalar quantization that maps speech into a finite and compact latent space, which aligns with the concept of discretizing continuous audio waveforms into discrete tokens; furthermore, it applies a transformer diffusion model on these discrete tokens for TTS, showing system design, tokenization, and usage in downstream generation tasks (TTS). The abstract mentions scalar quantization but does not detail multi-codebooks or vector quantization, yet scalar quantization still constitutes discretization, meeting core criteria for neural discrete audio tokens. The paper also meets English evaluability requirements. Therefore, it fits the inclusion criteria and does not meet any exclusions.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper describes a novel speech codec model, SQ-Codec, with scalar quantization that maps speech into a finite and compact latent space, which aligns with the concept of discretizing continuous audio waveforms into discrete tokens; furthermore, it applies a transformer diffusion model on these discrete tokens for TTS, showing system design, tokenization, and usage in downstream generation tasks (TTS). The abstract mentions scalar quantization but does not detail multi-codebooks or vector quantization, yet scalar quantization still constitutes discretization, meeting core criteria for neural discrete audio tokens. The paper also meets English evaluability requirements. Therefore, it fits the inclusion criteria and does not meet any exclusions.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "cuervo2024scalingpropertiesspeechlanguag", "query_title": "{{Scaling Properties of Speech Language Models}}", "normalized_title": "scaling properties of speech language models", "title": "Scaling Properties of Speech Language Models", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.", "source": "arxiv", "source_id": "2404.00685v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Scaling Properties of Speech Language Models", "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.", "metadata": {"arxiv_id": "2404.00685v2", "title": "Scaling Properties of Speech Language Models", "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.", "authors": ["Santiago Cuervo", "Ricard Marxer"], "published": "2024-03-31T13:30:12Z", "updated": "2024-04-16T06:46:18Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.NE"], "pdf_url": "https://arxiv.org/pdf/2404.00685v2", "landing_url": "https://arxiv.org/abs/2404.00685v2", "doi": "https://doi.org/10.18653/v1/2024.emnlp-main.21"}, "round-A_JuniorNano_output": {"reasoning": "Although the abstract mentions 'coarser speech tokenization', the paper's focus is on scaling properties of speech-language models rather than proposing or evaluating discrete audio token representations; it does not provide explicit token generation details or token-level analyses required by the inclusion criteria, so it should be excluded.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "Although the abstract mentions 'coarser speech tokenization', the paper's focus is on scaling properties of speech-language models rather than proposing or evaluating discrete audio token representations; it does not provide explicit token generation details or token-level analyses required by the inclusion criteria, so it should be excluded.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper discusses Speech Language Models learning from raw audio without textual resources, focusing on model scaling and syntactic/semantic abilities improvements. However, it does not explicitly describe the generation or use of discrete audio tokens through discretization or neural audio codecs, nor does it detail tokenization processes or vocabulary/codebook settings, which are core to the inclusion criteria. Therefore, it lacks clear evidence of using discrete audio tokens as core representations or analyzing their impact, making it not suitable under the given criteria.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper discusses Speech Language Models learning from raw audio without textual resources, focusing on model scaling and syntactic/semantic abilities improvements. However, it does not explicitly describe the generation or use of discrete audio tokens through discretization or neural audio codecs, nor does it detail tokenization processes or vocabulary/codebook settings, which are core to the inclusion criteria. Therefore, it lacks clear evidence of using discrete audio tokens as core representations or analyzing their impact, making it not suitable under the given criteria.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "hassid2023textually", "query_title": "{Textually pretrained speech language models}", "normalized_title": "textually pretrained speech language models", "title": "Textually Pretrained Speech Language Models", "abstract": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .", "source": "arxiv", "source_id": "2305.13009v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Textually Pretrained Speech Language Models", "abstract": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .", "metadata": {"arxiv_id": "2305.13009v3", "title": "Textually Pretrained Speech Language Models", "summary": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .", "authors": ["Michael Hassid", "Tal Remez", "Tu Anh Nguyen", "Itai Gat", "Alexis Conneau", "Felix Kreuk", "Jade Copet", "Alexandre Defossez", "Gabriel Synnaeve", "Emmanuel Dupoux", "Roy Schwartz", "Yossi Adi"], "published": "2023-05-22T13:12:16Z", "updated": "2024-01-30T11:52:51Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2305.13009v3", "landing_url": "https://arxiv.org/abs/2305.13009v3", "doi": "https://doi.org/10.48550/arXiv.2305.13009"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on textually pretrained SpeechLMs and warm-starting from textual models, but it does not discuss discrete audio tokens, tokenizers producing discrete vocabularies, or token-level analysis essential to the inclusion criteria, so it does not meet the required topic focus.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on textually pretrained SpeechLMs and warm-starting from textual models, but it does not discuss discrete audio tokens, tokenizers producing discrete vocabularies, or token-level analysis essential to the inclusion criteria, so it does not meet the required topic focus.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on speech language models that generate acoustic data but does not explicitly describe a discretization process converting continuous audio waveforms into discrete tokens, nor detail the tokenizer/codec architecture, quantization method, or codebook/vocabulary settings critical to confirming the use of discrete audio tokens. Although it mentions a speech tokenizer and pretrained textual models, the abstract lacks sufficient specifics on how discrete tokens are generated and used as core modeling units, which is required by the inclusion criteria. Hence, it fails to meet the core requirement of describing discrete audio token generation and their role in modeling.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study focuses on speech language models that generate acoustic data but does not explicitly describe a discretization process converting continuous audio waveforms into discrete tokens, nor detail the tokenizer/codec architecture, quantization method, or codebook/vocabulary settings critical to confirming the use of discrete audio tokens. Although it mentions a speech tokenizer and pretrained textual models, the abstract lacks sufficient specifics on how discrete tokens are generated and used as core modeling units, which is required by the inclusion criteria. Hence, it fails to meet the core requirement of describing discrete audio token generation and their role in modeling.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "copet2024musicgen", "query_title": "{Simple and controllable music generation}", "normalized_title": "simple and controllable music generation", "title": "Simple and Controllable Music Generation", "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft", "source": "arxiv", "source_id": "2306.05284v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Simple and Controllable Music Generation", "abstract": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft", "metadata": {"arxiv_id": "2306.05284v3", "title": "Simple and Controllable Music Generation", "summary": "We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft", "authors": ["Jade Copet", "Felix Kreuk", "Itai Gat", "Tal Remez", "David Kant", "Gabriel Synnaeve", "Yossi Adi", "Alexandre Défossez"], "published": "2023-06-08T15:31:05Z", "updated": "2024-01-30T04:49:16Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2306.05284v3", "landing_url": "https://arxiv.org/abs/2306.05284v3", "doi": "https://doi.org/10.48550/arXiv.2306.05284"}, "round-A_JuniorNano_output": {"reasoning": "The work uses compressed discrete tokens to model music, with a single-stage transformer conditioned on text or melodic features and demonstrated token-based generation, aligning with the discrete-token inclusion criteria and not triggering any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work uses compressed discrete tokens to model music, with a single-stage transformer conditioned on text or melodic features and demonstrated token-based generation, aligning with the discrete-token inclusion criteria and not triggering any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents MusicGen, a transformer-based language model for conditional music generation using multiple streams of compressed discrete music representations (tokens). It explicitly details a single-stage transformer operating over discrete tokens, which aligns with the inclusion criteria focusing on discrete audio tokens derived from neural audio codecs and used in downstream music generation tasks. The abstract mentions empirical evaluation and ablation studies, indicating a system-level analysis. Therefore, it meets the thematic requirements and provides sufficient methodological detail in English, with publicly available code and models, supporting reproducibility and evaluation.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents MusicGen, a transformer-based language model for conditional music generation using multiple streams of compressed discrete music representations (tokens). It explicitly details a single-stage transformer operating over discrete tokens, which aligns with the inclusion criteria focusing on discrete audio tokens derived from neural audio codecs and used in downstream music generation tasks. The abstract mentions empirical evaluation and ablation studies, indicating a system-level analysis. Therefore, it meets the thematic requirements and provides sufficient methodological detail in English, with publicly available code and models, supporting reproducibility and evaluation.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "wolf2020huggingfacestransformersstateoftheartnatural", "query_title": "{HuggingFace's Transformers: State-of-the-art Natural Language Processing}", "normalized_title": "huggingface s transformers state of the art natural language processing", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{https://github.com/huggingface/transformers}.", "source": "arxiv", "source_id": "1910.03771v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{https://github.com/huggingface/transformers}.", "metadata": {"arxiv_id": "1910.03771v5", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "summary": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{https://github.com/huggingface/transformers}.", "authors": ["Thomas Wolf", "Lysandre Debut", "Victor Sanh", "Julien Chaumond", "Clement Delangue", "Anthony Moi", "Pierric Cistac", "Tim Rault", "Rémi Louf", "Morgan Funtowicz", "Joe Davison", "Sam Shleifer", "Patrick von Platen", "Clara Ma", "Yacine Jernite", "Julien Plu", "Canwen Xu", "Teven Le Scao", "Sylvain Gugger", "Mariama Drame", "Quentin Lhoest", "Alexander M. Rush"], "published": "2019-10-09T03:23:22Z", "updated": "2020-07-14T03:42:34Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1910.03771v5", "landing_url": "https://arxiv.org/abs/1910.03771v5", "doi": "https://doi.org/10.48550/arXiv.1910.03771"}, "round-A_JuniorNano_output": {"reasoning": "This item concerns a general NLP transformer library (HuggingFace Transformers) and does not discuss discrete audio tokens, tokenizers, or audio-token-based modeling, so it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item concerns a general NLP transformer library (HuggingFace Transformers) and does not discuss discrete audio tokens, tokenizers, or audio-token-based modeling, so it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article focuses on HuggingFace's Transformers library, which is primarily about transformer-based models for natural language processing, not about discrete audio tokens or their generation from continuous audio waveforms. It lacks any mention of discretization of audio, neural audio codecs, vector quantization, or the use of discrete token sequences for audio generation or understanding, which are core to the inclusion criteria. Also, the abstract does not describe any discrete token generation process or experimental evaluation related to discrete audio tokens, failing to meet the inclusion criteria and matching exclusion criteria related to continuous audio or text-based token modeling.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article focuses on HuggingFace's Transformers library, which is primarily about transformer-based models for natural language processing, not about discrete audio tokens or their generation from continuous audio waveforms. It lacks any mention of discretization of audio, neural audio codecs, vector quantization, or the use of discrete token sequences for audio generation or understanding, which are core to the inclusion criteria. Also, the abstract does not describe any discrete token generation process or experimental evaluation related to discrete audio tokens, failing to meet the inclusion criteria and matching exclusion criteria related to continuous audio or text-based token modeling.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ji2024wavtokenizer", "query_title": "{WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling}", "normalized_title": "wavtokenizer an efficient acoustic discrete codec tokenizer for audio language modeling", "title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling", "abstract": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1)extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The related code, demos, and pre-trained models are available at https://github.com/jishengpeng/WavTokenizer.", "source": "arxiv", "source_id": "2408.16532v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling", "abstract": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1)extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The related code, demos, and pre-trained models are available at https://github.com/jishengpeng/WavTokenizer.", "metadata": {"arxiv_id": "2408.16532v3", "title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling", "summary": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1)extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2)improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The related code, demos, and pre-trained models are available at https://github.com/jishengpeng/WavTokenizer.", "authors": ["Shengpeng Ji", "Ziyue Jiang", "Wen Wang", "Yifu Chen", "Minghui Fang", "Jialong Zuo", "Qian Yang", "Xize Cheng", "Zehan Wang", "Ruiqi Li", "Ziang Zhang", "Xiaoda Yang", "Rongjie Huang", "Yidi Jiang", "Qian Chen", "Siqi Zheng", "Zhou Zhao"], "published": "2024-08-29T13:43:36Z", "updated": "2025-02-25T11:45:12Z", "categories": ["eess.AS", "cs.LG", "cs.MM", "cs.SD", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2408.16532v3", "landing_url": "https://arxiv.org/abs/2408.16532v3", "doi": "https://doi.org/10.48550/arXiv.2408.16532"}, "round-A_JuniorNano_output": {"reasoning": "WavTokenizer clearly describes a discrete acoustic tokenizer/codec and demonstrates its use for downstream audio language modeling, thus meeting the inclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "WavTokenizer clearly describes a discrete acoustic tokenizer/codec and demonstrates its use for downstream audio language modeling, thus meeting the inclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes WavTokenizer, a neural audio codec tokenizer that compresses continuous audio signals into discrete tokens via vector quantization, meeting the inclusion criteria of generating discrete audio tokens for use in modeling and generation tasks; it also provides English title/abstract and sufficient methodological detail, including code availability, supporting reproducibility. The paper focuses on design choices affecting quality and efficiency, evaluates across tasks, and emphasizes the use of discrete token sequences as core representations, aligning well with the defined topic and inclusion criteria without conflicting with any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes WavTokenizer, a neural audio codec tokenizer that compresses continuous audio signals into discrete tokens via vector quantization, meeting the inclusion criteria of generating discrete audio tokens for use in modeling and generation tasks; it also provides English title/abstract and sufficient methodological detail, including code availability, supporting reproducibility. The paper focuses on design choices affecting quality and efficiency, evaluates across tasks, and emphasizes the use of discrete token sequences as core representations, aligning well with the defined topic and inclusion criteria without conflicting with any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "gpt", "query_title": "{Language Models are Few-Shot Learners}", "normalized_title": "language models are few shot learners", "title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "source": "arxiv", "source_id": "2005.14165v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Language Models are Few-Shot Learners", "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "metadata": {"arxiv_id": "2005.14165v4", "title": "Language Models are Few-Shot Learners", "summary": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.", "authors": ["Tom B. Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Ariel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel M. Ziegler", "Jeffrey Wu", "Clemens Winter", "Christopher Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei"], "published": "2020-05-28T17:29:03Z", "updated": "2020-07-22T19:47:17Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2005.14165v4", "landing_url": "https://arxiv.org/abs/2005.14165v4", "doi": "https://doi.org/10.48550/arXiv.2005.14165"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract discuss GPT-3 NLP and few-shot learning; it does not address discrete audio tokens, tokenizers, or audio codec concepts, so it does not meet the 'Discrete Audio Tokens' criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract discuss GPT-3 NLP and few-shot learning; it does not address discrete audio tokens, tokenizers, or audio codec concepts, so it does not meet the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The article focuses on the few-shot learning capabilities of large language models like GPT-3 in the NLP domain, specifically dealing with text-based language tasks rather than audio or discretized audio token modeling. It does not discuss discretizing continuous audio waveforms into discrete tokens, nor does it cover neural audio codecs, vector quantization, or any specifics about discrete audio token design, generation, or downstream audio-related tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls under exclusion criteria due to lack of relevant discrete audio token content and focus on purely text-based tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article focuses on the few-shot learning capabilities of large language models like GPT-3 in the NLP domain, specifically dealing with text-based language tasks rather than audio or discretized audio token modeling. It does not discuss discretizing continuous audio waveforms into discrete tokens, nor does it cover neural audio codecs, vector quantization, or any specifics about discrete audio token design, generation, or downstream audio-related tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls under exclusion criteria due to lack of relevant discrete audio token content and focus on purely text-based tasks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "bark2023", "query_title": "{Bark: Text-Prompted Generative Audio Model}", "normalized_title": "bark text prompted generative audio model", "title": "{Bark: Text-Prompted Generative Audio Model}", "abstract": "> Notice: Bark is Suno's open-source text-to-speech+ model. If you are looking for our text-to-music models, please visit us on our [web page](https://suno.ai) and join our community on [Discord](https://suno.ai/discord).", "source": "github", "source_id": "suno-ai/bark", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{Bark: Text-Prompted Generative Audio Model}", "abstract": "> Notice: Bark is Suno's open-source text-to-speech+ model. If you are looking for our text-to-music models, please visit us on our [web page](https://suno.ai) and join our community on [Discord](https://suno.ai/discord).", "metadata": {"arxiv_id": "github:suno-ai/bark", "title": "{Bark: Text-Prompted Generative Audio Model}", "summary": "> Notice: Bark is Suno's open-source text-to-speech+ model. If you are looking for our text-to-music models, please visit us on our [web page](https://suno.ai) and join our community on [Discord](https://suno.ai/discord).", "source": "github", "source_id": "suno-ai/bark", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "Reasoning: The item is about Bark TTS and does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Reasoning: The item is about Bark TTS and does not discuss discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract simply describes Bark as an open-source text-to-speech model without any details on discrete audio tokens, neural audio codec architectures, quantization, vocabulary setups, or any relevant experimental or design details related to discretization of audio tokens, thus failing to meet the inclusion requirements and lacking sufficient English technical details.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract simply describes Bark as an open-source text-to-speech model without any details on discrete audio tokens, neural audio codec architectures, quantization, vocabulary setups, or any relevant experimental or design details related to discretization of audio tokens, thus failing to meet the inclusion requirements and lacking sufficient English technical details.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "guo2025recent", "query_title": "{Recent Advances in Discrete Speech Tokens: A Review}", "normalized_title": "recent advances in discrete speech tokens a review", "title": "Recent Advances in Discrete Speech Tokens: A Review", "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "source": "arxiv", "source_id": "2502.06490v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Recent Advances in Discrete Speech Tokens: A Review", "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "metadata": {"arxiv_id": "2502.06490v4", "title": "Recent Advances in Discrete Speech Tokens: A Review", "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.", "authors": ["Yiwei Guo", "Zhihan Li", "Hankun Wang", "Bohan Li", "Chongtian Shao", "Hanglei Zhang", "Chenpeng Du", "Xie Chen", "Shujie Liu", "Kai Yu"], "published": "2025-02-10T14:08:25Z", "updated": "2025-12-12T05:18:11Z", "categories": ["eess.AS", "cs.AI", "cs.MM", "cs.SD", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2502.06490v4", "landing_url": "https://arxiv.org/abs/2502.06490v4", "doi": "https://doi.org/10.48550/arXiv.2502.06490"}, "final_verdict": "include (seed_filter)", "review_skipped": true, "discard_reason": null, "force_include_reason": "seed_filter_selected"}]}
{"key": "zaiem2023speech", "query_title": "{Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?}", "normalized_title": "speech self supervised representation benchmarking are we doing it right", "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?", "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.", "source": "arxiv", "source_id": "2306.00452v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?", "abstract": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.", "metadata": {"arxiv_id": "2306.00452v1", "title": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?", "summary": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data. The high number of proposed approaches fostered the need and rise of extended benchmarks that evaluate their performance on a set of downstream tasks exploring various aspects of the speech signal. However, and while the number of considered tasks has been growing, most rely upon a single decoding architecture that maps the frozen SSL representations to the downstream labels. This work investigates the robustness of such benchmarking results to changes in the decoder architecture. Interestingly, it appears that varying the architecture of the downstream decoder leads to significant variations in the leaderboards of most tasks. Concerningly, our study reveals that benchmarking using limited decoders may cause a counterproductive increase in the sizes of the developed SSL models.", "authors": ["Salah Zaiem", "Youcef Kemiche", "Titouan Parcollet", "Slim Essid", "Mirco Ravanelli"], "published": "2023-06-01T08:51:18Z", "updated": "2023-06-01T08:51:18Z", "categories": ["eess.AS", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2306.00452v1", "landing_url": "https://arxiv.org/abs/2306.00452v1", "doi": "https://doi.org/10.48550/arXiv.2306.00452"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract discuss benchmarking SSL representations and decoder robustness, not discrete audio tokens or tokenization architectures; it lacks any discrete token generation, tokenizer/codec details, or token-level analyses required by the ‘Discrete Audio Tokens’ criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract discuss benchmarking SSL representations and decoder robustness, not discrete audio tokens or tokenization architectures; it lacks any discrete token generation, tokenizer/codec details, or token-level analyses required by the ‘Discrete Audio Tokens’ criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on self-supervised speech representations and evaluates benchmarking methodologies rather than proposing or analyzing discrete audio tokens derived via discretization or neural audio codecs; it lacks mention of discrete token production, vocabulary, or utilization in generation or cross-modal tasks, and thus does not meet the inclusion criteria focused on discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representations and evaluates benchmarking methodologies rather than proposing or analyzing discrete audio tokens derived via discretization or neural audio codecs; it lacks mention of discrete token production, vocabulary, or utilization in generation or cross-modal tasks, and thus does not meet the inclusion criteria focused on discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "watanabe2023tree", "query_title": "{Tree-structured parzen estimator: Understanding its algorithm components and their roles for better empirical performance}", "normalized_title": "tree structured parzen estimator understanding its algorithm components and their roles for better empirical performance", "title": "Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance", "abstract": "Recent scientific advances require complex experiment design, necessitating the meticulous tuning of many experiment parameters. Tree-structured Parzen estimator (TPE) is a widely used Bayesian optimization method in recent parameter tuning frameworks such as Hyperopt and Optuna. Despite its popularity, the roles of each control parameter in TPE and the algorithm intuition have not been discussed so far. The goal of this paper is to identify the roles of each control parameter and their impacts on parameter tuning based on the ablation studies using diverse benchmark datasets. The recommended setting concluded from the ablation studies is demonstrated to improve the performance of TPE. Our TPE implementation used in this paper is available at https://github.com/nabenabe0928/tpe/tree/single-opt.", "source": "arxiv", "source_id": "2304.11127v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance", "abstract": "Recent scientific advances require complex experiment design, necessitating the meticulous tuning of many experiment parameters. Tree-structured Parzen estimator (TPE) is a widely used Bayesian optimization method in recent parameter tuning frameworks such as Hyperopt and Optuna. Despite its popularity, the roles of each control parameter in TPE and the algorithm intuition have not been discussed so far. The goal of this paper is to identify the roles of each control parameter and their impacts on parameter tuning based on the ablation studies using diverse benchmark datasets. The recommended setting concluded from the ablation studies is demonstrated to improve the performance of TPE. Our TPE implementation used in this paper is available at https://github.com/nabenabe0928/tpe/tree/single-opt.", "metadata": {"arxiv_id": "2304.11127v4", "title": "Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance", "summary": "Recent scientific advances require complex experiment design, necessitating the meticulous tuning of many experiment parameters. Tree-structured Parzen estimator (TPE) is a widely used Bayesian optimization method in recent parameter tuning frameworks such as Hyperopt and Optuna. Despite its popularity, the roles of each control parameter in TPE and the algorithm intuition have not been discussed so far. The goal of this paper is to identify the roles of each control parameter and their impacts on parameter tuning based on the ablation studies using diverse benchmark datasets. The recommended setting concluded from the ablation studies is demonstrated to improve the performance of TPE. Our TPE implementation used in this paper is available at https://github.com/nabenabe0928/tpe/tree/single-opt.", "authors": ["Shuhei Watanabe"], "published": "2023-04-21T17:02:38Z", "updated": "2025-09-30T15:18:43Z", "categories": ["cs.LG", "cs.AI"], "pdf_url": "https://arxiv.org/pdf/2304.11127v4", "landing_url": "https://arxiv.org/abs/2304.11127v4", "doi": "https://doi.org/10.48550/arXiv.2304.11127"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract discuss Tree-Structured Parzen Estimator for hyperparameter optimization, focusing on algorithm components, not discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract discuss Tree-Structured Parzen Estimator for hyperparameter optimization, focusing on algorithm components, not discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on tuning parameters for the Tree-structured Parzen Estimator algorithm, a Bayesian optimization method, with no mention of discrete audio token generation, neural audio codecs, or any audio discretization process related to modeling or generation of audio as token sequences. It does not discuss the creation, usage, or evaluation of discrete audio tokens and thus does not meet the inclusion criteria related to discrete audio tokens representation or application.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on tuning parameters for the Tree-structured Parzen Estimator algorithm, a Bayesian optimization method, with no mention of discrete audio token generation, neural audio codecs, or any audio discretization process related to modeling or generation of audio as token sequences. It does not discuss the creation, usage, or evaluation of discrete audio tokens and thus does not meet the inclusion criteria related to discrete audio tokens representation or application.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "xavier_bouthillier_2022_0_2_6", "query_title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "normalized_title": "epistimio orion asynchronous distributed hyperparameter optimization", "title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "abstract": "***** Oríon *****", "source": "github", "source_id": "Epistimio/orion", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "abstract": "***** Oríon *****", "metadata": {"arxiv_id": "github:Epistimio/orion", "title": "{{Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization}}", "summary": "***** Oríon *****", "source": "github", "source_id": "Epistimio/orion", "year": 2017}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe asynchronous distributed hyperparameter optimization (Oríon), which is unrelated to Discrete Audio Tokens and tokenization; thus it does not meet inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe asynchronous distributed hyperparameter optimization (Oríon), which is unrelated to Discrete Audio Tokens and tokenization; thus it does not meet inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title 'Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization' and the abstract '***** Oríon *****' provide insufficient information and do not mention any aspects related to discrete audio tokens, neural audio codecs, or related tokenization processes required by the inclusion criteria. It does not meet any inclusion criteria and likely fits the exclusion for lack of sufficient English key information and relevance to discrete audio token studies.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title 'Epistimio/orion: Asynchronous Distributed Hyperparameter Optimization' and the abstract '***** Oríon *****' provide insufficient information and do not mention any aspects related to discrete audio tokens, neural audio codecs, or related tokenization processes required by the inclusion criteria. It does not meet any inclusion criteria and likely fits the exclusion for lack of sufficient English key information and relevance to discrete audio token studies.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mentzer2023finite", "query_title": "{Finite Scalar Quantization: VQ-VAE Made Simple}", "normalized_title": "finite scalar quantization vq vae made simple", "title": "Finite Scalar Quantization: VQ-VAE Made Simple", "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.", "source": "arxiv", "source_id": "2309.15505v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Finite Scalar Quantization: VQ-VAE Made Simple", "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.", "metadata": {"arxiv_id": "2309.15505v2", "title": "Finite Scalar Quantization: VQ-VAE Made Simple", "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.", "authors": ["Fabian Mentzer", "David Minnen", "Eirikur Agustsson", "Michael Tschannen"], "published": "2023-09-27T09:13:40Z", "updated": "2023-10-12T07:55:05Z", "categories": ["cs.CV", "cs.LG"], "pdf_url": "https://arxiv.org/pdf/2309.15505v2", "landing_url": "https://arxiv.org/abs/2309.15505v2", "doi": "https://doi.org/10.48550/arXiv.2309.15505"}, "round-A_JuniorNano_output": {"reasoning": "The paper proposes finite scalar quantization (FSQ) to replace VQ-VAE discretization in latent representations for image-related tasks, not addressing discrete audio tokens or audio-domain tokenization, so it does not meet the Discrete Audio Tokens criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper proposes finite scalar quantization (FSQ) to replace VQ-VAE discretization in latent representations for image-related tasks, not addressing discrete audio tokens or audio-domain tokenization, so it does not meet the Discrete Audio Tokens criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses finite scalar quantization methods within VQ-VAE for image and vision tasks without mentioning any audio, speech, or environmental sound waveform discretization or tokenization relevant to discrete audio tokens. It focuses on image generation and computer vision applications rather than audio, and does not provide details on neural audio codec or audio tokenization processes required by the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses finite scalar quantization methods within VQ-VAE for image and vision tasks without mentioning any audio, speech, or environmental sound waveform discretization or tokenization relevant to discrete audio tokens. It focuses on image generation and computer vision applications rather than audio, and does not provide details on neural audio codec or audio tokenization processes required by the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "du2024cosyvoiceAS", "query_title": "{CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens}", "normalized_title": "cosyvoice a scalable multilingual zero shot text to speech synthesizer based on supervised semantic tokens", "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "source": "arxiv", "source_id": "2407.05407v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "metadata": {"arxiv_id": "2407.05407v2", "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "authors": ["Zhihao Du", "Qian Chen", "Shiliang Zhang", "Kai Hu", "Heng Lu", "Yexin Yang", "Hangrui Hu", "Siqi Zheng", "Yue Gu", "Ziyang Ma", "Zhifu Gao", "Zhijie Yan"], "published": "2024-07-07T15:16:19Z", "updated": "2024-07-09T07:42:51Z", "categories": ["cs.SD", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2407.05407v2", "landing_url": "https://arxiv.org/abs/2407.05407v2", "doi": "https://doi.org/10.48550/arXiv.2407.05407"}, "round-A_JuniorNano_output": {"reasoning": "The item explicitly uses supervised semantic tokens derived from a neural/audio codec with vector quantization to represent speech as discrete tokens and builds a TTS system (LLM-based text-to-token generation and token-to-speech synthesis) around these tokens, targeting token-based generation and cross-modality modeling, which aligns with the discrete audio token criteria and excludes non-token-focused work.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The item explicitly uses supervised semantic tokens derived from a neural/audio codec with vector quantization to represent speech as discrete tokens and builds a TTS system (LLM-based text-to-token generation and token-to-speech synthesis) around these tokens, targeting token-based generation and cross-modality modeling, which aligns with the discrete audio token criteria and excludes non-token-focused work.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on discrete audio tokens derived from a multilingual speech recognition model with vector quantization, used in a text-to-speech synthesizer. The tokens are explicitly described as supervised semantic tokens, indicating a detailed discrete token generation process using neural audio codec techniques. The work contributes a new approach to discrete audio tokens and evaluates their impact on TTS quality and zero-shot voice cloning, aligning well with inclusion criteria emphasizing token design, evaluation, and applications in generation tasks. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on discrete audio tokens derived from a multilingual speech recognition model with vector quantization, used in a text-to-speech synthesizer. The tokens are explicitly described as supervised semantic tokens, indicating a detailed discrete token generation process using neural audio codec techniques. The work contributes a new approach to discrete audio tokens and evaluates their impact on TTS quality and zero-shot voice cloning, aligning well with inclusion criteria emphasizing token design, evaluation, and applications in generation tasks. Therefore, it meets the inclusion criteria and does not fall under any exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "du2024cosyvoice", "query_title": "{Cosyvoice 2: Scalable streaming speech synthesis with large language models}", "normalized_title": "cosyvoice 2 scalable streaming speech synthesis with large language models", "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.", "source": "arxiv", "source_id": "2412.10117v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.", "metadata": {"arxiv_id": "2412.10117v3", "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models", "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.", "authors": ["Zhihao Du", "Yuxuan Wang", "Qian Chen", "Xian Shi", "Xiang Lv", "Tianyu Zhao", "Zhifu Gao", "Yexin Yang", "Changfeng Gao", "Hui Wang", "Fan Yu", "Huadai Liu", "Zhengyan Sheng", "Yue Gu", "Chong Deng", "Wen Wang", "Shiliang Zhang", "Zhijie Yan", "Jingren Zhou"], "published": "2024-12-13T12:59:39Z", "updated": "2024-12-25T11:54:03Z", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2412.10117v3", "landing_url": "https://arxiv.org/abs/2412.10117v3", "doi": "https://doi.org/10.48550/arXiv.2412.10117"}, "round-A_JuniorNano_output": {"reasoning": "The abstract explicitly discusses discrete speech tokens, finite-scalar quantization, and codebooks in the context of speech synthesis, indicating token-level representation and potential downstream modeling, which matches the criteria for discrete audio tokens and token-based systems.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete speech tokens, finite-scalar quantization, and codebooks in the context of speech synthesis, indicating token-level representation and potential downstream modeling, which matches the criteria for discrete audio tokens and token-based systems.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a multilingual speech synthesis model, CosyVoice 2, which builds upon discrete speech tokens generated through supervised discrete speech tokenization, likely produced by neural audio encoding techniques. It discusses improvements in token utilization, model architecture to leverage large language models, and addresses streaming synthesis scenarios, aligning well with the inclusion criteria related to discrete audio token design and application in TTS using token sequences. The abstract provides sufficient technical details about quantization, token optimization, the use of a large-scale multilingual dataset, and synthesis quality, offering good English evaluability. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on a multilingual speech synthesis model, CosyVoice 2, which builds upon discrete speech tokens generated through supervised discrete speech tokenization, likely produced by neural audio encoding techniques. It discusses improvements in token utilization, model architecture to leverage large language models, and addresses streaming synthesis scenarios, aligning well with the inclusion criteria related to discrete audio token design and application in TTS using token sequences. The abstract provides sufficient technical details about quantization, token optimization, the use of a large-scale multilingual dataset, and synthesis quality, offering good English evaluability. Therefore, it meets all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "tong2023improving", "query_title": "{Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport}", "normalized_title": "improving and generalizing flow based generative models with minibatch optimal transport", "title": "Improving and generalizing flow-based generative models with minibatch optimal transport", "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference.", "source": "arxiv", "source_id": "2302.00482v4", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Improving and generalizing flow-based generative models with minibatch optimal transport", "abstract": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference.", "metadata": {"arxiv_id": "2302.00482v4", "title": "Improving and generalizing flow-based generative models with minibatch optimal transport", "summary": "Continuous normalizing flows (CNFs) are an attractive generative modeling technique, but they have been held back by limitations in their simulation-based maximum likelihood training. We introduce the generalized conditional flow matching (CFM) technique, a family of simulation-free training objectives for CNFs. CFM features a stable regression objective like that used to train the stochastic flow in diffusion models but enjoys the efficient inference of deterministic flow models. In contrast to both diffusion models and prior CNF training algorithms, CFM does not require the source distribution to be Gaussian or require evaluation of its density. A variant of our objective is optimal transport CFM (OT-CFM), which creates simpler flows that are more stable to train and lead to faster inference, as evaluated in our experiments. Furthermore, we show that when the true OT plan is available, our OT-CFM method approximates dynamic OT. Training CNFs with CFM improves results on a variety of conditional and unconditional generation tasks, such as inferring single cell dynamics, unsupervised image translation, and Schrödinger bridge inference.", "authors": ["Alexander Tong", "Kilian Fatras", "Nikolay Malkin", "Guillaume Huguet", "Yanlei Zhang", "Jarrid Rector-Brooks", "Guy Wolf", "Yoshua Bengio"], "published": "2023-02-01T14:47:17Z", "updated": "2024-03-11T14:27:48Z", "categories": ["cs.LG"], "pdf_url": "https://arxiv.org/pdf/2302.00482v4", "landing_url": "https://arxiv.org/abs/2302.00482v4", "doi": "https://doi.org/10.48550/arXiv.2302.00482"}, "round-A_JuniorNano_output": {"reasoning": "The paper discusses continuous normalizing flows and optimal transport for generative modeling and contains no discussion of discrete audio tokens or tokenization, thus it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper discusses continuous normalizing flows and optimal transport for generative modeling and contains no discussion of discrete audio tokens or tokenization, thus it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents advancements in flow-based generative models for continuous data without addressing discretization of audio signals into token sequences, nor does it discuss neural audio codecs, vector quantization, or token-based modeling relevant to discrete audio tokens. It therefore does not meet the inclusion criteria requiring discrete audio token generation or usage and focuses on continuous normalizing flows rather than discrete audio token representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents advancements in flow-based generative models for continuous data without addressing discretization of audio signals into token sequences, nor does it discuss neural audio codecs, vector quantization, or token-based modeling relevant to discrete audio tokens. It therefore does not meet the inclusion criteria requiring discrete audio token generation or usage and focuses on continuous normalizing flows rather than discrete audio token representation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ho2020denoising", "query_title": "{Denoising diffusion probabilistic models}", "normalized_title": "denoising diffusion probabilistic models", "title": "Denoising Diffusion Probabilistic Models", "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion", "source": "arxiv", "source_id": "2006.11239v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Denoising Diffusion Probabilistic Models", "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion", "metadata": {"arxiv_id": "2006.11239v2", "title": "Denoising Diffusion Probabilistic Models", "summary": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion", "authors": ["Jonathan Ho", "Ajay Jain", "Pieter Abbeel"], "published": "2020-06-19T17:24:44Z", "updated": "2020-12-16T21:15:05Z", "categories": ["cs.LG", "stat.ML"], "pdf_url": "https://arxiv.org/pdf/2006.11239v2", "landing_url": "https://arxiv.org/abs/2006.11239v2", "doi": "https://doi.org/10.48550/arXiv.2006.11239"}, "round-A_JuniorNano_output": {"reasoning": "The abstract discusses image diffusion models and image synthesis, with no mention of discrete audio tokens, tokenization, or audio-domain token generation/analysis, thus it does not meet the inclusion criteria for the discrete audio token topic.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract discusses image diffusion models and image synthesis, with no mention of discrete audio tokens, tokenization, or audio-domain token generation/analysis, thus it does not meet the inclusion criteria for the discrete audio token topic.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on diffusion probabilistic models for image synthesis, without any mention of discrete audio tokens or audio-related discrete token representations, nor does it discuss neural audio codecs, vector quantization, or tokenization of audio data. Thus, it does not meet the inclusion criteria related to discrete audio tokens and is centered on continuous image data modeling rather than discrete audio token sequences.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on diffusion probabilistic models for image synthesis, without any mention of discrete audio tokens or audio-related discrete token representations, nor does it discuss neural audio codecs, vector quantization, or tokenization of audio data. Thus, it does not meet the inclusion criteria related to discrete audio tokens and is centered on continuous image data modeling rather than discrete audio token sequences.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "rombach2022high", "query_title": "{High-resolution image synthesis with latent diffusion models}", "normalized_title": "high resolution image synthesis with latent diffusion models", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .", "source": "arxiv", "source_id": "2112.10752v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "High-Resolution Image Synthesis with Latent Diffusion Models", "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .", "metadata": {"arxiv_id": "2112.10752v2", "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "summary": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .", "authors": ["Robin Rombach", "Andreas Blattmann", "Dominik Lorenz", "Patrick Esser", "Björn Ommer"], "published": "2021-12-20T18:55:25Z", "updated": "2022-04-13T11:38:44Z", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/2112.10752v2", "landing_url": "https://arxiv.org/abs/2112.10752v2", "doi": "https://doi.org/10.48550/arXiv.2112.10752"}, "round-A_JuniorNano_output": {"reasoning": "This item focuses on image diffusion in latent space and does not involve discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item focuses on image diffusion in latent space and does not involve discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract focus on high-resolution image synthesis using latent diffusion models and do not mention any discretization of continuous audio waveforms into discrete audio tokens, neural audio codecs, or audio-related sequence modeling; thus, they do not meet the inclusion criteria focused on discrete audio tokens and are unrelated to audio processing or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract focus on high-resolution image synthesis using latent diffusion models and do not mention any discretization of continuous audio waveforms into discrete audio tokens, neural audio codecs, or audio-related sequence modeling; thus, they do not meet the inclusion criteria focused on discrete audio tokens and are unrelated to audio processing or generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kim2024neural", "query_title": "{Neural speech and audio coding}", "normalized_title": "neural speech and audio coding", "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs", "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "source": "arxiv", "source_id": "2408.06954", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs", "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "metadata": {"arxiv_id": "2408.06954", "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs", "summary": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.", "authors": ["Minje Kim", "Jan Skoglund"], "published": "2024-08-13T15:13:21Z", "updated": "2025-01-07T04:11:55Z", "categories": ["cs.SD", "cs.AI", "eess.AS", "eess.SP"], "pdf_url": "https://arxiv.org/pdf/2408.06954v2", "landing_url": "https://arxiv.org/abs/2408.06954v2", "doi": "https://doi.org/10.1109/MSP.2024.3444318"}, "round-A_JuniorNano_output": {"reasoning": "The item discusses neural and traditional audio coding with hybrid systems, but does not explicitly focus on discrete audio tokens or tokenization (no clear token generation/quantization schemes described), thus it does not satisfy the token-centric inclusion criteria and likely falls under exclusion.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The item discusses neural and traditional audio coding with hybrid systems, but does not explicitly focus on discrete audio tokens or tokenization (no clear token generation/quantization schemes described), thus it does not satisfy the token-centric inclusion criteria and likely falls under exclusion.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on integrating model-based and data-driven approaches for neural speech and audio coding systems, specifically enhancing traditional codecs with neural network components. However, it does not explicitly discuss the generation or use of discrete audio tokens derived via discretization for sequence modeling, nor does it detail a tokenizer or discrete token vocabulary, which are central to the inclusion criteria focused on discrete token representations. The study centers on coding and enhancement of audio signals rather than on tokenization or downstream tasks involving discrete tokens. Therefore, it does not meet the necessary criterion of using or analyzing discrete audio tokens as core representations.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper focuses on integrating model-based and data-driven approaches for neural speech and audio coding systems, specifically enhancing traditional codecs with neural network components. However, it does not explicitly discuss the generation or use of discrete audio tokens derived via discretization for sequence modeling, nor does it detail a tokenizer or discrete token vocabulary, which are central to the inclusion criteria focused on discrete token representations. The study centers on coding and enhancement of audio signals rather than on tokenization or downstream tasks involving discrete tokens. Therefore, it does not meet the necessary criterion of using or analyzing discrete audio tokens as core representations.", "round-A_JuniorMini_evaluation": 2, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "anees2024speech", "query_title": "{Speech coding techniques and challenges: A comprehensive literature survey}", "normalized_title": "speech coding techniques and challenges a comprehensive literature survey", "title": "Speech coding techniques and challenges: a comprehensive literature survey", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "source": "semantic_scholar", "source_id": "ba02cf95af70c5e29e26ad4db7f59360e85a3786", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Speech coding techniques and challenges: a comprehensive literature survey", "abstract": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "metadata": {"arxiv_id": "semantic_scholar:ba02cf95af70c5e29e26ad4db7f59360e85a3786", "title": "Speech coding techniques and challenges: a comprehensive literature survey", "summary": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.", "source": "semantic_scholar", "source_id": "ba02cf95af70c5e29e26ad4db7f59360e85a3786", "publication_date": "2023-09-14"}, "round-A_JuniorNano_output": {"reasoning": "The item is about region proposal networks for object detection in computer vision, with no discussion of discrete audio tokens, neural audio codecs, or token design; it does not meet the audio-token inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is about region proposal networks for object detection in computer vision, with no discussion of discrete audio tokens, neural audio codecs, or token design; it does not meet the audio-token inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title suggests a review of speech coding techniques, which might relate to audio tokenization. However, the abstract details a method for object detection in images using Region Proposal Networks, with no mention of audio, discrete audio tokens, or any related concepts such as neural audio codecs or tokenization processes. The abstract clearly focuses on computer vision tasks rather than audio tokenization or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title suggests a review of speech coding techniques, which might relate to audio tokenization. However, the abstract details a method for object detection in images using Region Proposal Networks, with no mention of audio, discrete audio tokens, or any related concepts such as neural audio codecs or tokenization processes. The abstract clearly focuses on computer vision tasks rather than audio tokenization or generation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "shannon1948mathematical", "query_title": "{A mathematical theory of communication}", "normalized_title": "a mathematical theory of communication", "title": "A Mathematical Theory of Communication", "abstract": "The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> and Hartley <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.", "source": "semantic_scholar", "source_id": "6d12a1d23b21a9b170118a56386552bc5d4727de", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "A Mathematical Theory of Communication", "abstract": "The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> and Hartley <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.", "metadata": {"arxiv_id": "semantic_scholar:6d12a1d23b21a9b170118a56386552bc5d4727de", "title": "A Mathematical Theory of Communication", "summary": "The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup> and Hartley <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sup> on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.", "source": "semantic_scholar", "source_id": "6d12a1d23b21a9b170118a56386552bc5d4727de", "year": 2006}, "round-A_JuniorNano_output": {"reasoning": "The abstract describes a general information-theoretic theory of communication with noise (Nyquist/Hartley lineage) rather than discrete audio tokenization or token-based modeling; there is no discussion of producing or using discrete tokens/codebooks for audio, so it does not meet the discrete audio tokens criteria and aligns with the exclusion of non-token-centric continuous representations.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract describes a general information-theoretic theory of communication with noise (Nyquist/Hartley lineage) rather than discrete audio tokenization or token-based modeling; there is no discussion of producing or using discrete tokens/codebooks for audio, so it does not meet the discrete audio tokens criteria and aligns with the exclusion of non-token-centric continuous representations.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper 'A Mathematical Theory of Communication' focuses on general communication theory and signal modulation methods rather than discrete audio tokenization for audio modeling or generation; it lacks any discussion of neural audio codecs, discrete token generation, or downstream tasks related to discrete audio token sequences.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper 'A Mathematical Theory of Communication' focuses on general communication theory and signal modulation methods rather than discrete audio tokenization for audio modeling or generation; it lacks any discussion of neural audio codecs, discrete token generation, or downstream tasks related to discrete audio token sequences.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "nyquist1928certain", "query_title": "{Certain topics in telegraph transmission theory}", "normalized_title": "certain topics in telegraph transmission theory", "title": "Certain Topics in Telegraph Transmission Theory", "abstract": "The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.", "source": "semantic_scholar", "source_id": "db0172576316dc748aea82e8f13fb4719ac933d5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Certain Topics in Telegraph Transmission Theory", "abstract": "The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.", "metadata": {"arxiv_id": "semantic_scholar:db0172576316dc748aea82e8f13fb4719ac933d5", "title": "Certain Topics in Telegraph Transmission Theory", "summary": "The most obvious method for determining the distortion of telegraph signals is to calculate the transients of the telegraph system. This method has been treated by various writers, and solutions are available for telegraph lines with simple terminal conditions. It is well known that the extension of the same methods to more complicated terminal conditions, which represent the usual terminal apparatus, leads to great difficulties. The present paper attacks the same problem from the alternative standpoint of the steady-state characteristics of the system. This method has the advantage over the method of transients that the complication of the circuit which results from the use of terminal apparatus does not complicate the calculations materially. This method of treatment necessitates expressing the criteria of distortionless transmission in terms of the steady-state characteristics. Accordingly, a considerable portion of the paper describes and illustrates a method for making this translation. A discussion is given of the minimum frequency range required for transmission at a given speed of signaling. In the case of carrier telegraphy, this discussion includes a comparison of single-sideband and double-sideband transmission. A number of incidental topics is also discussed.", "source": "semantic_scholar", "source_id": "db0172576316dc748aea82e8f13fb4719ac933d5", "publication_date": "1928-04-01"}, "round-A_JuniorNano_output": {"reasoning": "The article targets telegraph transmission theory with transient vs steady-state analysis and does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article targets telegraph transmission theory with transient vs steady-state analysis and does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper discusses theoretical aspects of telegraph transmission, focusing on signal distortion and steady-state characteristics without any mention of discretizing continuous audio into discrete tokens, neural audio codecs, or token-based modeling relevant to audio generation or cross-modal reasoning as specified in the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper discusses theoretical aspects of telegraph transmission, focusing on signal distortion and steady-state characteristics without any mention of discretizing continuous audio into discrete tokens, neural audio codecs, or token-based modeling relevant to audio generation or cross-modal reasoning as specified in the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "o1988linear", "query_title": "{Linear predictive coding}", "normalized_title": "linear predictive coding", "title": "{Linear predictive coding}", "abstract": "The basic principles of linear predictive coding (LPC) are presented. Least-squares methods for obtaining the LPC coefficients characterizing the all-pole filter are described. Computational factors, instantaneous updating, and spectral estimation are discussed.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>", "source": "openalex", "source_id": null, "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{Linear predictive coding}", "abstract": "The basic principles of linear predictive coding (LPC) are presented. Least-squares methods for obtaining the LPC coefficients characterizing the all-pole filter are described. Computational factors, instantaneous updating, and spectral estimation are discussed.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>", "metadata": {"arxiv_id": "openalex:unknown", "title": "{Linear predictive coding}", "summary": "The basic principles of linear predictive coding (LPC) are presented. Least-squares methods for obtaining the LPC coefficients characterizing the all-pole filter are described. Computational factors, instantaneous updating, and spectral estimation are discussed.< <ETX xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">&gt;</ETX>", "source": "openalex", "source_id": "", "year": 1988}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe linear predictive coding and continuous representations, not discrete audio tokens or tokenization methods, so it does not meet the discrete-audio-token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe linear predictive coding and continuous representations, not discrete audio tokens or tokenization methods, so it does not meet the discrete-audio-token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes linear predictive coding, which pertains to continuous audio signal processing without mention of discretization into tokens or neural audio codecs, and lacks detail on discrete token design or usage required by the criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes linear predictive coding, which pertains to continuous audio signal processing without mention of discretization into tokens or neural audio codecs, and lacks detail on discrete token design or usage required by the criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "wang2003modified", "query_title": "{Modified discrete cosine transform: Its implications for audio coding and error concealment}", "normalized_title": "modified discrete cosine transform its implications for audio coding and error concealment", "title": "Modified Discrete Cosine Transform: Its Implications for Audio Coding and Error Concealment", "abstract": "A study of the modified discrete cosine transform (MDCT) and its implications for audio coding and error concealment is presented from the perspective of Fourier frequency analysis. A relationship between MDCT and DFT via shifted discrete fourier transform (SDFT) is established, which provides a possible fast implementation of MDCT employing a fast Fourier transform (FFT) routine. The concept of time-domain alias cancellation (TDAC), the symmetric and nonorthogonal properties of MDCT, is analyzed and illustrated with intuitive examples. New insights are given for innovative solutions in audio codec design and MDCT-domain audio processing such as error concealment.", "source": "semantic_scholar", "source_id": "042aa1735de37d4b17972b795c412d572e0b32c9", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Modified Discrete Cosine Transform: Its Implications for Audio Coding and Error Concealment", "abstract": "A study of the modified discrete cosine transform (MDCT) and its implications for audio coding and error concealment is presented from the perspective of Fourier frequency analysis. A relationship between MDCT and DFT via shifted discrete fourier transform (SDFT) is established, which provides a possible fast implementation of MDCT employing a fast Fourier transform (FFT) routine. The concept of time-domain alias cancellation (TDAC), the symmetric and nonorthogonal properties of MDCT, is analyzed and illustrated with intuitive examples. New insights are given for innovative solutions in audio codec design and MDCT-domain audio processing such as error concealment.", "metadata": {"arxiv_id": "semantic_scholar:042aa1735de37d4b17972b795c412d572e0b32c9", "title": "Modified Discrete Cosine Transform: Its Implications for Audio Coding and Error Concealment", "summary": "A study of the modified discrete cosine transform (MDCT) and its implications for audio coding and error concealment is presented from the perspective of Fourier frequency analysis. A relationship between MDCT and DFT via shifted discrete fourier transform (SDFT) is established, which provides a possible fast implementation of MDCT employing a fast Fourier transform (FFT) routine. The concept of time-domain alias cancellation (TDAC), the symmetric and nonorthogonal properties of MDCT, is analyzed and illustrated with intuitive examples. New insights are given for innovative solutions in audio codec design and MDCT-domain audio processing such as error concealment.", "source": "semantic_scholar", "source_id": "042aa1735de37d4b17972b795c412d572e0b32c9", "publication_date": "2003-01-15"}, "round-A_JuniorNano_output": {"reasoning": "The article focuses on the Modified Discrete Cosine Transform in traditional audio coding and error concealment, without discussing discrete audio tokens, tokenization methods, or token-based modeling as core elements.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article focuses on the Modified Discrete Cosine Transform in traditional audio coding and error concealment, without discussing discrete audio tokens, tokenization methods, or token-based modeling as core elements.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study discusses the Modified Discrete Cosine Transform (MDCT) for audio coding and error concealment but does not describe generating discrete audio tokens or mapping continuous audio into discrete token sequences usable by sequence models like Transformers or LLMs. There is no mention of neural audio codecs, vector quantization, or discrete token design, nor does it present evaluations related to token-based audio generation or understanding tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study discusses the Modified Discrete Cosine Transform (MDCT) for audio coding and error concealment but does not describe generating discrete audio tokens or mapping continuous audio into discrete token sequences usable by sequence models like Transformers or LLMs. There is no mention of neural audio codecs, vector quantization, or discrete token design, nor does it present evaluations related to token-based audio generation or understanding tasks.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "noll1997mpeg", "query_title": "{MPEG digital audio coding}", "normalized_title": "mpeg digital audio coding", "title": "MPEG digital audio coding", "abstract": "The Moving Pictures Expert Group (MPEG) within the International Organization of Standardization (ISO) has developed a series of audio-visual standards known as MFEG-1 and MPEG-2. These audio-coding standards are the first international standards in the field of high-quality digital audio compression. MPEG-1 covers coding of stereophonic audio signals at high sampling rates aiming at transparent quality, whereas MPEG-2 also offers stereophonic audio coding at lower sampling rates. In addition, MPEG-2 introduces multichannel coding with and without backwards compatibility to MPEG-1 to provide an improved acoustical image for audio-only applications and for enhanced television and video-conferencing systems. MPEG-2 audio coding without backwards compatibility, called IMPEG-2 Advanced Audio Coding (AAC), offers the highest compression rates. Typical application areas for MPEG-based digital audio are in the fields of audio production, program distribution and exchange, digital sound broadcasting, digital storage, and various multimedia applications. We describe in some detail the key technologies and main features of MPEG-1 and MPEG-2 audio coders. We also present the MPEG-4 standard and discuss some of the typical applications for MPEG audio compression.", "source": "semantic_scholar", "source_id": "64dd904dc1a8eeb84dae812335a4d46b202f7f4b", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MPEG digital audio coding", "abstract": "The Moving Pictures Expert Group (MPEG) within the International Organization of Standardization (ISO) has developed a series of audio-visual standards known as MFEG-1 and MPEG-2. These audio-coding standards are the first international standards in the field of high-quality digital audio compression. MPEG-1 covers coding of stereophonic audio signals at high sampling rates aiming at transparent quality, whereas MPEG-2 also offers stereophonic audio coding at lower sampling rates. In addition, MPEG-2 introduces multichannel coding with and without backwards compatibility to MPEG-1 to provide an improved acoustical image for audio-only applications and for enhanced television and video-conferencing systems. MPEG-2 audio coding without backwards compatibility, called IMPEG-2 Advanced Audio Coding (AAC), offers the highest compression rates. Typical application areas for MPEG-based digital audio are in the fields of audio production, program distribution and exchange, digital sound broadcasting, digital storage, and various multimedia applications. We describe in some detail the key technologies and main features of MPEG-1 and MPEG-2 audio coders. We also present the MPEG-4 standard and discuss some of the typical applications for MPEG audio compression.", "metadata": {"arxiv_id": "semantic_scholar:64dd904dc1a8eeb84dae812335a4d46b202f7f4b", "title": "MPEG digital audio coding", "summary": "The Moving Pictures Expert Group (MPEG) within the International Organization of Standardization (ISO) has developed a series of audio-visual standards known as MFEG-1 and MPEG-2. These audio-coding standards are the first international standards in the field of high-quality digital audio compression. MPEG-1 covers coding of stereophonic audio signals at high sampling rates aiming at transparent quality, whereas MPEG-2 also offers stereophonic audio coding at lower sampling rates. In addition, MPEG-2 introduces multichannel coding with and without backwards compatibility to MPEG-1 to provide an improved acoustical image for audio-only applications and for enhanced television and video-conferencing systems. MPEG-2 audio coding without backwards compatibility, called IMPEG-2 Advanced Audio Coding (AAC), offers the highest compression rates. Typical application areas for MPEG-based digital audio are in the fields of audio production, program distribution and exchange, digital sound broadcasting, digital storage, and various multimedia applications. We describe in some detail the key technologies and main features of MPEG-1 and MPEG-2 audio coders. We also present the MPEG-4 standard and discuss some of the typical applications for MPEG audio compression.", "source": "semantic_scholar", "source_id": "64dd904dc1a8eeb84dae812335a4d46b202f7f4b", "publication_date": "1997-09-01"}, "round-A_JuniorNano_output": {"reasoning": "This item reviews MPEG-1/2/4 audio coding standards (traditional, non-neural codecs) and does not discuss discrete audio tokens, tokenizers, or token-based modeling, hence it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item reviews MPEG-1/2/4 audio coding standards (traditional, non-neural codecs) and does not discuss discrete audio tokens, tokenizers, or token-based modeling, hence it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study primarily discusses MPEG digital audio coding standards focused on traditional audio compression and coding techniques rather than discrete audio tokens derived from neural audio codecs or self-supervised models; it lacks details about tokenization processes, vocabulary, or the use of discrete tokens for downstream generative or cross-modal tasks, thus not meeting inclusion criteria and falling under exclusion criteria for traditional audio compression focus without neural discrete tokens.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study primarily discusses MPEG digital audio coding standards focused on traditional audio compression and coding techniques rather than discrete audio tokens derived from neural audio codecs or self-supervised models; it lacks details about tokenization processes, vocabulary, or the use of discrete tokens for downstream generative or cross-modal tasks, thus not meeting inclusion criteria and falling under exclusion criteria for traditional audio compression focus without neural discrete tokens.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "painter2000perceptual", "query_title": "{Perceptual coding of digital audio}", "normalized_title": "perceptual coding of digital audio", "title": "Perceptual coding of digital audio", "abstract": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (-1, -2, -4), the Lucent Technologies PAC/EPAC/MPAC, the Dolby AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.", "source": "semantic_scholar", "source_id": "ecf98b58e733badb55d9b6a449b943dbf58342fb", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Perceptual coding of digital audio", "abstract": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (-1, -2, -4), the Lucent Technologies PAC/EPAC/MPAC, the Dolby AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.", "metadata": {"arxiv_id": "semantic_scholar:ecf98b58e733badb55d9b6a449b943dbf58342fb", "title": "Perceptual coding of digital audio", "summary": "During the last decade, CD-quality digital audio has essentially replaced analog audio. Emerging digital audio applications for network, wireless, and multimedia computing systems face a series of constraints such as reduced channel bandwidth, limited storage capacity, and low cost. These new applications have created a demand for high-quality digital audio delivery at low bit rates. In response to this need, considerable research has been devoted to the development of algorithms for perceptually transparent coding of high-fidelity (CD-quality) digital audio. As a result, many algorithms have been proposed, and several have now become international and/or commercial product standards. This paper reviews algorithms for perceptually transparent coding of CD-quality digital audio, including both research and standardization activities. This paper is organized as follows. First, psychoacoustic principles are described, with the MPEG psychoacoustic signal analysis model 1 discussed in some detail. Next, filter bank design issues and algorithms are addressed, with a particular emphasis placed on the modified discrete cosine transform, a perfect reconstruction cosine-modulated filter bank that has become of central importance in perceptual audio coding. Then, we review methodologies that achieve perceptually transparent coding of FM- and CD-quality audio signals, including algorithms that manipulate transform components, subband signal decompositions, sinusoidal signal components, and linear prediction parameters, as well as hybrid algorithms that make use of more than one signal model. These discussions concentrate on architectures and applications of those techniques that utilize psychoacoustic models to exploit efficiently masking characteristics of the human receiver. Several algorithms that have become international and/or commercial standards receive in-depth treatment, including the ISO/IEC MPEG family (-1, -2, -4), the Lucent Technologies PAC/EPAC/MPAC, the Dolby AC-2/AC-3, and the Sony ATRAC/SDDS algorithms. Then, we describe subjective evaluation methodologies in some detail, including the ITU-R BS.1116 recommendation on subjective measurements of small impairments. This paper concludes with a discussion of future research directions.", "source": "semantic_scholar", "source_id": "ecf98b58e733badb55d9b6a449b943dbf58342fb", "year": 2000}, "round-A_JuniorNano_output": {"reasoning": "This item discusses perceptual coding of CD-quality audio and standards; it does not involve discrete audio tokens, tokenizers, codebooks, or downstream token-based modeling, so it fails inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item discusses perceptual coding of CD-quality audio and standards; it does not involve discrete audio tokens, tokenizers, codebooks, or downstream token-based modeling, so it fails inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper is a survey of perceptual audio coding algorithms focusing on psychoacoustic models and compression standards like MPEG and Dolby, without discussing discretization of audio waveform into token sequences nor neural audio codecs or self-supervised discrete token representations; thus, it does not meet the inclusion criteria centered on discrete audio token generation and usage.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper is a survey of perceptual audio coding algorithms focusing on psychoacoustic models and compression standards like MPEG and Dolby, without discussing discretization of audio waveform into token sequences nor neural audio codecs or self-supervised discrete token representations; thus, it does not meet the inclusion criteria centered on discrete audio token generation and usage.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "jage2016celp", "query_title": "{CELP and MELP speech coding techniques}", "normalized_title": "celp and melp speech coding techniques", "title": "CELP and MELP speech coding techniques", "abstract": "Speech is one of the natural ways of communication amongst humans. Nowadays there is insatiable demand for speech communication as it carries more information like speaker identity, emotional state, prosodic nuance which adds naturalness in communication. With rapid growth and increased number of applications there exists a need for devising an approach for data compression techniques which reduces communication cost by using available bandwidth and storage space effectively. The speech coding techniques helps to achieve bit rate reduction by simultaneously maintaining original speech quality. In this paper, Hybrid speech coding technique i.e. Code Excited Linear Prediction (CELP) and Parametric coding technique i.e. Mixed Excitation Linear Prediction (MELP) are discussed and CELP technique is implemented using MATLAB. The parameters like mean square error (MSE), Mean Opinion Score (MOS), and Signal to Noise Ratio are calculated for CELP technique which shows that CELP technique is an improvement to a coder called Linear Predictive Coder (LPC). It is an efficient coding technique for the bit rate of 16-9.6 kbps. The MELP coder discussed here helps to remove the voicing error in two state excitation model of LPC. It is a low bit rate coder having a bit rate of 2.4 kbps and mainly used by military and federal standards.", "source": "semantic_scholar", "source_id": "3f58e96c1b528458a5a9a629920f1bf6cda84793", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "CELP and MELP speech coding techniques", "abstract": "Speech is one of the natural ways of communication amongst humans. Nowadays there is insatiable demand for speech communication as it carries more information like speaker identity, emotional state, prosodic nuance which adds naturalness in communication. With rapid growth and increased number of applications there exists a need for devising an approach for data compression techniques which reduces communication cost by using available bandwidth and storage space effectively. The speech coding techniques helps to achieve bit rate reduction by simultaneously maintaining original speech quality. In this paper, Hybrid speech coding technique i.e. Code Excited Linear Prediction (CELP) and Parametric coding technique i.e. Mixed Excitation Linear Prediction (MELP) are discussed and CELP technique is implemented using MATLAB. The parameters like mean square error (MSE), Mean Opinion Score (MOS), and Signal to Noise Ratio are calculated for CELP technique which shows that CELP technique is an improvement to a coder called Linear Predictive Coder (LPC). It is an efficient coding technique for the bit rate of 16-9.6 kbps. The MELP coder discussed here helps to remove the voicing error in two state excitation model of LPC. It is a low bit rate coder having a bit rate of 2.4 kbps and mainly used by military and federal standards.", "metadata": {"arxiv_id": "semantic_scholar:3f58e96c1b528458a5a9a629920f1bf6cda84793", "title": "CELP and MELP speech coding techniques", "summary": "Speech is one of the natural ways of communication amongst humans. Nowadays there is insatiable demand for speech communication as it carries more information like speaker identity, emotional state, prosodic nuance which adds naturalness in communication. With rapid growth and increased number of applications there exists a need for devising an approach for data compression techniques which reduces communication cost by using available bandwidth and storage space effectively. The speech coding techniques helps to achieve bit rate reduction by simultaneously maintaining original speech quality. In this paper, Hybrid speech coding technique i.e. Code Excited Linear Prediction (CELP) and Parametric coding technique i.e. Mixed Excitation Linear Prediction (MELP) are discussed and CELP technique is implemented using MATLAB. The parameters like mean square error (MSE), Mean Opinion Score (MOS), and Signal to Noise Ratio are calculated for CELP technique which shows that CELP technique is an improvement to a coder called Linear Predictive Coder (LPC). It is an efficient coding technique for the bit rate of 16-9.6 kbps. The MELP coder discussed here helps to remove the voicing error in two state excitation model of LPC. It is a low bit rate coder having a bit rate of 2.4 kbps and mainly used by military and federal standards.", "source": "semantic_scholar", "source_id": "3f58e96c1b528458a5a9a629920f1bf6cda84793", "publication_date": "2016-03-23"}, "round-A_JuniorNano_output": {"reasoning": "The abstract discusses traditional speech codecs CELP/MELP and their bit-rate performance; there is no discussion of discrete audio tokenization, token vocabularies, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract discusses traditional speech codecs CELP/MELP and their bit-rate performance; there is no discussion of discrete audio tokenization, token vocabularies, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study discusses traditional speech coding techniques (CELP and MELP) focusing on compression and improving speech quality rather than generating or modeling discrete audio tokens as defined in the inclusion criteria; it does not address neural audio codecs, vector quantization, or discrete token sequence modeling relevant to Transformer/LLM-based audio representations, and thus does not meet the inclusion criteria and falls into the exclusion category of traditional coding without discrete token content.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study discusses traditional speech coding techniques (CELP and MELP) focusing on compression and improving speech quality rather than generating or modeling discrete audio tokens as defined in the inclusion criteria; it does not address neural audio codecs, vector quantization, or discrete token sequence modeling relevant to Transformer/LLM-based audio representations, and thus does not meet the inclusion criteria and falls into the exclusion category of traditional coding without discrete token content.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "maimon2025scaling", "query_title": "{Scaling Analysis of Interleaved Speech-Text Language Models}", "normalized_title": "scaling analysis of interleaved speech text language models", "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .", "source": "arxiv", "source_id": "2504.02398v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Scaling Analysis of Interleaved Speech-Text Language Models", "abstract": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .", "metadata": {"arxiv_id": "2504.02398v2", "title": "Scaling Analysis of Interleaved Speech-Text Language Models", "summary": "Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - \"Do interleaved SLMs scale more efficiently than textless-SLMs?\" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .", "authors": ["Gallil Maimon", "Michael Hassid", "Amit Roth", "Yossi Adi"], "published": "2025-04-03T08:46:56Z", "updated": "2025-07-27T17:33:45Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2504.02398v2", "landing_url": "https://arxiv.org/abs/2504.02398v2", "doi": "https://doi.org/10.48550/arXiv.2504.02398"}, "round-A_JuniorNano_output": {"reasoning": "The paper analyzes scaling of interleaved speech-text language models and does not address discrete audio tokens or token-based representations (tokenizers/codebooks), hence it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper analyzes scaling of interleaved speech-text language models and does not address discrete audio tokens or token-based representations (tokenizers/codebooks), hence it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on scaling analysis of interleaved speech-text language models but does not mention the use of discrete audio tokens derived from neural audio codecs or self-supervised representations as defined in the inclusion criteria; there is no description of tokenizer/codec architecture, quantization methods or token vocabulary details required to verify discrete token usage. Therefore, this study does not meet the core criteria for 'Discrete Audio Tokens' inclusion and aligns more with continuous representation modeling or general speech-text language model analysis without discrete token discretization details.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on scaling analysis of interleaved speech-text language models but does not mention the use of discrete audio tokens derived from neural audio codecs or self-supervised representations as defined in the inclusion criteria; there is no description of tokenizer/codec architecture, quantization methods or token vocabulary details required to verify discrete token usage. Therefore, this study does not meet the core criteria for 'Discrete Audio Tokens' inclusion and aligns more with continuous representation modeling or general speech-text language model analysis without discrete token discretization details.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "qwen2025qwen25technicalreport", "query_title": "{Qwen2.5 Technical Report}", "normalized_title": "qwen2 5 technical report", "title": "Qwen2.5 Technical Report", "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.", "source": "arxiv", "source_id": "2412.15115v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Qwen2.5 Technical Report", "abstract": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.", "metadata": {"arxiv_id": "2412.15115v2", "title": "Qwen2.5 Technical Report", "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.", "authors": ["Qwen", ":", "An Yang", "Baosong Yang", "Beichen Zhang", "Binyuan Hui", "Bo Zheng", "Bowen Yu", "Chengyuan Li", "Dayiheng Liu", "Fei Huang", "Haoran Wei", "Huan Lin", "Jian Yang", "Jianhong Tu", "Jianwei Zhang", "Jianxin Yang", "Jiaxi Yang", "Jingren Zhou", "Junyang Lin", "Kai Dang", "Keming Lu", "Keqin Bao", "Kexin Yang", "Le Yu", "Mei Li", "Mingfeng Xue", "Pei Zhang", "Qin Zhu", "Rui Men", "Runji Lin", "Tianhao Li", "Tianyi Tang", "Tingyu Xia", "Xingzhang Ren", "Xuancheng Ren", "Yang Fan", "Yang Su", "Yichang Zhang", "Yu Wan", "Yuqiong Liu", "Zeyu Cui", "Zhenru Zhang", "Zihan Qiu"], "published": "2024-12-19T17:56:09Z", "updated": "2025-01-03T02:18:21Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/2412.15115v2", "landing_url": "https://arxiv.org/abs/2412.15115v2", "doi": "https://doi.org/10.48550/arXiv.2412.15115"}, "round-A_JuniorNano_output": {"reasoning": "The item is about Qwen2.5 LLM training and deployment with no discussion of discrete audio tokens, tokenizers, or audio-domain modeling, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item is about Qwen2.5 LLM training and deployment with no discussion of discrete audio tokens, tokenizers, or audio-domain modeling, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract describe a large language model (LLM) named Qwen2.5 with improvements in pre-training and post-training, focusing mainly on language understanding, reasoning, mathematics, and coding tasks. There is no mention of discrete audio tokens, audio waveforms, neural audio codecs, or any audio-related tokenization or modeling process. Thus, the study does not meet the inclusion criteria related to discrete audio tokens and focuses solely on language models without addressing the discrete audio token domain.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract describe a large language model (LLM) named Qwen2.5 with improvements in pre-training and post-training, focusing mainly on language understanding, reasoning, mathematics, and coding tasks. There is no mention of discrete audio tokens, audio waveforms, neural audio codecs, or any audio-related tokenization or modeling process. Thus, the study does not meet the inclusion criteria related to discrete audio tokens and focuses solely on language models without addressing the discrete audio token domain.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "cuervo2025textspeechlanguagemodelsimproved", "query_title": "{Text-Speech Language Models with Improved Cross-Modal Transfer by Aligning Abstraction Levels}", "normalized_title": "text speech language models with improved cross modal transfer by aligning abstraction levels", "title": "Late Fusion and Multi-Level Fission Amplify Cross-Modal Transfer in Text-Speech LMs", "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly process and generate text and speech -- are commonly trained through an early modality fusion/fission approach, in which both modalities are fed and predicted from a shared backbone via linear layers. We hypothesize that this approach limits cross-modal transfer by neglecting feature compositionality -- specifically, the finer-grained nature of speech representations compared to text -- preventing the emergence of a shared feature hierarchy within model layers. In this paper, we argue that this limitation can be addressed through late fusion and fission, with a fission process that accesses both high- and low-level features for speech generation. Our models implementing these principles, SmolTolk, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute, and achieve significantly improved cross-modal performance relative to early fusion/fission baselines. Representation analyses further suggest that our method enhances the model's ability to abstract higher-level, more semantic features from speech, and leads to increasingly shared representation spaces across layers.", "source": "arxiv", "source_id": "2503.06211", "match_status": "exact_id", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Late Fusion and Multi-Level Fission Amplify Cross-Modal Transfer in Text-Speech LMs", "abstract": "Text-Speech Language Models (TSLMs) -- language models trained to jointly process and generate text and speech -- are commonly trained through an early modality fusion/fission approach, in which both modalities are fed and predicted from a shared backbone via linear layers. We hypothesize that this approach limits cross-modal transfer by neglecting feature compositionality -- specifically, the finer-grained nature of speech representations compared to text -- preventing the emergence of a shared feature hierarchy within model layers. In this paper, we argue that this limitation can be addressed through late fusion and fission, with a fission process that accesses both high- and low-level features for speech generation. Our models implementing these principles, SmolTolk, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute, and achieve significantly improved cross-modal performance relative to early fusion/fission baselines. Representation analyses further suggest that our method enhances the model's ability to abstract higher-level, more semantic features from speech, and leads to increasingly shared representation spaces across layers.", "metadata": {"arxiv_id": "2503.06211", "title": "Late Fusion and Multi-Level Fission Amplify Cross-Modal Transfer in Text-Speech LMs", "summary": "Text-Speech Language Models (TSLMs) -- language models trained to jointly process and generate text and speech -- are commonly trained through an early modality fusion/fission approach, in which both modalities are fed and predicted from a shared backbone via linear layers. We hypothesize that this approach limits cross-modal transfer by neglecting feature compositionality -- specifically, the finer-grained nature of speech representations compared to text -- preventing the emergence of a shared feature hierarchy within model layers. In this paper, we argue that this limitation can be addressed through late fusion and fission, with a fission process that accesses both high- and low-level features for speech generation. Our models implementing these principles, SmolTolk, rival or surpass state-of-the-art TSLMs trained with orders of magnitude more compute, and achieve significantly improved cross-modal performance relative to early fusion/fission baselines. Representation analyses further suggest that our method enhances the model's ability to abstract higher-level, more semantic features from speech, and leads to increasingly shared representation spaces across layers.", "authors": ["Santiago Cuervo", "Adel Moumen", "Yanis Labrak", "Sameer Khurana", "Antoine Laurent", "Mickael Rouvier", "Phil Woodland", "Ricard Marxer"], "published": "2025-03-08T13:28:50Z", "updated": "2025-10-20T09:06:09Z", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2503.06211v2", "landing_url": "https://arxiv.org/abs/2503.06211v2", "doi": "https://doi.org/10.48550/arXiv.2503.06211"}, "round-A_JuniorNano_output": {"reasoning": "The study focuses on late fusion in text-speech LMs and cross-modal transfer without discussing discrete audio tokenization, discretization processes, or token-based representations, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The study focuses on late fusion in text-speech LMs and cross-modal transfer without discussing discrete audio tokenization, discretization processes, or token-based representations, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study discusses improvements in Text-Speech Language Models using late fusion and multi-level fission to enhance cross-modal transfer but does not present any discrete audio tokens generated through neural audio codecs or self-supervised tokenization methods, nor does it describe any discrete token design, quantization, or vocabulary setup. Therefore, it does not meet the inclusion criteria requiring a focus on discrete audio tokens used as core representations for audio generation or modeling, and it lacks detailed description of discrete tokenization processes or evaluations relevant to the scope.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study discusses improvements in Text-Speech Language Models using late fusion and multi-level fission to enhance cross-modal transfer but does not present any discrete audio tokens generated through neural audio codecs or self-supervised tokenization methods, nor does it describe any discrete token design, quantization, or vocabulary setup. Therefore, it does not meet the inclusion criteria requiring a focus on discrete audio tokens used as core representations for audio generation or modeling, and it lacks detailed description of discrete tokenization processes or evaluations relevant to the scope.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kharitonov2022textfreeprosodyawaregenerativespoken", "query_title": "{Text-Free Prosody-Aware Generative Spoken Language Modeling}", "normalized_title": "text free prosody aware generative spoken language modeling", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "source": "arxiv", "source_id": "2109.03264v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "metadata": {"arxiv_id": "2109.03264v2", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "authors": ["Eugene Kharitonov", "Ann Lee", "Adam Polyak", "Yossi Adi", "Jade Copet", "Kushal Lakhotia", "Tu-Anh Nguyen", "Morgane Rivière", "Abdelrahman Mohamed", "Emmanuel Dupoux", "Wei-Ning Hsu"], "published": "2021-09-07T18:03:21Z", "updated": "2022-05-10T16:41:20Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2109.03264v2", "landing_url": "https://arxiv.org/abs/2109.03264v2", "doi": "https://doi.org/10.48550/arXiv.2109.03264"}, "round-A_JuniorNano_output": {"reasoning": "The paper presents a text-free, prosody-aware generative spoken language model that uses 'discovered unit streams' (token-like discrete units) alongside prosodic features to model and generate speech, which aligns with discrete audio token representations and downstream generation tasks though exact token design details are not fully specified in the abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The paper presents a text-free, prosody-aware generative spoken language model that uses 'discovered unit streams' (token-like discrete units) alongside prosodic features to model and generate speech, which aligns with discrete audio token representations and downstream generation tasks though exact token design details are not fully specified in the abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study presents a prosody-aware generative spoken language model that uses discovered unit streams—which are discrete and phone-like—as tokens for modeling and generating speech, aligning with the discrete audio tokens topic; it also provides codes, models, and detailed methodology supportive of reproducibility, and focuses on generative tasks rather than pure classification, satisfying all inclusion criteria without triggering any exclusions.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a prosody-aware generative spoken language model that uses discovered unit streams—which are discrete and phone-like—as tokens for modeling and generating speech, aligning with the discrete audio tokens topic; it also provides codes, models, and detailed methodology supportive of reproducibility, and focuses on generative tasks rather than pure classification, satisfying all inclusion criteria without triggering any exclusions.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}, {"title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "abstract": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "metadata": {"arxiv_id": "2109.03264v2", "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling", "summary": "Speech pre-training has primarily demonstrated efficacy on classification tasks, while its capability of generating novel speech, similar to how GPT-2 can generate coherent paragraphs, has barely been explored. Generative Spoken Language Modeling (GSLM) \\cite{Lakhotia2021} is the only prior work addressing the generative aspects of speech pre-training, which replaces text with discovered phone-like units for language modeling and shows the ability to generate meaningful novel sentences. Unfortunately, despite eliminating the need of text, the units used in GSLM discard most of the prosodic information. Hence, GSLM fails to leverage prosody for better comprehension, and does not generate expressive speech. In this work, we present a prosody-aware generative spoken language model (pGSLM). It is composed of a multi-stream transformer language model (MS-TLM) of speech, represented as discovered unit and prosodic feature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to waveforms. We devise a series of metrics for prosody modeling and generation, and re-use metrics from GSLM for content modeling. Experimental results show that the pGSLM can utilize prosody to improve both prosody and content modeling, and also generate natural, meaningful, and coherent speech given a spoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm. Codes and models are available at https://github.com/pytorch/fairseq/tree/main/examples/textless_nlp/pgslm.", "authors": ["Eugene Kharitonov", "Ann Lee", "Adam Polyak", "Yossi Adi", "Jade Copet", "Kushal Lakhotia", "Tu-Anh Nguyen", "Morgane Rivière", "Abdelrahman Mohamed", "Emmanuel Dupoux", "Wei-Ning Hsu"], "published": "2021-09-07T18:03:21Z", "updated": "2022-05-10T16:41:20Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2109.03264v2", "landing_url": "https://arxiv.org/abs/2109.03264v2", "doi": "https://doi.org/10.48550/arXiv.2109.03264"}, "round-A_JuniorNano_output": {"reasoning": "The abstract uses 'discovered unit' streams as discrete input to a text-free generative speech model, implying discrete audio tokens suitable for token-based modeling, which aligns with the inclusion criteria, though explicit tokenizer/codec details are not given in the abstract.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The abstract uses 'discovered unit' streams as discrete input to a text-free generative speech model, implying discrete audio tokens suitable for token-based modeling, which aligns with the inclusion criteria, though explicit tokenizer/codec details are not given in the abstract.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The study presents a generative spoken language model utilizing discovered phone-like units as tokens, which are discrete audio units derived from speech for language modeling; it details the representation methods and provides links to code and models, fulfilling the inclusion criteria related to discrete audio tokens and sufficient English technical details, while none of the exclusion criteria apply.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study presents a generative spoken language model utilizing discovered phone-like units as tokens, which are discrete audio units derived from speech for language modeling; it details the representation methods and provides links to code and models, fulfilling the inclusion criteria related to discrete audio tokens and sufficient English technical details, while none of the exclusion criteria apply.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:4)", "review_skipped": false, "discard_reason": null}]}
{"key": "agustsson2017soft", "query_title": "{Soft-to-hard vector quantization for end-to-end learning compressible representations}", "normalized_title": "soft to hard vector quantization for end to end learning compressible representations", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.", "source": "arxiv", "source_id": "1704.00648v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "abstract": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.", "metadata": {"arxiv_id": "1704.00648v2", "title": "Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations", "summary": "We present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives results competitive with the state-of-the-art for both.", "authors": ["Eirikur Agustsson", "Fabian Mentzer", "Michael Tschannen", "Lukas Cavigelli", "Radu Timofte", "Luca Benini", "Luc Van Gool"], "published": "2017-04-03T15:39:56Z", "updated": "2017-06-08T09:18:22Z", "categories": ["cs.LG", "cs.CV"], "pdf_url": "https://arxiv.org/pdf/1704.00648v2", "landing_url": "https://arxiv.org/abs/1704.00648v2", "doi": "https://doi.org/10.48550/arXiv.1704.00648"}, "round-A_JuniorNano_output": {"reasoning": "The work discusses soft-to-hard vector quantization for end-to-end learning in image and neural network compression, with no reference to discrete audio tokens, audio-specific tokenization, or audio-domain evaluation, thus it does not meet the discrete audio token inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The work discusses soft-to-hard vector quantization for end-to-end learning in image and neural network compression, with no reference to discrete audio tokens, audio-specific tokenization, or audio-domain evaluation, thus it does not meet the discrete audio token inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents a method for learning compressible representations via soft-to-hard vector quantization applied to image and neural network compression, but it does not focus on audio data nor discrete audio tokens mapping continuous audio waveforms to token sequences for modeling or generation. It lacks specific details on discrete tokenization of audio signals or neural audio codecs related to audio token generation as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria for discrete audio tokens and is outside the scope of the survey topic.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents a method for learning compressible representations via soft-to-hard vector quantization applied to image and neural network compression, but it does not focus on audio data nor discrete audio tokens mapping continuous audio waveforms to token sequences for modeling or generation. It lacks specific details on discrete tokenization of audio signals or neural audio codecs related to audio token generation as required by the inclusion criteria. Therefore, it does not meet the inclusion criteria for discrete audio tokens and is outside the scope of the survey topic.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kankanahalli2018end", "query_title": "{End-to-end optimized speech coding with deep neural networks}", "normalized_title": "end to end optimized speech coding with deep neural networks", "title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "abstract": "Modern compression algorithms are often the result of laborious domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took years to develop and were largely hand-designed. We present a deep neural network model which optimizes all the steps of a wideband speech coding pipeline (compression, quantization, entropy coding, and decompression) end-to-end directly from raw speech data -- no manual feature engineering necessary, and it trains in hours. In testing, our DNN-based coder performs on par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps). It also runs in realtime on a 3.8GhZ Intel CPU.", "source": "arxiv", "source_id": "1710.09064v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "abstract": "Modern compression algorithms are often the result of laborious domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took years to develop and were largely hand-designed. We present a deep neural network model which optimizes all the steps of a wideband speech coding pipeline (compression, quantization, entropy coding, and decompression) end-to-end directly from raw speech data -- no manual feature engineering necessary, and it trains in hours. In testing, our DNN-based coder performs on par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps). It also runs in realtime on a 3.8GhZ Intel CPU.", "metadata": {"arxiv_id": "1710.09064v3", "title": "End-to-End Optimized Speech Coding with Deep Neural Networks", "summary": "Modern compression algorithms are often the result of laborious domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took years to develop and were largely hand-designed. We present a deep neural network model which optimizes all the steps of a wideband speech coding pipeline (compression, quantization, entropy coding, and decompression) end-to-end directly from raw speech data -- no manual feature engineering necessary, and it trains in hours. In testing, our DNN-based coder performs on par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps). It also runs in realtime on a 3.8GhZ Intel CPU.", "authors": ["Srihari Kankanahalli"], "published": "2017-10-25T03:21:44Z", "updated": "2021-07-08T15:43:27Z", "categories": ["cs.SD", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1710.09064v3", "landing_url": "https://arxiv.org/abs/1710.09064v3", "doi": "https://doi.org/10.48550/arXiv.1710.09064"}, "round-A_JuniorNano_output": {"reasoning": "The paper discusses end-to-end neural speech coding with quantization for compression, but it does not explicitly address discrete audio tokens, tokenizers/codebooks, or token-based downstream tasks as required by the 'Discrete Audio Tokens' criteria, so it does not meet the core inclusion criteria and should be excluded.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "The paper discusses end-to-end neural speech coding with quantization for compression, but it does not explicitly address discrete audio tokens, tokenizers/codebooks, or token-based downstream tasks as required by the 'Discrete Audio Tokens' criteria, so it does not meet the core inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The paper presents an end-to-end optimized speech coding model using deep neural networks that performs compression, quantization, entropy coding, and decompression, but it does not explicitly state the use of discrete audio tokens derived from quantization or vector quantization methods to create a token vocabulary sequence for subsequent modeling; it focuses on compression performance and real-time operation rather than on token design or downstream tasks like generation or cross-modal modeling, which makes its alignment with the inclusion criteria for discrete audio tokens unclear and insufficient for guaranteed inclusion.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The paper presents an end-to-end optimized speech coding model using deep neural networks that performs compression, quantization, entropy coding, and decompression, but it does not explicitly state the use of discrete audio tokens derived from quantization or vector quantization methods to create a token vocabulary sequence for subsequent modeling; it focuses on compression performance and real-time operation rather than on token design or downstream tasks like generation or cross-modal modeling, which makes its alignment with the inclusion criteria for discrete audio tokens unclear and insufficient for guaranteed inclusion.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "choi2024selfsupervisedspeechrepresentationsphonetic", "query_title": "{Self-Supervised Speech Representations are More Phonetic than Semantic}", "normalized_title": "self supervised speech representations are more phonetic than semantic", "title": "Self-Supervised Speech Representations are More Phonetic than Semantic", "abstract": "Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.", "source": "arxiv", "source_id": "2406.08619v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Self-Supervised Speech Representations are More Phonetic than Semantic", "abstract": "Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.", "metadata": {"arxiv_id": "2406.08619v1", "title": "Self-Supervised Speech Representations are More Phonetic than Semantic", "summary": "Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.", "authors": ["Kwanghee Choi", "Ankita Pasad", "Tomohiko Nakamura", "Satoru Fukayama", "Karen Livescu", "Shinji Watanabe"], "published": "2024-06-12T20:04:44Z", "updated": "2024-06-12T20:04:44Z", "categories": ["cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2406.08619v1", "landing_url": "https://arxiv.org/abs/2406.08619v1", "doi": "https://doi.org/10.48550/arXiv.2406.08619"}, "round-A_JuniorNano_output": {"reasoning": "The article analyzes continuous self-supervised speech representations for phonetic vs semantic content and does not discuss discrete audio tokens, tokenizers, or token-level modeling, so it does not meet the discrete audio tokens criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article analyzes continuous self-supervised speech representations for phonetic vs semantic content and does not discuss discrete audio tokens, tokenizers, or token-level modeling, so it does not meet the discrete audio tokens criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study analyzes self-supervised speech models (S3Ms) and their encoding of phonetic versus semantic information, but it does not clearly describe the process of generating discrete audio tokens or provide specific tokenization details such as vocabularies or quantization methods. It focuses on analyzing continuous speech representations rather than discrete token sequences for generation or cross-modal modeling. Therefore, it does not meet the core inclusion criteria of using discrete audio tokens as the central methodology or analysis object.", "evaluation": 2}, "round-A_JuniorMini_reasoning": "The study analyzes self-supervised speech models (S3Ms) and their encoding of phonetic versus semantic information, but it does not clearly describe the process of generating discrete audio tokens or provide specific tokenization details such as vocabularies or quantization methods. It focuses on analyzing continuous speech representations rather than discrete token sequences for generation or cross-modal modeling. Therefore, it does not meet the core inclusion criteria of using discrete audio tokens as the central methodology or analysis object.", "round-A_JuniorMini_evaluation": 2, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "grattafiori2024llama3herdmodels", "query_title": "{The llama 3 herd of models}", "normalized_title": "the llama 3 herd of models", "title": "The Llama 3 Herd of Models", "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.", "source": "arxiv", "source_id": "2407.21783v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "The Llama 3 Herd of Models", "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.", "metadata": {"arxiv_id": "2407.21783v3", "title": "The Llama 3 Herd of Models", "summary": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.", "authors": ["Aaron Grattafiori", "Abhimanyu Dubey", "Abhinav Jauhri", "Abhinav Pandey", "Abhishek Kadian", "Ahmad Al-Dahle", "Aiesha Letman", "Akhil Mathur", "Alan Schelten", "Alex Vaughan", "Amy Yang", "Angela Fan", "Anirudh Goyal", "Anthony Hartshorn", "Aobo Yang", "Archi Mitra", "Archie Sravankumar", "Artem Korenev", "Arthur Hinsvark", "Arun Rao", "Aston Zhang", "Aurelien Rodriguez", "Austen Gregerson", "Ava Spataru", "Baptiste Roziere", "Bethany Biron", "Binh Tang", "Bobbie Chern", "Charlotte Caucheteux", "Chaya Nayak", "Chloe Bi", "Chris Marra", "Chris McConnell", "Christian Keller", "Christophe Touret", "Chunyang Wu", "Corinne Wong", "Cristian Canton Ferrer", "Cyrus Nikolaidis", "Damien Allonsius", "Daniel Song", "Danielle Pintz", "Danny Livshits", "Danny Wyatt", "David Esiobu", "Dhruv Choudhary", "Dhruv Mahajan", "Diego Garcia-Olano", "Diego Perino", "Dieuwke Hupkes", "Egor Lakomkin", "Ehab AlBadawy", "Elina Lobanova", "Emily Dinan", "Eric Michael Smith", "Filip Radenovic", "Francisco Guzmán", "Frank Zhang", "Gabriel Synnaeve", "Gabrielle Lee", "Georgia Lewis Anderson", "Govind Thattai", "Graeme Nail", "Gregoire Mialon", "Guan Pang", "Guillem Cucurell", "Hailey Nguyen", "Hannah Korevaar", "Hu Xu", "Hugo Touvron", "Iliyan Zarov", "Imanol Arrieta Ibarra", "Isabel Kloumann", "Ishan Misra", "Ivan Evtimov", "Jack Zhang", "Jade Copet", "Jaewon Lee", "Jan Geffert", "Jana Vranes", "Jason Park", "Jay Mahadeokar", "Jeet Shah", "Jelmer van der Linde", "Jennifer Billock", "Jenny Hong", "Jenya Lee", "Jeremy Fu", "Jianfeng Chi", "Jianyu Huang", "Jiawen Liu", "Jie Wang", "Jiecao Yu", "Joanna Bitton", "Joe Spisak", "Jongsoo Park", "Joseph Rocca", "Joshua Johnstun", "Joshua Saxe", "Junteng Jia", "Kalyan Vasuden Alwala", "Karthik Prasad", "Kartikeya Upasani", "Kate Plawiak", "Ke Li", "Kenneth Heafield", "Kevin Stone", "Khalid El-Arini", "Krithika Iyer", "Kshitiz Malik", "Kuenley Chiu", "Kunal Bhalla", "Kushal Lakhotia", "Lauren Rantala-Yeary", "Laurens van der Maaten", "Lawrence Chen", "Liang Tan", "Liz Jenkins", "Louis Martin", "Lovish Madaan", "Lubo Malo", "Lukas Blecher", "Lukas Landzaat", "Luke de Oliveira", "Madeline Muzzi", "Mahesh Pasupuleti", "Mannat Singh", "Manohar Paluri", "Marcin Kardas", "Maria Tsimpoukelli", "Mathew Oldham", "Mathieu Rita", "Maya Pavlova", "Melanie Kambadur", "Mike Lewis", "Min Si", "Mitesh Kumar Singh", "Mona Hassan", "Naman Goyal", "Narjes Torabi", "Nikolay Bashlykov", "Nikolay Bogoychev", "Niladri Chatterji", "Ning Zhang", "Olivier Duchenne", "Onur Çelebi", "Patrick Alrassy", "Pengchuan Zhang", "Pengwei Li", "Petar Vasic", "Peter Weng", "Prajjwal Bhargava", "Pratik Dubal", "Praveen Krishnan", "Punit Singh Koura", "Puxin Xu", "Qing He", "Qingxiao Dong", "Ragavan Srinivasan", "Raj Ganapathy", "Ramon Calderer", "Ricardo Silveira Cabral", "Robert Stojnic", "Roberta Raileanu", "Rohan Maheswari", "Rohit Girdhar", "Rohit Patel", "Romain Sauvestre", "Ronnie Polidoro", "Roshan Sumbaly", "Ross Taylor", "Ruan Silva", "Rui Hou", "Rui Wang", "Saghar Hosseini", "Sahana Chennabasappa", "Sanjay Singh", "Sean Bell", "Seohyun Sonia Kim", "Sergey Edunov", "Shaoliang Nie", "Sharan Narang", "Sharath Raparthy", "Sheng Shen", "Shengye Wan", "Shruti Bhosale", "Shun Zhang", "Simon Vandenhende", "Soumya Batra", "Spencer Whitman", "Sten Sootla", "Stephane Collot", "Suchin Gururangan", "Sydney Borodinsky", "Tamar Herman", "Tara Fowler", "Tarek Sheasha", "Thomas Georgiou", "Thomas Scialom", "Tobias Speckbacher", "Todor Mihaylov", "Tong Xiao", "Ujjwal Karn", "Vedanuj Goswami", "Vibhor Gupta", "Vignesh Ramanathan", "Viktor Kerkez", "Vincent Gonguet", "Virginie Do", "Vish Vogeti", "Vítor Albiero", "Vladan Petrovic", "Weiwei Chu", "Wenhan Xiong", "Wenyin Fu", "Whitney Meers", "Xavier Martinet", "Xiaodong Wang", "Xiaofang Wang", "Xiaoqing Ellen Tan", "Xide Xia", "Xinfeng Xie", "Xuchao Jia", "Xuewei Wang", "Yaelle Goldschlag", "Yashesh Gaur", "Yasmine Babaei", "Yi Wen", "Yiwen Song", "Yuchen Zhang", "Yue Li", "Yuning Mao", "Zacharie Delpierre Coudert", "Zheng Yan", "Zhengxing Chen", "Zoe Papakipos", "Aaditya Singh", "Aayushi Srivastava", "Abha Jain", "Adam Kelsey", "Adam Shajnfeld", "Adithya Gangidi", "Adolfo Victoria", "Ahuva Goldstand", "Ajay Menon", "Ajay Sharma", "Alex Boesenberg", "Alexei Baevski", "Allie Feinstein", "Amanda Kallet", "Amit Sangani", "Amos Teo", "Anam Yunus", "Andrei Lupu", "Andres Alvarado", "Andrew Caples", "Andrew Gu", "Andrew Ho", "Andrew Poulton", "Andrew Ryan", "Ankit Ramchandani", "Annie Dong", "Annie Franco", "Anuj Goyal", "Aparajita Saraf", "Arkabandhu Chowdhury", "Ashley Gabriel", "Ashwin Bharambe", "Assaf Eisenman", "Azadeh Yazdan", "Beau James", "Ben Maurer", "Benjamin Leonhardi", "Bernie Huang", "Beth Loyd", "Beto De Paola", "Bhargavi Paranjape", "Bing Liu", "Bo Wu", "Boyu Ni", "Braden Hancock", "Bram Wasti", "Brandon Spence", "Brani Stojkovic", "Brian Gamido", "Britt Montalvo", "Carl Parker", "Carly Burton", "Catalina Mejia", "Ce Liu", "Changhan Wang", "Changkyu Kim", "Chao Zhou", "Chester Hu", "Ching-Hsiang Chu", "Chris Cai", "Chris Tindal", "Christoph Feichtenhofer", "Cynthia Gao", "Damon Civin", "Dana Beaty", "Daniel Kreymer", "Daniel Li", "David Adkins", "David Xu", "Davide Testuggine", "Delia David", "Devi Parikh", "Diana Liskovich", "Didem Foss", "Dingkang Wang", "Duc Le", "Dustin Holland", "Edward Dowling", "Eissa Jamil", "Elaine Montgomery", "Eleonora Presani", "Emily Hahn", "Emily Wood", "Eric-Tuan Le", "Erik Brinkman", "Esteban Arcaute", "Evan Dunbar", "Evan Smothers", "Fei Sun", "Felix Kreuk", "Feng Tian", "Filippos Kokkinos", "Firat Ozgenel", "Francesco Caggioni", "Frank Kanayet", "Frank Seide", "Gabriela Medina Florez", "Gabriella Schwarz", "Gada Badeer", "Georgia Swee", "Gil Halpern", "Grant Herman", "Grigory Sizov", "Guangyi", "Zhang", "Guna Lakshminarayanan", "Hakan Inan", "Hamid Shojanazeri", "Han Zou", "Hannah Wang", "Hanwen Zha", "Haroun Habeeb", "Harrison Rudolph", "Helen Suk", "Henry Aspegren", "Hunter Goldman", "Hongyuan Zhan", "Ibrahim Damlaj", "Igor Molybog", "Igor Tufanov", "Ilias Leontiadis", "Irina-Elena Veliche", "Itai Gat", "Jake Weissman", "James Geboski", "James Kohli", "Janice Lam", "Japhet Asher", "Jean-Baptiste Gaya", "Jeff Marcus", "Jeff Tang", "Jennifer Chan", "Jenny Zhen", "Jeremy Reizenstein", "Jeremy Teboul", "Jessica Zhong", "Jian Jin", "Jingyi Yang", "Joe Cummings", "Jon Carvill", "Jon Shepard", "Jonathan McPhie", "Jonathan Torres", "Josh Ginsburg", "Junjie Wang", "Kai Wu", "Kam Hou U", "Karan Saxena", "Kartikay Khandelwal", "Katayoun Zand", "Kathy Matosich", "Kaushik Veeraraghavan", "Kelly Michelena", "Keqian Li", "Kiran Jagadeesh", "Kun Huang", "Kunal Chawla", "Kyle Huang", "Lailin Chen", "Lakshya Garg", "Lavender A", "Leandro Silva", "Lee Bell", "Lei Zhang", "Liangpeng Guo", "Licheng Yu", "Liron Moshkovich", "Luca Wehrstedt", "Madian Khabsa", "Manav Avalani", "Manish Bhatt", "Martynas Mankus", "Matan Hasson", "Matthew Lennie", "Matthias Reso", "Maxim Groshev", "Maxim Naumov", "Maya Lathi", "Meghan Keneally", "Miao Liu", "Michael L. Seltzer", "Michal Valko", "Michelle Restrepo", "Mihir Patel", "Mik Vyatskov", "Mikayel Samvelyan", "Mike Clark", "Mike Macey", "Mike Wang", "Miquel Jubert Hermoso", "Mo Metanat", "Mohammad Rastegari", "Munish Bansal", "Nandhini Santhanam", "Natascha Parks", "Natasha White", "Navyata Bawa", "Nayan Singhal", "Nick Egebo", "Nicolas Usunier", "Nikhil Mehta", "Nikolay Pavlovich Laptev", "Ning Dong", "Norman Cheng", "Oleg Chernoguz", "Olivia Hart", "Omkar Salpekar", "Ozlem Kalinli", "Parkin Kent", "Parth Parekh", "Paul Saab", "Pavan Balaji", "Pedro Rittner", "Philip Bontrager", "Pierre Roux", "Piotr Dollar", "Polina Zvyagina", "Prashant Ratanchandani", "Pritish Yuvraj", "Qian Liang", "Rachad Alao", "Rachel Rodriguez", "Rafi Ayub", "Raghotham Murthy", "Raghu Nayani", "Rahul Mitra", "Rangaprabhu Parthasarathy", "Raymond Li", "Rebekkah Hogan", "Robin Battey", "Rocky Wang", "Russ Howes", "Ruty Rinott", "Sachin Mehta", "Sachin Siby", "Sai Jayesh Bondu", "Samyak Datta", "Sara Chugh", "Sara Hunt", "Sargun Dhillon", "Sasha Sidorov", "Satadru Pan", "Saurabh Mahajan", "Saurabh Verma", "Seiji Yamamoto", "Sharadh Ramaswamy", "Shaun Lindsay", "Shaun Lindsay", "Sheng Feng", "Shenghao Lin", "Shengxin Cindy Zha", "Shishir Patil", "Shiva Shankar", "Shuqiang Zhang", "Shuqiang Zhang", "Sinong Wang", "Sneha Agarwal", "Soji Sajuyigbe", "Soumith Chintala", "Stephanie Max", "Stephen Chen", "Steve Kehoe", "Steve Satterfield", "Sudarshan Govindaprasad", "Sumit Gupta", "Summer Deng", "Sungmin Cho", "Sunny Virk", "Suraj Subramanian", "Sy Choudhury", "Sydney Goldman", "Tal Remez", "Tamar Glaser", "Tamara Best", "Thilo Koehler", "Thomas Robinson", "Tianhe Li", "Tianjun Zhang", "Tim Matthews", "Timothy Chou", "Tzook Shaked", "Varun Vontimitta", "Victoria Ajayi", "Victoria Montanez", "Vijai Mohan", "Vinay Satish Kumar", "Vishal Mangla", "Vlad Ionescu", "Vlad Poenaru", "Vlad Tiberiu Mihailescu", "Vladimir Ivanov", "Wei Li", "Wenchen Wang", "Wenwen Jiang", "Wes Bouaziz", "Will Constable", "Xiaocheng Tang", "Xiaojian Wu", "Xiaolan Wang", "Xilun Wu", "Xinbo Gao", "Yaniv Kleinman", "Yanjun Chen", "Ye Hu", "Ye Jia", "Ye Qi", "Yenda Li", "Yilin Zhang", "Ying Zhang", "Yossi Adi", "Youngjin Nam", "Yu", "Wang", "Yu Zhao", "Yuchen Hao", "Yundi Qian", "Yunlu Li", "Yuzi He", "Zach Rait", "Zachary DeVito", "Zef Rosnbrick", "Zhaoduo Wen", "Zhenyu Yang", "Zhiwei Zhao", "Zhiyu Ma"], "published": "2024-07-31T17:54:27Z", "updated": "2024-11-23T23:27:33Z", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf_url": "https://arxiv.org/pdf/2407.21783v3", "landing_url": "https://arxiv.org/abs/2407.21783v3", "doi": "https://doi.org/10.48550/arXiv.2407.21783"}, "round-A_JuniorNano_output": {"reasoning": "The item concerns Llama 3 language models with multimodal capabilities, not discrete audio token design or token-based audio modeling, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item concerns Llama 3 language models with multimodal capabilities, not discrete audio token design or token-based audio modeling, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract do not mention any discrete audio tokens or the discretization of continuous audio waveforms into token sequences; they focus on large language models with multimodal capabilities including image, video, and speech recognition, but no indication of neural audio codec or discrete token mechanism as defined in the inclusion criteria. Therefore, the paper does not meet the core inclusion criteria regarding discrete audio tokens and related system design or evaluation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract do not mention any discrete audio tokens or the discretization of continuous audio waveforms into token sequences; they focus on large language models with multimodal capabilities including image, video, and speech recognition, but no indication of neural audio codec or discrete token mechanism as defined in the inclusion criteria. Therefore, the paper does not meet the core inclusion criteria regarding discrete audio tokens and related system design or evaluation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "radhakrishnan2023whispering", "query_title": "{Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition}", "normalized_title": "whispering llama a cross modal generative error correction framework for speech recognition", "title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition", "abstract": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.", "source": "arxiv", "source_id": "2310.06434v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition", "abstract": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.", "metadata": {"arxiv_id": "2310.06434v2", "title": "Whispering LLaMA: A Cross-Modal Generative Error Correction Framework for Speech Recognition", "summary": "We introduce a new cross-modal fusion technique designed for generative error correction in automatic speech recognition (ASR). Our methodology leverages both acoustic information and external linguistic representations to generate accurate speech transcription contexts. This marks a step towards a fresh paradigm in generative error correction within the realm of n-best hypotheses. Unlike the existing ranking-based rescoring methods, our approach adeptly uses distinct initialization techniques and parameter-efficient algorithms to boost ASR performance derived from pre-trained speech and text models. Through evaluation across diverse ASR datasets, we evaluate the stability and reproducibility of our fusion technique, demonstrating its improved word error rate relative (WERR) performance in comparison to n-best hypotheses by relatively 37.66%. To encourage future research, we have made our code and pre-trained models open source at https://github.com/Srijith-rkr/Whispering-LLaMA.", "authors": ["Srijith Radhakrishnan", "Chao-Han Huck Yang", "Sumeer Ahmad Khan", "Rohit Kumar", "Narsis A. Kiani", "David Gomez-Cabrero", "Jesper N. Tegner"], "published": "2023-10-10T09:04:33Z", "updated": "2023-10-16T21:32:56Z", "categories": ["cs.CL", "cs.AI", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2310.06434v2", "landing_url": "https://arxiv.org/abs/2310.06434v2", "doi": "https://doi.org/10.18653/v1/2023.emnlp-main.618"}, "round-A_JuniorNano_output": {"reasoning": "No discussion of discrete audio tokens, tokenizers, or token-based modeling; focuses on cross-modal error correction for ASR using pre-trained models, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "No discussion of discrete audio tokens, tokenizers, or token-based modeling; focuses on cross-modal error correction for ASR using pre-trained models, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on a cross-modal generative error correction framework for speech recognition utilizing acoustic and external linguistic representations to improve ASR. However, the abstract does not mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it explain any detail on discretization, token vocabularies, or quantization processes. Therefore, it does not meet the core criteria for studying discrete audio tokens as defined, and it focuses more on generative error correction rather than token-based audio modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on a cross-modal generative error correction framework for speech recognition utilizing acoustic and external linguistic representations to improve ASR. However, the abstract does not mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it explain any detail on discretization, token vocabularies, or quantization processes. Therefore, it does not meet the core criteria for studying discrete audio tokens as defined, and it focuses more on generative error correction rather than token-based audio modeling or generation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "libritts-alignments", "query_title": "{LibriTTS-Phones-and-Mel}", "normalized_title": "libritts phones and mel", "title": "{LibriTTS-Phones-and-Mel}", "abstract": "Dataset containing Mel Spectrograms, Prosody and Phone Alignments for the LibriTTS dataset.", "source": "huggingface", "source_id": "cdminix/libritts-phones-and-mel", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{LibriTTS-Phones-and-Mel}", "abstract": "Dataset containing Mel Spectrograms, Prosody and Phone Alignments for the LibriTTS dataset.", "metadata": {"arxiv_id": "huggingface:cdminix/libritts-phones-and-mel", "title": "{LibriTTS-Phones-and-Mel}", "summary": "Dataset containing Mel Spectrograms, Prosody and Phone Alignments for the LibriTTS dataset.", "source": "huggingface", "source_id": "cdminix/libritts-phones-and-mel", "year": 2023}, "round-A_JuniorNano_output": {"reasoning": "This item centers on continuous representations (Mel spectrograms, prosody, and phone alignments) and does not describe generation or use of discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item centers on continuous representations (Mel spectrograms, prosody, and phone alignments) and does not describe generation or use of discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The dataset described provides Mel spectrograms, prosody, and phone alignments for the LibriTTS dataset but does not describe the generation or use of discrete audio tokens as defined by the inclusion criteria (i.e., no mention of neural audio codec, quantization, token vocabularies, or their use in modeling or generating audio). Thus, it lacks the core features required for including discrete audio tokens research and appears to be a continuous audio feature dataset without discrete token representations.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The dataset described provides Mel spectrograms, prosody, and phone alignments for the LibriTTS dataset but does not describe the generation or use of discrete audio tokens as defined by the inclusion criteria (i.e., no mention of neural audio codec, quantization, token vocabularies, or their use in modeling or generating audio). Thus, it lacks the core features required for including discrete audio tokens research and appears to be a continuous audio feature dataset without discrete token representations.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ren2019fastspeech", "query_title": "{Fastspeech: Fast, robust and controllable text to speech}", "normalized_title": "fastspeech fast robust and controllable text to speech", "title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "abstract": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.", "source": "arxiv", "source_id": "1905.09263v5", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "abstract": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.", "metadata": {"arxiv_id": "1905.09263v5", "title": "FastSpeech: Fast, Robust and Controllable Text to Speech", "summary": "Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.", "authors": ["Yi Ren", "Yangjun Ruan", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu"], "published": "2019-05-22T17:50:21Z", "updated": "2019-11-20T09:37:22Z", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/1905.09263v5", "landing_url": "https://arxiv.org/abs/1905.09263v5", "doi": "https://doi.org/10.48550/arXiv.1905.09263"}, "round-A_JuniorNano_output": {"reasoning": "Reasoning: The paper describes FastSpeech, a parallel mel-spectrogram-based TTS model, with no discussion of discrete audio tokens, tokenizers, or neural/audio codecs required by the discrete audio tokens criteria, thus it should be excluded under the exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "Reasoning: The paper describes FastSpeech, a parallel mel-spectrogram-based TTS model, with no discussion of discrete audio tokens, tokenizers, or neural/audio codecs required by the discrete audio tokens criteria, thus it should be excluded under the exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study proposes FastSpeech, a neural TTS model generating mel-spectrograms in parallel but does not involve discrete audio tokenization, vector quantization, or any neural codec producing discrete token sequences as core representations for downstream modeling or alignment; thus it does not meet the inclusion criteria focusing on discrete audio tokens and their use in generation or understanding tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study proposes FastSpeech, a neural TTS model generating mel-spectrograms in parallel but does not involve discrete audio tokenization, vector quantization, or any neural codec producing discrete token sequences as core representations for downstream modeling or alignment; thus it does not meet the inclusion criteria focusing on discrete audio tokens and their use in generation or understanding tasks.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "morise2016world", "query_title": "{WORLD: a vocoder-based high-quality speech synthesis system for real-time applications}", "normalized_title": "world a vocoder based high quality speech synthesis system for real time applications", "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications", "abstract": "SUMMARY A vocoder-based speech synthesis system, named WORLD, was developed in an e ﬀ ort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been di ﬃ cult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The e ﬀ ectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.", "source": "semantic_scholar", "source_id": "ba91dabec842d507a647aab97ad224b4abdc1635", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications", "abstract": "SUMMARY A vocoder-based speech synthesis system, named WORLD, was developed in an e ﬀ ort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been di ﬃ cult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The e ﬀ ectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.", "metadata": {"arxiv_id": "semantic_scholar:ba91dabec842d507a647aab97ad224b4abdc1635", "title": "WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications", "summary": "SUMMARY A vocoder-based speech synthesis system, named WORLD, was developed in an e ﬀ ort to improve the sound quality of real-time applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been di ﬃ cult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The e ﬀ ectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.", "source": "semantic_scholar", "source_id": "ba91dabec842d507a647aab97ad224b4abdc1635", "publication_date": "2016-07-01"}, "round-A_JuniorNano_output": {"reasoning": "This paper describes a vocoder-based real-time speech synthesis system (WORLD) with no discrete-token/tokenization design or analysis, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This paper describes a vocoder-based real-time speech synthesis system (WORLD) with no discrete-token/tokenization design or analysis, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents a vocoder-based speech synthesis system focused on improving sound quality and real-time processing speed, but it does not discuss the discretization of audio into tokens, nor does it mention neural audio codecs, vector quantization, or any discrete token representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their role in downstream generative or alignment tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents a vocoder-based speech synthesis system focused on improving sound quality and real-time processing speed, but it does not discuss the discretization of audio into tokens, nor does it mention neural audio codecs, vector quantization, or any discrete token representations. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their role in downstream generative or alignment tasks.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ren2020fastspeech", "query_title": "{FastSpeech 2: Fast and High-Quality End-to-End Text to Speech}", "normalized_title": "fastspeech 2 fast and high quality end to end text to speech", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.", "source": "arxiv", "source_id": "2006.04558v8", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.", "metadata": {"arxiv_id": "2006.04558v8", "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech", "summary": "Non-autoregressive text to speech (TTS) models such as FastSpeech can synthesize speech significantly faster than previous autoregressive models with comparable quality. The training of FastSpeech model relies on an autoregressive teacher model for duration prediction (to provide more information as input) and knowledge distillation (to simplify the data distribution in output), which can ease the one-to-many mapping problem (i.e., multiple speech variations correspond to the same text) in TTS. However, FastSpeech has several disadvantages: 1) the teacher-student distillation pipeline is complicated and time-consuming, 2) the duration extracted from the teacher model is not accurate enough, and the target mel-spectrograms distilled from teacher model suffer from information loss due to data simplification, both of which limit the voice quality. In this paper, we propose FastSpeech 2, which addresses the issues in FastSpeech and better solves the one-to-many mapping problem in TTS by 1) directly training the model with ground-truth target instead of the simplified output from teacher, and 2) introducing more variation information of speech (e.g., pitch, energy and more accurate duration) as conditional inputs. Specifically, we extract duration, pitch and energy from speech waveform and directly take them as conditional inputs in training and use predicted values in inference. We further design FastSpeech 2s, which is the first attempt to directly generate speech waveform from text in parallel, enjoying the benefit of fully end-to-end inference. Experimental results show that 1) FastSpeech 2 achieves a 3x training speed-up over FastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech 2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even surpass autoregressive models. Audio samples are available at https://speechresearch.github.io/fastspeech2/.", "authors": ["Yi Ren", "Chenxu Hu", "Xu Tan", "Tao Qin", "Sheng Zhao", "Zhou Zhao", "Tie-Yan Liu"], "published": "2020-06-08T13:05:40Z", "updated": "2022-08-08T01:53:05Z", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2006.04558v8", "landing_url": "https://arxiv.org/abs/2006.04558v8", "doi": "https://doi.org/10.48550/arXiv.2006.04558"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on FastSpeech 2, a non-autoregressive TTS system using mel-spectrograms and waveform generation, with no discretized audio tokens or tokenization design discussed; thus it fails to meet the 'Discrete Audio Tokens' criterion and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on FastSpeech 2, a non-autoregressive TTS system using mel-spectrograms and waveform generation, with no discretized audio tokens or tokenization design discussed; thus it fails to meet the 'Discrete Audio Tokens' criterion and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on FastSpeech 2, an end-to-end TTS model improving speed and quality by using ground-truth speech features (duration, pitch, energy) as conditional inputs, but it does not discuss discretization of continuous audio into discrete token sequences or the design and use of discrete audio tokens as core representations in modeling, generation, or cross-modal tasks; therefore, it does not meet the inclusion criteria which require discrete token usage and description.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on FastSpeech 2, an end-to-end TTS model improving speed and quality by using ground-truth speech features (duration, pitch, energy) as conditional inputs, but it does not discuss discretization of continuous audio into discrete token sequences or the design and use of discrete audio tokens as core representations in modeling, generation, or cross-modal tasks; therefore, it does not meet the inclusion criteria which require discrete token usage and description.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kim2021conditional", "query_title": "{Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech}", "normalized_title": "conditional variational autoencoder with adversarial learning for end to end text to speech", "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech", "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.", "source": "arxiv", "source_id": "2106.06103v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech", "abstract": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.", "metadata": {"arxiv_id": "2106.06103v1", "title": "Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech", "summary": "Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best publicly available TTS systems and achieves a MOS comparable to ground truth.", "authors": ["Jaehyeon Kim", "Jungil Kong", "Juhee Son"], "published": "2021-06-11T01:07:12Z", "updated": "2021-06-11T01:07:12Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2106.06103v1", "landing_url": "https://arxiv.org/abs/2106.06103v1", "doi": "https://doi.org/10.48550/arXiv.2106.06103"}, "round-A_JuniorNano_output": {"reasoning": "The abstract focuses on end-to-end TTS using variational autoencoder, normalizing flows, and adversarial training, with no mention of discrete audio tokens, tokenization, or token-based modeling.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract focuses on end-to-end TTS using variational autoencoder, normalizing flows, and adversarial training, with no mention of discrete audio tokens, tokenization, or token-based modeling.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes an end-to-end text-to-speech system that improves sample quality using variational inference, adversarial training, and a stochastic duration predictor, but it does not mention the use or design of discrete audio tokens, neural audio codecs, or quantization methods that generate token sequences representing audio; therefore, it lacks evidence of employing or analyzing discrete token representations as defined in the inclusion criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes an end-to-end text-to-speech system that improves sample quality using variational inference, adversarial training, and a stochastic duration predictor, but it does not mention the use or design of discrete audio tokens, neural audio codecs, or quantization methods that generate token sequences representing audio; therefore, it lacks evidence of employing or analyzing discrete token representations as defined in the inclusion criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "ju2024naturalspeech", "query_title": "{NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models}", "normalized_title": "naturalspeech 3 zero shot speech synthesis with factorized codec and diffusion models", "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models", "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.", "source": "arxiv", "source_id": "2403.03100v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models", "abstract": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.", "metadata": {"arxiv_id": "2403.03100v3", "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models", "summary": "While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can effectively and efficiently model intricate speech with disentangled subspaces in a divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the state-of-the-art TTS systems on quality, similarity, prosody, and intelligibility, and achieves on-par quality with human recordings. Furthermore, we achieve better performance by scaling to 1B parameters and 200K hours of training data.", "authors": ["Zeqian Ju", "Yuancheng Wang", "Kai Shen", "Xu Tan", "Detai Xin", "Dongchao Yang", "Yanqing Liu", "Yichong Leng", "Kaitao Song", "Siliang Tang", "Zhizheng Wu", "Tao Qin", "Xiang-Yang Li", "Wei Ye", "Shikun Zhang", "Jiang Bian", "Lei He", "Jinyu Li", "Sheng Zhao"], "published": "2024-03-05T16:35:25Z", "updated": "2024-04-23T08:38:03Z", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "pdf_url": "https://arxiv.org/pdf/2403.03100v3", "landing_url": "https://arxiv.org/abs/2403.03100v3", "doi": "https://doi.org/10.48550/arXiv.2403.03100"}, "round-A_JuniorNano_output": {"reasoning": "The paper uses a neural codec with factorized vector quantization to produce discrete tokens (content, prosody, timbre, etc.) and a factorized diffusion model to generate these discrete subspaces, aligning with the discrete audio token criteria for TTS applications.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper uses a neural codec with factorized vector quantization to produce discrete tokens (content, prosody, timbre, etc.) and a factorized diffusion model to generate these discrete subspaces, aligning with the discrete audio token criteria for TTS applications.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The paper presents NaturalSpeech 3, which uses a neural codec with factorized vector quantization to disentangle speech into subspaces (content, prosody, timbre, acoustic details), thus representing speech as discrete tokens; this aligns with the inclusion criteria focusing on neural audio codecs producing discrete token sequences for speech synthesis. Moreover, the work addresses token design and its effects on quality and controllability in a TTS task, fitting the theme's emphasis on token design choices and downstream tasks, and the paper is in English with sufficient methodological detail. Hence, the study satisfies all inclusion criteria and none of the exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The paper presents NaturalSpeech 3, which uses a neural codec with factorized vector quantization to disentangle speech into subspaces (content, prosody, timbre, acoustic details), thus representing speech as discrete tokens; this aligns with the inclusion criteria focusing on neural audio codecs producing discrete token sequences for speech synthesis. Moreover, the work addresses token design and its effects on quality and controllability in a TTS task, fitting the theme's emphasis on token design choices and downstream tasks, and the paper is in English with sufficient methodological detail. Hence, the study satisfies all inclusion criteria and none of the exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "mcauliffe2017montreal", "query_title": "{Montreal forced aligner: Trainable text-speech alignment using {k}aldi.}", "normalized_title": "montreal forced aligner trainable text speech alignment using kaldi", "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi", "abstract": "We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.", "source": "semantic_scholar", "source_id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi", "abstract": "We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.", "metadata": {"arxiv_id": "semantic_scholar:9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7", "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi", "summary": "We present a novel deep-learning based approach to producing animator-centric speech motion curves that drive a JALI or standard FACS-based production face-rig, directly from input audio. Our three-stage Long Short-Term Memory (LSTM) network architecture is motivated by psycho-linguistic insights: segmenting speech audio into a stream of phonetic-groups is sufficient for viseme construction; speech styles like mumbling or shouting are strongly co-related to the motion of facial landmarks; and animator style is encoded in viseme motion curve profiles. Our contribution is an automatic real-time lip-synchronization from audio solution that integrates seamlessly into existing animation pipelines. We evaluate our results by: cross-validation to ground-truth data; animator critique and edits; visual comparison to recent deep-learning lip-synchronization solutions; and showing our approach to be resilient to diversity in speaker and language.", "source": "semantic_scholar", "source_id": "9e8b06c60722fee06d7f01d4eeaf3ae81e0247d7", "publication_date": "2017-08-20"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on lip-sync visemes for animation and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item centers on lip-sync visemes for animation and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a deep-learning approach for real-time lip-synchronization from audio, involving speech segmentation and facial motion curve generation, but it does not describe any generation or use of discrete audio tokens derived from audio discretization or neural audio codec quantization, nor provide details about discrete token vocabularies or quantization methods; therefore, it does not meet the core inclusion criteria focused on discrete audio tokens for audio modeling or generation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on a deep-learning approach for real-time lip-synchronization from audio, involving speech segmentation and facial motion curve generation, but it does not describe any generation or use of discrete audio tokens derived from audio discretization or neural audio codec quantization, nor provide details about discrete token vocabularies or quantization methods; therefore, it does not meet the core inclusion criteria focused on discrete audio tokens for audio modeling or generation.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "har2025past", "query_title": "{PAST: Phonetic-Acoustic Speech Tokenizer}", "normalized_title": "past phonetic acoustic speech tokenizer", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST", "source": "arxiv", "source_id": "2505.14470v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "PAST: Phonetic-Acoustic Speech Tokenizer", "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST", "metadata": {"arxiv_id": "2505.14470v2", "title": "PAST: Phonetic-Acoustic Speech Tokenizer", "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST", "authors": ["Nadav Har-Tuv", "Or Tal", "Yossi Adi"], "published": "2025-05-20T15:05:14Z", "updated": "2025-06-04T08:23:18Z", "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2505.14470v2", "landing_url": "https://arxiv.org/abs/2505.14470v2", "doi": "https://doi.org/10.48550/arXiv.2505.14470"}, "round-A_JuniorNano_output": {"reasoning": "The item centers on a phonetic-acoustic speech tokenizer and tokenization process, implying discrete audio tokens suitable for downstream modeling, thus matching the inclusion criteria.", "evaluation": 4}, "round-A_JuniorNano_reasoning": "The item centers on a phonetic-acoustic speech tokenizer and tokenization process, implying discrete audio tokens suitable for downstream modeling, thus matching the inclusion criteria.", "round-A_JuniorNano_evaluation": 4, "round-A_JuniorMini_output": {"reasoning": "The abstract presents PAST, an end-to-end speech tokenizer integrating phonetic information without relying on pretrained self-supervised models, implying a supervised phonetic tokenization approach. Although it deals with tokenization of speech, the abstract does not explicitly describe the process of discretization into token sequences, the construction of vocabulary/codebooks, or quantization details necessary to confirm the presence of discrete audio tokens as defined. It highlights applications in speech representation and language modeling but lacks clear evidence on discrete token design choices, vocab settings, or systematic evaluation metrics related to token quality or downstream tasks. Thus, it partially meets the inclusion criteria but misses critical details about discrete token generation and evaluation, making it ambiguous whether the method centers on discrete audio tokens per the topic definition.", "evaluation": 3}, "round-A_JuniorMini_reasoning": "The abstract presents PAST, an end-to-end speech tokenizer integrating phonetic information without relying on pretrained self-supervised models, implying a supervised phonetic tokenization approach. Although it deals with tokenization of speech, the abstract does not explicitly describe the process of discretization into token sequences, the construction of vocabulary/codebooks, or quantization details necessary to confirm the presence of discrete audio tokens as defined. It highlights applications in speech representation and language modeling but lacks clear evidence on discrete token design choices, vocab settings, or systematic evaluation metrics related to token quality or downstream tasks. Thus, it partially meets the inclusion criteria but misses critical details about discrete token generation and evaluation, making it ambiguous whether the method centers on discrete audio tokens per the topic definition.", "round-A_JuniorMini_evaluation": 3, "round-B_SeniorLead_output": {"reasoning": "The paper describes a phonetic-acoustic “tokenizer” that jointly models phonetics and signal reconstruction and is reported to serve as a representation for speech language models, but the abstract does not explicitly state key discrete-token details required by the inclusion criteria (e.g., quantization/codebook/vocabulary, bitrate or explicit discretization procedure); junior reviewers were split (4 vs 3), so I judge this as uncertain.", "evaluation": 3}, "round-B_SeniorLead_reasoning": "The paper describes a phonetic-acoustic “tokenizer” that jointly models phonetics and signal reconstruction and is reported to serve as a representation for speech language models, but the abstract does not explicitly state key discrete-token details required by the inclusion criteria (e.g., quantization/codebook/vocabulary, bitrate or explicit discretization procedure); junior reviewers were split (4 vs 3), so I judge this as uncertain.", "round-B_SeniorLead_evaluation": 3, "final_verdict": "maybe (senior:3)", "review_skipped": false, "discard_reason": "review_needs_followup"}]}
{"key": "10889092", "query_title": "{What Are They Doing? Joint Audio-Speech Co-Reasoning}", "normalized_title": "what are they doing joint audio speech co reasoning", "title": "What Are They Doing? Joint Audio-Speech Co-Reasoning", "abstract": "In audio and speech processing, tasks usually focus on either the audio or speech modality, even when both sounds and human speech are present in the same audio clip. Recent Auditory Large Language Models (ALLMs) have made it possible to process audio and speech simultaneously within a single model, leading to further considerations of joint audio-speech tasks. In this paper, we establish a novel benchmark to investigate how well ALLMs can perform joint audio-speech processing. Specifically, we introduce Joint Audio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech processing, strictly requiring co-reasoning across both modalities. We also release a scene-reasoning dataset called \"What Are They Doing\". Additionally, we provide deeper insights into the models' behaviors by analyzing their dependence on each modality.", "source": "arxiv", "source_id": "2409.14526v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "What Are They Doing? Joint Audio-Speech Co-Reasoning", "abstract": "In audio and speech processing, tasks usually focus on either the audio or speech modality, even when both sounds and human speech are present in the same audio clip. Recent Auditory Large Language Models (ALLMs) have made it possible to process audio and speech simultaneously within a single model, leading to further considerations of joint audio-speech tasks. In this paper, we establish a novel benchmark to investigate how well ALLMs can perform joint audio-speech processing. Specifically, we introduce Joint Audio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech processing, strictly requiring co-reasoning across both modalities. We also release a scene-reasoning dataset called \"What Are They Doing\". Additionally, we provide deeper insights into the models' behaviors by analyzing their dependence on each modality.", "metadata": {"arxiv_id": "2409.14526v2", "title": "What Are They Doing? Joint Audio-Speech Co-Reasoning", "summary": "In audio and speech processing, tasks usually focus on either the audio or speech modality, even when both sounds and human speech are present in the same audio clip. Recent Auditory Large Language Models (ALLMs) have made it possible to process audio and speech simultaneously within a single model, leading to further considerations of joint audio-speech tasks.\n  In this paper, we establish a novel benchmark to investigate how well ALLMs can perform joint audio-speech processing. Specifically, we introduce Joint Audio-Speech Co-Reasoning (JASCO), a novel task that unifies audio and speech processing, strictly requiring co-reasoning across both modalities. We also release a scene-reasoning dataset called \"What Are They Doing\". Additionally, we provide deeper insights into the models' behaviors by analyzing their dependence on each modality.", "authors": ["Yingzhi Wang", "Pooneh Mousavi", "Artem Ploujnikov", "Mirco Ravanelli"], "published": "2024-09-22T16:45:57Z", "updated": "2025-01-12T10:48:00Z", "categories": ["cs.SD", "cs.CL", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.14526v2", "landing_url": "https://arxiv.org/abs/2409.14526v2", "doi": "https://doi.org/10.48550/arXiv.2409.14526"}, "round-A_JuniorNano_output": {"reasoning": "The item lacks any discussion of discrete audio tokens, tokenizers, or token-based representations; it centers on joint audio-speech co-reasoning with ALLMs rather than discrete-token modeling, so it fails the inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item lacks any discussion of discrete audio tokens, tokenizers, or token-based representations; it centers on joint audio-speech co-reasoning with ALLMs rather than discrete-token modeling, so it fails the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract focus on joint audio-speech co-reasoning using Auditory Large Language Models (ALLMs), but they do not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of audio into token sequences. The study appears to focus on multimodal reasoning rather than discrete token generation or analysis, which is central to the inclusion criteria. Therefore, it does not meet the key requirements about discrete audio token representation or methodology.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract focus on joint audio-speech co-reasoning using Auditory Large Language Models (ALLMs), but they do not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of audio into token sequences. The study appears to focus on multimodal reasoning rather than discrete token generation or analysis, which is central to the inclusion criteria. Therefore, it does not meet the key requirements about discrete audio token representation or methodology.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "zengscaling", "query_title": "{Scaling Speech-Text Pre-training with Synthetic Interleaved Data}", "normalized_title": "scaling speech text pre training with synthetic interleaved data", "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.", "source": "arxiv", "source_id": "2411.17607v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.", "metadata": {"arxiv_id": "2411.17607v2", "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data", "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.", "authors": ["Aohan Zeng", "Zhengxiao Du", "Mingdao Liu", "Lei Zhang", "Shengmin Jiang", "Yuxiao Dong", "Jie Tang"], "published": "2024-11-26T17:19:09Z", "updated": "2024-12-02T16:13:24Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.17607v2", "landing_url": "https://arxiv.org/abs/2411.17607v2", "doi": "https://doi.org/10.48550/arXiv.2411.17607"}, "round-A_JuniorNano_output": {"reasoning": "This item explicitly centers on discrete speech tokens produced via vector-quantized bottlenecks/multi-codebooks and examines their design and impact on downstream speech-language tasks, aligning with the Discrete Audio Tokens topic; it provides English abstract with tokenization details and does not appear to meet any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "This item explicitly centers on discrete speech tokens produced via vector-quantized bottlenecks/multi-codebooks and examines their design and impact on downstream speech-language tasks, aligning with the Discrete Audio Tokens topic; it provides English abstract with tokenization details and does not appear to meet any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a method involving a supervised speech tokenizer with a vector-quantized bottleneck, producing discrete speech tokens from continuous speech, which aligns with the inclusion criteria of discrete audio tokens generated via neural audio codecs with vector quantization. The work focuses on scalability and quality of these discrete tokens and applies them in speech language modeling and spoken question answering, demonstrating empirical evaluation. This satisfies the inclusion criteria around discrete token generation, technical detail, and downstream usage. There is no indication of exclusion criteria being met, such as lacking discrete tokens or focusing on only classification tasks without token design analysis. Therefore, this study is highly relevant and meets the requirements for inclusion.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The abstract describes a method involving a supervised speech tokenizer with a vector-quantized bottleneck, producing discrete speech tokens from continuous speech, which aligns with the inclusion criteria of discrete audio tokens generated via neural audio codecs with vector quantization. The work focuses on scalability and quality of these discrete tokens and applies them in speech language modeling and spoken question answering, demonstrating empirical evaluation. This satisfies the inclusion criteria around discrete token generation, technical detail, and downstream usage. There is no indication of exclusion criteria being met, such as lacking discrete tokens or focusing on only classification tasks without token design analysis. Therefore, this study is highly relevant and meets the requirements for inclusion.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "yosha2025stresstest", "query_title": "{{StressTest}: Can YOUR Speech {LM} Handle the Stress?}", "normalized_title": "stresstest can your speech lm handle the stress", "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "abstract": "Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.", "source": "arxiv", "source_id": "2505.22765v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "StressTest: Can YOUR Speech LM Handle the Stress?", "abstract": "Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.", "metadata": {"arxiv_id": "2505.22765v2", "title": "StressTest: Can YOUR Speech LM Handle the Stress?", "summary": "Sentence stress refers to emphasis on words within a spoken utterance to highlight or contrast an idea. It is often used to imply an underlying intention not explicitly stated. Recent speech-aware language models (SLMs) have enabled direct audio processing, allowing models to access the full richness of speech to perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and intent, it remains largely overlooked in evaluation and development of SLMs. We address this gap by introducing StressTest, a benchmark designed to evaluate models' ability to distinguish between meanings of speech based on the stress pattern. We evaluate leading SLMs, and find that despite their overall capabilities, they perform poorly on such tasks. Hence, we propose a novel data generation pipeline, and create Stress-17k, a training set that simulates change of meaning implied by stress variation. Results suggest, that our finetuned model, StresSLM, generalizes well to real recordings and notably outperforms existing SLMs on sentence stress reasoning and detection. Models, code, data, samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest.", "authors": ["Iddo Yosha", "Gallil Maimon", "Yossi Adi"], "published": "2025-05-28T18:32:56Z", "updated": "2025-10-05T12:21:35Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2505.22765v2", "landing_url": "https://arxiv.org/abs/2505.22765v2", "doi": "https://doi.org/10.48550/arXiv.2505.22765"}, "round-A_JuniorNano_output": {"reasoning": "The item focuses on sentence stress and benchmarking speech models without detailing discrete audio token generation or token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item focuses on sentence stress and benchmarking speech models without detailing discrete audio token generation or token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study focuses on evaluating speech language models' ability to interpret sentence stress, using continuous speech data for audio reasoning tasks; however, it does not describe or propose any discrete audio tokenization method, neural audio codec, or vector quantization process to convert continuous waveform into discrete tokens. There is no mention of token vocabularies, codebooks, or discrete token sequences being core to the methodology or evaluation. Therefore, it does not meet the inclusion criteria related to discrete audio token-based approaches, and it aligns more with modeling continuous speech representations for downstream tasks rather than discrete tokenization for generation, alignment, or multimodal LLM use indicated by the criteria.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study focuses on evaluating speech language models' ability to interpret sentence stress, using continuous speech data for audio reasoning tasks; however, it does not describe or propose any discrete audio tokenization method, neural audio codec, or vector quantization process to convert continuous waveform into discrete tokens. There is no mention of token vocabularies, codebooks, or discrete token sequences being core to the methodology or evaluation. Therefore, it does not meet the inclusion criteria related to discrete audio token-based approaches, and it aligns more with modeling continuous speech representations for downstream tasks rather than discrete tokenization for generation, alignment, or multimodal LLM use indicated by the criteria.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "mousavi2025listen", "query_title": "{LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs}", "normalized_title": "listen learning soft token embeddings for neural audio llms", "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "abstract": "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.", "source": "arxiv", "source_id": "2505.18517v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "abstract": "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.", "metadata": {"arxiv_id": "2505.18517v1", "title": "LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs", "summary": "Foundation models based on large language models (LLMs) have shown great success in handling various tasks and modalities. However, adapting these models for general-purpose audio-language tasks is challenging due to differences in acoustic environments and task variations. In this work, we introduce LiSTEN Learning Soft Token Embeddings for Neural Audio LLMs), a framework for adapting LLMs to speech and audio tasks. LiSTEN uses a dynamic prompt selection strategy with learnable key-value pairs, allowing the model to balance general and task-specific knowledge while avoiding overfitting in a multitask setting. Our approach reduces dependence on large-scale ASR or captioning datasets, achieves competitive performance with fewer trainable parameters, and simplifies training by using a single-stage process. Additionally, LiSTEN enhances interpretability by analyzing the diversity and overlap of selected prompts across different tasks.", "authors": ["Pooneh Mousavi", "Shubham Gupta", "Cem Subakan", "Mirco Ravanelli"], "published": "2025-05-24T05:28:22Z", "updated": "2025-05-24T05:28:22Z", "categories": ["cs.AI", "cs.LG", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2505.18517v1", "landing_url": "https://arxiv.org/abs/2505.18517v1", "doi": "https://doi.org/10.48550/arXiv.2505.18517"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on learning soft token embeddings and dynamic prompts to adapt LLMs to audio tasks, without describing discrete audio token generation or a token/codec vocabulary, so it does not meet the discrete audio token criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on learning soft token embeddings and dynamic prompts to adapt LLMs to audio tasks, without describing discrete audio token generation or a token/codec vocabulary, so it does not meet the discrete audio token criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper LiSTEN introduces a framework for adapting LLMs to speech and audio tasks using soft token embeddings and dynamic prompt selection, but it does not describe discrete audio tokens derived from discretizing continuous audio waveforms into finite vocabularies with explicit tokenizer/codec architectures, quantization methods, or codebook settings. It focuses more on embeddings and model adaptation rather than proposing or analyzing discrete token representations or their impacts on downstream tasks, thus failing to meet the inclusion criteria focused on discrete audio tokens for generation or cross-modal alignment.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper LiSTEN introduces a framework for adapting LLMs to speech and audio tasks using soft token embeddings and dynamic prompt selection, but it does not describe discrete audio tokens derived from discretizing continuous audio waveforms into finite vocabularies with explicit tokenizer/codec architectures, quantization methods, or codebook settings. It focuses more on embeddings and model adaptation rather than proposing or analyzing discrete token representations or their impacts on downstream tasks, thus failing to meet the inclusion criteria focused on discrete audio tokens for generation or cross-modal alignment.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "kumar2024sila", "query_title": "{SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation}", "normalized_title": "sila signal to language augmentation for enhanced control in text to audio generation", "title": "SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation", "abstract": "The field of text-to-audio generation has seen significant advancements, and yet the ability to finely control the acoustic characteristics of generated audio remains under-explored. In this paper, we introduce a novel yet simple approach to generate sound effects with control over key acoustic parameters such as loudness, pitch, reverb, fade, brightness, noise and duration, enabling creative applications in sound design and content creation. These parameters extend beyond traditional Digital Signal Processing (DSP) techniques, incorporating learned representations that capture the subtleties of how sound characteristics can be shaped in context, enabling a richer and more nuanced control over the generated audio. Our approach is model-agnostic and is based on learning the disentanglement between audio semantics and its acoustic features. Our approach not only enhances the versatility and expressiveness of text-to-audio generation but also opens new avenues for creative audio production and sound design. Our objective and subjective evaluation results demonstrate the effectiveness of our approach in producing high-quality, customizable audio outputs that align closely with user specifications.", "source": "arxiv", "source_id": "2412.09789v1", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation", "abstract": "The field of text-to-audio generation has seen significant advancements, and yet the ability to finely control the acoustic characteristics of generated audio remains under-explored. In this paper, we introduce a novel yet simple approach to generate sound effects with control over key acoustic parameters such as loudness, pitch, reverb, fade, brightness, noise and duration, enabling creative applications in sound design and content creation. These parameters extend beyond traditional Digital Signal Processing (DSP) techniques, incorporating learned representations that capture the subtleties of how sound characteristics can be shaped in context, enabling a richer and more nuanced control over the generated audio. Our approach is model-agnostic and is based on learning the disentanglement between audio semantics and its acoustic features. Our approach not only enhances the versatility and expressiveness of text-to-audio generation but also opens new avenues for creative audio production and sound design. Our objective and subjective evaluation results demonstrate the effectiveness of our approach in producing high-quality, customizable audio outputs that align closely with user specifications.", "metadata": {"arxiv_id": "2412.09789v1", "title": "SILA: Signal-to-Language Augmentation for Enhanced Control in Text-to-Audio Generation", "summary": "The field of text-to-audio generation has seen significant advancements, and yet the ability to finely control the acoustic characteristics of generated audio remains under-explored. In this paper, we introduce a novel yet simple approach to generate sound effects with control over key acoustic parameters such as loudness, pitch, reverb, fade, brightness, noise and duration, enabling creative applications in sound design and content creation. These parameters extend beyond traditional Digital Signal Processing (DSP) techniques, incorporating learned representations that capture the subtleties of how sound characteristics can be shaped in context, enabling a richer and more nuanced control over the generated audio. Our approach is model-agnostic and is based on learning the disentanglement between audio semantics and its acoustic features. Our approach not only enhances the versatility and expressiveness of text-to-audio generation but also opens new avenues for creative audio production and sound design. Our objective and subjective evaluation results demonstrate the effectiveness of our approach in producing high-quality, customizable audio outputs that align closely with user specifications.", "authors": ["Sonal Kumar", "Prem Seetharaman", "Justin Salamon", "Dinesh Manocha", "Oriol Nieto"], "published": "2024-12-13T02:07:21Z", "updated": "2024-12-13T02:07:21Z", "categories": ["cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2412.09789v1", "landing_url": "https://arxiv.org/abs/2412.09789v1", "doi": "https://doi.org/10.48550/arXiv.2412.09789"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on continuous control of text-to-audio generation without describing discrete audio tokens, tokenizers, or codec-based tokenization, thus it does not meet the inclusion criteria and aligns with the exclusion criteria for 'Discrete Audio Tokens'.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The paper focuses on continuous control of text-to-audio generation without describing discrete audio tokens, tokenizers, or codec-based tokenization, thus it does not meet the inclusion criteria and aligns with the exclusion criteria for 'Discrete Audio Tokens'.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The study presents a novel approach for controllable text-to-audio generation focusing on acoustic parameters control but does not mention any use or design of discrete audio tokens, neural audio codecs, vector quantization, or any tokenization processes central to the inclusion criteria. The emphasis lies on learning disentangled acoustic features rather than discretizing audio into token sequences for modeling or generation. Furthermore, there are no details about tokenizer architecture, quantization methods, codebook configurations, or any evaluation related to discrete token design or usage. Thus, it does not meet the core inclusion criteria focused on discrete audio token research and should be excluded.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The study presents a novel approach for controllable text-to-audio generation focusing on acoustic parameters control but does not mention any use or design of discrete audio tokens, neural audio codecs, vector quantization, or any tokenization processes central to the inclusion criteria. The emphasis lies on learning disentangled acoustic features rather than discretizing audio into token sequences for modeling or generation. Furthermore, there are no details about tokenizer architecture, quantization methods, codebook configurations, or any evaluation related to discrete token design or usage. Thus, it does not meet the core inclusion criteria focused on discrete audio token research and should be excluded.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "chen2025video", "query_title": "{Video-guided foley sound generation with multimodal controls}", "normalized_title": "video guided foley sound generation with multimodal controls", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "source": "arxiv", "source_id": "2411.17698v4", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "metadata": {"arxiv_id": "2411.17698v4", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "summary": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "authors": ["Ziyang Chen", "Prem Seetharaman", "Bryan Russell", "Oriol Nieto", "David Bourgin", "Andrew Owens", "Justin Salamon"], "published": "2024-11-26T18:59:58Z", "updated": "2025-03-17T17:44:37Z", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.17698v4", "landing_url": "https://arxiv.org/abs/2411.17698v4", "doi": "https://doi.org/10.48550/arXiv.2411.17698"}, "round-A_JuniorNano_output": {"reasoning": "The abstract covers video-guided sound generation with multimodal conditioning but does not discuss discrete audio tokens, tokenizers, or neural codec-based token representations; thus it does not meet the ‘Discrete Audio Tokens’ criterion and should be excluded under the stated exclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The abstract covers video-guided sound generation with multimodal conditioning but does not discuss discrete audio tokens, tokenizers, or neural codec-based token representations; thus it does not meet the ‘Discrete Audio Tokens’ criterion and should be excluded under the stated exclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The abstract describes a model for video-guided Foley sound generation using multimodal inputs, focusing on generating synchronized high-quality sounds. However, there is no explicit mention of the use of discrete audio tokens, such as neural audio codecs or vector quantization processes that produce token sequences, nor details about tokenization, codebooks, or discrete representation design, which are required to meet the inclusion criteria. The study seems to focus on continuous audio generation conditioned on multimodal inputs rather than discrete token-based audio modeling, thus failing the core inclusion requirements.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The abstract describes a model for video-guided Foley sound generation using multimodal inputs, focusing on generating synchronized high-quality sounds. However, there is no explicit mention of the use of discrete audio tokens, such as neural audio codecs or vector quantization processes that produce token sequences, nor details about tokenization, codebooks, or discrete representation design, which are required to meet the inclusion criteria. The study seems to focus on continuous audio generation conditioned on multimodal inputs rather than discrete token-based audio modeling, thus failing the core inclusion requirements.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "Video-Guided Foley Sound Generation with Multimodal Controls", "abstract": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "metadata": {"arxiv_id": "2411.17698v4", "title": "Video-Guided Foley Sound Generation with Multimodal Controls", "summary": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/", "authors": ["Ziyang Chen", "Prem Seetharaman", "Bryan Russell", "Oriol Nieto", "David Bourgin", "Andrew Owens", "Justin Salamon"], "published": "2024-11-26T18:59:58Z", "updated": "2025-03-17T17:44:37Z", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2411.17698v4", "landing_url": "https://arxiv.org/abs/2411.17698v4", "doi": "https://doi.org/10.48550/arXiv.2411.17698"}, "round-A_JuniorNano_output": {"reasoning": "The item describes video-guided sound generation with multimodal controls, but there is no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the discrete-audio-token inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The item describes video-guided sound generation with multimodal controls, but there is no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the discrete-audio-token inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on video-guided sound generation using multimodal controls, particularly generating synchronized high-quality sounds conditioned on video, text, and audio inputs. However, the abstract lacks any mention or indication of discrete audio tokenization methods, such as the use of neural audio codecs or vector quantization producing discrete token sequences; nor does it specify any quantization or tokenizer design details, vocabulary, codebook settings, or discrete unit representations. The methodology seems to be based on direct sound generation rather than using discrete token representations as core modeling units. Therefore, it does not meet the inclusion criteria requiring clear evidence of discrete audio token generation and modeling, and falls under the exclusion criteria of continuous audio modeling without discrete token use.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on video-guided sound generation using multimodal controls, particularly generating synchronized high-quality sounds conditioned on video, text, and audio inputs. However, the abstract lacks any mention or indication of discrete audio tokenization methods, such as the use of neural audio codecs or vector quantization producing discrete token sequences; nor does it specify any quantization or tokenizer design details, vocabulary, codebook settings, or discrete unit representations. The methodology seems to be based on direct sound generation rather than using discrete token representations as core modeling units. Therefore, it does not meet the inclusion criteria requiring clear evidence of discrete audio token generation and modeling, and falls under the exclusion criteria of continuous audio modeling without discrete token use.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "international1993coding", "query_title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "normalized_title": "coding of moving pictures and associated audio for digital storage media at up to about 1 5 mbit s", "title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "abstract": "BS EN ISO/IEC 11172-4:1997: The Standard for Information technology. Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s - Compliance testing", "source": "bsi", "source_id": "https://knowledge.bsigroup.com/products/information-technology-coding-of-moving-pictures-and-associated-audio-for-digital-storage-media-at-up-to-about-1-5-mbit-s-compliance-testing", "match_status": "fuzzy_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "abstract": "BS EN ISO/IEC 11172-4:1997: The Standard for Information technology. Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s - Compliance testing", "metadata": {"arxiv_id": "bsi:https://knowledge.bsigroup.com/products/information-technology-coding-of-moving-pictures-and-associated-audio-for-digital-storage-media-at-up-to-about-1-5-mbit-s-compliance-testing", "title": "{Coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s}", "summary": "BS EN ISO/IEC 11172-4:1997: The Standard for Information technology. Coding of moving pictures and associated audio for digital storage media at up to about 1,5 Mbit/s - Compliance testing", "source": "bsi", "source_id": "https://knowledge.bsigroup.com/products/information-technology-coding-of-moving-pictures-and-associated-audio-for-digital-storage-media-at-up-to-about-1-5-mbit-s-compliance-testing", "year": 1997}, "round-A_JuniorNano_output": {"reasoning": "This item is a standards/compliance document (BS EN ISO/IEC 11172-4:1997) about coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s and compliance testing; it discusses traditional coding standards and does not mention discrete audio tokens, tokenizers, neural audio codecs, or token-based downstream modeling, so it does not meet the inclusion criteria and should be excluded. Explanation: No discrete tokenization or token-based modeling content is present.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "This item is a standards/compliance document (BS EN ISO/IEC 11172-4:1997) about coding of moving pictures and associated audio for digital storage media at up to about 1.5 Mbit/s and compliance testing; it discusses traditional coding standards and does not mention discrete audio tokens, tokenizers, neural audio codecs, or token-based downstream modeling, so it does not meet the inclusion criteria and should be excluded. Explanation: No discrete tokenization or token-based modeling content is present.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The title and abstract refer solely to a standard for coding moving pictures and associated audio for digital storage media at a specified bitrate, without discussion of discrete audio tokens, neural audio codecs, tokenization processes, or downstream generation or multimodal tasks; thus, it does not meet the inclusion criteria which require research on discrete audio tokens or related mechanisms and excludes traditional audio compression standards without focus on discrete token representation.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The title and abstract refer solely to a standard for coding moving pictures and associated audio for digital storage media at a specified bitrate, without discussion of discrete audio tokens, neural audio codecs, tokenization processes, or downstream generation or multimodal tasks; thus, it does not meet the inclusion criteria which require research on discrete audio tokens or related mechanisms and excludes traditional audio compression standards without focus on discrete token representation.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "6530580", "query_title": "{MPEG Unified Speech and Audio Coding}", "normalized_title": "mpeg unified speech and audio coding", "title": "MPEG Unified Speech and Audio Coding", "abstract": "The MPEG Audio Subgroup has a rich history of accomplishments in creating music coding technology. At higher bit rates, MPEG technology can represent arbitrary sounds, including the human voice, with excellent quality. MPEG-1 and MPEG-2 Audio coders use perceptually shaped quantization noise as the primary tool for achieving compression. The MPEG-4 High-Efficiency Advanced Audio Coding (AAC) standard is a single technology capable of compressing speech, speech mixed with music, or music signals with quality that is always at least as good as the best of two state-of-the-art reference codecs, one optimized for speech and mixed content (AMR-WB B;) and the other optimized for music and general audio (HE-AACv2). This article provides an overview of the USAC architecture and summarizes the performance relative to the best state-of-the-art speech and audio codecs.", "source": "semantic_scholar", "source_id": "9d0ea1deb125e736478c6a07c89519c40eb7f154", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "MPEG Unified Speech and Audio Coding", "abstract": "The MPEG Audio Subgroup has a rich history of accomplishments in creating music coding technology. At higher bit rates, MPEG technology can represent arbitrary sounds, including the human voice, with excellent quality. MPEG-1 and MPEG-2 Audio coders use perceptually shaped quantization noise as the primary tool for achieving compression. The MPEG-4 High-Efficiency Advanced Audio Coding (AAC) standard is a single technology capable of compressing speech, speech mixed with music, or music signals with quality that is always at least as good as the best of two state-of-the-art reference codecs, one optimized for speech and mixed content (AMR-WB B;) and the other optimized for music and general audio (HE-AACv2). This article provides an overview of the USAC architecture and summarizes the performance relative to the best state-of-the-art speech and audio codecs.", "metadata": {"arxiv_id": "semantic_scholar:9d0ea1deb125e736478c6a07c89519c40eb7f154", "title": "MPEG Unified Speech and Audio Coding", "summary": "The MPEG Audio Subgroup has a rich history of accomplishments in creating music coding technology. At higher bit rates, MPEG technology can represent arbitrary sounds, including the human voice, with excellent quality. MPEG-1 and MPEG-2 Audio coders use perceptually shaped quantization noise as the primary tool for achieving compression. The MPEG-4 High-Efficiency Advanced Audio Coding (AAC) standard is a single technology capable of compressing speech, speech mixed with music, or music signals with quality that is always at least as good as the best of two state-of-the-art reference codecs, one optimized for speech and mixed content (AMR-WB B;) and the other optimized for music and general audio (HE-AACv2). This article provides an overview of the USAC architecture and summarizes the performance relative to the best state-of-the-art speech and audio codecs.", "source": "semantic_scholar", "source_id": "9d0ea1deb125e736478c6a07c89519c40eb7f154", "publication_date": "2011-09-29"}, "round-A_JuniorNano_output": {"reasoning": "This article surveys MPEG audio coding standards and perceptual quantization, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "evaluation": 2}, "round-A_JuniorNano_reasoning": "This article surveys MPEG audio coding standards and perceptual quantization, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 2, "round-A_JuniorMini_output": {"reasoning": "The article discusses MPEG audio coding technologies focusing on compression standards and codec performance but does not specifically address the generation of discrete audio tokens via neural audio codecs or self-supervised learning models, nor does it provide details about tokenization processes, vocabulary, or the use of discrete tokens for modeling or generation tasks as required by the inclusion criteria. Thus, it falls under the exclusion of traditional audio compression without focus on discrete token representations for sequence modeling or downstream generative/understanding tasks.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The article discusses MPEG audio coding technologies focusing on compression standards and codec performance but does not specifically address the generation of discrete audio tokens via neural audio codecs or self-supervised learning models, nor does it provide details about tokenization processes, vocabulary, or the use of discrete tokens for modeling or generation tasks as required by the inclusion criteria. Thus, it falls under the exclusion of traditional audio compression without focus on discrete token representations for sequence modeling or downstream generative/understanding tasks.", "round-A_JuniorMini_evaluation": 1, "final_verdict": "exclude (junior:2)", "review_skipped": false, "discard_reason": "exclude (junior:2)"}]}
{"key": "devlin2019bert", "query_title": "{Bert: Pre-training of deep bidirectional transformers for language understanding}", "normalized_title": "bert pre training of deep bidirectional transformers for language understanding", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "source": "arxiv", "source_id": "1810.04805v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "metadata": {"arxiv_id": "1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "published": "2018-10-11T00:50:01Z", "updated": "2019-05-24T20:37:26Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1810.04805v2", "landing_url": "https://arxiv.org/abs/1810.04805v2", "doi": "https://doi.org/10.48550/arXiv.1810.04805"}, "round-A_JuniorNano_output": {"reasoning": "此項目為BERT的英文語言模型，與離散音訊 token 的主題不符，且未涉及離散音訊 token 的產生、設計或下游任務，因此不符合納入標準。", "evaluation": 1}, "round-A_JuniorNano_reasoning": "此項目為BERT的英文語言模型，與離散音訊 token 的主題不符，且未涉及離散音訊 token 的產生、設計或下游任務，因此不符合納入標準。", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper presents BERT, a language representation model for text, focusing on natural language processing tasks without any mention of processing audio signals or discrete audio tokens as defined in the inclusion criteria. There is no indication the study involves discretization of audio waveforms, neural audio codecs, or tokenization related to audio, thus not meeting the core requirements for inclusion focused on discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper presents BERT, a language representation model for text, focusing on natural language processing tasks without any mention of processing audio signals or discrete audio tokens as defined in the inclusion criteria. There is no indication the study involves discretization of audio waveforms, neural audio codecs, or tokenization related to audio, thus not meeting the core requirements for inclusion focused on discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "metadata": {"arxiv_id": "1810.04805v2", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "summary": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "published": "2018-10-11T00:50:01Z", "updated": "2019-05-24T20:37:26Z", "categories": ["cs.CL"], "pdf_url": "https://arxiv.org/pdf/1810.04805v2", "landing_url": "https://arxiv.org/abs/1810.04805v2", "doi": "https://doi.org/10.48550/arXiv.1810.04805"}, "round-A_JuniorNano_output": {"reasoning": "The article is about BERT for language understanding and does not discuss discrete audio tokens, tokenizers for audio, or audio generation/understanding, thus it does not meet the inclusion criteria.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The article is about BERT for language understanding and does not discuss discrete audio tokens, tokenizers for audio, or audio generation/understanding, thus it does not meet the inclusion criteria.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" focuses exclusively on natural language processing and language representation using bidirectional transformers; it does not discuss or involve discrete audio tokens, audio tokenization methods, or any audio-related token discretization required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for lack of relevance to discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" focuses exclusively on natural language processing and language representation using bidirectional transformers; it does not discuss or involve discrete audio tokens, audio tokenization methods, or any audio-related token discretization required by the inclusion criteria. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria for lack of relevance to discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
{"key": "turetzky2024last", "query_title": "{Last: Language model aware speech tokenization}", "normalized_title": "last language model aware speech tokenization", "title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "source": "arxiv", "source_id": "2409.03701v2", "match_status": "exact_title", "missing_reason": null, "review_count": 2, "reviews": [{"title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "metadata": {"arxiv_id": "2409.03701v2", "title": "LAST: Language Model Aware Speech Tokenization", "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "authors": ["Arnon Turetzky", "Yossi Adi"], "published": "2024-09-05T16:57:39Z", "updated": "2024-09-10T14:45:15Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.03701v2", "landing_url": "https://arxiv.org/abs/2409.03701v2", "doi": "https://doi.org/10.48550/arXiv.2409.03701"}, "round-A_JuniorNano_output": {"reasoning": "The work focuses on discrete speech tokens via a tokenizer/codec for LM-based speech processing and reports downstream evaluation, meeting the inclusion criteria and not falling under any exclusion.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The work focuses on discrete speech tokens via a tokenizer/codec for LM-based speech processing and reports downstream evaluation, meeting the inclusion criteria and not falling under any exclusion.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on a novel approach to speech tokenization that involves discrete speech representations aligned with LMs, aiming to create discrete tokens from continuous speech for improved modeling; it describes the tokenization process, design choices (vocabulary size, feature space), and evaluates impact on speech modeling tasks, aligning well with the discrete audio tokens definition and evaluation criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on a novel approach to speech tokenization that involves discrete speech representations aligned with LMs, aiming to create discrete tokens from continuous speech for improved modeling; it describes the tokenization process, design choices (vocabulary size, feature space), and evaluates impact on speech modeling tasks, aligning well with the discrete audio tokens definition and evaluation criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}, {"title": "LAST: Language Model Aware Speech Tokenization", "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "metadata": {"arxiv_id": "2409.03701v2", "title": "LAST: Language Model Aware Speech Tokenization", "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.", "authors": ["Arnon Turetzky", "Yossi Adi"], "published": "2024-09-05T16:57:39Z", "updated": "2024-09-10T14:45:15Z", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf_url": "https://arxiv.org/pdf/2409.03701v2", "landing_url": "https://arxiv.org/abs/2409.03701v2", "doi": "https://doi.org/10.48550/arXiv.2409.03701"}, "round-A_JuniorNano_output": {"reasoning": "The title and abstract clearly focus on discrete speech tokens/tokenization and their use in downstream speech-LM tasks, covering token design and evaluation of token-level representations, aligning with the inclusion criteria and not triggering any exclusion criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The title and abstract clearly focus on discrete speech tokens/tokenization and their use in downstream speech-LM tasks, covering token design and evaluation of token-level representations, aligning with the inclusion criteria and not triggering any exclusion criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study proposes a novel speech tokenizer leveraging textual language model objectives to generate discrete speech representations suitable for spoken language modeling and speech-to-text tasks. It clearly involves mapping continuous speech into discrete token sequences aligned with language models, fitting within the theme of discrete audio tokens produced via neural methods. The abstract suggests design choices impacting token vocabulary and integration with language models, aligned with systematic analysis and evaluation, which meets inclusion criteria without conflicting with exclusion criteria.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study proposes a novel speech tokenizer leveraging textual language model objectives to generate discrete speech representations suitable for spoken language modeling and speech-to-text tasks. It clearly involves mapping continuous speech into discrete token sequences aligned with language models, fitting within the theme of discrete audio tokens produced via neural methods. The abstract suggests design choices impacting token vocabulary and integration with language models, aligned with systematic analysis and evaluation, which meets inclusion criteria without conflicting with exclusion criteria.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "zheng2025ervq", "query_title": "{ERVQ: Enhanced residual vector quantization with intra-and-inter-codebook optimization for neural audio codecs}", "normalized_title": "ervq enhanced residual vector quantization with intra and inter codebook optimization for neural audio codecs", "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.", "source": "arxiv", "source_id": "2410.12359v2", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.", "metadata": {"arxiv_id": "2410.12359v2", "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs", "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.", "authors": ["Rui-Chen Zheng", "Hui-Peng Du", "Xiao-Hang Jiang", "Yang Ai", "Zhen-Hua Ling"], "published": "2024-10-16T08:21:37Z", "updated": "2025-06-11T08:43:47Z", "categories": ["eess.AS"], "pdf_url": "https://arxiv.org/pdf/2410.12359v2", "landing_url": "https://arxiv.org/abs/2410.12359v2", "doi": "https://doi.org/10.48550/arXiv.2410.12359"}, "round-A_JuniorNano_output": {"reasoning": "The paper focuses on generating discrete tokens via residual vector quantization in neural audio codecs and proposes intra- and inter-codebook optimization to improve token quality/diversity, with an English abstract and discussion of downstream use, matching the 'Discrete Audio Tokens' criteria.", "evaluation": 5}, "round-A_JuniorNano_reasoning": "The paper focuses on generating discrete tokens via residual vector quantization in neural audio codecs and proposes intra- and inter-codebook optimization to improve token quality/diversity, with an English abstract and discussion of downstream use, matching the 'Discrete Audio Tokens' criteria.", "round-A_JuniorNano_evaluation": 5, "round-A_JuniorMini_output": {"reasoning": "The study focuses on Enhanced Residual Vector Quantization in neural audio codecs, which inherently involve discretizing continuous audio waveforms into discrete tokens (codebook entries) for use in downstream models, including large language models for speech generation. It explicitly addresses codebook utilization, quantization processes, and improvements in audio quality and modeling, satisfying the core criteria of discrete audio token generation and use. The abstract is in English and provides sufficient detail on the codebook design and token utilization. Furthermore, it demonstrates downstream application in text-to-speech tasks with large language models, aligning with the research topic's emphasis on token design's impact on downstream tasks and cross-modal modeling. It does not meet any exclusion criteria as it focuses on neural, discrete token-based audio representations rather than continuous representations, traditional codecs, or purely classification tasks.", "evaluation": 5}, "round-A_JuniorMini_reasoning": "The study focuses on Enhanced Residual Vector Quantization in neural audio codecs, which inherently involve discretizing continuous audio waveforms into discrete tokens (codebook entries) for use in downstream models, including large language models for speech generation. It explicitly addresses codebook utilization, quantization processes, and improvements in audio quality and modeling, satisfying the core criteria of discrete audio token generation and use. The abstract is in English and provides sufficient detail on the codebook design and token utilization. Furthermore, it demonstrates downstream application in text-to-speech tasks with large language models, aligning with the research topic's emphasis on token design's impact on downstream tasks and cross-modal modeling. It does not meet any exclusion criteria as it focuses on neural, discrete token-based audio representations rather than continuous representations, traditional codecs, or purely classification tasks.", "round-A_JuniorMini_evaluation": 5, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "include (junior:5)", "review_skipped": false, "discard_reason": null}]}
{"key": "esser2021taming", "query_title": "{Taming transformers for high-resolution image synthesis}", "normalized_title": "taming transformers for high resolution image synthesis", "title": "Taming Transformers for High-Resolution Image Synthesis", "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .", "source": "arxiv", "source_id": "2012.09841v3", "match_status": "exact_title", "missing_reason": null, "review_count": 1, "reviews": [{"title": "Taming Transformers for High-Resolution Image Synthesis", "abstract": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .", "metadata": {"arxiv_id": "2012.09841v3", "title": "Taming Transformers for High-Resolution Image Synthesis", "summary": "Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .", "authors": ["Patrick Esser", "Robin Rombach", "Björn Ommer"], "published": "2020-12-17T18:57:28Z", "updated": "2021-06-23T16:07:21Z", "categories": ["cs.CV"], "pdf_url": "https://arxiv.org/pdf/2012.09841v3", "landing_url": "https://arxiv.org/abs/2012.09841v3", "doi": "https://doi.org/10.48550/arXiv.2012.09841"}, "round-A_JuniorNano_output": {"reasoning": "The title/abstract describe high‑resolution image synthesis with transformers and CNNs, with no discrete audio token design, tokenization method, or audio-domain modeling; therefore it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "evaluation": 1}, "round-A_JuniorNano_reasoning": "The title/abstract describe high‑resolution image synthesis with transformers and CNNs, with no discrete audio token design, tokenization method, or audio-domain modeling; therefore it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.", "round-A_JuniorNano_evaluation": 1, "round-A_JuniorMini_output": {"reasoning": "The paper focuses on high-resolution image synthesis using a combination of CNNs and transformers to model images, without any mention of discrete audio tokens, neural audio codecs, or audio waveforms and their discretization; therefore, it does not meet the inclusion criteria centered on discrete audio token research.", "evaluation": 1}, "round-A_JuniorMini_reasoning": "The paper focuses on high-resolution image synthesis using a combination of CNNs and transformers to model images, without any mention of discrete audio tokens, neural audio codecs, or audio waveforms and their discretization; therefore, it does not meet the inclusion criteria centered on discrete audio token research.", "round-A_JuniorMini_evaluation": 1, "round-B_SeniorLead_output": null, "round-B_SeniorLead_reasoning": null, "round-B_SeniorLead_evaluation": null, "final_verdict": "exclude (junior:1)", "review_skipped": false, "discard_reason": "exclude (junior:1)"}]}
