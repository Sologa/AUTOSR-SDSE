[
  {
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "final_verdict": "include (seed_filter)",
    "review_skipped": true,
    "discard_reason": null,
    "force_include_reason": "seed_filter_selected"
  },
  {
    "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
    "abstract": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
    "metadata": {
      "arxiv_id": "2206.06192",
      "title": "Toward Zero Oracle Word Error Rate on the Switchboard Benchmark",
      "summary": "The \"Switchboard benchmark\" is a very well-known test set in automatic speech recognition (ASR) research, establishing record-setting performance for systems that claim human-level transcription accuracy. This work highlights lesser-known practical considerations of this evaluation, demonstrating major improvements in word error rate (WER) by correcting the reference transcriptions and deviating from the official scoring methodology. In this more detailed and reproducible scheme, even commercial ASR systems can score below 5% WER and the established record for a research system is lowered to 2.3%. An alternative metric of transcript precision is proposed, which does not penalize deletions and appears to be more discriminating for human vs. machine performance. While commercial ASR systems are still below this threshold, a research system is shown to clearly surpass the accuracy of commercial human speech recognition. This work also explores using standardized scoring tools to compute oracle WER by selecting the best among a list of alternatives. A phrase alternatives representation is compared to utterance-level N-best lists and word-level data structures; using dense lattices and adding out-of-vocabulary words, this achieves an oracle WER of 0.18%.",
      "authors": [
        "Arlo Faria",
        "Adam Janin",
        "Korbinian Riedhammer",
        "Sidhi Adkoli"
      ],
      "published": "2022-06-13T14:26:40Z",
      "updated": "2022-06-27T14:44:58Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.06192v2",
      "landing_url": "https://arxiv.org/abs/2206.06192v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.06192"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on ASR evaluation and oracle WER for the Switchboard benchmark, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on ASR evaluation and oracle WER for the Switchboard benchmark, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on automatic speech recognition performance and evaluation metrics on the Switchboard benchmark, without discussing discrete audio tokenization, neural audio codec architectures, or any form of discretization of continuous audio into token sequences; hence it does not meet the inclusion criteria related to discrete audio tokens and their generation, nor does it provide relevant tokenization or codec details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on automatic speech recognition performance and evaluation metrics on the Switchboard benchmark, without discussing discrete audio tokenization, neural audio codec architectures, or any form of discretization of continuous audio into token sequences; hence it does not meet the inclusion criteria related to discrete audio tokens and their generation, nor does it provide relevant tokenization or codec details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synthesizing Mathematical Identities with E-Graphs",
    "abstract": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
    "metadata": {
      "arxiv_id": "2206.07086",
      "title": "Synthesizing Mathematical Identities with E-Graphs",
      "summary": "Identities compactly describe properties of a mathematical expression and can be leveraged into faster and more accurate function implementations. However, identities must currently be discovered manually, which requires a lot of expertise. We propose a two-phase synthesis and deduplication pipeline that discovers these identities automatically. In the synthesis step, a set of rewrite rules is composed, using an e-graph, to discover candidate identities. However, most of these candidates are duplicates, which a secondary deduplication step discards using integer linear programming and another e-graph. Applied to a set of 61 benchmarks, the synthesis phase generates 7215 candidate identities which the deduplication phase then reduces down to 125 core identities.",
      "authors": [
        "Ian Briggs",
        "Pavel Panchekha"
      ],
      "published": "2022-06-14T18:21:01Z",
      "updated": "2022-06-14T18:21:01Z",
      "categories": [
        "cs.SC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07086v1",
      "landing_url": "https://arxiv.org/abs/2206.07086v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07086"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses mathematical identity synthesis using e-graphs and does not address discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses mathematical identity synthesis using e-graphs and does not address discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method for synthesizing mathematical identities using e-graphs and optimization techniques, with no mention or indication of discrete audio tokens, neural audio codecs, quantization, or audio-related generation or modeling tasks. Therefore, it does not meet any inclusion criteria related to discrete audio tokens and audio modeling, and falls under the exclusion criteria of research not involving discrete audio token representations or related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method for synthesizing mathematical identities using e-graphs and optimization techniques, with no mention or indication of discrete audio tokens, neural audio codecs, quantization, or audio-related generation or modeling tasks. Therefore, it does not meet any inclusion criteria related to discrete audio tokens and audio modeling, and falls under the exclusion criteria of research not involving discrete audio token representations or related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Streaming non-autoregressive model for any-to-many voice conversion",
    "abstract": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
    "metadata": {
      "arxiv_id": "2206.07288",
      "title": "Streaming non-autoregressive model for any-to-many voice conversion",
      "summary": "Voice conversion models have developed for decades, and current mainstream research focuses on non-streaming voice conversion. However, streaming voice conversion is more suitable for practical application scenarios than non-streaming voice conversion. In this paper, we propose a streaming any-to-many voice conversion based on fully non-autoregressive model, which includes a streaming transformer based acoustic model and a streaming vocoder. Streaming transformer based acoustic model is composed of a pre-trained encoder from streaming end-to-end based automatic speech recognition model and a decoder modified on FastSpeech blocks. Streaming vocoder is designed for streaming task with pseudo quadrature mirror filter bank and causal convolution. Experimental results show that the proposed method achieves significant performance both in latency and conversion quality and can be real-time on CPU and GPU.",
      "authors": [
        "Ziyi Chen",
        "Haoran Miao",
        "Pengyuan Zhang"
      ],
      "published": "2022-06-15T04:04:14Z",
      "updated": "2022-06-15T04:04:14Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07288v1",
      "landing_url": "https://arxiv.org/abs/2206.07288v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07288"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses streaming non-autoregressive voice conversion without mentioning discrete audio tokens, tokenization, or token-based representations, thus it does not meet the ‘Discrete Audio Tokens’ inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses streaming non-autoregressive voice conversion without mentioning discrete audio tokens, tokenization, or token-based representations, thus it does not meet the ‘Discrete Audio Tokens’ inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a streaming any-to-many voice conversion system using a fully non-autoregressive model but does not mention the use of discrete audio tokens or any neural audio codec producing discrete token sequences, nor does it describe tokenization, quantization, or codebook settings; the focus is on streaming voice conversion latency and quality rather than discrete token representations, thus it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a streaming any-to-many voice conversion system using a fully non-autoregressive model but does not mention the use of discrete audio tokens or any neural audio codec producing discrete token sequences, nor does it describe tokenization, quantization, or codebook settings; the focus is on streaming voice conversion latency and quality rather than discrete token representations, thus it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "End-to-End Voice Conversion with Information Perturbation",
    "abstract": "The ideal goal of voice conversion is to convert the source speaker's speech to sound naturally like the target speaker while maintaining the linguistic content and the prosody of the source speech. However, current approaches are insufficient to achieve comprehensive source prosody transfer and target speaker timbre preservation in the converted speech, and the quality of the converted speech is also unsatisfied due to the mismatch between the acoustic model and the vocoder. In this paper, we leverage the recent advances in information perturbation and propose a fully end-to-end approach to conduct high-quality voice conversion. We first adopt information perturbation to remove speaker-related information in the source speech to disentangle speaker timbre and linguistic content and thus the linguistic information is subsequently modeled by a content encoder. To better transfer the prosody of the source speech to the target, we particularly introduce a speaker-related pitch encoder which can maintain the general pitch pattern of the source speaker while flexibly modifying the pitch intensity of the generated speech. Finally, one-shot voice conversion is set up through continuous speaker space modeling. Experimental results indicate that the proposed end-to-end approach significantly outperforms the state-of-the-art models in terms of intelligibility, naturalness, and speaker similarity.",
    "metadata": {
      "arxiv_id": "2206.07569",
      "title": "End-to-End Voice Conversion with Information Perturbation",
      "summary": "The ideal goal of voice conversion is to convert the source speaker's speech to sound naturally like the target speaker while maintaining the linguistic content and the prosody of the source speech. However, current approaches are insufficient to achieve comprehensive source prosody transfer and target speaker timbre preservation in the converted speech, and the quality of the converted speech is also unsatisfied due to the mismatch between the acoustic model and the vocoder. In this paper, we leverage the recent advances in information perturbation and propose a fully end-to-end approach to conduct high-quality voice conversion. We first adopt information perturbation to remove speaker-related information in the source speech to disentangle speaker timbre and linguistic content and thus the linguistic information is subsequently modeled by a content encoder. To better transfer the prosody of the source speech to the target, we particularly introduce a speaker-related pitch encoder which can maintain the general pitch pattern of the source speaker while flexibly modifying the pitch intensity of the generated speech. Finally, one-shot voice conversion is set up through continuous speaker space modeling. Experimental results indicate that the proposed end-to-end approach significantly outperforms the state-of-the-art models in terms of intelligibility, naturalness, and speaker similarity.",
      "authors": [
        "Qicong Xie",
        "Shan Yang",
        "Yi Lei",
        "Lei Xie",
        "Dan Su"
      ],
      "published": "2022-06-15T14:38:31Z",
      "updated": "2022-06-15T14:38:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07569v1",
      "landing_url": "https://arxiv.org/abs/2206.07569v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.07569"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper proposes end-to-end voice conversion with information perturbation but does not discuss discrete audio tokens, neural audio codecs with vector quantization, or token-based representations necessary to meet the token-discretization inclusion criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper proposes end-to-end voice conversion with information perturbation but does not discuss discrete audio tokens, neural audio codecs with vector quantization, or token-based representations necessary to meet the token-discretization inclusion criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an end-to-end voice conversion approach leveraging information perturbation to separate speaker-related information from linguistic content but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization process as defined in the inclusion criteria. Therefore, it does not meet the core requirement of involving discrete audio tokens or their design and use for audio generation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an end-to-end voice conversion approach leveraging information perturbation to separate speaker-related information from linguistic content but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization process as defined in the inclusion criteria. Therefore, it does not meet the core requirement of involving discrete audio tokens or their design and use for audio generation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
    "abstract": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
    "metadata": {
      "arxiv_id": "2206.07860",
      "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
      "summary": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
      "authors": [
        "Li-Chin Chen",
        "Po-Hsun Chen",
        "Richard Tzong-Han Tsai",
        "Yu Tsao"
      ],
      "published": "2022-06-16T00:33:20Z",
      "updated": "2022-06-16T00:33:20Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07860v1",
      "landing_url": "https://arxiv.org/abs/2206.07860v1",
      "doi": "https://doi.org/10.1109/LSP.2022.3184636"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on EPG-based speech generation/enhancement using continuous audio/EPG signals and does not discuss discrete audio tokens or tokenization, thus it does not meet the inclusion criteria and fails due to exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on EPG-based speech generation/enhancement using continuous audio/EPG signals and does not discuss discrete audio tokens or tokenization, thus it does not meet the inclusion criteria and fails due to exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech generation and enhancement using electropalatography (EPG) and audio signals with multimodal learning, but there is no mention of discretizing continuous audio waveforms into discrete tokens or describing any neural audio codec or tokenizer that produces discrete token sequences; hence, it does not meet the core inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech generation and enhancement using electropalatography (EPG) and audio signals with multimodal learning, but there is no mention of discretizing continuous audio waveforms into discrete tokens or describing any neural audio codec or tokenizer that produces discrete token sequences; hence, it does not meet the core inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
    "abstract": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner.",
    "metadata": {
      "arxiv_id": "2206.08790",
      "title": "Self-supervised speech unit discovery from articulatory and acoustic features using VQ-VAE",
      "summary": "The human perception system is often assumed to recruit motor knowledge when processing auditory speech inputs. Using articulatory modeling and deep learning, this study examines how this articulatory information can be used for discovering speech units in a self-supervised setting. We used vector-quantized variational autoencoders (VQ-VAE) to learn discrete representations from articulatory and acoustic speech data. In line with the zero-resource paradigm, an ABX test was then used to investigate how the extracted representations encode phonetically relevant properties. Experiments were conducted on three different corpora in English and French. We found that articulatory information rather organises the latent representations in terms of place of articulation whereas the speech acoustics mainly structure the latent space in terms of manner of articulation. We show that an optimal fusion of the two modalities can lead to a joint representation of these phonetic dimensions more accurate than each modality considered individually. Since articulatory information is usually not available in a practical situation, we finally investigate the benefit it provides when inferred from the speech acoustics in a self-supervised manner.",
      "authors": [
        "Marc-Antoine Georges",
        "Jean-Luc Schwartz",
        "Thomas Hueber"
      ],
      "published": "2022-06-17T14:04:24Z",
      "updated": "2022-06-17T14:04:24Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.08790v1",
      "landing_url": "https://arxiv.org/abs/2206.08790v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.08790"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item uses a neural discrete-token learning approach (VQ-VAE) to discover speech units from articulatory and acoustic features and evaluates discrete representations with phonetic-relevant tasks, aligning with the discrete audio token criteria and lacking exclusion factors.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item uses a neural discrete-token learning approach (VQ-VAE) to discover speech units from articulatory and acoustic features and evaluates discrete representations with phonetic-relevant tasks, aligning with the discrete audio token criteria and lacking exclusion factors.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning discrete speech units via vector-quantized variational autoencoders (VQ-VAE) from articulatory and acoustic features, which aligns with generating discrete audio tokens from self-supervised representation learning. The methodology describes producing discrete token sequences useful for downstream tasks consistent with the discrete audio tokens' definition. The work also discusses phonetic properties encoded in the discrete representations, meeting the inclusion criteria of neural audio codec/tokenizer use and evaluation. The abstract is in English and provides sufficient insight into the tokenization mechanism, supporting reproducibility evaluation. No exclusion criteria are triggered since the study clearly centers on discrete audio token representations rather than continuous audio or purely discriminative tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning discrete speech units via vector-quantized variational autoencoders (VQ-VAE) from articulatory and acoustic features, which aligns with generating discrete audio tokens from self-supervised representation learning. The methodology describes producing discrete token sequences useful for downstream tasks consistent with the discrete audio tokens' definition. The work also discusses phonetic properties encoded in the discrete representations, meeting the inclusion criteria of neural audio codec/tokenizer use and evaluation. The abstract is in English and provides sufficient insight into the tokenization mechanism, supporting reproducibility evaluation. No exclusion criteria are triggered since the study clearly centers on discrete audio token representations rather than continuous audio or purely discriminative tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems",
    "abstract": "An automatic speaker verification system aims to verify the speaker identity of a speech signal. However, a voice conversion system could manipulate a person's speech signal to make it sound like another speaker's voice and deceive the speaker verification system. Most countermeasures for voice conversion-based spoofing attacks are designed to discriminate bona fide speech from spoofed speech for speaker verification systems. In this paper, we investigate the problem of source speaker identification -- inferring the identity of the source speaker given the voice converted speech. To perform source speaker identification, we simply add voice-converted speech data with the label of source speaker identity to the genuine speech dataset during speaker embedding network training. Experimental results show the feasibility of source speaker identification when training and testing with converted speeches from the same voice conversion model(s). In addition, our results demonstrate that having more converted utterances from various voice conversion model for training helps improve the source speaker identification performance on converted utterances from unseen voice conversion models.",
    "metadata": {
      "arxiv_id": "2206.09103",
      "title": "Identifying Source Speakers for Voice Conversion based Spoofing Attacks on Speaker Verification Systems",
      "summary": "An automatic speaker verification system aims to verify the speaker identity of a speech signal. However, a voice conversion system could manipulate a person's speech signal to make it sound like another speaker's voice and deceive the speaker verification system. Most countermeasures for voice conversion-based spoofing attacks are designed to discriminate bona fide speech from spoofed speech for speaker verification systems. In this paper, we investigate the problem of source speaker identification -- inferring the identity of the source speaker given the voice converted speech. To perform source speaker identification, we simply add voice-converted speech data with the label of source speaker identity to the genuine speech dataset during speaker embedding network training. Experimental results show the feasibility of source speaker identification when training and testing with converted speeches from the same voice conversion model(s). In addition, our results demonstrate that having more converted utterances from various voice conversion model for training helps improve the source speaker identification performance on converted utterances from unseen voice conversion models.",
      "authors": [
        "Danwei Cai",
        "Zexin Cai",
        "Ming Li"
      ],
      "published": "2022-06-18T03:45:34Z",
      "updated": "2022-10-31T18:50:41Z",
      "categories": [
        "eess.AS",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09103v2",
      "landing_url": "https://arxiv.org/abs/2206.09103v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.09103"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper studies source speaker identification for voice-conversion spoofing in speaker verification and does not address discrete audio tokens or tokenization; it does not meet the 'Discrete Audio Tokens' criteria and aligns with the exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper studies source speaker identification for voice-conversion spoofing in speaker verification and does not address discrete audio tokens or tokenization; it does not meet the 'Discrete Audio Tokens' criteria and aligns with the exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker verification and voice conversion attacks using voice-converted speech data for source speaker identification and does not discuss or involve discrete audio tokens, tokenization processes, or neural audio codecs for discretizing audio into token sequences for downstream modeling tasks, as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker verification and voice conversion attacks using voice-converted speech data for source speaker identification and does not discuss or involve discrete audio tokens, tokenization processes, or neural audio codecs for discretizing audio into token sequences for downstream modeling tasks, as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Misspelling Semantics In Thai",
    "abstract": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
    "metadata": {
      "arxiv_id": "2206.09680",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "authors": [
        "Pakawat Nakwijit",
        "Matthew Purver"
      ],
      "published": "2022-06-20T09:42:50Z",
      "updated": "2022-06-20T09:42:50Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09680v1",
      "landing_url": "https://arxiv.org/abs/2206.09680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.09680"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns Thai misspelling semantics for NLP sentiment analysis, not discrete audio tokens or any discrete audio token generation/encoding; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns Thai misspelling semantics for NLP sentiment analysis, not discrete audio tokens or any discrete audio token generation/encoding; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on analyzing misspellings in Thai user-generated content and their semantics for language understanding tasks, with no mention of discrete audio tokens, neural audio codecs, quantization, or tokenization of continuous audio waveforms into discrete tokens. It does not address audio-related token design, discrete audio representations for generative or cross-modal models, or audio token evaluation, thus not meeting the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on analyzing misspellings in Thai user-generated content and their semantics for language understanding tasks, with no mention of discrete audio tokens, neural audio codecs, quantization, or tokenization of continuous audio waveforms into discrete tokens. It does not address audio-related token design, discrete audio representations for generative or cross-modal models, or audio token evaluation, thus not meeting the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vicinity Vision Transformer",
    "abstract": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
    "metadata": {
      "arxiv_id": "2206.10552",
      "title": "Vicinity Vision Transformer",
      "summary": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
      "authors": [
        "Weixuan Sun",
        "Zhen Qin",
        "Hui Deng",
        "Jianyuan Wang",
        "Yi Zhang",
        "Kaihao Zhang",
        "Nick Barnes",
        "Stan Birchfield",
        "Lingpeng Kong",
        "Yiran Zhong"
      ],
      "published": "2022-06-21T17:33:53Z",
      "updated": "2023-07-20T08:57:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.10552v2",
      "landing_url": "https://arxiv.org/abs/2206.10552v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.10552"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns Vicinity Vision Transformer and locality-based attention for images; there is no discussion of discrete audio tokens, tokenization, or audio-focused generation/understanding, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns Vicinity Vision Transformer and locality-based attention for images; there is no discussion of discrete audio tokens, tokenization, or audio-focused generation/understanding, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vision transformers and image processing techniques rather than any form of audio signal discretization or audio tokenization; no mention of discrete audio tokens or related neural audio codec processes is found, thus failing to meet the inclusion criteria centered on discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vision transformers and image processing techniques rather than any form of audio signal discretization or audio tokenization; no mention of discrete audio tokens or related neural audio codec processes is found, thus failing to meet the inclusion criteria centered on discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
    "abstract": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
    "metadata": {
      "arxiv_id": "2206.12351",
      "title": "Megapixel Image Generation with Step-Unrolled Denoising Autoencoders",
      "summary": "An ongoing trend in generative modelling research has been to push sample resolutions higher whilst simultaneously reducing computational requirements for training and sampling. We aim to push this trend further via the combination of techniques - each component representing the current pinnacle of efficiency in their respective areas. These include vector-quantized GAN (VQ-GAN), a vector-quantization (VQ) model capable of high levels of lossy - but perceptually insignificant - compression; hourglass transformers, a highly scaleable self-attention model; and step-unrolled denoising autoencoders (SUNDAE), a non-autoregressive (NAR) text generative model. Unexpectedly, our method highlights weaknesses in the original formulation of hourglass transformers when applied to multidimensional data. In light of this, we propose modifications to the resampling mechanism, applicable in any task applying hierarchical transformers to multidimensional data. Additionally, we demonstrate the scalability of SUNDAE to long sequence lengths - four times longer than prior work. Our proposed framework scales to high-resolutions ($1024 \\times 1024$) and trains quickly (2-4 days). Crucially, the trained model produces diverse and realistic megapixel samples in approximately 2 seconds on a consumer-grade GPU (GTX 1080Ti). In general, the framework is flexible: supporting an arbitrary number of sampling steps, sample-wise self-stopping, self-correction capabilities, conditional generation, and a NAR formulation that allows for arbitrary inpainting masks. We obtain FID scores of 10.56 on FFHQ256 - close to the original VQ-GAN in less than half the sampling steps - and 21.85 on FFHQ1024 in only 100 sampling steps.",
      "authors": [
        "Alex F. McKinney",
        "Chris G. Willcocks"
      ],
      "published": "2022-06-24T15:47:42Z",
      "updated": "2022-06-24T15:47:42Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12351v1",
      "landing_url": "https://arxiv.org/abs/2206.12351v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12351"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on image generation with SUNDAE/VQ-GAN and does not address discrete audio tokens or tokenization, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on image generation with SUNDAE/VQ-GAN and does not address discrete audio tokens or tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on high-resolution image generation techniques, involving vector-quantized GANs and autoencoders, but there is no indication of dealing with audio data, audio tokenization, or discrete audio token representations as required by the inclusion criteria. It mainly covers visual generative modeling rather than discrete audio token methods or their evaluation in audio or multimodal tasks, thus failing to meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on high-resolution image generation techniques, involving vector-quantized GANs and autoencoders, but there is no indication of dealing with audio data, audio tokenization, or discrete audio token representations as required by the inclusion criteria. It mainly covers visual generative modeling rather than discrete audio token methods or their evaluation in audio or multimodal tasks, thus failing to meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction",
    "abstract": "This paper presents TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model. This takes advantage of the fact that if the language model will reliably and accurately predict a token anyway, then the acoustic model doesn't need to be accurate in recognizing it. We train German ASR models with 900 million parameters and show that on CommonVoice German, TEVR scores a very competitive 3.64% word error rate, which outperforms the best reported results by a relative 16.89% reduction in word error rate. We hope that releasing our fully trained speech recognition pipeline to the community will lead to privacy-preserving offline virtual assistants in the future.",
    "metadata": {
      "arxiv_id": "2206.12693",
      "title": "TEVR: Improving Speech Recognition by Token Entropy Variance Reduction",
      "summary": "This paper presents TEVR, a speech recognition model designed to minimize the variation in token entropy w.r.t. to the language model. This takes advantage of the fact that if the language model will reliably and accurately predict a token anyway, then the acoustic model doesn't need to be accurate in recognizing it. We train German ASR models with 900 million parameters and show that on CommonVoice German, TEVR scores a very competitive 3.64% word error rate, which outperforms the best reported results by a relative 16.89% reduction in word error rate. We hope that releasing our fully trained speech recognition pipeline to the community will lead to privacy-preserving offline virtual assistants in the future.",
      "authors": [
        "Hajo Nils Krabbenhöft",
        "Erhardt Barth"
      ],
      "published": "2022-06-25T16:42:05Z",
      "updated": "2022-06-25T16:42:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.12693v1",
      "landing_url": "https://arxiv.org/abs/2206.12693v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.12693"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses reducing token entropy in ASR but does not specify discrete audio token generation (tokenizers/codebooks) or discrete token representations as core to the method, so it may not meet the discrete audio token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses reducing token entropy in ASR but does not specify discrete audio token generation (tokenizers/codebooks) or discrete token representations as core to the method, so it may not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving speech recognition by reducing token entropy variance but does not explicitly describe producing discrete audio tokens via discretization methods such as vector quantization or tokenizer architectures; it primarily uses acoustic and language model components without detailing a discrete token vocabulary or codec, which are core criteria for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving speech recognition by reducing token entropy variance but does not explicitly describe producing discrete audio tokens via discretization methods such as vector quantization or tokenizer architectures; it primarily uses acoustic and language model components without detailing a discrete token vocabulary or codec, which are core criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
    "abstract": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
    "metadata": {
      "arxiv_id": "2206.14962",
      "title": "GLD-Net: Improving Monaural Speech Enhancement by Learning Global and Local Dependency Features with GLD Block",
      "summary": "For monaural speech enhancement, contextual information is important for accurate speech estimation. However, commonly used convolution neural networks (CNNs) are weak in capturing temporal contexts since they only build blocks that process one local neighborhood at a time. To address this problem, we learn from human auditory perception to introduce a two-stage trainable reasoning mechanism, referred as global-local dependency (GLD) block. GLD blocks capture long-term dependency of time-frequency bins both in global level and local level from the noisy spectrogram to help detecting correlations among speech part, noise part, and whole noisy input. What is more, we conduct a monaural speech enhancement network called GLD-Net, which adopts encoder-decoder architecture and consists of speech object branch, interference branch, and global noisy branch. The extracted speech feature at global-level and local-level are efficiently reasoned and aggregated in each of the branches. We compare the proposed GLD-Net with existing state-of-art methods on WSJ0 and DEMAND dataset. The results show that GLD-Net outperforms the state-of-the-art methods in terms of PESQ and STOI.",
      "authors": [
        "Xinmeng Xu",
        "Yang Wang",
        "Jie Jia",
        "Binbin Chen",
        "Jianjun Hao"
      ],
      "published": "2022-06-30T01:16:40Z",
      "updated": "2022-06-30T01:16:40Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.14962v1",
      "landing_url": "https://arxiv.org/abs/2206.14962v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.14962"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes GLD-Net for monaural speech enhancement using global-local dependency blocks and does not discuss discrete audio tokens, tokenizers, or neural codecs producing discrete token sequences, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes GLD-Net for monaural speech enhancement using global-local dependency blocks and does not discuss discrete audio tokens, tokenizers, or neural codecs producing discrete token sequences, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on monaural speech enhancement using a neural network architecture with a novel block design to capture global and local dependencies, but it does not discuss any discretization of continuous audio signals into discrete token sequences or neural audio codecs for token generation, nor does it mention any vocabulary or codebook settings related to discrete audio tokens, which are central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on monaural speech enhancement using a neural network architecture with a novel block design to capture global and local dependencies, but it does not discuss any discretization of continuous audio signals into discrete token sequences or neural audio codecs for token generation, nor does it mention any vocabulary or codebook settings related to discrete audio tokens, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "esCorpius: A Massive Spanish Crawling Corpus",
    "abstract": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
    "metadata": {
      "arxiv_id": "2206.15147",
      "title": "esCorpius: A Massive Spanish Crawling Corpus",
      "summary": "In the recent years, transformer-based models have lead to significant advances in language modelling for natural language processing. However, they require a vast amount of data to be (pre-)trained and there is a lack of corpora in languages other than English. Recently, several initiatives have presented multilingual datasets obtained from automatic web crawling. However, the results in Spanish present important shortcomings, as they are either too small in comparison with other languages, or present a low quality derived from sub-optimal cleaning and deduplication. In this paper, we introduce esCorpius, a Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is the most extensive corpus in Spanish with this level of quality in the extraction, purification and deduplication of web textual content. Our data curation process involves a novel highly parallel cleaning pipeline and encompasses a series of deduplication mechanisms that together ensure the integrity of both document and paragraph boundaries. Additionally, we maintain both the source web page URL and the WARC shard origin URL in order to complain with EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license and is available on HuggingFace.",
      "authors": [
        "Asier Gutiérrez-Fandiño",
        "David Pérez-Fernández",
        "Jordi Armengol-Estapé",
        "David Griol",
        "Zoraida Callejas"
      ],
      "published": "2022-06-30T09:29:18Z",
      "updated": "2022-07-01T08:22:32Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.15147v2",
      "landing_url": "https://arxiv.org/abs/2206.15147v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.15147"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a Spanish crawling corpus (esCorpius) and data curation/cleaning, with licensing, but it does not address discrete audio tokens or tokenization designs, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a Spanish crawling corpus (esCorpius) and data curation/cleaning, with licensing, but it does not address discrete audio tokens or tokenization designs, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a large Spanish crawling corpus for language modeling but do not discuss any discrete audio tokens, neural audio codecs, vector quantization, or related audio discretization techniques; thus, it does not meet the inclusion criteria focused on discrete audio tokens and related audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a large Spanish crawling corpus for language modeling but do not discuss any discrete audio tokens, neural audio codecs, vector quantization, or related audio discretization techniques; thus, it does not meet the inclusion criteria focused on discrete audio tokens and related audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
    "abstract": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
    "metadata": {
      "arxiv_id": "2207.00756",
      "title": "Learning Noise-independent Speech Representation for High-quality Voice Conversion for Noisy Target Speakers",
      "summary": "Building a voice conversion system for noisy target speakers, such as users providing noisy samples or Internet found data, is a challenging task since the use of contaminated speech in model training will apparently degrade the conversion performance. In this paper, we leverage the advances of our recently proposed Glow-WaveGAN and propose a noise-independent speech representation learning approach for high-quality voice conversion for noisy target speakers. Specifically, we learn a latent feature space where we ensure that the target distribution modeled by the conversion model is exactly from the modeled distribution of the waveform generator. With this premise, we further manage to make the latent feature to be noise-invariant. Specifically, we introduce a noise-controllable WaveGAN, which directly learns the noise-independent acoustic representation from waveform by the encoder and conducts noise control in the hidden space through a FiLM module in the decoder. As for the conversion model, importantly, we use a flow-based model to learn the distribution of noise-independent but speaker-related latent features from phoneme posteriorgrams. Experimental results demonstrate that the proposed model achieves high speech quality and speaker similarity in the voice conversion for noisy target speakers.",
      "authors": [
        "Liumeng Xue",
        "Shan Yang",
        "Na Hu",
        "Dan Su",
        "Lei Xie"
      ],
      "published": "2022-07-02T06:51:12Z",
      "updated": "2022-07-02T06:51:12Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00756v1",
      "landing_url": "https://arxiv.org/abs/2207.00756v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00756"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on learning noise-robust continuous latent representations for voice conversion and does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on learning noise-robust continuous latent representations for voice conversion and does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on learning noise-independent speech representations for voice conversion, but it does not mention converting continuous audio waveforms to discrete token sequences nor details about tokenization, codebooks, or any neural codec generating discrete representations; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on learning noise-independent speech representations for voice conversion, but it does not mention converting continuous audio waveforms to discrete token sequences nor details about tokenization, codebooks, or any neural codec generating discrete representations; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
    "abstract": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
    "metadata": {
      "arxiv_id": "2207.04356",
      "title": "A Comparative Study of Self-supervised Speech Representation Based Voice Conversion",
      "summary": "We present a large-scale comparative study of self-supervised speech representation (S3R)-based voice conversion (VC). In the context of recognition-synthesis VC, S3Rs are attractive owing to their potential to replace expensive supervised representations such as phonetic posteriorgrams (PPGs), which are commonly adopted by state-of-the-art VC systems. Using S3PRL-VC, an open-source VC software we previously developed, we provide a series of in-depth objective and subjective analyses under three VC settings: intra-/cross-lingual any-to-one (A2O) and any-to-any (A2A) VC, using the voice conversion challenge 2020 (VCC2020) dataset. We investigated S3R-based VC in various aspects, including model type, multilinguality, and supervision. We also studied the effect of a post-discretization process with k-means clustering and showed how it improves in the A2A setting. Finally, the comparison with state-of-the-art VC systems demonstrates the competitiveness of S3R-based VC and also sheds light on the possible improving directions.",
      "authors": [
        "Wen-Chin Huang",
        "Shu-Wen Yang",
        "Tomoki Hayashi",
        "Tomoki Toda"
      ],
      "published": "2022-07-10T01:02:22Z",
      "updated": "2022-07-10T01:02:22Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04356v1",
      "landing_url": "https://arxiv.org/abs/2207.04356v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3193761"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly discusses discretizing self-supervised speech representations into discrete tokens via a post-discretization (k-means) step and uses them in a voice-conversion system, aligning with the token-discretization criteria (token source via self-supervised modeling; token-based downstream task) and providing English abstract and methods, thus include.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly discusses discretizing self-supervised speech representations into discrete tokens via a post-discretization (k-means) step and uses them in a voice-conversion system, aligning with the token-discretization criteria (token source via self-supervised modeling; token-based downstream task) and providing English abstract and methods, thus include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representations (S3R) for voice conversion, including the use of a post-discretization process with k-means clustering, which suggests the generation of discrete tokens from continuous audio for downstream modeling. However, the abstract does not sufficiently detail the discrete token generation process, vocabulary/codebook settings, or explicit use of discrete tokens as core inputs or analyses, which limits full eligibility assessment against inclusion criteria. The research involves systematic comparison and evaluation of representation types in voice conversion, aligning with some evaluation and design choice considerations but lacks explicit and detailed description of discrete token design and usage as core method, leaving some uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representations (S3R) for voice conversion, including the use of a post-discretization process with k-means clustering, which suggests the generation of discrete tokens from continuous audio for downstream modeling. However, the abstract does not sufficiently detail the discrete token generation process, vocabulary/codebook settings, or explicit use of discrete tokens as core inputs or analyses, which limits full eligibility assessment against inclusion criteria. The research involves systematic comparison and evaluation of representation types in voice conversion, aligning with some evaluation and design choice considerations but lacks explicit and detailed description of discrete token design and usage as core method, leaving some uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged include (5) noting the paper explicitly uses a post-discretization (k-means) step to produce discrete units from self-supervised speech representations for VC; JuniorMini was more cautious (3) because the abstract lacks detailed tokenizer/codebook settings required by the strict reproducibility clause. The abstract, however, clearly reports using S3R-derived discrete units via k-means clustering as core inputs and analyzes their impact in downstream any-to-any VC, which satisfies the inclusion group that accepts self-supervised-model-derived discrete tokens, even though the abstract omits low-level codebook/bitrate details required for full reproducibility — therefore I recommend inclusion but note limited methodological detail (better to include).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged include (5) noting the paper explicitly uses a post-discretization (k-means) step to produce discrete units from self-supervised speech representations for VC; JuniorMini was more cautious (3) because the abstract lacks detailed tokenizer/codebook settings required by the strict reproducibility clause. The abstract, however, clearly reports using S3R-derived discrete units via k-means clustering as core inputs and analyzes their impact in downstream any-to-any VC, which satisfies the inclusion group that accepts self-supervised-model-derived discrete tokens, even though the abstract omits low-level codebook/bitrate details required for full reproducibility — therefore I recommend inclusion but note limited methodological detail (better to include).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
    "abstract": "Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system.",
    "metadata": {
      "arxiv_id": "2207.04646",
      "title": "DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders",
      "summary": "Current text to speech (TTS) systems usually leverage a cascaded acoustic model and vocoder pipeline with mel-spectrograms as the intermediate representations, which suffer from two limitations: 1) the acoustic model and vocoder are separately trained instead of jointly optimized, which incurs cascaded errors; 2) the intermediate speech representations (e.g., mel-spectrogram) are pre-designed and lose phase information, which are sub-optimal. To solve these problems, in this paper, we develop DelightfulTTS 2, a new end-to-end speech synthesis system with automatically learned speech representations and jointly optimized acoustic model and vocoder. Specifically, 1) we propose a new codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN) to extract intermediate frame-level speech representations (instead of traditional representations like mel-spectrograms) and reconstruct speech waveform; 2) we jointly optimize the acoustic model (based on DelightfulTTS) and the vocoder (the decoder of VQ-GAN), with an auxiliary loss on the acoustic model to predict intermediate speech representations. Experiments show that DelightfulTTS 2 achieves a CMOS gain +0.14 over DelightfulTTS, and more method analyses further verify the effectiveness of the developed system.",
      "authors": [
        "Yanqing Liu",
        "Ruiqing Xue",
        "Lei He",
        "Xu Tan",
        "Sheng Zhao"
      ],
      "published": "2022-07-11T06:15:45Z",
      "updated": "2022-07-11T06:15:45Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04646v1",
      "landing_url": "https://arxiv.org/abs/2207.04646v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.04646"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses vector-quantized auto-encoders (VQ-GAN) to produce discrete audio representations (tokens) for end-to-end TTS, aligning with the discrete audio token theme and providing token/codec design and evaluable experiments.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses vector-quantized auto-encoders (VQ-GAN) to produce discrete audio representations (tokens) for end-to-end TTS, aligning with the discrete audio token theme and providing token/codec design and evaluable experiments.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents DelightfulTTS 2, which uses a vector-quantized auto-encoder (VQ-GAN) codec network to extract discrete intermediate frame-level speech representations for end-to-end TTS, thus involving a neural audio codec producing discrete tokens as core representations used for speech synthesis. However, the abstract lacks detailed information on the discrete token vocabulary/codebook configurations, quantization specifics, or systematic evaluation of token design choices, so while it fits the basic scope of discrete audio tokens, some key details required for reproducibility and thorough token assessment are missing.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper presents DelightfulTTS 2, which uses a vector-quantized auto-encoder (VQ-GAN) codec network to extract discrete intermediate frame-level speech representations for end-to-end TTS, thus involving a neural audio codec producing discrete tokens as core representations used for speech synthesis. However, the abstract lacks detailed information on the discrete token vocabulary/codebook configurations, quantization specifics, or systematic evaluation of token design choices, so while it fits the basic scope of discrete audio tokens, some key details required for reproducibility and thorough token assessment are missing.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Lexicon and Depth-wise Separable Convolution Based Handwritten Text Recognition System",
    "abstract": "Cursive handwritten text recognition is a challenging research problem in the domain of pattern recognition. The current state-of-the-art approaches include models based on convolutional recurrent neural networks and multi-dimensional long short-term memory recurrent neural networks techniques. These methods are highly computationally extensive as well model is complex at design level. In recent studies, combination of convolutional neural network and gated convolutional neural networks based models demonstrated less number of parameters in comparison to convolutional recurrent neural networks based models. In the direction to reduced the total number of parameters to be trained, in this work, we have used depthwise convolution in place of standard convolutions with a combination of gated-convolutional neural network and bidirectional gated recurrent unit to reduce the total number of parameters to be trained. Additionally, we have also included a lexicon based word beam search decoder at testing step. It also helps in improving the the overall accuracy of the model. We have obtained 3.84% character error rate and 9.40% word error rate on IAM dataset; 4.88% character error rate and 14.56% word error rate in George Washington dataset, respectively.",
    "metadata": {
      "arxiv_id": "2207.04651",
      "title": "A Lexicon and Depth-wise Separable Convolution Based Handwritten Text Recognition System",
      "summary": "Cursive handwritten text recognition is a challenging research problem in the domain of pattern recognition. The current state-of-the-art approaches include models based on convolutional recurrent neural networks and multi-dimensional long short-term memory recurrent neural networks techniques. These methods are highly computationally extensive as well model is complex at design level. In recent studies, combination of convolutional neural network and gated convolutional neural networks based models demonstrated less number of parameters in comparison to convolutional recurrent neural networks based models. In the direction to reduced the total number of parameters to be trained, in this work, we have used depthwise convolution in place of standard convolutions with a combination of gated-convolutional neural network and bidirectional gated recurrent unit to reduce the total number of parameters to be trained. Additionally, we have also included a lexicon based word beam search decoder at testing step. It also helps in improving the the overall accuracy of the model. We have obtained 3.84% character error rate and 9.40% word error rate on IAM dataset; 4.88% character error rate and 14.56% word error rate in George Washington dataset, respectively.",
      "authors": [
        "Lalita Kumari",
        "Sukhdeep Singh",
        "VVS Rathore",
        "Anuj Sharma"
      ],
      "published": "2022-07-11T06:24:26Z",
      "updated": "2022-07-11T06:24:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04651v1",
      "landing_url": "https://arxiv.org/abs/2207.04651v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.04651"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Topic mismatch: the item studies handwritten text recognition with CNN/GRU architectures and lexicon decoding; it does not involve discrete audio tokens or tokenization/discretization of audio, thus it should be excluded per exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Topic mismatch: the item studies handwritten text recognition with CNN/GRU architectures and lexicon decoding; it does not involve discrete audio tokens or tokenization/discretization of audio, thus it should be excluded per exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on handwritten text recognition using convolutional and gated recurrent neural networks, without any mention or discussion of discrete audio tokens, neural audio codecs, or audio discretization for generative or cross-modal tasks, which are required by the inclusion criteria. Hence, it does not meet the criteria of studying discrete audio tokens or related audio representation methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on handwritten text recognition using convolutional and gated recurrent neural networks, without any mention or discussion of discrete audio tokens, neural audio codecs, or audio discretization for generative or cross-modal tasks, which are required by the inclusion criteria. Hence, it does not meet the criteria of studying discrete audio tokens or related audio representation methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer",
    "abstract": "Pathologists have a rich vocabulary with which they can describe all the nuances of cellular morphology. In their world, there is a natural pairing of images and words. Recent advances demonstrate that machine learning models can now be trained to learn high-quality image features and represent them as discrete units of information. This enables natural language, which is also discrete, to be jointly modelled alongside the imaging, resulting in a description of the contents of the imaging. Here we present experiments in applying discrete modelling techniques to the problem domain of non-melanoma skin cancer, specifically, histological images of Intraepidermal Carcinoma (IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256) images of IEC images, we trained a sequence-to-sequence transformer to generate natural language descriptions using pathologist terminology. Combined with the idea of interactive concept vectors available by using continuous generative methods, we demonstrate an additional angle of interpretability. The result is a promising means of working towards highly expressive machine learning systems which are not only useful as predictive/classification tools, but also means to further our scientific understanding of disease.",
    "metadata": {
      "arxiv_id": "2207.05749",
      "title": "Towards Highly Expressive Machine Learning Models of Non-Melanoma Skin Cancer",
      "summary": "Pathologists have a rich vocabulary with which they can describe all the nuances of cellular morphology. In their world, there is a natural pairing of images and words. Recent advances demonstrate that machine learning models can now be trained to learn high-quality image features and represent them as discrete units of information. This enables natural language, which is also discrete, to be jointly modelled alongside the imaging, resulting in a description of the contents of the imaging. Here we present experiments in applying discrete modelling techniques to the problem domain of non-melanoma skin cancer, specifically, histological images of Intraepidermal Carcinoma (IEC). Implementing a VQ-GAN model to reconstruct high-resolution (256x256) images of IEC images, we trained a sequence-to-sequence transformer to generate natural language descriptions using pathologist terminology. Combined with the idea of interactive concept vectors available by using continuous generative methods, we demonstrate an additional angle of interpretability. The result is a promising means of working towards highly expressive machine learning systems which are not only useful as predictive/classification tools, but also means to further our scientific understanding of disease.",
      "authors": [
        "Simon M. Thomas",
        "James G. Lefevre",
        "Glenn Baxter",
        "Nicholas A. Hamilton"
      ],
      "published": "2022-07-09T04:53:25Z",
      "updated": "2022-07-09T04:53:25Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05749v1",
      "landing_url": "https://arxiv.org/abs/2207.05749v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05749"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on discrete modelling of histology images (using VQ-GAN and a transformer to produce pathology terminology) rather than discrete audio tokens; the inclusion criteria require discrete audio token design/evaluation for audio, so this item does not meet the scope and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on discrete modelling of histology images (using VQ-GAN and a transformer to produce pathology terminology) rather than discrete audio tokens; the inclusion criteria require discrete audio token design/evaluation for audio, so this item does not meet the scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on machine learning models applied to non-melanoma skin cancer histological images, using VQ-GAN and transformers for image reconstruction and natural language description generation related to pathology. However, it does not involve discretization of continuous audio waveforms into discrete audio tokens for modeling, generation, or cross-modal reasoning, nor does it discuss neural audio codecs or discrete token vocabularies from audio data. Therefore, it does not meet the inclusion criteria of studies centered on discrete audio token representation and processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on machine learning models applied to non-melanoma skin cancer histological images, using VQ-GAN and transformers for image reconstruction and natural language description generation related to pathology. However, it does not involve discretization of continuous audio waveforms into discrete audio tokens for modeling, generation, or cross-modal reasoning, nor does it discuss neural audio codecs or discrete token vocabularies from audio data. Therefore, it does not meet the inclusion criteria of studies centered on discrete audio token representation and processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
    "abstract": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
    "metadata": {
      "arxiv_id": "2207.05913",
      "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
      "summary": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
      "authors": [
        "Yi-Chiao Wu",
        "Patrick Lumban Tobing",
        "Kazuki Yasuhara",
        "Noriyuki Matsunaga",
        "Yamato Ohtani",
        "Tomoki Toda"
      ],
      "published": "2022-07-13T01:40:59Z",
      "updated": "2022-07-13T01:40:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05913v1",
      "landing_url": "https://arxiv.org/abs/2207.05913v1",
      "doi": "https://doi.org/10.1561/116.00000020"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on neural post-filtering for low-cost TTS and neural vocoders; there is no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on neural post-filtering for low-cost TTS and neural vocoders; there is no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving neural post-filters for low-cost TTS systems using neural vocoders but does not mention or describe the use of discrete audio tokens, tokenization, vector quantization, or any discretization process mapping continuous audio waveforms to discrete token sequences; thus, it lacks key elements required by the inclusion criteria about discrete audio tokens and their design or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving neural post-filters for low-cost TTS systems using neural vocoders but does not mention or describe the use of discrete audio tokens, tokenization, vector quantization, or any discretization process mapping continuous audio waveforms to discrete token sequences; thus, it lacks key elements required by the inclusion criteria about discrete audio tokens and their design or evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
    "abstract": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
    "metadata": {
      "arxiv_id": "2207.05920",
      "title": "Online Target Speaker Voice Activity Detection for Speaker Diarization",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. First, we employ a ResNet-based front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. We iteratively extract the results for each block and update the target speaker embedding until reaching the end of the signal. Experimental results show that the proposed method is better than the offline clustering-based diarization system on the AliMeeting dataset.",
      "authors": [
        "Weiqing Wang",
        "Qingjian Lin",
        "Ming Li"
      ],
      "published": "2022-07-13T01:56:31Z",
      "updated": "2022-07-13T01:56:31Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05920v1",
      "landing_url": "https://arxiv.org/abs/2207.05920v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on online target speaker VAD for diarization using continuous frame-level embeddings, with no discrete audio token production or token-based modeling described, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on online target speaker VAD for diarization using continuous frame-level embeddings, with no discrete audio token production or token-based modeling described, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an online target speaker voice activity detection system for speaker diarization, but it does not discuss or propose any discrete audio tokens derived via neural audio codec or self-supervised learning; nor does it provide details about tokenization, vector quantization, or discrete token sequences as required by the inclusion criteria. The study is about speaker diarization and voice activity detection without touching upon discrete token extraction, design, or usage for audio generation or cross-modal modeling, so it does not meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an online target speaker voice activity detection system for speaker diarization, but it does not discuss or propose any discrete audio tokens derived via neural audio codec or self-supervised learning; nor does it provide details about tokenization, vector quantization, or discrete token sequences as required by the inclusion criteria. The study is about speaker diarization and voice activity detection without touching upon discrete token extraction, design, or usage for audio generation or cross-modal modeling, so it does not meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
    "abstract": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
    "metadata": {
      "arxiv_id": "2207.06011",
      "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
      "summary": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2022-07-13T07:35:23Z",
      "updated": "2022-07-13T07:35:23Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06011v1",
      "landing_url": "https://arxiv.org/abs/2207.06011v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06011"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on SATTS and zero-shot speaker adaptation via speaker attractors in multi-speaker TTS; it does not address discrete audio tokens, tokenizers, or token-based representations, so it fails the inclusion criteria and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on SATTS and zero-shot speaker adaptation via speaker attractors in multi-speaker TTS; it does not address discrete audio tokens, tokenizers, or token-based representations, so it fails the inclusion criteria and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on zero-shot speaker adaptation in multi-speaker TTS synthesis using speaker attractors but does not describe the use or generation of discrete audio tokens through discretization methods like neural audio codecs or self-supervised models, nor does it provide any details on token vocabularies, quantization, or token-based modeling which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and mainly deals with continuous speech representations and adaptation, leading to exclusion based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on zero-shot speaker adaptation in multi-speaker TTS synthesis using speaker attractors but does not describe the use or generation of discrete audio tokens through discretization methods like neural audio codecs or self-supervised models, nor does it provide any details on token vocabularies, quantization, or token-based modeling which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and mainly deals with continuous speech representations and adaptation, leading to exclusion based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
    "abstract": "Voice conversion is to generate a new speech with the source content and a target voice style. In this paper, we focus on one general setting, i.e., non-parallel many-to-many voice conversion, which is close to the real-world scenario. As the name implies, non-parallel many-to-many voice conversion does not require the paired source and reference speeches and can be applied to arbitrary voice transfer. In recent years, Generative Adversarial Networks (GANs) and other techniques such as Conditional Variational Autoencoders (CVAEs) have made considerable progress in this field. However, due to the sophistication of voice conversion, the style similarity of the converted speech is still unsatisfactory. Inspired by the inherent structure of mel-spectrogram, we propose a new voice conversion framework, i.e., Subband-based Generative Adversarial Network for Voice Conversion (SGAN-VC). SGAN-VC converts each subband content of the source speech separately by explicitly utilizing the spatial characteristics between different subbands. SGAN-VC contains one style encoder, one content encoder, and one decoder. In particular, the style encoder network is designed to learn style codes for different subbands of the target speaker. The content encoder network can capture the content information on the source speech. Finally, the decoder generates particular subband content. In addition, we propose a pitch-shift module to fine-tune the pitch of the source speaker, making the converted tone more accurate and explainable. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on VCTK Corpus and AISHELL3 datasets both qualitatively and quantitatively, whether on seen or unseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data even exceeds that of StarGANv2-VC with ASR network assistance.",
    "metadata": {
      "arxiv_id": "2207.06057",
      "title": "Subband-based Generative Adversarial Network for Non-parallel Many-to-many Voice Conversion",
      "summary": "Voice conversion is to generate a new speech with the source content and a target voice style. In this paper, we focus on one general setting, i.e., non-parallel many-to-many voice conversion, which is close to the real-world scenario. As the name implies, non-parallel many-to-many voice conversion does not require the paired source and reference speeches and can be applied to arbitrary voice transfer. In recent years, Generative Adversarial Networks (GANs) and other techniques such as Conditional Variational Autoencoders (CVAEs) have made considerable progress in this field. However, due to the sophistication of voice conversion, the style similarity of the converted speech is still unsatisfactory. Inspired by the inherent structure of mel-spectrogram, we propose a new voice conversion framework, i.e., Subband-based Generative Adversarial Network for Voice Conversion (SGAN-VC). SGAN-VC converts each subband content of the source speech separately by explicitly utilizing the spatial characteristics between different subbands. SGAN-VC contains one style encoder, one content encoder, and one decoder. In particular, the style encoder network is designed to learn style codes for different subbands of the target speaker. The content encoder network can capture the content information on the source speech. Finally, the decoder generates particular subband content. In addition, we propose a pitch-shift module to fine-tune the pitch of the source speaker, making the converted tone more accurate and explainable. Extensive experiments demonstrate that the proposed approach achieves state-of-the-art performance on VCTK Corpus and AISHELL3 datasets both qualitatively and quantitatively, whether on seen or unseen data. Furthermore, the content intelligibility of SGAN-VC on unseen data even exceeds that of StarGANv2-VC with ASR network assistance.",
      "authors": [
        "Jian Ma",
        "Zhedong Zheng",
        "Hao Fei",
        "Feng Zheng",
        "Tat-seng Chua",
        "Yi Yang"
      ],
      "published": "2022-07-13T09:03:28Z",
      "updated": "2022-07-27T07:31:57Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06057v2",
      "landing_url": "https://arxiv.org/abs/2207.06057v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.06057"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets non-parallel many-to-many voice conversion using SGANs and subband processing, with no discussion of discrete audio tokens or tokenization; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets non-parallel many-to-many voice conversion using SGANs and subband processing, with no discussion of discrete audio tokens or tokenization; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on non-parallel many-to-many voice conversion using a GAN-based framework but does not describe any discretization process mapping continuous audio waveforms into discrete token sequences or mention neural audio codecs or vector quantization used to create discrete tokens. The work centers on continuous speech feature processing (mel-spectrogram subbands) and GAN modeling for voice style transfer rather than discrete audio token representation, making it not aligned with the inclusion criteria requiring discrete audio tokens as core representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on non-parallel many-to-many voice conversion using a GAN-based framework but does not describe any discretization process mapping continuous audio waveforms into discrete token sequences or mention neural audio codecs or vector quantization used to create discrete tokens. The work centers on continuous speech feature processing (mel-spectrogram subbands) and GAN modeling for voice style transfer rather than discrete audio token representation, making it not aligned with the inclusion criteria requiring discrete audio tokens as core representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
    "abstract": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
    "metadata": {
      "arxiv_id": "2207.06088",
      "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
      "summary": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
      "authors": [
        "Zhengxi Liu",
        "Qiao Tian",
        "Chenxu Hu",
        "Xudong Liu",
        "Menglin Wu",
        "Yuping Wang",
        "Hang Zhao",
        "Yuxuan Wang"
      ],
      "published": "2022-07-13T09:57:06Z",
      "updated": "2022-07-13T09:57:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06088v1",
      "landing_url": "https://arxiv.org/abs/2207.06088v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06088"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on end-to-end waveform generation without using discrete audio tokens or tokenization, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on end-to-end waveform generation without using discrete audio tokens or tokenization, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on end-to-end neural text-to-speech generation producing raw waveforms directly from text, addressing prosody modeling and high-frequency reconstruction, but does not mention the use or generation of discrete audio tokens, tokenizers, vector quantization, or any discretization processes mapping continuous audio into discrete token sequences as required by the inclusion criteria. Hence, it does not fulfill the core criterion of using discrete audio tokens as a representation, and therefore should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on end-to-end neural text-to-speech generation producing raw waveforms directly from text, addressing prosody modeling and high-frequency reconstruction, but does not mention the use or generation of discrete audio tokens, tokenizers, vector quantization, or any discretization processes mapping continuous audio into discrete token sequences as required by the inclusion criteria. Hence, it does not fulfill the core criterion of using discrete audio tokens as a representation, and therefore should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
    "metadata": {
      "arxiv_id": "2207.06389",
      "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
      "summary": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
      "authors": [
        "Rongjie Huang",
        "Zhou Zhao",
        "Huadai Liu",
        "Jinglin Liu",
        "Chenye Cui",
        "Yi Ren"
      ],
      "published": "2022-07-13T17:45:43Z",
      "updated": "2022-07-13T17:45:43Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06389v1",
      "landing_url": "https://arxiv.org/abs/2207.06389v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06389"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on diffusion-based TTS that outputs mel-spectrograms (continuous representations) and does not introduce or analyze discrete audio tokens, so it does not meet the discrete-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on diffusion-based TTS that outputs mel-spectrograms (continuous representations) and does not introduce or analyze discrete audio tokens, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study ProDiff focuses on a diffusion model-based approach for fast and high-quality text-to-speech synthesis. However, the abstract does not mention any discrete audio tokenization process or neural audio codec producing discrete tokens; it mainly discusses continuous mel-spectrogram generation through diffusion models. There is no indication of discretization mapping continuous waveform into token sequences, vocabulary design, codebooks, or any evaluation relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focusing on discrete audio tokens as core representations for modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study ProDiff focuses on a diffusion model-based approach for fast and high-quality text-to-speech synthesis. However, the abstract does not mention any discrete audio tokenization process or neural audio codec producing discrete tokens; it mainly discusses continuous mel-spectrogram generation through diffusion models. There is no indication of discretization mapping continuous waveform into token sequences, vocabulary design, codebooks, or any evaluation relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focusing on discrete audio tokens as core representations for modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
    "abstract": "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region.",
    "metadata": {
      "arxiv_id": "2207.07850",
      "title": "Reducing Geographic Disparities in Automatic Speech Recognition via Elastic Weight Consolidation",
      "summary": "We present an approach to reduce the performance disparity between geographic regions without degrading performance on the overall user population for ASR. A popular approach is to fine-tune the model with data from regions where the ASR model has a higher word error rate (WER). However, when the ASR model is adapted to get better performance on these high-WER regions, its parameters wander from the previous optimal values, which can lead to worse performance in other regions. In our proposed method, we utilize the elastic weight consolidation (EWC) regularization loss to identify directions in parameters space along which the ASR weights can vary to improve for high-error regions, while still maintaining performance on the speaker population overall. Our results demonstrate that EWC can reduce the word error rate (WER) in the region with highest WER by 3.2% relative while reducing the overall WER by 1.3% relative. We also evaluate the role of language and acoustic models in ASR fairness and propose a clustering algorithm to identify WER disparities based on geographic region.",
      "authors": [
        "Viet Anh Trinh",
        "Pegah Ghahremani",
        "Brian King",
        "Jasha Droppo",
        "Andreas Stolcke",
        "Roland Maas"
      ],
      "published": "2022-07-16T06:04:52Z",
      "updated": "2022-07-16T06:04:52Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.07850v1",
      "landing_url": "https://arxiv.org/abs/2207.07850v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-11063"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on fairness in ASR using Elastic Weight Consolidation to reduce geographic WER disparities; it does not involve discrete audio tokens, tokenizers, or token-based representations, thus does not meet the inclusion criteria for discrete audio token studies.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on fairness in ASR using Elastic Weight Consolidation to reduce geographic WER disparities; it does not involve discrete audio tokens, tokenizers, or token-based representations, thus does not meet the inclusion criteria for discrete audio token studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving automatic speech recognition fairness via Elastic Weight Consolidation but does not mention any discretization of audio waveforms into token sequences or the use of neural audio codecs, vector quantization, or discrete token design, which are essential for inclusion under the 'Discrete Audio Tokens' theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving automatic speech recognition fairness via Elastic Weight Consolidation but does not mention any discretization of audio waveforms into token sequences or the use of neural audio codecs, vector quantization, or discrete token design, which are essential for inclusion under the 'Discrete Audio Tokens' theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cancer Subtyping by Improved Transcriptomic Features Using Vector Quantized Variational Autoencoder",
    "abstract": "Defining and separating cancer subtypes is essential for facilitating personalized therapy modality and prognosis of patients. The definition of subtypes has been constantly recalibrated as a result of our deepened understanding. During this recalibration, researchers often rely on clustering of cancer data to provide an intuitive visual reference that could reveal the intrinsic characteristics of subtypes. The data being clustered are often omics data such as transcriptomics that have strong correlations to the underlying biological mechanism. However, while existing studies have shown promising results, they suffer from issues associated with omics data: sample scarcity and high dimensionality. As such, existing methods often impose unrealistic assumptions to extract useful features from the data while avoiding overfitting to spurious correlations. In this paper, we propose to leverage a recent strong generative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle the data issues and extract informative latent features that are crucial to the quality of subsequent clustering by retaining only information relevant to reconstructing the input. VQ-VAE does not impose strict assumptions and hence its latent features are better representations of the input, capable of yielding superior clustering performance with any mainstream clustering method. Extensive experiments and medical analysis on multiple datasets comprising 10 distinct cancers demonstrate the VQ-VAE clustering results can significantly and robustly improve prognosis over prevalent subtyping systems.",
    "metadata": {
      "arxiv_id": "2207.09783",
      "title": "Cancer Subtyping by Improved Transcriptomic Features Using Vector Quantized Variational Autoencoder",
      "summary": "Defining and separating cancer subtypes is essential for facilitating personalized therapy modality and prognosis of patients. The definition of subtypes has been constantly recalibrated as a result of our deepened understanding. During this recalibration, researchers often rely on clustering of cancer data to provide an intuitive visual reference that could reveal the intrinsic characteristics of subtypes. The data being clustered are often omics data such as transcriptomics that have strong correlations to the underlying biological mechanism. However, while existing studies have shown promising results, they suffer from issues associated with omics data: sample scarcity and high dimensionality. As such, existing methods often impose unrealistic assumptions to extract useful features from the data while avoiding overfitting to spurious correlations. In this paper, we propose to leverage a recent strong generative model, Vector Quantized Variational AutoEncoder (VQ-VAE), to tackle the data issues and extract informative latent features that are crucial to the quality of subsequent clustering by retaining only information relevant to reconstructing the input. VQ-VAE does not impose strict assumptions and hence its latent features are better representations of the input, capable of yielding superior clustering performance with any mainstream clustering method. Extensive experiments and medical analysis on multiple datasets comprising 10 distinct cancers demonstrate the VQ-VAE clustering results can significantly and robustly improve prognosis over prevalent subtyping systems.",
      "authors": [
        "Zheng Chen",
        "Ziwei Yang",
        "Lingwei Zhu",
        "Guang Shi",
        "Kun Yue",
        "Takashi Matsubara",
        "Shigehiko Kanaya",
        "MD Altaf-Ul-Amin"
      ],
      "published": "2022-07-20T09:47:53Z",
      "updated": "2022-07-20T09:47:53Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.09783v1",
      "landing_url": "https://arxiv.org/abs/2207.09783v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.09783"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究雖使用向量量化自編碼器得到離散化 latent 表徵，但主題為癌症亞型鑑定與轉錄組資料分析，並非離散音訊 token 的設計、產生流程或用於音訊生成/跨模態推理等下游任務，因此不符合「Discrete Audio Tokens」的收錄與評估條件，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究雖使用向量量化自編碼器得到離散化 latent 表徵，但主題為癌症亞型鑑定與轉錄組資料分析，並非離散音訊 token 的設計、產生流程或用於音訊生成/跨模態推理等下游任務，因此不符合「Discrete Audio Tokens」的收錄與評估條件，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cancer subtyping using a Vector Quantized Variational AutoEncoder (VQ-VAE) applied to transcriptomic data, which is biological data related to cancer, not audio signals. Although it uses vector quantization, it does not involve discretizing audio waveforms into tokens for modeling or generation, nor does it address discrete audio tokens, neural audio codecs, or any audio-related applications as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cancer subtyping using a Vector Quantized Variational AutoEncoder (VQ-VAE) applied to transcriptomic data, which is biological data related to cancer, not audio signals. Although it uses vector quantization, it does not involve discretizing audio waveforms into tokens for modeling or generation, nor does it address discrete audio tokens, neural audio codecs, or any audio-related applications as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
    "abstract": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
    "metadata": {
      "arxiv_id": "2207.11226",
      "title": "FewGAN: Generating from the Joint Distribution of a Few Images",
      "summary": "We introduce FewGAN, a generative model for generating novel, high-quality and diverse images whose patch distribution lies in the joint patch distribution of a small number of N>1 training samples. The method is, in essence, a hierarchical patch-GAN that applies quantization at the first coarse scale, in a similar fashion to VQ-GAN, followed by a pyramid of residual fully convolutional GANs at finer scales. Our key idea is to first use quantization to learn a fixed set of patch embeddings for training images. We then use a separate set of side images to model the structure of generated images using an autoregressive model trained on the learned patch embeddings of training images. Using quantization at the coarsest scale allows the model to generate both conditional and unconditional novel images. Subsequently, a patch-GAN renders the fine details, resulting in high-quality images. In an extensive set of experiments, it is shown that FewGAN outperforms baselines both quantitatively and qualitatively.",
      "authors": [
        "Lior Ben-Moshe",
        "Sagie Benaim",
        "Lior Wolf"
      ],
      "published": "2022-07-18T07:11:28Z",
      "updated": "2022-07-18T07:11:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.11226v1",
      "landing_url": "https://arxiv.org/abs/2207.11226v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.11226"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on FewGAN for image generation using quantization and patch-GAN, with no discrete audio tokens or audio tokenization methodology, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on FewGAN for image generation using quantization and patch-GAN, with no discrete audio tokens or audio tokenization methodology, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe FewGAN, a generative model for images based on patch distributions. The work focuses on generating novel images and uses patch-GANs and quantization but is centered on image generation rather than audio tokenization or audio-related tasks. There is no mention or indication of the method producing discrete audio tokens, neural audio codecs, audio discretization, or related tasks such as TTS, speech or music generation, or cross-modal audio-text/image alignment. Therefore, it does not meet the inclusion criteria of discrete audio tokens and is unrelated to audio, leading to its exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe FewGAN, a generative model for images based on patch distributions. The work focuses on generating novel images and uses patch-GANs and quantization but is centered on image generation rather than audio tokenization or audio-related tasks. There is no mention or indication of the method producing discrete audio tokens, neural audio codecs, audio discretization, or related tasks such as TTS, speech or music generation, or cross-modal audio-text/image alignment. Therefore, it does not meet the inclusion criteria of discrete audio tokens and is unrelated to audio, leading to its exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantized Image-to-Image Translation",
    "abstract": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
    "metadata": {
      "arxiv_id": "2207.13286",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "authors": [
        "Yu-Jie Chen",
        "Shin-I Cheng",
        "Wei-Chen Chiu",
        "Hung-Yu Tseng",
        "Hsin-Ying Lee"
      ],
      "published": "2022-07-27T04:22:29Z",
      "updated": "2022-07-27T04:22:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13286v1",
      "landing_url": "https://arxiv.org/abs/2207.13286v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13286"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets vector-quantized image-to-image translation in the visual domain, not discrete audio tokens, hence it does not satisfy the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets vector-quantized image-to-image translation in the visual domain, not discrete audio tokens, hence it does not satisfy the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization techniques in image-to-image translation and does not address audio signals, discrete audio tokens, or related neural audio codec methodologies relevant to audio tokenization and modeling; thus, it falls outside the inclusion domain of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization techniques in image-to-image translation and does not address audio signals, discrete audio tokens, or related neural audio codec methodologies relevant to audio tokenization and modeling; thus, it falls outside the inclusion domain of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
    "abstract": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2207.13861",
      "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
      "summary": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
      "authors": [
        "Hao Li",
        "Zhijing Yang",
        "Xiaobin Hong",
        "Ziying Zhao",
        "Junyang Chen",
        "Yukai Shi",
        "Jinshan Pan"
      ],
      "published": "2022-07-28T02:33:57Z",
      "updated": "2022-09-13T05:14:07Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13861v2",
      "landing_url": "https://arxiv.org/abs/2207.13861v2",
      "doi": "https://doi.org/10.1016/j.knosys.2022.109815"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about real-world image denoising using a continuous wavelet sliding-transformer (DnSwin), involving CNN encoders/decoders and wavelet-based transformers. It does not involve discrete audio tokens, tokenization, or token-based modeling, and thus fails the 'Discrete Audio Tokens' criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about real-world image denoising using a continuous wavelet sliding-transformer (DnSwin), involving CNN encoders/decoders and wavelet-based transformers. It does not involve discrete audio tokens, tokenization, or token-based modeling, and thus fails the 'Discrete Audio Tokens' criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image denoising using a Wavelet Sliding-Transformer and Vision Transformer architecture, which is unrelated to discrete audio tokens or audio processing as defined by the inclusion criteria. There is no mention of discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-related token modeling or evaluation. Therefore, it does not meet the core inclusion requirements nor addresses any audio token-related issues outlined in the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image denoising using a Wavelet Sliding-Transformer and Vision Transformer architecture, which is unrelated to discrete audio tokens or audio processing as defined by the inclusion criteria. There is no mention of discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-related token modeling or evaluation. Therefore, it does not meet the core inclusion requirements nor addresses any audio token-related issues outlined in the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
    "abstract": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
    "metadata": {
      "arxiv_id": "2207.14686",
      "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
      "summary": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
      "authors": [
        "Denise Moussa",
        "Anatol Maier",
        "Andreas Spruck",
        "Jürgen Seiler",
        "Christian Riess"
      ],
      "published": "2022-07-29T13:58:24Z",
      "updated": "2024-05-03T15:15:27Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.14686v3",
      "landing_url": "https://arxiv.org/abs/2207.14686v3",
      "doi": "https://doi.org/10.1109/ICIP46576.2022.9897178"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns forensic license plate recognition with a compression-informed Transformer, which does not involve discrete audio tokens or tokenization as defined in the inclusion criteria; it lacks any discussion of neural audio codecs, discrete token design, or downstream tasks enabled by discrete tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns forensic license plate recognition with a compression-informed Transformer, which does not involve discrete audio tokens or tokenization as defined in the inclusion criteria; it lacks any discussion of neural audio codecs, discrete token design, or downstream tasks enabled by discrete tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on forensic license plate recognition from compressed and low-resolution images using a Transformer architecture informed by compression levels, but it does not involve discrete audio tokens, neural audio codecs, or audio tokenization, nor does it address audio generation, understanding, or cross-modal modeling related to audio tokens as outlined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on forensic license plate recognition from compressed and low-resolution images using a Transformer architecture informed by compression levels, but it does not involve discrete audio tokens, neural audio codecs, or audio tokenization, nor does it address audio generation, understanding, or cross-modal modeling related to audio tokens as outlined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
    "abstract": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
    "metadata": {
      "arxiv_id": "2208.03987",
      "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
      "summary": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
      "authors": [
        "Di Wang",
        "Qiming Zhang",
        "Yufei Xu",
        "Jing Zhang",
        "Bo Du",
        "Dacheng Tao",
        "Liangpei Zhang"
      ],
      "published": "2022-08-08T09:08:40Z",
      "updated": "2022-12-08T13:51:33Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.03987v4",
      "landing_url": "https://arxiv.org/abs/2208.03987v4",
      "doi": "https://doi.org/10.48550/arXiv.2208.03987"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vision transformers for remote sensing and does not involve discrete audio tokens, tokenizers/codecs, or downstream audio generation/understanding tasks; it fails the inclusion criteria for the discrete audio tokens topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vision transformers for remote sensing and does not involve discrete audio tokens, tokenizers/codecs, or downstream audio generation/understanding tasks; it fails the inclusion criteria for the discrete audio tokens topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on large-scale vision transformers for remote sensing image analysis and does not discuss any aspect related to discrete audio tokens, audio discretization, or neural audio codecs; hence it fails to meet the inclusion criteria centered on discrete audio token representations and processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on large-scale vision transformers for remote sensing image analysis and does not discuss any aspect related to discrete audio tokens, audio discretization, or neural audio codecs; hence it fails to meet the inclusion criteria centered on discrete audio token representations and processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA",
    "abstract": "Convolutional Neural Networks (CNNs) reach high accuracies in various application domains, but require large amounts of computation and incur costly data movements. One method to decrease these costs while trading accuracy is weight and/or activation word-length reduction. Thereby, layer-wise mixed-precision quantization allows for more efficient results while inflating the design space. In this work, we present an in-depth quantitative methodology to efficiently explore the design space considering the limited hardware resources of a given FPGA. Our holistic exploration approach vertically traverses the various design entry levels from the architectural down to the logic level, and laterally covers optimization from processing elements to dataflow for an efficient mixed-precision CNN accelerator. Our resulting hardware accelerators implement truly mixed-precision operations that enable efficient execution of layer-wise and channel-wise quantized CNNs. Mapping feed-forward and identity-shortcut-connection mixed-precision CNNs result in competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5 accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for ResNet-152, respectively. Thereby, the required memory footprint for parameters is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",
    "metadata": {
      "arxiv_id": "2208.04854",
      "title": "Design of High-Throughput Mixed-Precision CNN Accelerators on FPGA",
      "summary": "Convolutional Neural Networks (CNNs) reach high accuracies in various application domains, but require large amounts of computation and incur costly data movements. One method to decrease these costs while trading accuracy is weight and/or activation word-length reduction. Thereby, layer-wise mixed-precision quantization allows for more efficient results while inflating the design space. In this work, we present an in-depth quantitative methodology to efficiently explore the design space considering the limited hardware resources of a given FPGA. Our holistic exploration approach vertically traverses the various design entry levels from the architectural down to the logic level, and laterally covers optimization from processing elements to dataflow for an efficient mixed-precision CNN accelerator. Our resulting hardware accelerators implement truly mixed-precision operations that enable efficient execution of layer-wise and channel-wise quantized CNNs. Mapping feed-forward and identity-shortcut-connection mixed-precision CNNs result in competitive accuracy-throughout trade-offs: 245 frames/s with 87.48% Top-5 accuracy for ResNet-18 and 92.9% Top-5 accuracy with 1.13 TOps/s for ResNet-152, respectively. Thereby, the required memory footprint for parameters is reduced by 4.9x and 9.4x compared to the respective floating-point baseline.",
      "authors": [
        "Cecilia Latotzke",
        "Tim Ciesielski",
        "Tobias Gemmeke"
      ],
      "published": "2022-08-09T15:32:51Z",
      "updated": "2022-08-09T15:32:51Z",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.04854v1",
      "landing_url": "https://arxiv.org/abs/2208.04854v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.04854"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses FPGA-based CNN accelerators with mixed-precision precision and performance metrics, not discrete audio tokens; it does not meet the inclusion criteria for Discrete Audio Tokens and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses FPGA-based CNN accelerators with mixed-precision precision and performance metrics, not discrete audio tokens; it does not meet the inclusion criteria for Discrete Audio Tokens and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe the design and optimization of high-throughput mixed-precision CNN accelerators on FPGA focused on computational efficiency and trade-offs in accuracy for CNN models. There is no mention of audio waveform discretization, neural audio codecs, discrete audio tokens, or token generation related to audio modeling, generation, or alignment. The study is centered on hardware accelerator design for CNN computations rather than discrete audio token representation or audio-related downstream tasks like speech/audio/music generation or multimodal reasoning. Hence, it does not meet the inclusion criteria focused on discrete audio token topics and related neural or self-supervised token generation methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe the design and optimization of high-throughput mixed-precision CNN accelerators on FPGA focused on computational efficiency and trade-offs in accuracy for CNN models. There is no mention of audio waveform discretization, neural audio codecs, discrete audio tokens, or token generation related to audio modeling, generation, or alignment. The study is centered on hardware accelerator design for CNN computations rather than discrete audio token representation or audio-related downstream tasks like speech/audio/music generation or multimodal reasoning. Hence, it does not meet the inclusion criteria focused on discrete audio token topics and related neural or self-supervised token generation methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
    "abstract": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
    "metadata": {
      "arxiv_id": "2208.08757",
      "title": "Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion",
      "summary": "One-shot voice conversion (VC) with only a single target speaker's speech for reference has become a hot research topic. Existing works generally disentangle timbre, while information about pitch, rhythm and content is still mixed together. To perform one-shot VC effectively with further disentangling these speech components, we employ random resampling for pitch and content encoder and use the variational contrastive log-ratio upper bound of mutual information and gradient reversal layer based adversarial mutual information learning to ensure the different parts of the latent space containing only the desired disentangled representation during training. Experiments on the VCTK dataset show the model achieves state-of-the-art performance for one-shot VC in terms of naturalness and intellgibility. In addition, we can transfer characteristics of one-shot VC on timbre, pitch and rhythm separately by speech representation disentanglement. Our code, pre-trained models and demo are available at https://im1eon.github.io/IS2022-SRDVC/.",
      "authors": [
        "SiCheng Yang",
        "Methawee Tantrawenith",
        "Haolin Zhuang",
        "Zhiyong Wu",
        "Aolan Sun",
        "Jianzong Wang",
        "Ning Cheng",
        "Huaizhen Tang",
        "Xintao Zhao",
        "Jie Wang",
        "Helen Meng"
      ],
      "published": "2022-08-18T10:36:27Z",
      "updated": "2022-08-18T10:36:27Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.08757v1",
      "landing_url": "https://arxiv.org/abs/2208.08757v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.08757"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on continuous latent representation disentanglement for one-shot voice conversion and does not discuss discrete audio tokens, codecs, or tokenization, thus it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on continuous latent representation disentanglement for one-shot voice conversion and does not discuss discrete audio tokens, codecs, or tokenization, thus it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on one-shot voice conversion using disentangled speech representations, but there is no mention of discrete audio tokens, neural audio codec, vector quantization, or discrete token sequences central to modeling or downstream tasks as defined in the inclusion criteria; the study appears to address continuous representation disentanglement rather than discrete token-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on one-shot voice conversion using disentangled speech representations, but there is no mention of discrete audio tokens, neural audio codec, vector quantization, or discrete token sequences central to modeling or downstream tasks as defined in the inclusion criteria; the study appears to address continuous representation disentanglement rather than discrete token-based modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
    "abstract": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
    "metadata": {
      "arxiv_id": "2208.09030",
      "title": "A Secure and Efficient Data Deduplication Scheme with Dynamic Ownership Management in Cloud Computing",
      "summary": "Encrypted data deduplication is an important technique for saving storage space and network bandwidth, which has been widely used in cloud storage. Recently, a number of schemes that solve the problem of data deduplication with dynamic ownership management have been proposed. However, these schemes suffer from low efficiency when the dynamic ownership changes a lot. To this end, in this paper, we propose a novel server-side deduplication scheme for encrypted data in a hybrid cloud architecture, where a public cloud (Pub-CSP) manages the storage and a private cloud (Pri-CSP) plays a role as the data owner to perform deduplication and dynamic ownership management. Further, to reduce the communication overhead we use an initial uploader check mechanism to ensure only the first uploader needs to perform encryption, and adopt an access control technique that verifies the validity of the data users before they download data. Our security analysis and performance evaluation demonstrate that our proposed server-side deduplication scheme has better performance in terms of security, effectiveness, and practicability compared with previous schemes. Meanwhile, our method can efficiently resist collusion attacks and duplicate faking attacks.",
      "authors": [
        "Xuewei Ma",
        "Wenyuan Yang",
        "Yuesheng Zhu",
        "Zhiqiang Bai"
      ],
      "published": "2022-08-18T19:02:30Z",
      "updated": "2022-08-31T15:47:52Z",
      "categories": [
        "cs.CR",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.09030v3",
      "landing_url": "https://arxiv.org/abs/2208.09030v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.09030"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about encrypted data deduplication and dynamic ownership in cloud computing; it does not address discrete audio tokens, tokenization methods, or audio generation/understanding, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about encrypted data deduplication and dynamic ownership in cloud computing; it does not address discrete audio tokens, tokenization methods, or audio generation/understanding, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a data deduplication scheme in cloud computing with no mention of discrete audio tokens, neural audio codecs, or related audio tokenization processes; hence, it does not meet the inclusion criteria centered on discrete audio tokens and their applications in modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a data deduplication scheme in cloud computing with no mention of discrete audio tokens, neural audio codecs, or related audio tokenization processes; hence, it does not meet the inclusion criteria centered on discrete audio tokens and their applications in modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Planning in a Compact Latent Action Space",
    "abstract": "Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision-making, and scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.",
    "metadata": {
      "arxiv_id": "2208.10291",
      "title": "Efficient Planning in a Compact Latent Action Space",
      "summary": "Planning-based reinforcement learning has shown strong performance in tasks in discrete and low-dimensional continuous action spaces. However, planning usually brings significant computational overhead for decision-making, and scaling such methods to high-dimensional action spaces remains challenging. To advance efficient planning for high-dimensional continuous control, we propose Trajectory Autoencoding Planner (TAP), which learns low-dimensional latent action codes with a state-conditional VQ-VAE. The decoder of the VQ-VAE thus serves as a novel dynamics model that takes latent actions and current state as input and reconstructs long-horizon trajectories. During inference time, given a starting state, TAP searches over discrete latent actions to find trajectories that have both high probability under the training distribution and high predicted cumulative reward. Empirical evaluation in the offline RL setting demonstrates low decision latency which is indifferent to the growing raw action dimensionality. For Adroit robotic hand manipulation tasks with high-dimensional continuous action space, TAP surpasses existing model-based methods by a large margin and also beats strong model-free actor-critic baselines.",
      "authors": [
        "Zhengyao Jiang",
        "Tianjun Zhang",
        "Michael Janner",
        "Yueying Li",
        "Tim Rocktäschel",
        "Edward Grefenstette",
        "Yuandong Tian"
      ],
      "published": "2022-08-22T13:19:02Z",
      "updated": "2023-01-24T11:09:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.10291v3",
      "landing_url": "https://arxiv.org/abs/2208.10291v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.10291"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses discrete latent actions for planning in reinforcement learning, not discrete audio tokens or tokenization of audio; it does not meet the inclusion criteria focused on discrete audio tokens and related token-based modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses discrete latent actions for planning in reinforcement learning, not discrete audio tokens or tokenization of audio; it does not meet the inclusion criteria focused on discrete audio tokens and related token-based modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on planning-based reinforcement learning in continuous action spaces and does not address discrete audio tokens, neural audio codecs, or tokenization of continuous audio waveforms as per the inclusion criteria. There is no mention of audio-related discrete token generation, modeling, or cross-modal tasks involving audio tokens. Therefore, it does not meet the topic-specific inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on planning-based reinforcement learning in continuous action spaces and does not address discrete audio tokens, neural audio codecs, or tokenization of continuous audio waveforms as per the inclusion criteria. There is no mention of audio-related discrete token generation, modeling, or cross-modal tasks involving audio tokens. Therefore, it does not meet the topic-specific inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
    "abstract": "Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data. However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction. Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case. Therefore, the contribution of this work is two-fold. First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction. Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker.",
    "metadata": {
      "arxiv_id": "2209.01978",
      "title": "Investigation into Target Speaking Rate Adaptation for Voice Conversion",
      "summary": "Disentangling speaker and content attributes of a speech signal into separate latent representations followed by decoding the content with an exchanged speaker representation is a popular approach for voice conversion, which can be trained with non-parallel and unlabeled speech data. However, previous approaches perform disentanglement only implicitly via some sort of information bottleneck or normalization, where it is usually hard to find a good trade-off between voice conversion and content reconstruction. Further, previous works usually do not consider an adaptation of the speaking rate to the target speaker or they put some major restrictions to the data or use case. Therefore, the contribution of this work is two-fold. First, we employ an explicit and fully unsupervised disentanglement approach, which has previously only been used for representation learning, and show that it allows to obtain both superior voice conversion and content reconstruction. Second, we investigate simple and generic approaches to linearly adapt the length of a speech signal, and hence the speaking rate, to a target speaker and show that the proposed adaptation allows to increase the speaking rate similarity with respect to the target speaker.",
      "authors": [
        "Michael Kuhlmann",
        "Fritz Seebauer",
        "Janek Ebbers",
        "Petra Wagner",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-09-05T14:20:42Z",
      "updated": "2022-09-05T14:20:42Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.01978v1",
      "landing_url": "https://arxiv.org/abs/2209.01978v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.01978"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on voice conversion and speaking-rate adaptation without discussing discrete audio tokens, tokenization, or token-based representations, thus it fails the 'Discrete Audio Tokens' inclusion criteria and aligns with the exclusion criteria for not addressing token discretization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on voice conversion and speaking-rate adaptation without discussing discrete audio tokens, tokenization, or token-based representations, thus it fails the 'Discrete Audio Tokens' inclusion criteria and aligns with the exclusion criteria for not addressing token discretization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion via disentanglement of speaker and content attributes and adaptation of speaking rate, but it does not discuss or involve discrete audio tokens, tokenizer architectures, quantization methods, or the mapping of continuous audio waveforms to discrete token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion via disentanglement of speaker and content attributes and adaptation of speaking rate, but it does not discuss or involve discrete audio tokens, tokenizer architectures, quantization methods, or the mapping of continuous audio waveforms to discrete token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
    "abstract": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
    "metadata": {
      "arxiv_id": "2209.04213",
      "title": "Autoencoder Based Iterative Modeling and Multivariate Time-Series Subsequence Clustering Algorithm",
      "summary": "This paper introduces an algorithm for the detection of change-points and the identification of the corresponding subsequences in transient multivariate time-series data (MTSD). The analysis of such data has become more and more important due to the increase of availability in many industrial fields. Labeling, sorting or filtering highly transient measurement data for training condition based maintenance (CbM) models is cumbersome and error-prone. For some applications it can be sufficient to filter measurements by simple thresholds or finding change-points based on changes in mean value and variation. But a robust diagnosis of a component within a component group for example, which has a complex non-linear correlation between multiple sensor values, a simple approach would not be feasible. No meaningful and coherent measurement data which could be used for training a CbM model would emerge. Therefore, we introduce an algorithm which uses a recurrent neural network (RNN) based Autoencoder (AE) which is iteratively trained on incoming data. The scoring function uses the reconstruction error and latent space information. A model of the identified subsequence is saved and used for recognition of repeating subsequences as well as fast offline clustering. For evaluation, we propose a new similarity measure based on the curvature for a more intuitive time-series subsequence clustering metric. A comparison with seven other state-of-the-art algorithms and eight datasets shows the capability and the increased performance of our algorithm to cluster MTSD online and offline in conjunction with mechatronic systems.",
      "authors": [
        "Jonas Köhne",
        "Lars Henning",
        "Clemens Gühmann"
      ],
      "published": "2022-09-09T09:59:56Z",
      "updated": "2022-09-23T11:07:27Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04213v2",
      "landing_url": "https://arxiv.org/abs/2209.04213v2",
      "doi": "https://doi.org/10.1109/ACCESS.2023.3247564"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe change-point detection and subsequence clustering in multivariate time-series for mechatronic systems using an RNN autoencoder, with no mention of discrete audio tokens, tokenization, or token-based modeling, thus failing the inclusion criteria and meeting exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe change-point detection and subsequence clustering in multivariate time-series for mechatronic systems using an RNN autoencoder, with no mention of discrete audio tokens, tokenization, or token-based modeling, thus failing the inclusion criteria and meeting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an algorithm for detecting and clustering subsequences in multivariate time-series sensor data using an autoencoder-based method, but it does not discuss discretization of audio waveforms into discrete audio tokens, neural audio codecs, or leveraging discrete token sequences for audio modeling or generation, which are core to the inclusion criteria. It deals with transient multivariate time-series from industrial sensors rather than discrete audio token representations or audio-related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an algorithm for detecting and clustering subsequences in multivariate time-series sensor data using an autoencoder-based method, but it does not discuss discretization of audio waveforms into discrete audio tokens, neural audio codecs, or leveraging discrete token sequences for audio modeling or generation, which are core to the inclusion criteria. It deals with transient multivariate time-series from industrial sensors rather than discrete audio token representations or audio-related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lexicon and Attention based Handwritten Text Recognition System",
    "abstract": "The handwritten text recognition problem is widely studied by the researchers of computer vision community due to its scope of improvement and applicability to daily lives, It is a sub-domain of pattern recognition. Due to advancement of computational power of computers since last few decades neural networks based systems heavily contributed towards providing the state-of-the-art handwritten text recognizers. In the same direction, we have taken two state-of-the art neural networks systems and merged the attention mechanism with it. The attention technique has been widely used in the domain of neural machine translations and automatic speech recognition and now is being implemented in text recognition domain. In this study, we are able to achieve 4.15% character error rate and 9.72% word error rate on IAM dataset, 7.07% character error rate and 16.14% word error rate on GW dataset after merging the attention and word beam search decoder with existing Flor et al. architecture. To analyse further, we have also used system similar to Shi et al. neural network system with greedy decoder and observed 23.27% improvement in character error rate from the base model.",
    "metadata": {
      "arxiv_id": "2209.04817",
      "title": "Lexicon and Attention based Handwritten Text Recognition System",
      "summary": "The handwritten text recognition problem is widely studied by the researchers of computer vision community due to its scope of improvement and applicability to daily lives, It is a sub-domain of pattern recognition. Due to advancement of computational power of computers since last few decades neural networks based systems heavily contributed towards providing the state-of-the-art handwritten text recognizers. In the same direction, we have taken two state-of-the art neural networks systems and merged the attention mechanism with it. The attention technique has been widely used in the domain of neural machine translations and automatic speech recognition and now is being implemented in text recognition domain. In this study, we are able to achieve 4.15% character error rate and 9.72% word error rate on IAM dataset, 7.07% character error rate and 16.14% word error rate on GW dataset after merging the attention and word beam search decoder with existing Flor et al. architecture. To analyse further, we have also used system similar to Shi et al. neural network system with greedy decoder and observed 23.27% improvement in character error rate from the base model.",
      "authors": [
        "Lalita Kumari",
        "Sukhdeep Singh",
        "VVS Rathore",
        "Anuj Sharma"
      ],
      "published": "2022-09-11T09:26:45Z",
      "updated": "2022-09-11T09:26:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.04817v1",
      "landing_url": "https://arxiv.org/abs/2209.04817v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.04817"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses handwritten text recognition with attention, not discrete audio tokens or tokenization; it does not meet the inclusion criteria focused on discrete audio token modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses handwritten text recognition with attention, not discrete audio tokens or tokenization; it does not meet the inclusion criteria focused on discrete audio token modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on handwritten text recognition using neural networks and attention mechanisms, with no mention of discretizing continuous audio waveforms into token sequences or use of discrete audio tokens for audio generation or understanding; hence it does not meet the inclusion criteria related to discrete audio tokens and instead falls into the exclusion criteria regarding lack of discrete token usage and focus on audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on handwritten text recognition using neural networks and attention mechanisms, with no mention of discretizing continuous audio waveforms into token sequences or use of discrete audio tokens for audio generation or understanding; hence it does not meet the inclusion criteria related to discrete audio tokens and instead falls into the exclusion criteria regarding lack of discrete token usage and focus on audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Parallel Voice Conversion for ASR Augmentation",
    "abstract": "Automatic speech recognition (ASR) needs to be robust to speaker differences. Voice Conversion (VC) modifies speaker characteristics of input speech. This is an attractive feature for ASR data augmentation. In this paper, we demonstrate that voice conversion can be used as a data augmentation technique to improve ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR augmentation, it is necessary that the VC model be robust to a wide range of input speech. This motivates the use of a non-autoregressive, non-parallel VC model, and the use of a pretrained ASR encoder within the VC model. This work suggests that despite including many speakers, speaker diversity may remain a limitation to ASR quality. Finally, interrogation of our VC performance has provided useful metrics for objective evaluation of VC quality.",
    "metadata": {
      "arxiv_id": "2209.06987",
      "title": "Non-Parallel Voice Conversion for ASR Augmentation",
      "summary": "Automatic speech recognition (ASR) needs to be robust to speaker differences. Voice Conversion (VC) modifies speaker characteristics of input speech. This is an attractive feature for ASR data augmentation. In this paper, we demonstrate that voice conversion can be used as a data augmentation technique to improve ASR performance, even on LibriSpeech, which contains 2,456 speakers. For ASR augmentation, it is necessary that the VC model be robust to a wide range of input speech. This motivates the use of a non-autoregressive, non-parallel VC model, and the use of a pretrained ASR encoder within the VC model. This work suggests that despite including many speakers, speaker diversity may remain a limitation to ASR quality. Finally, interrogation of our VC performance has provided useful metrics for objective evaluation of VC quality.",
      "authors": [
        "Gary Wang",
        "Andrew Rosenberg",
        "Bhuvana Ramabhadran",
        "Fadi Biadsy",
        "Yinghui Huang",
        "Jesse Emond",
        "Pedro Moreno Mengibar"
      ],
      "published": "2022-09-15T00:40:35Z",
      "updated": "2022-09-15T00:40:35Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.06987v1",
      "landing_url": "https://arxiv.org/abs/2209.06987v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.06987"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於用非平行聲碼轉換作為 ASR 資料增強，未討論離散音訊 token 的產生、碼本/字彙表設定，且把焦點放在連續聲音表示與轉換，故不符合離散音訊 token 相關納入條件，屬於排除範圍。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於用非平行聲碼轉換作為 ASR 資料增強，未討論離散音訊 token 的產生、碼本/字彙表設定，且把焦點放在連續聲音表示與轉換，故不符合離散音訊 token 相關納入條件，屬於排除範圍。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion for ASR data augmentation and does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria. The abstract lacks details on discrete token generation processes, vocabularies, or token-based modeling approaches, and primarily addresses continuous speech features in ASR augmentation rather than discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion for ASR data augmentation and does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria. The abstract lacks details on discrete token generation processes, vocabularies, or token-based modeling approaches, and primarily addresses continuous speech features in ASR augmentation rather than discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator",
    "abstract": "Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating high-fidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high-resolution video prediction on complex and large-scale datasets. Videos are available at https://sites.google.com/view/harp-videos/home.",
    "metadata": {
      "arxiv_id": "2209.07143",
      "title": "HARP: Autoregressive Latent Video Prediction with High-Fidelity Image Generator",
      "summary": "Video prediction is an important yet challenging problem; burdened with the tasks of generating future frames and learning environment dynamics. Recently, autoregressive latent video models have proved to be a powerful video prediction tool, by separating the video prediction into two sub-problems: pre-training an image generator model, followed by learning an autoregressive prediction model in the latent space of the image generator. However, successfully generating high-fidelity and high-resolution videos has yet to be seen. In this work, we investigate how to train an autoregressive latent video prediction model capable of predicting high-fidelity future frames with minimal modification to existing models, and produce high-resolution (256x256) videos. Specifically, we scale up prior models by employing a high-fidelity image generator (VQ-GAN) with a causal transformer model, and introduce additional techniques of top-k sampling and data augmentation to further improve video prediction quality. Despite the simplicity, the proposed method achieves competitive performance to state-of-the-art approaches on standard video prediction benchmarks with fewer parameters, and enables high-resolution video prediction on complex and large-scale datasets. Videos are available at https://sites.google.com/view/harp-videos/home.",
      "authors": [
        "Younggyo Seo",
        "Kimin Lee",
        "Fangchen Liu",
        "Stephen James",
        "Pieter Abbeel"
      ],
      "published": "2022-09-15T08:41:57Z",
      "updated": "2022-09-15T08:41:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.07143v1",
      "landing_url": "https://arxiv.org/abs/2209.07143v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.07143"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on autoregressive latent video prediction using a high-fidelity image generator (VQ-GAN) and does not involve discrete audio tokens, tokenizers, or audio-centric discrete representations, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on autoregressive latent video prediction using a high-fidelity image generator (VQ-GAN) and does not involve discrete audio tokens, tokenizers, or audio-centric discrete representations, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video prediction using autoregressive latent models with a high-fidelity image generator and does not involve discretization of audio waveforms into token sequences, nor does it discuss discrete audio tokens or neural audio codecs in the context of audio generation or understanding, thus failing to meet inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video prediction using autoregressive latent models with a high-fidelity image generator and does not involve discretization of audio waveforms into token sequences, nor does it discuss discrete audio tokens or neural audio codecs in the context of audio generation or understanding, thus failing to meet inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Relaxed Attention for Transformer Models",
    "abstract": "The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models. In this paper, we explore relaxed attention, a simple and easy-to-implement smoothing of the attention weights, yielding a two-fold improvement to the general transformer architecture: First, relaxed attention provides regularization when applied to the self-attention layers in the encoder. Second, we show that it naturally supports the integration of an external language model as it suppresses the implicitly learned internal language model by relaxing the cross attention in the decoder. We demonstrate the benefit of relaxed attention across several tasks with clear improvement in combination with recent benchmark approaches. Specifically, we exceed the former state-of-the-art performance of 26.90% word error rate on the largest public lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as we achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE$\\rightarrow$EN) machine translation task without external language models and virtually no additional model parameters. Code and models will be made publicly available.",
    "metadata": {
      "arxiv_id": "2209.09735",
      "title": "Relaxed Attention for Transformer Models",
      "summary": "The powerful modeling capabilities of all-attention-based transformer architectures often cause overfitting and - for natural language processing tasks - lead to an implicitly learned internal language model in the autoregressive transformer decoder complicating the integration of external language models. In this paper, we explore relaxed attention, a simple and easy-to-implement smoothing of the attention weights, yielding a two-fold improvement to the general transformer architecture: First, relaxed attention provides regularization when applied to the self-attention layers in the encoder. Second, we show that it naturally supports the integration of an external language model as it suppresses the implicitly learned internal language model by relaxing the cross attention in the decoder. We demonstrate the benefit of relaxed attention across several tasks with clear improvement in combination with recent benchmark approaches. Specifically, we exceed the former state-of-the-art performance of 26.90% word error rate on the largest public lip-reading LRS3 benchmark with a word error rate of 26.31%, as well as we achieve a top-performing BLEU score of 37.67 on the IWSLT14 (DE$\\rightarrow$EN) machine translation task without external language models and virtually no additional model parameters. Code and models will be made publicly available.",
      "authors": [
        "Timo Lohrenz",
        "Björn Möller",
        "Zhengyang Li",
        "Tim Fingscheidt"
      ],
      "published": "2022-09-20T14:10:28Z",
      "updated": "2022-09-20T14:10:28Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09735v1",
      "landing_url": "https://arxiv.org/abs/2209.09735v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.09735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on relaxed attention in transformer models for lip-reading and translation, but it does not describe discrete audio token generation, tokenizers/codebooks, or token-based modeling, so it fails to meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on relaxed attention in transformer models for lip-reading and translation, but it does not describe discrete audio token generation, tokenizers/codebooks, or token-based modeling, so it fails to meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Transformer model improvements related to attention mechanisms but does not describe the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or self-supervised models, nor does it address token design, discretization, or related evaluation; thus, it does not meet the inclusion criteria centered on discrete audio tokens and focuses primarily on continuous signal modeling within transformer architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Transformer model improvements related to attention mechanisms but does not describe the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or self-supervised models, nor does it address token design, discretization, or related evaluation; thus, it does not meet the inclusion criteria centered on discrete audio tokens and focuses primarily on continuous signal modeling within transformer architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
    "abstract": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.",
    "metadata": {
      "arxiv_id": "2209.10887",
      "title": "A Multi-Stage Multi-Codebook VQ-VAE Approach to High-Performance Neural TTS",
      "summary": "We propose a Multi-Stage, Multi-Codebook (MSMC) approach to high-performance neural TTS synthesis. A vector-quantized, variational autoencoder (VQ-VAE) based feature analyzer is used to encode Mel spectrograms of speech training data by down-sampling progressively in multiple stages into MSMC Representations (MSMCRs) with different time resolutions, and quantizing them with multiple VQ codebooks, respectively. Multi-stage predictors are trained to map the input text sequence to MSMCRs progressively by minimizing a combined loss of the reconstruction Mean Square Error (MSE) and \"triplet loss\". In synthesis, the neural vocoder converts the predicted MSMCRs into final speech waveforms. The proposed approach is trained and tested with an English TTS database of 16 hours by a female speaker. The proposed TTS achieves an MOS score of 4.41, which outperforms the baseline with an MOS of 3.62. Compact versions of the proposed TTS with much less parameters can still preserve high MOS scores. Ablation studies show that both multiple stages and multiple codebooks are effective for achieving high TTS performance.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Frank K. Soong",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2022-09-22T09:43:17Z",
      "updated": "2022-09-22T09:43:17Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.10887v1",
      "landing_url": "https://arxiv.org/abs/2209.10887v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.10887"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work explicitly employs discrete audio tokens via a vector-quantized, multi-codebook VQ-VAE to encode audio into MSMCRs and uses these tokens as targets for downstream TTS modeling, addressing token design and their impact on synthesis—directly matching the discrete audio token criteria and presenting English abstract content.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work explicitly employs discrete audio tokens via a vector-quantized, multi-codebook VQ-VAE to encode audio into MSMCRs and uses these tokens as targets for downstream TTS modeling, addressing token design and their impact on synthesis—directly matching the discrete audio token criteria and presenting English abstract content.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a neural TTS approach using a Multi-Stage Multi-Codebook VQ-VAE that produces quantized MSMC Representations via multiple VQ codebooks, indicating the usage of discrete audio tokens generated by a neural codec with multiple codebooks and down-sampling; the study focuses on TTS generation, provides English abstract and details on tokenizer/codebook settings, fitting the inclusion criteria well and none of the exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a neural TTS approach using a Multi-Stage Multi-Codebook VQ-VAE that produces quantized MSMC Representations via multiple VQ codebooks, indicating the usage of discrete audio tokens generated by a neural codec with multiple codebooks and down-sampling; the study focuses on TTS generation, provides English abstract and details on tokenizer/codebook settings, fitting the inclusion criteria well and none of the exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
    "abstract": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
    "metadata": {
      "arxiv_id": "2209.10890",
      "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
      "summary": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
      "authors": [
        "Perry Lam",
        "Huayun Zhang",
        "Nancy F. Chen",
        "Berrak Sisman"
      ],
      "published": "2022-09-22T09:47:25Z",
      "updated": "2022-09-22T09:47:25Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.10890v1",
      "landing_url": "https://arxiv.org/abs/2209.10890v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10626"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses pruning methods and sparsity in TTS models (Tacotron2 baseline) and evaluates naturalness, intelligibility, prosody, and training time; it does not mention discrete audio tokens, tokenizers, or token-based representations, so it does not meet the Discrete Audio Tokens inclusion criteria and may be excluded per the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses pruning methods and sparsity in TTS models (Tacotron2 baseline) and evaluates naturalness, intelligibility, prosody, and training time; it does not mention discrete audio tokens, tokenizers, or token-based representations, so it does not meet the Discrete Audio Tokens inclusion criteria and may be excluded per the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on pruning techniques for text-to-speech (TTS) models and evaluates model sparsity and performance but does not discuss or involve any form of discrete audio tokenization, discretization, or neural audio codecs for token generation, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to the use or analysis of discrete audio tokens, nor does it describe a discrete token generation process or evaluate such tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on pruning techniques for text-to-speech (TTS) models and evaluates model sparsity and performance but does not discuss or involve any form of discrete audio tokenization, discretization, or neural audio codecs for token generation, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to the use or analysis of discrete audio tokens, nor does it describe a discrete token generation process or evaluate such tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
    "abstract": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
    "metadata": {
      "arxiv_id": "2209.11750",
      "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
      "summary": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
      "authors": [
        "Sannara EK",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-09-22T09:42:08Z",
      "updated": "2025-08-23T20:07:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11750v2",
      "landing_url": "https://arxiv.org/abs/2209.11750v2",
      "doi": "https://doi.org/10.1007/s00779-023-01776-3"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on Transformer-based HAR under heterogeneous environments and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on Transformer-based HAR under heterogeneous environments and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Human Activity Recognition using Transformer models on sensor data from inertial measurement units, which is unrelated to discrete audio token representations or their application in audio generation, understanding, or cross-modal modeling. It does not mention discrete tokenization of audio signals, neural audio codecs, or any quantization mechanisms relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Human Activity Recognition using Transformer models on sensor data from inertial measurement units, which is unrelated to discrete audio token representations or their application in audio generation, understanding, or cross-modal modeling. It does not mention discrete tokenization of audio signals, neural audio codecs, or any quantization mechanisms relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
    "abstract": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
    "metadata": {
      "arxiv_id": "2209.12139",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "authors": [
        "Yifan Wang",
        "Zhanxuan Mei",
        "Ioannis Katsavounidis",
        "C. -C. Jay Kuo"
      ],
      "published": "2022-09-25T04:14:26Z",
      "updated": "2022-09-25T04:14:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.12139v1",
      "landing_url": "https://arxiv.org/abs/2209.12139v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.12139"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a lightweight image codec using multi-grid multi-block-size vector quantization for images; there is no discussion of discrete audio tokens, tokenization methods, or audio-domain modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a lightweight image codec using multi-grid multi-block-size vector quantization for images; there is no discussion of discrete audio tokens, tokenization methods, or audio-domain modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image coding using multi-grid multi-block-size vector quantization, aiming to compress images efficiently, but does not touch upon discrete audio tokens, audio quantization, or audio generation tasks, nor does it discuss neural audio codecs or discrete token sequences relevant to audio signals. Hence, it fails to meet the inclusion criteria, which are specific to discrete audio tokens and their applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image coding using multi-grid multi-block-size vector quantization, aiming to compress images efficiently, but does not touch upon discrete audio tokens, audio quantization, or audio generation tasks, nor does it discuss neural audio codecs or discrete token sequences relevant to audio signals. Hence, it fails to meet the inclusion criteria, which are specific to discrete audio tokens and their applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dilated Neighborhood Attention Transformer",
    "abstract": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
    "metadata": {
      "arxiv_id": "2209.15001",
      "title": "Dilated Neighborhood Attention Transformer",
      "summary": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
      "authors": [
        "Ali Hassani",
        "Humphrey Shi"
      ],
      "published": "2022-09-29T17:57:08Z",
      "updated": "2023-01-16T18:58:58Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15001v3",
      "landing_url": "https://arxiv.org/abs/2209.15001v3",
      "doi": "https://doi.org/10.48550/arXiv.2209.15001"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses Dilated Neighborhood Attention Transformer for vision tasks, with no discussion of discrete audio tokens, tokenizers, or audio codecs, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses Dilated Neighborhood Attention Transformer for vision tasks, with no discussion of discrete audio tokens, tokenizers, or audio codecs, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improvements in vision transformer architectures and their applications in visual tasks such as object detection and segmentation, without any mention of discrete audio tokens, neural audio codecs, or audio-related modeling; thus, it does not meet the inclusion criteria related to discrete audio token representation or audio sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improvements in vision transformer architectures and their applications in visual tasks such as object detection and segmentation, without any mention of discrete audio tokens, neural audio codecs, or audio-related modeling; thus, it does not meet the inclusion criteria related to discrete audio token representation or audio sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
    "abstract": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
    "metadata": {
      "arxiv_id": "2209.15472",
      "title": "Binaural Speech Enhancement Using STOI-Optimal Masks",
      "summary": "STOI-optimal masking has been previously proposed and developed for single-channel speech enhancement. In this paper, we consider the extension to the task of binaural speech enhancement in which spatial information is known to be important to speech understanding and therefore should be preserved by the enhancement processing. Masks are estimated for each of the binaural channels individually and a `better-ear listening' mask is computed by choosing the maximum of the two masks. The estimated mask is used to supply probability information about the speech presence in each time-frequency bin to an Optimally-modified Log Spectral Amplitude (OM-LSA) enhancer. We show that using the proposed method for binaural signals with a directional noise not only improves the SNR of the noisy signal but also preserves the binaural cues and intelligibility.",
      "authors": [
        "Vikas Tokala",
        "Mike Brookes",
        "Patrick A. Naylor"
      ],
      "published": "2022-09-30T13:56:25Z",
      "updated": "2022-09-30T13:56:25Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15472v1",
      "landing_url": "https://arxiv.org/abs/2209.15472v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.15472"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses binaural speech enhancement and STOI-optimal masks without any discrete audio tokenization or token/codec design; thus it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses binaural speech enhancement and STOI-optimal masks without any discrete audio tokenization or token/codec design; thus it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks and describes a method for improving signal-to-noise ratio and preserving binaural cues; however, it does not discuss or involve discrete audio tokenization, discretization processes, neural audio codecs, or any representation of audio as discrete token sequences, which are the core inclusion criteria. Therefore, it does not meet the inclusion criteria and aligns with the exclusion criteria regarding studies that focus on continuous audio representations and enhancement rather than discrete token modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on binaural speech enhancement using STOI-optimal masks and describes a method for improving signal-to-noise ratio and preserving binaural cues; however, it does not discuss or involve discrete audio tokenization, discretization processes, neural audio codecs, or any representation of audio as discrete token sequences, which are the core inclusion criteria. Therefore, it does not meet the inclusion criteria and aligns with the exclusion criteria regarding studies that focus on continuous audio representations and enhancement rather than discrete token modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
    "abstract": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
    "metadata": {
      "arxiv_id": "2210.01765",
      "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
      "summary": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
      "authors": [
        "Shengjie Luo",
        "Tianlang Chen",
        "Yixian Xu",
        "Shuxin Zheng",
        "Tie-Yan Liu",
        "Liwei Wang",
        "Di He"
      ],
      "published": "2022-10-04T17:30:31Z",
      "updated": "2023-03-28T03:01:29Z",
      "categories": [
        "cs.LG",
        "q-bio.BM",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.01765v4",
      "landing_url": "https://arxiv.org/abs/2210.01765v4",
      "doi": "https://doi.org/10.48550/arXiv.2210.01765"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a Transformer model for 2D/3D molecular data without any discussion of discrete audio tokens, tokenization, or audio-related downstream tasks; it does not meet the inclusion criteria and clearly aligns with the exclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents a Transformer model for 2D/3D molecular data without any discussion of discrete audio tokens, tokenization, or audio-related downstream tasks; it does not meet the inclusion criteria and clearly aligns with the exclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Transformer model for molecular data representation learning, specifically handling 2D and 3D molecular structures, with no relation to discrete audio tokens or audio waveform discretization into token sequences. It does not address neural audio codecs, token vocabularies, or any audio generation or alignment tasks, and thus does not meet the inclusion criteria centered on discrete audio tokens and their applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Transformer model for molecular data representation learning, specifically handling 2D and 3D molecular structures, with no relation to discrete audio tokens or audio waveform discretization into token sequences. It does not address neural audio codecs, token vocabularies, or any audio generation or alignment tasks, and thus does not meet the inclusion criteria centered on discrete audio tokens and their applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "abstract": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
    "metadata": {
      "arxiv_id": "2210.04062",
      "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
      "summary": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
      "authors": [
        "Chutong Meng",
        "Junyi Ao",
        "Tom Ko",
        "Mingxuan Wang",
        "Haizhou Li"
      ],
      "published": "2022-10-08T17:15:46Z",
      "updated": "2023-07-05T16:30:48Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.04062v3",
      "landing_url": "https://arxiv.org/abs/2210.04062v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.04062"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper converts speech into a sequence of discrete codes via self-supervised learning (Code BERT) and uses these discrete tokens for modeling and downstream tasks, which fits the discrete audio token criteria and reports downstream benefits.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper converts speech into a sequence of discrete codes via self-supervised learning (Code BERT) and uses these discrete tokens for modeling and downstream tasks, which fits the discrete audio token criteria and reports downstream benefits.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract clearly states that the study involves converting speech into discrete codes (tokens) and performing code representation learning, which aligns with the definition of discrete audio tokens. It uses self-supervised learning to generate these tokens from speech and applies them for downstream tasks like ASR and speech translation, with code and models released publicly, indicating reproducibility. The study thus meets the inclusion criteria regarding token generation, usage for modeling, and evaluation, with a detailed English abstract and technical information. No exclusion criteria are met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract clearly states that the study involves converting speech into discrete codes (tokens) and performing code representation learning, which aligns with the definition of discrete audio tokens. It uses self-supervised learning to generate these tokens from speech and applies them for downstream tasks like ASR and speech translation, with code and models released publicly, indicating reproducibility. The study thus meets the inclusion criteria regarding token generation, usage for modeling, and evaluation, with a detailed English abstract and technical information. No exclusion criteria are met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding",
    "abstract": "In this paper we examine the use of semantically-aligned speech representations for end-to-end spoken language understanding (SLU). We employ the recently-introduced SAMU-XLSR model, which is designed to generate a single embedding that captures the semantics at the utterance level, semantically aligned across different languages. This model combines the acoustic frame-level speech representation learning model (XLS-R) with the Language Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the SAMU-XLSR model instead of the initial XLS-R model improves significantly the performance in the framework of end-to-end SLU. Finally, we present the benefits of using this model towards language portability in SLU.",
    "metadata": {
      "arxiv_id": "2210.05291",
      "title": "On the Use of Semantically-Aligned Speech Representations for Spoken Language Understanding",
      "summary": "In this paper we examine the use of semantically-aligned speech representations for end-to-end spoken language understanding (SLU). We employ the recently-introduced SAMU-XLSR model, which is designed to generate a single embedding that captures the semantics at the utterance level, semantically aligned across different languages. This model combines the acoustic frame-level speech representation learning model (XLS-R) with the Language Agnostic BERT Sentence Embedding (LaBSE) model. We show that the use of the SAMU-XLSR model instead of the initial XLS-R model improves significantly the performance in the framework of end-to-end SLU. Finally, we present the benefits of using this model towards language portability in SLU.",
      "authors": [
        "Gaëlle Laperrière",
        "Valentin Pelloin",
        "Mickaël Rouvier",
        "Themos Stafylakis",
        "Yannick Estève"
      ],
      "published": "2022-10-11T09:40:34Z",
      "updated": "2022-10-11T09:40:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.05291v1",
      "landing_url": "https://arxiv.org/abs/2210.05291v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.05291"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on semantically-aligned continuous speech embeddings for end-to-end SLU and does not describe discrete audio token generation, tokenizers, or neural audio codecs; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on semantically-aligned continuous speech embeddings for end-to-end SLU and does not describe discrete audio token generation, tokenizers, or neural audio codecs; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semantically-aligned speech representations for spoken language understanding, using models like SAMU-XLSR and XLS-R to improve end-to-end SLU performance; however, there is no explicit mention of discrete audio tokens, tokenization through vector quantization, or a discretization process mapping continuous audio waveforms into discrete token sequences. Therefore, it does not meet the key inclusion criteria related to the discrete audio token framework, nor does it provide details of tokenizers, codebooks, or discrete token generation mechanisms required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semantically-aligned speech representations for spoken language understanding, using models like SAMU-XLSR and XLS-R to improve end-to-end SLU performance; however, there is no explicit mention of discrete audio tokens, tokenization through vector quantization, or a discretization process mapping continuous audio waveforms into discrete token sequences. Therefore, it does not meet the key inclusion criteria related to the discrete audio token framework, nor does it provide details of tokenizers, codebooks, or discrete token generation mechanisms required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
    "abstract": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
    "metadata": {
      "arxiv_id": "2210.06007",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "authors": [
        "Yueh-Kao Wu",
        "Ching-Yu Chiu",
        "Yi-Hsuan Yang"
      ],
      "published": "2022-10-12T08:23:20Z",
      "updated": "2022-10-31T08:54:08Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06007v2",
      "landing_url": "https://arxiv.org/abs/2210.06007v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06007"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The paper uses a neural audio codec (VQ-VAE) to obtain discrete tokens for input and output drum tracks and employs a Transformer to model the token sequence for generation, aligning with the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper uses a neural audio codec (VQ-VAE) to obtain discrete tokens for input and output drum tracks and employs a Transformer to model the token sequence for generation, aligning with the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a model using vector-quantized variational autoencoders (VQ-VAE) to encode audio as discrete codes, which fit the definition of discrete audio tokens derived by neural audio codecs; it further utilizes a Transformer on these tokens for audio generation, meeting the inclusion criteria of research on discrete audio tokens for audio generation and providing clear methodology in English. No exclusion criteria are met as the study focuses on generation from discrete tokens, not just continuous audio or discriminative tasks, and sufficient English detail is provided.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a model using vector-quantized variational autoencoders (VQ-VAE) to encode audio as discrete codes, which fit the definition of discrete audio tokens derived by neural audio codecs; it further utilizes a Transformer on these tokens for audio generation, meeting the inclusion criteria of research on discrete audio tokens for audio generation and providing clear methodology in English. No exclusion criteria are met as the study focuses on generation from discrete tokens, not just continuous audio or discriminative tasks, and sufficient English detail is provided.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
    "abstract": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
    "metadata": {
      "arxiv_id": "2210.07323",
      "title": "Experiments on Turkish ASR with Self-Supervised Speech Representation Learning",
      "summary": "While the Turkish language is listed among low-resource languages, literature on Turkish automatic speech recognition (ASR) is relatively old. In this report, we present our findings on Turkish ASR with speech representation learning using HUBERT. We investigate pre-training HUBERT for Turkish with large-scale data curated from online resources. We pre-train our model using 6,500 hours of speech data from YouTube. The results show that the models are not ready for commercial use since they are not robust against disturbances that typically occur in real-world settings such as variations in accents, slang, background noise and interference. We analyze typical errors and the limitations of the models for use in commercial settings.",
      "authors": [
        "Ali Safaya",
        "Engin Erzin"
      ],
      "published": "2022-10-13T19:46:39Z",
      "updated": "2022-12-23T11:11:03Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.07323v3",
      "landing_url": "https://arxiv.org/abs/2210.07323v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.07323"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on Turkish ASR with HuBERT self-supervised representations and data scaling, but provides no details or emphasis on discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on Turkish ASR with HuBERT self-supervised representations and data scaling, but provides no details or emphasis on discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Turkish ASR with self-supervised learning using HUBERT, but it does not explicitly describe the generation or use of discrete audio tokens via quantization or clustering to form a finite vocabulary for downstream modeling; moreover, the abstract lacks details on tokenization mechanisms or design choices affecting tasks like generation or alignment, making it unclear if discrete token representations as defined in the inclusion criteria are central to this work.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Turkish ASR with self-supervised learning using HUBERT, but it does not explicitly describe the generation or use of discrete audio tokens via quantization or clustering to form a finite vocabulary for downstream modeling; moreover, the abstract lacks details on tokenization mechanisms or design choices affecting tasks like generation or alignment, making it unclear if discrete token representations as defined in the inclusion criteria are central to this work.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning",
    "abstract": "We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss the main findings from those submissions and the future directions of SSL research.",
    "metadata": {
      "arxiv_id": "2210.08634",
      "title": "SUPERB @ SLT 2022: Challenge on Generalization and Efficiency of Self-Supervised Speech Representation Learning",
      "summary": "We present the SUPERB challenge at SLT 2022, which aims at learning self-supervised speech representation for better performance, generalization, and efficiency. The challenge builds upon the SUPERB benchmark and implements metrics to measure the computation requirements of self-supervised learning (SSL) representation and to evaluate its generalizability and performance across the diverse SUPERB tasks. The SUPERB benchmark provides comprehensive coverage of popular speech processing tasks, from speech and speaker recognition to audio generation and semantic understanding. As SSL has gained interest in the speech community and showed promising outcomes, we envision the challenge to uplevel the impact of SSL techniques by motivating more practical designs of techniques beyond task performance. We summarize the results of 14 submitted models in this paper. We also discuss the main findings from those submissions and the future directions of SSL research.",
      "authors": [
        "Tzu-hsun Feng",
        "Annie Dong",
        "Ching-Feng Yeh",
        "Shu-wen Yang",
        "Tzu-Quan Lin",
        "Jiatong Shi",
        "Kai-Wei Chang",
        "Zili Huang",
        "Haibin Wu",
        "Xuankai Chang",
        "Shinji Watanabe",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Hung-yi Lee"
      ],
      "published": "2022-10-16T20:50:04Z",
      "updated": "2022-10-29T14:00:14Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.08634v2",
      "landing_url": "https://arxiv.org/abs/2210.08634v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.08634"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a self-supervised speech representation challenge and benchmarks, not on discrete audio tokens or tokenization; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a self-supervised speech representation challenge and benchmarks, not on discrete audio tokens or tokenization; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper summarizes the SUPERB challenge on self-supervised speech representation learning, focusing on generalization, performance, and efficiency across speech tasks. However, it does not explicitly discuss or provide details about discrete audio tokenization (e.g., tokenizer/codec architecture, quantization methods, codebook configurations) which is a core inclusion requirement. The study primarily evaluates SSL models at an aggregate level without detailing the generation or use of discrete tokens as core representations for downstream tasks. Therefore, it does not meet the strict criteria for inclusion regarding discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper summarizes the SUPERB challenge on self-supervised speech representation learning, focusing on generalization, performance, and efficiency across speech tasks. However, it does not explicitly discuss or provide details about discrete audio tokenization (e.g., tokenizer/codec architecture, quantization methods, codebook configurations) which is a core inclusion requirement. The study primarily evaluates SSL models at an aggregate level without detailing the generation or use of discrete tokens as core representations for downstream tasks. Therefore, it does not meet the strict criteria for inclusion regarding discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
    "abstract": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
    "metadata": {
      "arxiv_id": "2210.12995",
      "title": "TridentSE: Guiding Speech Enhancement with 32 Global Tokens",
      "summary": "In this paper, we present TridentSE, a novel architecture for speech enhancement, which is capable of efficiently capturing both global information and local details. TridentSE maintains T-F bin level representation to capture details, and uses a small number of global tokens to process the global information. Information is propagated between the local and the global representations through cross attention modules. To capture both inter- and intra-frame information, the global tokens are divided into two groups to process along the time and the frequency axis respectively. A metric discriminator is further employed to guide our model to achieve higher perceptual quality. Even with significantly lower computational cost, TridentSE outperforms a variety of previous speech enhancement methods, achieving a PESQ of 3.47 on VoiceBank+DEMAND dataset and a PESQ of 3.44 on DNS no-reverb test set. Visualization shows that the global tokens learn diverse and interpretable global patterns.",
      "authors": [
        "Dacheng Yin",
        "Zhiyuan Zhao",
        "Chuanxin Tang",
        "Zhiwei Xiong",
        "Chong Luo"
      ],
      "published": "2022-10-24T07:30:42Z",
      "updated": "2022-10-24T07:30:42Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12995v1",
      "landing_url": "https://arxiv.org/abs/2210.12995v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.12995"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes TridentSE with global tokens for speech enhancement but does not discuss discrete audio tokens, tokenizers/quantization, or token-based modeling as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes TridentSE with global tokens for speech enhancement but does not discuss discrete audio tokens, tokenizers/quantization, or token-based modeling as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a novel architecture for speech enhancement using global tokens and local details; however, it does not describe discretization of continuous audio waveforms into discrete token sequences, nor does it provide details on tokenizer/codec architecture, vocabulary, or quantization; thus, it does not meet the core inclusion criteria regarding discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a novel architecture for speech enhancement using global tokens and local details; however, it does not describe discretization of continuous audio waveforms into discrete token sequences, nor does it provide details on tokenizer/codec architecture, vocabulary, or quantization; thus, it does not meet the core inclusion criteria regarding discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
    "abstract": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
    "metadata": {
      "arxiv_id": "2210.13771",
      "title": "Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using $β$-VAE",
      "summary": "We propose an unsupervised learning method to disentangle speech into content representation and speaker identity representation. We apply this method to the challenging one-shot cross-lingual voice conversion task to demonstrate the effectiveness of the disentanglement. Inspired by $β$-VAE, we introduce a learning objective that balances between the information captured by the content and speaker representations. In addition, the inductive biases from the architectural design and the training dataset further encourage the desired disentanglement. Both objective and subjective evaluations show the effectiveness of the proposed method in speech disentanglement and in one-shot cross-lingual voice conversion.",
      "authors": [
        "Hui Lu",
        "Disong Wang",
        "Xixin Wu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "published": "2022-10-25T05:12:47Z",
      "updated": "2022-10-25T05:12:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13771v1",
      "landing_url": "https://arxiv.org/abs/2210.13771v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13771"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on disentangling continuous speech representations (content vs. speaker) using β-VAE for one-shot cross-lingual voice conversion and does not describe discrete audio tokens, tokenizers, or any discretization/quantization setup, so it fails the ‘Discrete Audio Tokens’ inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on disentangling continuous speech representations (content vs. speaker) using β-VAE for one-shot cross-lingual voice conversion and does not describe discrete audio tokens, tokenizers, or any discretization/quantization setup, so it fails the ‘Discrete Audio Tokens’ inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on disentangled speech representation and one-shot cross-lingual voice conversion using $\\beta$-VAE, but the abstract does not mention the use or generation of discrete audio tokens, quantization, or tokenization processes; it centers on continuous representation disentanglement rather than discrete token mapping, thus it does not meet the inclusion criteria requiring explicit discrete token generation and related details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on disentangled speech representation and one-shot cross-lingual voice conversion using $\\beta$-VAE, but the abstract does not mention the use or generation of discrete audio tokens, quantization, or tokenization processes; it centers on continuous representation disentanglement rather than discrete token mapping, thus it does not meet the inclusion criteria requiring explicit discrete token generation and related details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
    "abstract": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
    "metadata": {
      "arxiv_id": "2210.13805",
      "title": "Improving Speech Representation Learning via Speech-level and Phoneme-level Masking Approach",
      "summary": "Recovering the masked speech frames is widely applied in speech representation learning. However, most of these models use random masking in the pre-training. In this work, we proposed two kinds of masking approaches: (1) speech-level masking, making the model to mask more speech segments than silence segments, (2) phoneme-level masking, forcing the model to mask the whole frames of the phoneme, instead of phoneme pieces. We pre-trained the model via these two approaches, and evaluated on two downstream tasks, phoneme classification and speaker recognition. The experiments demonstrated that the proposed masking approaches are beneficial to improve the performance of speech representation.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Kexin Zhu",
        "Jing Xiao"
      ],
      "published": "2022-10-25T07:26:47Z",
      "updated": "2022-10-25T07:26:47Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.13805v1",
      "landing_url": "https://arxiv.org/abs/2210.13805v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.13805"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on masking strategies for continuous speech representations and does not define or utilize discrete audio tokens (no tokenizer/codec, codebook, or quantization details), failing the core criterion for discrete audio tokens; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on masking strategies for continuous speech representations and does not define or utilize discrete audio tokens (no tokenizer/codec, codebook, or quantization details), failing the core criterion for discrete audio tokens; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes novel masking approaches to improve speech representation learning but does not mention the discretization of continuous audio waveforms into discrete token sequences or the use of neural audio codecs/vector quantization for token generation, which are central to the inclusion criteria. The abstract focuses on continuous speech frame masking rather than discrete token design, generation, or usage in downstream tasks, making it unclear if discrete audio tokens are involved.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes novel masking approaches to improve speech representation learning but does not mention the discretization of continuous audio waveforms into discrete token sequences or the use of neural audio codecs/vector quantization for token generation, which are central to the inclusion criteria. The abstract focuses on continuous speech frame masking rather than discrete token design, generation, or usage in downstream tasks, making it unclear if discrete audio tokens are involved.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations",
    "abstract": "This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.",
    "metadata": {
      "arxiv_id": "2210.15131",
      "title": "Towards High-Quality Neural TTS for Low-Resource Languages by Learning Compact Speech Representations",
      "summary": "This paper aims to enhance low-resource TTS by reducing training data requirements using compact speech representations. A Multi-Stage Multi-Codebook (MSMC) VQ-GAN is trained to learn the representation, MSMCR, and decode it to waveforms. Subsequently, we train the multi-stage predictor to predict MSMCRs from the text for TTS synthesis. Moreover, we optimize the training strategy by leveraging more audio to learn MSMCRs better for low-resource languages. It selects audio from other languages using speaker similarity metric to augment the training set, and applies transfer learning to improve training quality. In MOS tests, the proposed system significantly outperforms FastSpeech and VITS in standard and low-resource scenarios, showing lower data requirements. The proposed training strategy effectively enhances MSMCRs on waveform reconstruction. It improves TTS performance further, which wins 77% votes in the preference test for the low-resource TTS with only 15 minutes of paired data.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Xixin Wu",
        "Hui Lu",
        "Helen Meng"
      ],
      "published": "2022-10-27T02:32:00Z",
      "updated": "2022-10-27T02:32:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15131v1",
      "landing_url": "https://arxiv.org/abs/2210.15131v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper employs discrete audio tokens via a multi-codebook vector-quantized neural codec (MSMC VQ-GAN) to learn compact speech representations used for TTS, with English title/abstract and explicit discussion of token generation/settings, fitting the discrete audio token inclusion criteria and not triggering exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper employs discrete audio tokens via a multi-codebook vector-quantized neural codec (MSMC VQ-GAN) to learn compact speech representations used for TTS, with English title/abstract and explicit discussion of token generation/settings, fitting the discrete audio token inclusion criteria and not triggering exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes training a Multi-Stage Multi-Codebook (MSMC) VQ-GAN to learn compact speech representations (MSMCR), which are then used as discrete tokens for TTS, fulfilling the inclusion focus on neural audio codecs producing discrete tokens with multi-codebook design; it also presents experimental results validating this approach for low-resource languages, meeting criteria for including empirical evaluations and clearly described tokenization methods, with no indication of exclusion criteria such as continuous-only representations or lack of token detail.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper describes training a Multi-Stage Multi-Codebook (MSMC) VQ-GAN to learn compact speech representations (MSMCR), which are then used as discrete tokens for TTS, fulfilling the inclusion focus on neural audio codecs producing discrete tokens with multi-codebook design; it also presents experimental results validating this approach for low-resource languages, meeting criteria for including empirical evaluations and clearly described tokenization methods, with no indication of exclusion criteria such as continuous-only representations or lack of token detail.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
    "abstract": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
    "metadata": {
      "arxiv_id": "2210.16611",
      "title": "Application of Knowledge Distillation to Multi-task Speech Representation Learning",
      "summary": "Model architectures such as wav2vec 2.0 and HuBERT have been proposed to learn speech representations from audio waveforms in a self-supervised manner. When they are combined with downstream tasks such as keyword spotting and speaker verification, they provide state-of-the-art performance. However, these models use a large number of parameters, the smallest version of which has 95 million parameters. This constitutes a challenge for edge AI device deployments. In this paper, we investigate the application of knowledge distillation to speech representation learning (SRL) models followed by joint fine-tuning with multiple downstream voice-activated tasks. In our experiments on two such tasks, our approach results in nearly 75% reduction in model size while suffering only 0.1% accuracy and 0.9% equal error rate degradation compared to the full-size model. In addition, we show that fine-tuning the SRL models results in a significant performance boost compared to using frozen SRL models.",
      "authors": [
        "Mine Kerpicci",
        "Van Nguyen",
        "Shuhua Zhang",
        "Erik Visser"
      ],
      "published": "2022-10-29T14:22:43Z",
      "updated": "2023-05-19T17:16:53Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16611v2",
      "landing_url": "https://arxiv.org/abs/2210.16611v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.16611"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on applying knowledge distillation to speech representation learning and multi-task fine-tuning with an emphasis on model compression, and does not discuss discrete audio tokens, tokenizers, or token-based representations as core methods or analyses.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on applying knowledge distillation to speech representation learning and multi-task fine-tuning with an emphasis on model compression, and does not discuss discrete audio tokens, tokenizers, or token-based representations as core methods or analyses.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on knowledge distillation applied to multi-task speech representation learning models like wav2vec 2.0 and HuBERT, which are continuous, large-parameter self-supervised models aiming to reduce size for edge deployment; however, it does not explicitly mention discrete audio tokens, discrete token generation processes, codebooks, quantization, or token vocabularies, nor does it appear to use discrete tokens as core representations or for downstream generative or alignment tasks. Therefore, the paper primarily addresses continuous speech representations without meeting the discrete token-related inclusion criteria, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on knowledge distillation applied to multi-task speech representation learning models like wav2vec 2.0 and HuBERT, which are continuous, large-parameter self-supervised models aiming to reduce size for edge deployment; however, it does not explicitly mention discrete audio tokens, discrete token generation processes, codebooks, quantization, or token vocabularies, nor does it appear to use discrete tokens as core representations or for downstream generative or alignment tasks. Therefore, the paper primarily addresses continuous speech representations without meeting the discrete token-related inclusion criteria, and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
    "abstract": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discretizes speech into discrete tokens and uses token-level masking in a joint speech-text pre-training framework, directly addressing discrete audio tokens and tokenizers, with English abstract; thus it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper discretizes speech into discrete tokens and uses token-level masking in a joint speech-text pre-training framework, directly addressing discrete audio tokens and tokenizers, with English abstract; thus it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes token2vec, a joint self-supervised pre-training framework that discretizes continuous speech into discrete speech tokens to overcome modality mismatch and length differences between speech and text. The study clearly describes the discretization process, including conversion of speech to discrete tokens and phoneme-based text token processing, feeding them into a Transformer encoder for modeling. This aligns well with the inclusion criteria of using discrete audio tokens derived from neural representations and applying them for downstream tasks like speech recognition and intent classification, with experimental validation. The study is presented in English with detailed methodological descriptions, supporting reproducibility. Therefore, it satisfies all inclusion criteria and none of the exclusion criteria, making it suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes token2vec, a joint self-supervised pre-training framework that discretizes continuous speech into discrete speech tokens to overcome modality mismatch and length differences between speech and text. The study clearly describes the discretization process, including conversion of speech to discrete tokens and phoneme-based text token processing, feeding them into a Transformer encoder for modeling. This aligns well with the inclusion criteria of using discrete audio tokens derived from neural representations and applying them for downstream tasks like speech recognition and intent classification, with experimental validation. The study is presented in English with detailed methodological descriptions, supporting reproducibility. Therefore, it satisfies all inclusion criteria and none of the exclusion criteria, making it suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
    "abstract": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
    "metadata": {
      "arxiv_id": "2211.05239",
      "title": "RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure",
      "summary": "We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. RecD improves the training and preprocessing throughput and storage efficiency by up to 2.48x, 1.79x, and 3.71x, respectively, in an industry-scale DLRM training system.",
      "authors": [
        "Mark Zhao",
        "Dhruv Choudhary",
        "Devashish Tyagi",
        "Ajay Somani",
        "Max Kaplan",
        "Sung-Han Lin",
        "Sarunya Pumma",
        "Jongsoo Park",
        "Aarti Basant",
        "Niket Agarwal",
        "Carole-Jean Wu",
        "Christos Kozyrakis"
      ],
      "published": "2022-11-09T22:21:19Z",
      "updated": "2023-05-01T19:37:39Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "cs.IR",
        "cs.PF"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.05239v4",
      "landing_url": "https://arxiv.org/abs/2211.05239v4",
      "doi": "https://doi.org/10.48550/arXiv.2211.05239"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes Deduplication infrastructure for DLRM training and does not address discrete audio tokens or tokenization design, so it fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes Deduplication infrastructure for DLRM training and does not address discrete audio tokens or tokenization design, so it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes infrastructure optimizations for deep learning recommendation model training, focusing on deduplication to reduce storage and processing overhead; it does not discuss discrete audio tokens, audio discretization, neural audio codecs, nor any audio-related token design or applications, thus not meeting any inclusion criteria and matching exclusion criteria related to lack of discrete audio token content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes infrastructure optimizations for deep learning recommendation model training, focusing on deduplication to reduce storage and processing overhead; it does not discuss discrete audio tokens, audio discretization, neural audio codecs, nor any audio-related token design or applications, thus not meeting any inclusion criteria and matching exclusion criteria related to lack of discrete audio token content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
    "abstract": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
    "metadata": {
      "arxiv_id": "2211.07321",
      "title": "MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets",
      "summary": "In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Changli Tang",
        "Yujin Wang",
        "Xie Chen"
      ],
      "published": "2022-11-14T13:00:47Z",
      "updated": "2023-05-31T11:45:38Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.07321v3",
      "landing_url": "https://arxiv.org/abs/2211.07321v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.07321"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes self-supervised speech learning using multiple targets and explicitly employs K-means as an offline target extractor, which yields discrete cluster targets, aligning with the discrete audio token criterion, thus meeting inclusion criteria while not being primarily about continuous representations only.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes self-supervised speech learning using multiple targets and explicitly employs K-means as an offline target extractor, which yields discrete cluster targets, aligning with the discrete audio token criterion, thus meeting inclusion criteria while not being primarily about continuous representations only.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a self-supervised speech representation learning method using multiple training targets, including one based on K-means clustering (an Offline Targets Extractor) and another from a teacher network (Online Targets Extractor). However, it does not explicitly describe the generation of discrete audio tokens through neural audio codec or vector quantization mechanisms, nor does it detail token vocabularies, quantization, or tokenization processes as required by the inclusion criteria. Instead, it focuses on improving SSL models via multi-task learning frameworks, and the study lacks explicit information on discrete token design, discrete token usage for downstream tasks, or systematic evaluation of discrete token properties. Hence, it does not meet the specific inclusion criteria centered on discrete token usage and design, leading to exclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents a self-supervised speech representation learning method using multiple training targets, including one based on K-means clustering (an Offline Targets Extractor) and another from a teacher network (Online Targets Extractor). However, it does not explicitly describe the generation of discrete audio tokens through neural audio codec or vector quantization mechanisms, nor does it detail token vocabularies, quantization, or tokenization processes as required by the inclusion criteria. Instead, it focuses on improving SSL models via multi-task learning frameworks, and the study lacks explicit information on discrete token design, discrete token usage for downstream tasks, or systematic evaluation of discrete token properties. Hence, it does not meet the specific inclusion criteria centered on discrete token usage and design, leading to exclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers disagree: JuniorNano leaned toward inclusion, noting K-means yields discrete cluster targets, while JuniorMini recommended exclusion due to missing tokenizer/quantization/vocabulary details. The abstract indeed mentions using K-means as an Offline Targets Extractor (producing cluster targets) and a teacher network as an Online Targets Extractor, but it does not explicitly describe producing reproducible discrete token sequences (no vocab/codebook/quantization/time‑downsampling/rate details) nor framing those discrete units as core tokens for generation/alignment downstream — therefore it fails the required inclusion criteria and matches the exclusion criteria for lacking discrete-token design and evaluable details. Score 2: better to exclude — the abstract lacks the explicit, reproducible discrete-token design/usage required by the Discrete Audio Tokens scope.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers disagree: JuniorNano leaned toward inclusion, noting K-means yields discrete cluster targets, while JuniorMini recommended exclusion due to missing tokenizer/quantization/vocabulary details. The abstract indeed mentions using K-means as an Offline Targets Extractor (producing cluster targets) and a teacher network as an Online Targets Extractor, but it does not explicitly describe producing reproducible discrete token sequences (no vocab/codebook/quantization/time‑downsampling/rate details) nor framing those discrete units as core tokens for generation/alignment downstream — therefore it fails the required inclusion criteria and matches the exclusion criteria for lacking discrete-token design and evaluable details. Score 2: better to exclude — the abstract lacks the explicit, reproducible discrete-token design/usage required by the Discrete Audio Tokens scope.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "L2 proficiency assessment using self-supervised speech representations",
    "abstract": "There has been a growing demand for automated spoken language assessment systems in recent years. A standard pipeline for this process is to start with a speech recognition system and derive features, either hand-crafted or based on deep-learning, that exploit the transcription and audio. Though these approaches can yield high performance systems, they require speech recognition systems that can be used for L2 speakers, and preferably tuned to the specific form of test being deployed. Recently a self-supervised speech representation based scheme, requiring no speech recognition, was proposed. This work extends the initial analysis conducted on this approach to a large scale proficiency test, Linguaskill, that comprises multiple parts, each designed to assess different attributes of a candidate's speaking proficiency. The performance of the self-supervised, wav2vec 2.0, system is compared to a high performance hand-crafted assessment system and a BERT-based text system both of which use speech transcriptions. Though the wav2vec 2.0 based system is found to be sensitive to the nature of the response, it can be configured to yield comparable performance to systems requiring a speech transcription, and yields gains when appropriately combined with standard approaches.",
    "metadata": {
      "arxiv_id": "2211.08849",
      "title": "L2 proficiency assessment using self-supervised speech representations",
      "summary": "There has been a growing demand for automated spoken language assessment systems in recent years. A standard pipeline for this process is to start with a speech recognition system and derive features, either hand-crafted or based on deep-learning, that exploit the transcription and audio. Though these approaches can yield high performance systems, they require speech recognition systems that can be used for L2 speakers, and preferably tuned to the specific form of test being deployed. Recently a self-supervised speech representation based scheme, requiring no speech recognition, was proposed. This work extends the initial analysis conducted on this approach to a large scale proficiency test, Linguaskill, that comprises multiple parts, each designed to assess different attributes of a candidate's speaking proficiency. The performance of the self-supervised, wav2vec 2.0, system is compared to a high performance hand-crafted assessment system and a BERT-based text system both of which use speech transcriptions. Though the wav2vec 2.0 based system is found to be sensitive to the nature of the response, it can be configured to yield comparable performance to systems requiring a speech transcription, and yields gains when appropriately combined with standard approaches.",
      "authors": [
        "Stefano Bannò",
        "Kate M. Knill",
        "Marco Matassoni",
        "Vyas Raina",
        "Mark J. F. Gales"
      ],
      "published": "2022-11-16T11:47:20Z",
      "updated": "2022-11-16T11:47:20Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.08849v1",
      "landing_url": "https://arxiv.org/abs/2211.08849v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.08849"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes using self-supervised speech representations for L2 proficiency assessment but does not discuss discrete audio tokens, tokenizers/codebooks, or token-based downstream modeling, hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes using self-supervised speech representations for L2 proficiency assessment but does not discuss discrete audio tokens, tokenizers/codebooks, or token-based downstream modeling, hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on L2 proficiency assessment using self-supervised speech representations like wav2vec 2.0, which is a continuous speech representation model; however, the abstract does not mention any discretization into discrete audio tokens or the creation/use of token vocabularies as defined by the inclusion criteria. No detailed description of a tokenizer, codec architecture, or quantization method for discrete token generation is provided, which are essential for meeting the inclusion criteria. Therefore, the study does not seem to involve discrete audio token-based modeling or analysis, nor does it evaluate token designs or employs tokens for downstream generation or alignment tasks. Thus, it fails the key inclusion criteria and fits the exclusion criteria of continuous audio representation modeling without discrete token usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on L2 proficiency assessment using self-supervised speech representations like wav2vec 2.0, which is a continuous speech representation model; however, the abstract does not mention any discretization into discrete audio tokens or the creation/use of token vocabularies as defined by the inclusion criteria. No detailed description of a tokenizer, codec architecture, or quantization method for discrete token generation is provided, which are essential for meeting the inclusion criteria. Therefore, the study does not seem to involve discrete audio token-based modeling or analysis, nor does it evaluate token designs or employs tokens for downstream generation or alignment tasks. Thus, it fails the key inclusion criteria and fits the exclusion criteria of continuous audio representation modeling without discrete token usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
    "abstract": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
    "metadata": {
      "arxiv_id": "2211.09117",
      "title": "MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis",
      "summary": "Generative modeling and representation learning are two key tasks in computer vision. However, these models are typically trained independently, which ignores the potential for each task to help the other, and leads to training and model maintenance overheads. In this work, we propose MAsked Generative Encoder (MAGE), the first framework to unify SOTA image generation and self-supervised representation learning. Our key insight is that using variable masking ratios in masked image modeling pre-training can allow generative training (very high masking ratio) and representation learning (lower masking ratio) under the same training framework. Inspired by previous generative models, MAGE uses semantic tokens learned by a vector-quantized GAN at inputs and outputs, combining this with masking. We can further improve the representation by adding a contrastive loss to the encoder output. We extensively evaluate the generation and representation learning capabilities of MAGE. On ImageNet-1K, a single MAGE ViT-L model obtains 9.10 FID in the task of class-unconditional image generation and 78.9% top-1 accuracy for linear probing, achieving state-of-the-art performance in both image generation and representation learning. Code is available at https://github.com/LTH14/mage.",
      "authors": [
        "Tianhong Li",
        "Huiwen Chang",
        "Shlok Kumar Mishra",
        "Han Zhang",
        "Dina Katabi",
        "Dilip Krishnan"
      ],
      "published": "2022-11-16T18:59:02Z",
      "updated": "2023-06-29T15:30:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.09117v2",
      "landing_url": "https://arxiv.org/abs/2211.09117v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.09117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about image generation and self-supervised representation learning with masked encoding; it does not involve discrete audio tokens, tokenization, or audio-specific discretization relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about image generation and self-supervised representation learning with masked encoding; it does not involve discrete audio tokens, tokenization, or audio-specific discretization relevant to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image generation and representation learning using masked generative encoding applied to images, without involving discrete audio tokens or any audio signal processing or discretization related to audio waveforms; hence, it does not meet any inclusion criteria related to discrete audio tokens as defined, nor is it relevant to audio tokenization or audio-related downstream tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image generation and representation learning using masked generative encoding applied to images, without involving discrete audio tokens or any audio signal processing or discretization related to audio waveforms; hence, it does not meet any inclusion criteria related to discrete audio tokens as defined, nor is it relevant to audio tokenization or audio-related downstream tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Asymptotics for The $k$-means",
    "abstract": "The $k$-means is one of the most important unsupervised learning techniques in statistics and computer science. The goal is to partition a data set into many clusters, such that observations within clusters are the most homogeneous and observations between clusters are the most heterogeneous. Although it is well known, the investigation of the asymptotic properties is far behind, leading to difficulties in developing more precise $k$-means methods in practice. To address this issue, a new concept called clustering consistency is proposed. Fundamentally, the proposed clustering consistency is more appropriate than the previous criterion consistency for the clustering methods. Using this concept, a new $k$-means method is proposed. It is found that the proposed $k$-means method has lower clustering error rates and is more robust to small clusters and outliers than existing $k$-means methods. When $k$ is unknown, using the Gap statistics, the proposed method can also identify the number of clusters. This is rarely achieved by existing $k$-means methods adopted by many software packages.",
    "metadata": {
      "arxiv_id": "2211.10015",
      "title": "Asymptotics for The $k$-means",
      "summary": "The $k$-means is one of the most important unsupervised learning techniques in statistics and computer science. The goal is to partition a data set into many clusters, such that observations within clusters are the most homogeneous and observations between clusters are the most heterogeneous. Although it is well known, the investigation of the asymptotic properties is far behind, leading to difficulties in developing more precise $k$-means methods in practice. To address this issue, a new concept called clustering consistency is proposed. Fundamentally, the proposed clustering consistency is more appropriate than the previous criterion consistency for the clustering methods. Using this concept, a new $k$-means method is proposed. It is found that the proposed $k$-means method has lower clustering error rates and is more robust to small clusters and outliers than existing $k$-means methods. When $k$ is unknown, using the Gap statistics, the proposed method can also identify the number of clusters. This is rarely achieved by existing $k$-means methods adopted by many software packages.",
      "authors": [
        "Tonglin Zhang"
      ],
      "published": "2022-11-18T03:36:58Z",
      "updated": "2022-11-18T03:36:58Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.10015v1",
      "landing_url": "https://arxiv.org/abs/2211.10015v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.10015"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article concerns asymptotics of k-means and clustering consistency; it does not discuss discrete audio tokens, tokenizers, neural audio codecs, or any audio-token design/assessment, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article concerns asymptotics of k-means and clustering consistency; it does not discuss discrete audio tokens, tokenizers, neural audio codecs, or any audio-token design/assessment, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on clustering algorithms (k-means) and its asymptotic properties, which are unrelated to the discrete audio tokens topic involving discretization of continuous audio waveforms into token sequences for modeling or generation. No information about neural audio codecs, discretization processes, token vocabularies, or audio-related token modeling is provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on clustering algorithms (k-means) and its asymptotic properties, which are unrelated to the discrete audio tokens topic involving discretization of continuous audio waveforms into token sequences for modeling or generation. No information about neural audio codecs, discretization processes, token vocabularies, or audio-related token modeling is provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
    "abstract": "Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.",
    "metadata": {
      "arxiv_id": "2211.11275",
      "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
      "summary": "Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.",
      "authors": [
        "Qiushi Zhu",
        "Long Zhou",
        "Ziqiang Zhang",
        "Shujie Liu",
        "Binxing Jiao",
        "Jie Zhang",
        "Lirong Dai",
        "Daxin Jiang",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2022-11-21T09:10:10Z",
      "updated": "2023-05-19T10:03:56Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.CV",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.11275v2",
      "landing_url": "https://arxiv.org/abs/2211.11275v2",
      "doi": "https://doi.org/10.1109/TMM.2023.3275873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes VATLM, which uses a unified tokenizer to produce discrete tokens for visual, audio, and text modalities and trains with masked prediction in a cross-modal setting, fitting the focus on discrete audio tokens and token-based representation; it provides an English title/abstract, and the approach centers on token-based modeling and cross-modal alignment rather than purely continuous representations, aligning with the inclusion criteria (token sources via tokenizer/codec and downstream multi-modal tasks) and not falling under the exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item describes VATLM, which uses a unified tokenizer to produce discrete tokens for visual, audio, and text modalities and trains with masked prediction in a cross-modal setting, fitting the focus on discrete audio tokens and token-based representation; it provides an English title/abstract, and the approach centers on token-based modeling and cross-modal alignment rather than purely continuous representations, aligning with the inclusion criteria (token sources via tokenizer/codec and downstream multi-modal tasks) and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes VATLM, a unified multimodal framework for speech representation learning integrating visual, audio, and text modalities, optimizing with a masked prediction task over unified tokens from a proposed tokenizer. However, the abstract does not clearly describe the generation or discretization process of discrete audio tokens (e.g., no detailed tokenizer/codec architecture, quantization method, or vocabulary/codebook settings) which is central to the inclusion criteria. Although it involves unified tokens, the lack of explicit description on discrete token formation and neural audio codec or clustering-based discrete unit derivation makes it unclear whether this study focuses on or advances discrete audio token representations. Thus, it does not meet the core criteria about discrete audio tokens needed for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes VATLM, a unified multimodal framework for speech representation learning integrating visual, audio, and text modalities, optimizing with a masked prediction task over unified tokens from a proposed tokenizer. However, the abstract does not clearly describe the generation or discretization process of discrete audio tokens (e.g., no detailed tokenizer/codec architecture, quantization method, or vocabulary/codebook settings) which is central to the inclusion criteria. Although it involves unified tokens, the lack of explicit description on discrete token formation and neural audio codec or clustering-based discrete unit derivation makes it unclear whether this study focuses on or advances discrete audio token representations. Thus, it does not meet the core criteria about discrete audio tokens needed for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior assessments (one rated 4, one rated 2): both note VATLM uses a “unified tokenizer” and masked-token prediction across visual/audio/text, but the abstract does not specify how discrete audio tokens are produced (no details on tokenizer/codec architecture, quantization/clustering, vocab/codebook, bitrate or time downsampling) — which is required by the inclusion criteria to confirm this is a discrete-audio-token paper. Because the core inclusion rules require clear, reproducible description of discrete token generation (neural codec or clustering-derived units) and the abstract is ambiguous, I cannot verify it meets the discrete-audio-token definition. One-sentence explanation: Although the work mentions unified tokens, the abstract lacks the explicit, reproducible details on how discrete audio tokens are generated, so it should better be excluded (score=2).",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior assessments (one rated 4, one rated 2): both note VATLM uses a “unified tokenizer” and masked-token prediction across visual/audio/text, but the abstract does not specify how discrete audio tokens are produced (no details on tokenizer/codec architecture, quantization/clustering, vocab/codebook, bitrate or time downsampling) — which is required by the inclusion criteria to confirm this is a discrete-audio-token paper. Because the core inclusion rules require clear, reproducible description of discrete token generation (neural codec or clustering-derived units) and the abstract is ambiguous, I cannot verify it meets the discrete-audio-token definition. One-sentence explanation: Although the work mentions unified tokens, the abstract lacks the explicit, reproducible details on how discrete audio tokens are generated, so it should better be excluded (score=2).",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
    "abstract": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
    "metadata": {
      "arxiv_id": "2211.11386",
      "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
      "summary": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
      "authors": [
        "Satoshi Ikehata"
      ],
      "published": "2022-11-21T11:58:25Z",
      "updated": "2022-11-21T11:58:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.11386v1",
      "landing_url": "https://arxiv.org/abs/2211.11386v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.11386"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about photometric stereo and self-attention in vision, not about discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about photometric stereo and self-attention in vision, not about discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study does not focus on discrete audio tokens or audio waveform discretization; instead, it presents a photometric stereo network for surface normal prediction using a self-attention mechanism. There is no mention of audio data, tokenization, neural audio codecs, or any audio-related discrete token representations, thereby not meeting any inclusion criteria and fitting exclusion criteria related to not addressing discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study does not focus on discrete audio tokens or audio waveform discretization; instead, it presents a photometric stereo network for surface normal prediction using a self-attention mechanism. There is no mention of audio data, tokenization, neural audio codecs, or any audio-related discrete token representations, thereby not meeting any inclusion criteria and fitting exclusion criteria related to not addressing discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
    "abstract": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
    "metadata": {
      "arxiv_id": "2211.12271",
      "title": "Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm",
      "summary": "The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \\emph{global $k$-means\\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\\texttt{++} algorithm. The proposed method has been tested and compared in various benchmark datasets yielding very satisfactory results in terms of clustering quality and execution speed.",
      "authors": [
        "Georgios Vardakas",
        "Aristidis Likas"
      ],
      "published": "2022-11-22T13:42:53Z",
      "updated": "2023-07-14T11:39:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12271v3",
      "landing_url": "https://arxiv.org/abs/2211.12271v3",
      "doi": "https://doi.org/10.48550/arXiv.2211.12271"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss global k-means clustering algorithms and computational aspects, with no mention of discrete audio tokens, tokenization, or audio-centric downstream tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss global k-means clustering algorithms and computational aspects, with no mention of discrete audio tokens, tokenization, or audio-centric downstream tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on k-means clustering algorithm enhancements and does not involve any audio processing, discretization of audio signals, neural audio codecs, or discrete audio tokens as described in the inclusion criteria. Therefore, it does not meet the required topic definition related to discrete audio token research and does not address any relevant token design, representation, or downstream tasks within audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on k-means clustering algorithm enhancements and does not involve any audio processing, discretization of audio signals, neural audio codecs, or discrete audio tokens as described in the inclusion criteria. Therefore, it does not meet the required topic definition related to discrete audio token research and does not address any relevant token design, representation, or downstream tasks within audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homology-constrained vector quantization entropy regularizer",
    "abstract": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on an entropy regularizer for vector quantization in autoencoders, with examples like VQ-VAE, but it does not explicitly address discrete audio tokens or audio-specific token generation/encoding, which is required by the discrete audio tokens criteria to be included.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on an entropy regularizer for vector quantization in autoencoders, with examples like VQ-VAE, but it does not explicitly address discrete audio tokens or audio-specific token generation/encoding, which is required by the discrete audio tokens criteria to be included.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes an entropy regularizer for vector quantization embeddings and integrates it into a VQ-VAE model, but it does not explicitly describe or evaluate discrete audio tokens derived from neural audio codecs for speech, music, or environmental audio targeted at sequence modeling or downstream audio tasks, nor does it provide detailed tokenizer or codec architecture specifics relevant to discrete audio tokens for cross-modal tasks as required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes an entropy regularizer for vector quantization embeddings and integrates it into a VQ-VAE model, but it does not explicitly describe or evaluate discrete audio tokens derived from neural audio codecs for speech, music, or environmental audio targeted at sequence modeling or downstream audio tasks, nor does it provide detailed tokenizer or codec architecture specifics relevant to discrete audio tokens for cross-modal tasks as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Contextual Expressive Text-to-Speech",
    "abstract": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
    "metadata": {
      "arxiv_id": "2211.14548",
      "title": "Contextual Expressive Text-to-Speech",
      "summary": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
      "authors": [
        "Jianhong Tu",
        "Zeyu Cui",
        "Xiaohuan Zhou",
        "Siqi Zheng",
        "Kai Hu",
        "Ju Fan",
        "Chang Zhou"
      ],
      "published": "2022-11-26T12:06:21Z",
      "updated": "2022-11-26T12:06:21Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14548v1",
      "landing_url": "https://arxiv.org/abs/2211.14548v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14548"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on context-based expressive TTS using textual context, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, thus not meeting the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on context-based expressive TTS using textual context, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, thus not meeting the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a Contextual Expressive Text-to-Speech system that synthesizes speech based on contextual text rather than fixed style or emotion labels, but does not mention any discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes involved in speech representation; hence, it does not meet the core inclusion criteria related to discrete audio tokens used for generation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a Contextual Expressive Text-to-Speech system that synthesizes speech based on contextual text rather than fixed style or emotion labels, but does not mention any discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes involved in speech representation; hence, it does not meet the core inclusion criteria related to discrete audio tokens used for generation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Faster $k$-means++ Algorithm",
    "abstract": "$k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\\widetilde{O}(k )$ iterations, and each iteration takes $\\widetilde{O}(nd k)$ time. The overall running time is thus $\\widetilde{O}(n d k^2)$. We propose a new algorithm \\textsc{FastKmeans++} that only takes in $\\widetilde{O}(nd + nk^2)$ time, in total.",
    "metadata": {
      "arxiv_id": "2211.15118",
      "title": "A Faster $k$-means++ Algorithm",
      "summary": "$k$-means++ is an important algorithm for choosing initial cluster centers for the $k$-means clustering algorithm. In this work, we present a new algorithm that can solve the $k$-means++ problem with nearly optimal running time. Given $n$ data points in $\\mathbb{R}^d$, the current state-of-the-art algorithm runs in $\\widetilde{O}(k )$ iterations, and each iteration takes $\\widetilde{O}(nd k)$ time. The overall running time is thus $\\widetilde{O}(n d k^2)$. We propose a new algorithm \\textsc{FastKmeans++} that only takes in $\\widetilde{O}(nd + nk^2)$ time, in total.",
      "authors": [
        "Jiehao Liang",
        "Somdeb Sarkhel",
        "Zhao Song",
        "Chenbo Yin",
        "Junze Yin",
        "Danyang Zhuo"
      ],
      "published": "2022-11-28T08:17:12Z",
      "updated": "2024-02-13T19:39:48Z",
      "categories": [
        "cs.DS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.15118v2",
      "landing_url": "https://arxiv.org/abs/2211.15118v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.15118"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about speeding up k-means++ and does not involve discrete audio tokens or tokenization; it does not address the token design, generation, or downstream multi-modal tasks defined in the inclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about speeding up k-means++ and does not involve discrete audio tokens or tokenization; it does not address the token design, generation, or downstream multi-modal tasks defined in the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe an improved algorithm for k-means++ clustering, focusing on computational efficiency for general data points, with no mention of audio data, discrete audio tokens, tokenization processes, neural audio codecs, or self-supervised discrete token representations. The work does not relate to audio signal discretization or token-based audio modeling/generation, thus failing to meet the inclusion criteria related to discrete audio tokens, and clearly falls into exclusion as it focuses on a general machine learning clustering algorithm not related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe an improved algorithm for k-means++ clustering, focusing on computational efficiency for general data points, with no mention of audio data, discrete audio tokens, tokenization processes, neural audio codecs, or self-supervised discrete token representations. The work does not relate to audio signal discretization or token-based audio modeling/generation, thus failing to meet the inclusion criteria related to discrete audio tokens, and clearly falls into exclusion as it focuses on a general machine learning clustering algorithm not related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
    "abstract": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
    "metadata": {
      "arxiv_id": "2211.16112",
      "title": "On Word Error Rate Definitions and their Efficient Computation for Multi-Speaker Speech Recognition Systems",
      "summary": "We propose a general framework to compute the word error rate (WER) of ASR systems that process recordings containing multiple speakers at their input and that produce multiple output word sequences (MIMO). Such ASR systems are typically required, e.g., for meeting transcription. We provide an efficient implementation based on a dynamic programming search in a multi-dimensional Levenshtein distance tensor under the constraint that a reference utterance must be matched consistently with one hypothesis output. This also results in an efficient implementation of the ORC WER which previously suffered from exponential complexity. We give an overview of commonly used WER definitions for multi-speaker scenarios and show that they are specializations of the above MIMO WER tuned to particular application scenarios. We conclude with a discussion of the pros and cons of the various WER definitions and a recommendation when to use which.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Keisuke Kinoshita",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2022-11-29T11:35:13Z",
      "updated": "2023-07-21T07:28:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16112v2",
      "landing_url": "https://arxiv.org/abs/2211.16112v2",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10094784"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes WER definitions for multi-speaker ASR and efficient computation, with no discussion of discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes WER definitions for multi-speaker ASR and efficient computation, with no discussion of discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on word error rate definitions and their efficient computation for multi-speaker speech recognition systems, discussing evaluation metrics rather than proposing or analyzing discrete audio tokens derived from neural audio codecs or self-supervised learning models. It does not mention discrete token generation, tokenizer architectures, quantization methods, or token-based modeling relevant to discrete audio token sequences as defined in the inclusion criteria. Since the work addresses continuous speech recognition evaluation without the core use of discrete tokens or their systemic analysis, it falls outside the scope of the defined inclusion criteria and meets exclusion criteria related to continuous audio representation modeling without discrete token details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on word error rate definitions and their efficient computation for multi-speaker speech recognition systems, discussing evaluation metrics rather than proposing or analyzing discrete audio tokens derived from neural audio codecs or self-supervised learning models. It does not mention discrete token generation, tokenizer architectures, quantization methods, or token-based modeling relevant to discrete audio token sequences as defined in the inclusion criteria. Since the work addresses continuous speech recognition evaluation without the core use of discrete tokens or their systemic analysis, it falls outside the scope of the defined inclusion criteria and meets exclusion criteria related to continuous audio representation modeling without discrete token details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech",
    "abstract": "This work adapts two recent architectures of generative models and evaluates their effectiveness for the conversion of whispered speech to normal speech. We incorporate the normal target speech into the training criterion of vector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby conditioning the systems to recover voiced speech from whispered inputs. Objective and subjective quality measures indicate that both VQ-VAEs and MelGANs can be modified to perform the conversion task. We find that the proposed approaches significantly improve the Mel cepstral distortion (MCD) metric by at least 25% relative to a DiscoGAN baseline. Subjective listening tests suggest that the MelGAN-based system significantly improves naturalness, intelligibility, and voicing compared to the whispered input speech. A novel evaluation measure based on differences between latent speech representations also indicates that our MelGAN-based approach yields improvements relative to the baseline.",
    "metadata": {
      "arxiv_id": "2212.01775",
      "title": "Generative Models for Improved Naturalness, Intelligibility, and Voicing of Whispered Speech",
      "summary": "This work adapts two recent architectures of generative models and evaluates their effectiveness for the conversion of whispered speech to normal speech. We incorporate the normal target speech into the training criterion of vector-quantized variational autoencoders (VQ-VAEs) and MelGANs, thereby conditioning the systems to recover voiced speech from whispered inputs. Objective and subjective quality measures indicate that both VQ-VAEs and MelGANs can be modified to perform the conversion task. We find that the proposed approaches significantly improve the Mel cepstral distortion (MCD) metric by at least 25% relative to a DiscoGAN baseline. Subjective listening tests suggest that the MelGAN-based system significantly improves naturalness, intelligibility, and voicing compared to the whispered input speech. A novel evaluation measure based on differences between latent speech representations also indicates that our MelGAN-based approach yields improvements relative to the baseline.",
      "authors": [
        "Dominik Wagner",
        "Sebastian P. Bayerl",
        "Hector A. Cordourier Maruri",
        "Tobias Bocklet"
      ],
      "published": "2022-12-04T09:06:18Z",
      "updated": "2023-01-30T10:03:00Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.01775v2",
      "landing_url": "https://arxiv.org/abs/2212.01775v2",
      "doi": "https://doi.org/10.1109/SLT54892.2023.10022796"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item uses vector-quantized VAE (VQ-VAE) to obtain discrete latent tokens as part of the generation process for whispered-to-voiced speech, indicating discrete audio token usage, even though the paper mainly targets speech conversion performance rather than token-design analysis; thus it aligns with the discrete-audio-token theme but is not solely token-design focused.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item uses vector-quantized VAE (VQ-VAE) to obtain discrete latent tokens as part of the generation process for whispered-to-voiced speech, indicating discrete audio token usage, even though the paper mainly targets speech conversion performance rather than token-design analysis; thus it aligns with the discrete-audio-token theme but is not solely token-design focused.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on converting whispered speech to voiced speech using vector-quantized variational autoencoders (VQ-VAEs) and MelGANs; however, the abstract does not describe the generation or use of discrete audio tokens as defined by the inclusion criteria (discretization into token sequences with vocabulary/codebook specifications). It mentions vector quantization but lacks details on tokenization, vocabulary, or discrete token usage for downstream modeling or generation in the style the topic requires. Therefore, it does not meet the main inclusion criteria and falls under exclusion for lacking discrete token definition and usage as the core method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on converting whispered speech to voiced speech using vector-quantized variational autoencoders (VQ-VAEs) and MelGANs; however, the abstract does not describe the generation or use of discrete audio tokens as defined by the inclusion criteria (discretization into token sequences with vocabulary/codebook specifications). It mentions vector quantization but lacks details on tokenization, vocabulary, or discrete token usage for downstream modeling or generation in the style the topic requires. Therefore, it does not meet the main inclusion criteria and falls under exclusion for lacking discrete token definition and usage as the core method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses VQ-VAE (vector quantization) for whispered-to-voiced conversion: one argued this implies discrete audio tokens, while the other correctly pointed out the abstract lacks any detail on tokenization (vocabulary/codebook, token sequences, or using tokens as the core representation for downstream modeling). The submission appears to be an audio-conversion paper using VQ-VAE/MelGAN to improve speech quality rather than a study of discrete audio-token design, usage, or evaluation; the abstract does not provide the required English-method details to confirm discrete-token generation or that tokens are central to modeling. Therefore, exclude. (Brief explanation: despite mentioning VQ-VAE, the abstract lacks explicit tokenizer/codebook/token-sequence details and treats VQ as a modeling component for conversion rather than presenting discrete audio tokens as the study's core.)",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses VQ-VAE (vector quantization) for whispered-to-voiced conversion: one argued this implies discrete audio tokens, while the other correctly pointed out the abstract lacks any detail on tokenization (vocabulary/codebook, token sequences, or using tokens as the core representation for downstream modeling). The submission appears to be an audio-conversion paper using VQ-VAE/MelGAN to improve speech quality rather than a study of discrete audio-token design, usage, or evaluation; the abstract does not provide the required English-method details to confirm discrete-token generation or that tokens are central to modeling. Therefore, exclude. (Brief explanation: despite mentioning VQ-VAE, the abstract lacks explicit tokenizer/codebook/token-sequence details and treats VQ as a modeling component for conversion rather than presenting discrete audio tokens as the study's core.)",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information",
    "abstract": "Multilingual end-to-end models have shown great improvement over monolingual systems. With the development of pre-training methods on speech, self-supervised multilingual speech representation learning like XLSR has shown success in improving the performance of multilingual automatic speech recognition (ASR). However, similar to the supervised learning, multilingual pre-training may also suffer from language interference and further affect the application of multilingual system. In this paper, we introduce several techniques for improving self-supervised multilingual pre-training by leveraging auxiliary language information, including the language adversarial training, language embedding and language adaptive training during the pre-training stage. We conduct experiments on a multilingual ASR task consisting of 16 languages. Our experimental results demonstrate 14.3% relative gain over the standard XLSR model, and 19.8% relative gain over the no pre-training multilingual model.",
    "metadata": {
      "arxiv_id": "2212.03476",
      "title": "Improved Self-Supervised Multilingual Speech Representation Learning Combined with Auxiliary Language Information",
      "summary": "Multilingual end-to-end models have shown great improvement over monolingual systems. With the development of pre-training methods on speech, self-supervised multilingual speech representation learning like XLSR has shown success in improving the performance of multilingual automatic speech recognition (ASR). However, similar to the supervised learning, multilingual pre-training may also suffer from language interference and further affect the application of multilingual system. In this paper, we introduce several techniques for improving self-supervised multilingual pre-training by leveraging auxiliary language information, including the language adversarial training, language embedding and language adaptive training during the pre-training stage. We conduct experiments on a multilingual ASR task consisting of 16 languages. Our experimental results demonstrate 14.3% relative gain over the standard XLSR model, and 19.8% relative gain over the no pre-training multilingual model.",
      "authors": [
        "Fenglin Ding",
        "Genshun Wan",
        "Pengcheng Li",
        "Jia Pan",
        "Cong Liu"
      ],
      "published": "2022-12-07T06:18:59Z",
      "updated": "2022-12-07T06:18:59Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03476v1",
      "landing_url": "https://arxiv.org/abs/2212.03476v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03476"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study concerns self-supervised multilingual speech representations and auxiliary language information, with no mention of discrete audio tokens, tokenization, quantization, or token-level analyses; thus does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study concerns self-supervised multilingual speech representations and auxiliary language information, with no mention of discrete audio tokens, tokenization, quantization, or token-level analyses; thus does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving self-supervised multilingual speech representation learning for ASR without describing a process of discretizing continuous audio into discrete tokens or detailing any tokenizer, codec, quantization method, or codebook configuration, which are key to the inclusion criteria for discrete audio tokens research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving self-supervised multilingual speech representation learning for ASR without describing a process of discretizing continuous audio into discrete tokens or detailing any tokenizer, codec, quantization method, or codebook configuration, which are key to the inclusion criteria for discrete audio tokens research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit",
    "abstract": "Speech pre-training has shown great success in learning useful and general latent representations from large-scale unlabeled data. Based on a well-designed self-supervised learning pattern, pre-trained models can be used to serve lots of downstream speech tasks such as automatic speech recognition. In order to take full advantage of the labed data in low resource task, we present an improved pre-training method by introducing a supervision-enhanced acoustic unit (SEAU) pattern to intensify the expression of comtext information and ruduce the training cost. Encoder representations extracted from the SEAU pattern are used to generate more representative target units for HuBERT pre-training process. The proposed method, named SeHuBERT, achieves a relative word error rate reductions of 10.5% and 4.9% comared with the standard HuBERT on Turkmen speech recognition task with 500 hours and 100 hours fine-tuning data respectively. Extended to more languages and more data, SeHuBERT can aslo achieve a relative word error rate reductions of approximately 10% at half of the training cost compared with HuBERT.",
    "metadata": {
      "arxiv_id": "2212.03482",
      "title": "Improved Speech Pre-Training with Supervision-Enhanced Acoustic Unit",
      "summary": "Speech pre-training has shown great success in learning useful and general latent representations from large-scale unlabeled data. Based on a well-designed self-supervised learning pattern, pre-trained models can be used to serve lots of downstream speech tasks such as automatic speech recognition. In order to take full advantage of the labed data in low resource task, we present an improved pre-training method by introducing a supervision-enhanced acoustic unit (SEAU) pattern to intensify the expression of comtext information and ruduce the training cost. Encoder representations extracted from the SEAU pattern are used to generate more representative target units for HuBERT pre-training process. The proposed method, named SeHuBERT, achieves a relative word error rate reductions of 10.5% and 4.9% comared with the standard HuBERT on Turkmen speech recognition task with 500 hours and 100 hours fine-tuning data respectively. Extended to more languages and more data, SeHuBERT can aslo achieve a relative word error rate reductions of approximately 10% at half of the training cost compared with HuBERT.",
      "authors": [
        "Pengcheng Li",
        "Genshun Wan",
        "Fenglin Ding",
        "Hang Chen",
        "Jianqing Gao",
        "Jia Pan",
        "Cong Liu"
      ],
      "published": "2022-12-07T06:31:31Z",
      "updated": "2022-12-07T06:31:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.03482v1",
      "landing_url": "https://arxiv.org/abs/2212.03482v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.03482"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on using supervision-enhanced acoustic/target units for HuBERT pre-training, implying discretized or semi-discrete representations derived from a self-supervised model, which aligns with the discrete audio token criterion. However, the abstract does not explicitly detail a tokenization/codec or vocab/qment settings, introducing some ambiguity about explicit token discretization.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on using supervision-enhanced acoustic/target units for HuBERT pre-training, implying discretized or semi-discrete representations derived from a self-supervised model, which aligns with the discrete audio token criterion. However, the abstract does not explicitly detail a tokenization/codec or vocab/qment settings, introducing some ambiguity about explicit token discretization.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving speech pre-training using a supervision-enhanced acoustic unit for better performance in speech recognition tasks. However, the abstract does not explicitly describe the production of discrete audio tokens through quantization or clustering, nor does it detail tokenization or vocabulary construction essential to meet the discrete audio token inclusion criteria. It appears to emphasize continuous representation learning and downstream ASR improvements rather than the design or usage of discrete tokens as core methodology.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving speech pre-training using a supervision-enhanced acoustic unit for better performance in speech recognition tasks. However, the abstract does not explicitly describe the production of discrete audio tokens through quantization or clustering, nor does it detail tokenization or vocabulary construction essential to meet the discrete audio token inclusion criteria. It appears to emphasize continuous representation learning and downstream ASR improvements rather than the design or usage of discrete tokens as core methodology.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged ambiguity: the work aims to generate improved “target units” for HuBERT via a supervision-enhanced acoustic unit (SEAU), which suggests discrete pseudo-units but the abstract does not explicitly describe any discretization/tokenization procedure (e.g., clustering/quantization, codebook/vocab size, tokenization pipeline, or codec settings) required by our Discrete Audio Tokens inclusion criteria; given this lack of explicit token-generation details and reproducibility information, the paper should be excluded/lean-excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged ambiguity: the work aims to generate improved “target units” for HuBERT via a supervision-enhanced acoustic unit (SEAU), which suggests discrete pseudo-units but the abstract does not explicitly describe any discretization/tokenization procedure (e.g., clustering/quantization, codebook/vocab size, tokenization pipeline, or codec settings) required by our Discrete Audio Tokens inclusion criteria; given this lack of explicit token-generation details and reproducibility information, the paper should be excluded/lean-excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "SpeechLMScore: Evaluating speech generation using speech language model",
    "abstract": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
    "metadata": {
      "arxiv_id": "2212.04559",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Takaaki Saeki",
        "Shinji Watanabe"
      ],
      "published": "2022-12-08T21:00:15Z",
      "updated": "2022-12-08T21:00:15Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04559v1",
      "landing_url": "https://arxiv.org/abs/2212.04559v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.04559"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses evaluating speech generation via discretized tokens produced by a speech-language model, aligning with the discrete audio tokens theme, but the abstract does not provide explicit details on token generation/codec/tokenizer design required for reproducibility.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item discusses evaluating speech generation via discretized tokens produced by a speech-language model, aligning with the discrete audio tokens theme, but the abstract does not provide explicit details on token generation/codec/tokenizer design required for reproducibility.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes SpeechLMScore, an unsupervised metric using a speech-language model that converts speech into discrete tokens to evaluate speech generation quality. This aligns with the discrete audio tokens theme, where continuous speech is discretized into token sequences modeled similarly to text sequences. The study focuses on discrete token generation and usage for speech evaluation, which supports reproducibility and evaluation of token design impact, matching inclusion criteria. The abstract provides an English title and abstract, and describes the tokenization mapping process, so it meets the English and reproducibility requirements. The study is not based on continuous representations alone, nor does it focus on discriminative tasks or traditional audio compression without neural discrete tokens. Therefore, it satisfies the inclusion criteria and meets none of the exclusion criteria, supporting inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes SpeechLMScore, an unsupervised metric using a speech-language model that converts speech into discrete tokens to evaluate speech generation quality. This aligns with the discrete audio tokens theme, where continuous speech is discretized into token sequences modeled similarly to text sequences. The study focuses on discrete token generation and usage for speech evaluation, which supports reproducibility and evaluation of token design impact, matching inclusion criteria. The abstract provides an English title and abstract, and describes the tokenization mapping process, so it meets the English and reproducibility requirements. The study is not based on continuous representations alone, nor does it focus on discriminative tasks or traditional audio compression without neural discrete tokens. Therefore, it satisfies the inclusion criteria and meets none of the exclusion criteria, supporting inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
    "abstract": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
    "metadata": {
      "arxiv_id": "2212.08329",
      "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
      "summary": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
      "authors": [
        "Yusuke Yasuda",
        "Tomoki Toda"
      ],
      "published": "2022-12-16T08:14:04Z",
      "updated": "2022-12-16T08:14:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08329v1",
      "landing_url": "https://arxiv.org/abs/2212.08329v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08329"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on continuous latent-variable diffusion/VAE for TTS and does not introduce or evaluate discrete audio tokens/tokenizers, thus it does not meet the discrete audio tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on continuous latent-variable diffusion/VAE for TTS and does not introduce or evaluate discrete audio tokens/tokenizers, thus it does not meet the discrete audio tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text-to-speech synthesis using latent variable conversion based on diffusion probabilistic models and variational autoencoders, but it does not describe any discrete tokenization or production of discrete audio tokens from continuous audio waveforms, nor does it provide details on tokenizer/codec architecture, quantization methods, or token vocabularies; hence it does not meet the inclusion criteria centered on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text-to-speech synthesis using latent variable conversion based on diffusion probabilistic models and variational autoencoders, but it does not describe any discrete tokenization or production of discrete audio tokens from continuous audio waveforms, nor does it provide details on tokenizer/codec architecture, quantization methods, or token vocabularies; hence it does not meet the inclusion criteria centered on discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
    "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item BEATs: Audio Pre-Training with Acoustic Tokenizers explicitly centers on discrete audio tokens via an acoustic tokenizer, using self-supervised learning to derive discrete units and iteratively refine both the tokenizer and the SSL model; this aligns with the 'Discrete Audio Tokens' topic and includes English abstract with methodological detail to judge token generation and evaluation. 1-sentence explanation: The work clearly focuses on discrete token representations for audio and reports downstream performance, meeting the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item BEATs: Audio Pre-Training with Acoustic Tokenizers explicitly centers on discrete audio tokens via an acoustic tokenizer, using self-supervised learning to derive discrete units and iteratively refine both the tokenizer and the SSL model; this aligns with the 'Discrete Audio Tokens' topic and includes English abstract with methodological detail to judge token generation and evaluation. 1-sentence explanation: The work clearly focuses on discrete token representations for audio and reports downstream performance, meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes BEATs, an iterative audio pre-training framework that uses an acoustic tokenizer generating discrete labels (tokens) with rich audio semantics for audio transformers, clearly addressing discrete audio token generation and use in downstream tasks like classification. It details the token generation process through an acoustic tokenizer optimized iteratively, meeting inclusion criteria regarding discrete token definition and use, with sufficient methodological description and empirical evaluation, and does not fall under any exclusion criteria such as focusing solely on continuous representations or lacking discrete token details. Therefore, it aligns well with the discrete audio token theme and includes detailed token design and evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes BEATs, an iterative audio pre-training framework that uses an acoustic tokenizer generating discrete labels (tokens) with rich audio semantics for audio transformers, clearly addressing discrete audio token generation and use in downstream tasks like classification. It details the token generation process through an acoustic tokenizer optimized iteratively, meeting inclusion criteria regarding discrete token definition and use, with sufficient methodological description and empirical evaluation, and does not fall under any exclusion criteria such as focusing solely on continuous representations or lacking discrete token details. Therefore, it aligns well with the discrete audio token theme and includes detailed token design and evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
    "abstract": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
    "metadata": {
      "arxiv_id": "2212.09096",
      "title": "FileDAG: A Multi-Version Decentralized Storage Network Built on DAG-based Blockchain",
      "summary": "Decentralized Storage Networks (DSNs) can gather storage resources from mutually untrusted providers and form worldwide decentralized file systems. Compared to traditional storage networks, DSNs are built on top of blockchains, which can incentivize service providers and ensure strong security. However, existing DSNs face two major challenges. First, deduplication can only be achieved at the directory-level. Missing file-level deduplication leads to unavoidable extra storage and bandwidth cost. Second, current DSNs realize file indexing by storing extra metadata while blockchain ledgers are not fully exploited. To overcome these problems, we propose FileDAG, a DSN built on DAG-based blockchain to support file-level deduplication in storing multi-versioned files. When updating files, we adopt an increment generation method to calculate and store only the increments instead of the entire updated files. Besides, we introduce a two-layer DAG-based blockchain ledger, by which FileDAG can provide flexible and storage-saving file indexing by directly using the blockchain database without incurring extra storage overhead. We implement FileDAG and evaluate its performance with extensive experiments. The results demonstrate that FileDAG outperforms the state-of-the-art industrial DSNs considering storage cost and latency.",
      "authors": [
        "Hechuan Guo",
        "Minghui Xu",
        "Jiahao Zhang",
        "Chunchi Liu",
        "Dongxiao Yu",
        "Schahram Dustdar",
        "Xiuzhen Cheng"
      ],
      "published": "2022-12-18T14:40:52Z",
      "updated": "2022-12-18T14:40:52Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09096v1",
      "landing_url": "https://arxiv.org/abs/2212.09096v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09096"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a decentralized storage network and blockchain-based deduplication, with no discussion of discrete audio tokens or related tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a decentralized storage network and blockchain-based deduplication, with no discussion of discrete audio tokens or related tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on decentralized storage networks built on DAG-based blockchain technology and does not discuss discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio tokenization relevant to the defined inclusion criteria; therefore it is unrelated to discrete audio tokens and audio generation or modeling tasks specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on decentralized storage networks built on DAG-based blockchain technology and does not discuss discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio tokenization relevant to the defined inclusion criteria; therefore it is unrelated to discrete audio tokens and audio generation or modeling tasks specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emotion Selectable End-to-End Text-based Speech Editing",
    "abstract": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
    "metadata": {
      "arxiv_id": "2212.10191",
      "title": "Emotion Selectable End-to-End Text-based Speech Editing",
      "summary": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
      "authors": [
        "Tao Wang",
        "Jiangyan Yi",
        "Ruibo Fu",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Chu Yuan Zhang"
      ],
      "published": "2022-12-20T12:02:40Z",
      "updated": "2022-12-20T12:02:40Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.10191v1",
      "landing_url": "https://arxiv.org/abs/2212.10191v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.10191"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on emotion-controlled end-to-end text-based speech editing and does not discuss discrete audio tokens or tokenization, thus fails inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on emotion-controlled end-to-end text-based speech editing and does not discuss discrete audio tokens or tokenization, thus fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on emotional control in text-based speech editing using an end-to-end model, but it does not describe or utilize any discrete audio tokens derived from vector quantization or similar discretization methods. The methodology centers around speech editing and emotional attributes rather than tokenizing continuous audio into discrete token sequences for modeling, generation, or alignment tasks, which is the core inclusion criteria. There is no mention of discrete token vocabularies, neural audio codec-based tokenization, or evaluation of token design choices, making it not relevant to the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on emotional control in text-based speech editing using an end-to-end model, but it does not describe or utilize any discrete audio tokens derived from vector quantization or similar discretization methods. The methodology centers around speech editing and emotional attributes rather than tokenizing continuous audio into discrete token sequences for modeling, generation, or alignment tasks, which is the core inclusion criteria. There is no mention of discrete token vocabularies, neural audio codec-based tokenization, or evaluation of token design choices, making it not relevant to the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
    "abstract": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
    "metadata": {
      "arxiv_id": "2212.11444",
      "title": "Offline Clustering Approach to Self-supervised Learning for Class-imbalanced Image Data",
      "summary": "Class-imbalanced datasets are known to cause the problem of model being biased towards the majority classes. In this project, we set up two research questions: 1) when is the class-imbalance problem more prevalent in self-supervised pre-training? and 2) can offline clustering of feature representations help pre-training on class-imbalanced data? Our experiments investigate the former question by adjusting the degree of {\\it class-imbalance} when training the baseline models, namely SimCLR and SimSiam on CIFAR-10 database. To answer the latter question, we train each expert model on each subset of the feature clusters. We then distill the knowledge of expert models into a single model, so that we will be able to compare the performance of this model to our baselines.",
      "authors": [
        "Hye-min Chang",
        "Sungkyun Chang"
      ],
      "published": "2022-12-22T01:26:38Z",
      "updated": "2022-12-22T01:26:38Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.11444v1",
      "landing_url": "https://arxiv.org/abs/2212.11444v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.11444"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on offline clustering for self-supervised learning on image data (class-imbalanced CIFAR-10) and does not involve discrete audio tokens, tokenization, or audio-centered modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on offline clustering for self-supervised learning on image data (class-imbalanced CIFAR-10) and does not involve discrete audio tokens, tokenization, or audio-centered modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on offline clustering in self-supervised learning for class-imbalanced image data, without any mention of audio data, discrete audio tokens, or related neural audio codecs, vector quantization, or tokenization processes as defined in the inclusion criteria. It is therefore not relevant to the topic of discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on offline clustering in self-supervised learning for class-imbalanced image data, without any mention of audio data, discrete audio tokens, or related neural audio codecs, vector quantization, or tokenization processes as defined in the inclusion criteria. It is therefore not relevant to the topic of discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models",
    "abstract": "One-shot voice conversion (VC) aims to convert speech from any source speaker to an arbitrary target speaker with only a few seconds of reference speech from the target speaker. This relies heavily on disentangling the speaker's identity and speech content, a task that still remains challenging. Here, we propose a novel approach to learning disentangled speech representation by transfer learning from style-based text-to-speech (TTS) models. With cycle consistent and adversarial training, the style-based TTS models can perform transcription-guided one-shot VC with high fidelity and similarity. By learning an additional mel-spectrogram encoder through a teacher-student knowledge transfer and novel data augmentation scheme, our approach results in disentangled speech representation without needing the input text. The subjective evaluation shows that our approach can significantly outperform the previous state-of-the-art one-shot voice conversion models in both naturalness and similarity.",
    "metadata": {
      "arxiv_id": "2212.14227",
      "title": "StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models",
      "summary": "One-shot voice conversion (VC) aims to convert speech from any source speaker to an arbitrary target speaker with only a few seconds of reference speech from the target speaker. This relies heavily on disentangling the speaker's identity and speech content, a task that still remains challenging. Here, we propose a novel approach to learning disentangled speech representation by transfer learning from style-based text-to-speech (TTS) models. With cycle consistent and adversarial training, the style-based TTS models can perform transcription-guided one-shot VC with high fidelity and similarity. By learning an additional mel-spectrogram encoder through a teacher-student knowledge transfer and novel data augmentation scheme, our approach results in disentangled speech representation without needing the input text. The subjective evaluation shows that our approach can significantly outperform the previous state-of-the-art one-shot voice conversion models in both naturalness and similarity.",
      "authors": [
        "Yinghao Aaron Li",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2022-12-29T08:56:20Z",
      "updated": "2022-12-29T08:56:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.14227v1",
      "landing_url": "https://arxiv.org/abs/2212.14227v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.14227"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses one-shot voice conversion using disentangled representations and a mel-spectrogram encoder with knowledge transfer; it does not present or rely on discrete audio tokens, tokenizers, or codebooks, so it does not meet the discrete-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses one-shot voice conversion using disentangled representations and a mel-spectrogram encoder with knowledge transfer; it does not present or rely on discrete audio tokens, tokenizers, or codebooks, so it does not meet the discrete-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using style-based TTS models and knowledge transfer but does not mention producing or utilizing discrete audio tokens from continuous waveforms through discretization methods like neural codecs or vector quantization, nor does it describe any token vocabularies, tokenizers, or discrete token representations as required by the inclusion criteria. Therefore, it does not meet the core theme of discrete audio tokens and appears to use continuous representations instead.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using style-based TTS models and knowledge transfer but does not mention producing or utilizing discrete audio tokens from continuous waveforms through discretization methods like neural codecs or vector quantization, nor does it describe any token vocabularies, tokenizers, or discrete token representations as required by the inclusion criteria. Therefore, it does not meet the core theme of discrete audio tokens and appears to use continuous representations instead.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
    "abstract": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
    "metadata": {
      "arxiv_id": "2212.14538",
      "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
      "summary": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
      "authors": [
        "Hangyu Mao",
        "Rui Zhao",
        "Hao Chen",
        "Jianye Hao",
        "Yiqun Chen",
        "Dong Li",
        "Junge Zhang",
        "Zhen Xiao"
      ],
      "published": "2022-12-30T03:50:38Z",
      "updated": "2023-01-03T06:51:22Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.14538v2",
      "landing_url": "https://arxiv.org/abs/2212.14538v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.14538"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目專注於在深度強化學習中使用純 Transformer 作為骨幹網路的設計，與離散音訊 token 的產生、編碼或用 token 作核心建模的主題無關，因此不符合本題的離散音訊 token 納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目專注於在深度強化學習中使用純 Transformer 作為骨幹網路的設計，與離散音訊 token 的產生、編碼或用 token 作核心建模的主題無關，因此不符合本題的離散音訊 token 納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on designing Transformer-based networks for deep reinforcement learning, without any mention of discrete audio tokens, audio discretization, or neural audio codecs. It does not discuss audio processing, tokenization of audio signals, or any related audio generation or understanding tasks, which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria and falls under the exclusion criteria of being unrelated to discrete audio token research and focusing on reinforcement learning network design instead.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on designing Transformer-based networks for deep reinforcement learning, without any mention of discrete audio tokens, audio discretization, or neural audio codecs. It does not discuss audio processing, tokenization of audio signals, or any related audio generation or understanding tasks, which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria and falls under the exclusion criteria of being unrelated to discrete audio token research and focusing on reinforcement learning network design instead.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
    "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
    "metadata": {
      "arxiv_id": "2301.00591",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "authors": [
        "Amitay Sicherman",
        "Yossi Adi"
      ],
      "published": "2023-01-02T10:36:40Z",
      "updated": "2023-03-01T09:59:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00591v3",
      "landing_url": "https://arxiv.org/abs/2301.00591v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly studies discrete self-supervised speech representations (units) as tokens, analyzes their relation to phonemes, proposes an unsupervised redundancy metric, and develops methods to improve unit clustering for downstream Generative Spoken Language Modeling tasks, which aligns with the discrete audio tokens criterion and includes evaluation and code.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly studies discrete self-supervised speech representations (units) as tokens, analyzes their relation to phonemes, proposes an unsupervised redundancy metric, and develops methods to improve unit clustering for downstream Generative Spoken Language Modeling tasks, which aligns with the discrete audio tokens criterion and includes evaluation and code.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper analyzes discrete self-supervised speech representations (units) used as tokens in a generative spoken language model, including analysis of token interpretation, visualization, resynthesis, and proposes improved methods to reduce redundancy and enhance clustering robustness with evaluation on zero-resource speech metrics; it involves discrete units generated from self-supervised learning models and focuses on token design and evaluation relevant to discrete audio tokens for speech modeling, meeting inclusion criteria regarding token origin, analysis, and documentation, and is presented in English with sufficient methodological detail.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper analyzes discrete self-supervised speech representations (units) used as tokens in a generative spoken language model, including analysis of token interpretation, visualization, resynthesis, and proposes improved methods to reduce redundancy and enhance clustering robustness with evaluation on zero-resource speech metrics; it involves discrete units generated from self-supervised learning models and focuses on token design and evaluation relevant to discrete audio tokens for speech modeling, meeting inclusion criteria regarding token origin, analysis, and documentation, and is presented in English with sufficient methodological detail.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
    "abstract": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
    "metadata": {
      "arxiv_id": "2301.00652",
      "title": "Efficient Speech Representation Learning with Low-Bit Quantization",
      "summary": "With the development of hardware for machine learning, newer models often come at the cost of both increased sizes and computational complexity. In effort to improve the efficiency for these models, we apply and investigate recent quantization techniques on speech representation learning models. The quantization techniques were evaluated on the SUPERB benchmark. On the ASR task, with aggressive quantization to 1 bit, we achieved 86.32% storage reduction (184.42 -> 25.23), 88% estimated runtime reduction (1.00 -> 0.12) with increased word error rate (7.06 -> 15.96). In comparison with DistillHuBERT which also aims for model compression, the 2-bit configuration yielded slightly smaller storage (35.84 vs. 46.98), better word error rate (12.68 vs. 13.37) and more efficient estimated runtime (0.15 vs. 0.73).",
      "authors": [
        "Ching-Feng Yeh",
        "Wei-Ning Hsu",
        "Paden Tomasello",
        "Abdelrahman Mohamed"
      ],
      "published": "2022-12-14T06:09:08Z",
      "updated": "2022-12-14T06:09:08Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00652v1",
      "landing_url": "https://arxiv.org/abs/2301.00652v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.00652"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets low-bit quantization for speech representations and model compression, but does not describe discrete audio token production (tokenizers/codebooks/vocabularies) or use discrete tokens as the core representation, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets low-bit quantization for speech representations and model compression, but does not describe discrete audio token production (tokenizers/codebooks/vocabularies) or use discrete tokens as the core representation, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on quantization techniques for speech representation learning, aiming at model compression and efficiency gains; however, it does not clearly describe generating discrete audio tokens as defined by the inclusion criteria, such as discrete tokenization processes, vocabulary/codebook configurations, or using tokens as core modeling units for generation or cross-modal reasoning. Instead, it focuses on continuous model compression and runtime impacts without emphasizing discrete token sequences or their design/evaluation. Therefore, it does not sufficiently meet the inclusion criteria nor violate exclusion criteria related to discrete token definitions, leading to uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on quantization techniques for speech representation learning, aiming at model compression and efficiency gains; however, it does not clearly describe generating discrete audio tokens as defined by the inclusion criteria, such as discrete tokenization processes, vocabulary/codebook configurations, or using tokens as core modeling units for generation or cross-modal reasoning. Instead, it focuses on continuous model compression and runtime impacts without emphasizing discrete token sequences or their design/evaluation. Therefore, it does not sufficiently meet the inclusion criteria nor violate exclusion criteria related to discrete token definitions, leading to uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews noted the paper focuses on quantization/model compression of continuous speech representations rather than producing or using discrete audio token sequences; the abstract reports low-bit quantization results (storage/runtime/WER) on SUPERB but gives no details of any tokenizer/codec, codebook/vocabulary, or discrete-token-based modeling or generation. This matches the exclusion criterion (continuous representation/quantization work without discrete tokenization) and fails all inclusion requirements. (JuniorNano: exclude=1; JuniorMini: unsure=3). I therefore score: 1 (absolutely exclude).",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews noted the paper focuses on quantization/model compression of continuous speech representations rather than producing or using discrete audio token sequences; the abstract reports low-bit quantization results (storage/runtime/WER) on SUPERB but gives no details of any tokenizer/codec, codebook/vocabulary, or discrete-token-based modeling or generation. This matches the exclusion criterion (continuous representation/quantization work without discrete tokenization) and fails all inclusion requirements. (JuniorNano: exclude=1; JuniorMini: unsure=3). I therefore score: 1 (absolutely exclude).",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses discrete audio tokens produced by a neural audio codec to model TTS, treating TTS as a conditional language modeling task, which aligns with discrete-token criteria; it provides empirical results and English abstracts, and does not fall under exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses discrete audio tokens produced by a neural audio codec to model TTS, treating TTS as a conditional language modeling task, which aligns with discrete-token criteria; it provides empirical results and English abstracts, and does not fall under exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a neural codec language model (Vall-E) for TTS that employs discrete codes derived from a neural audio codec, which aligns with the inclusion criteria of using neural audio codecs to generate discrete token sequences as core representations for audio modeling. It also reports large-scale training, systematic evaluation, and application in downstream TTS generation tasks, meeting the 'More Than a Survey' focus on token design impact and empirical evaluation. The abstract provides sufficient English details on token derivation and use, supporting reproducibility assessment. There is no indication the work falls under exclusion criteria like continuous-only models or lack of discrete token generation details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a neural codec language model (Vall-E) for TTS that employs discrete codes derived from a neural audio codec, which aligns with the inclusion criteria of using neural audio codecs to generate discrete token sequences as core representations for audio modeling. It also reports large-scale training, systematic evaluation, and application in downstream TTS generation tasks, meeting the 'More Than a Survey' focus on token design impact and empirical evaluation. The abstract provides sufficient English details on token derivation and use, supporting reproducibility assessment. There is no indication the work falls under exclusion criteria like continuous-only models or lack of discrete token generation details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
    "abstract": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
    "metadata": {
      "arxiv_id": "2301.04388",
      "title": "Perceive and predict: self-supervised speech representation based loss functions for speech enhancement",
      "summary": "Recent work in the domain of speech enhancement has explored the use of self-supervised speech representations to aid in the training of neural speech enhancement models. However, much of this work focuses on using the deepest or final outputs of self supervised speech representation models, rather than the earlier feature encodings. The use of self supervised representations in such a way is often not fully motivated. In this work it is shown that the distance between the feature encodings of clean and noisy speech correlate strongly with psychoacoustically motivated measures of speech quality and intelligibility, as well as with human Mean Opinion Score (MOS) ratings. Experiments using this distance as a loss function are performed and improved performance over the use of STFT spectrogram distance based loss as well as other common loss functions from speech enhancement literature is demonstrated using objective measures such as perceptual evaluation of speech quality (PESQ) and short-time objective intelligibility (STOI).",
      "authors": [
        "George Close",
        "William Ravenscroft",
        "Thomas Hain",
        "Stefan Goetze"
      ],
      "published": "2023-01-11T10:20:56Z",
      "updated": "2023-06-26T09:31:53Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.04388v3",
      "landing_url": "https://arxiv.org/abs/2301.04388v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095666"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on self-supervised continuous speech representations and loss functions, with no mention of discrete audio tokens, tokenizers, codebooks, or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on self-supervised continuous speech representations and loss functions, with no mention of discrete audio tokens, tokenizers, codebooks, or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representations applied as loss functions for speech enhancement, without evidence of discretizing continuous audio into discrete token sequences or involving neural audio codecs or vector quantization as core methodology. It also lacks details on token vocabularies or discrete token generation relevant to the definition of \"Discrete Audio Tokens\" and their usage in downstream tasks, thus it does not meet the inclusion criteria and matches the exclusion criteria of continuous audio representation modeling without discrete token use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representations applied as loss functions for speech enhancement, without evidence of discretizing continuous audio into discrete token sequences or involving neural audio codecs or vector quantization as core methodology. It also lacks details on token vocabularies or discrete token generation relevant to the definition of \"Discrete Audio Tokens\" and their usage in downstream tasks, thus it does not meet the inclusion criteria and matches the exclusion criteria of continuous audio representation modeling without discrete token use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations",
    "abstract": "In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.",
    "metadata": {
      "arxiv_id": "2301.06052",
      "title": "T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations",
      "summary": "In this work, we investigate a simple and must-known conditional generative framework based on Vector Quantised-Variational AutoEncoder (VQ-VAE) and Generative Pre-trained Transformer (GPT) for human motion generation from textural descriptions. We show that a simple CNN-based VQ-VAE with commonly used training recipes (EMA and Code Reset) allows us to obtain high-quality discrete representations. For GPT, we incorporate a simple corruption strategy during the training to alleviate training-testing discrepancy. Despite its simplicity, our T2M-GPT shows better performance than competitive approaches, including recent diffusion-based approaches. For example, on HumanML3D, which is currently the largest dataset, we achieve comparable performance on the consistency between text and generated motion (R-Precision), but with FID 0.116 largely outperforming MotionDiffuse of 0.630. Additionally, we conduct analyses on HumanML3D and observe that the dataset size is a limitation of our approach. Our work suggests that VQ-VAE still remains a competitive approach for human motion generation.",
      "authors": [
        "Jianrong Zhang",
        "Yangsong Zhang",
        "Xiaodong Cun",
        "Shaoli Huang",
        "Yong Zhang",
        "Hongwei Zhao",
        "Hongtao Lu",
        "Xi Shen"
      ],
      "published": "2023-01-15T09:34:42Z",
      "updated": "2023-09-24T17:00:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.06052v4",
      "landing_url": "https://arxiv.org/abs/2301.06052v4",
      "doi": "https://doi.org/10.48550/arXiv.2301.06052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete representations for human motion (VQ-VAE/GPT) rather than discrete audio tokens; it does not discuss tokenizing audio signals or using discrete audio vocabularies, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete representations for human motion (VQ-VAE/GPT) rather than discrete audio tokens; it does not discuss tokenizing audio signals or using discrete audio vocabularies, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on human motion generation from textual descriptions using a VQ-VAE and GPT framework, where VQ-VAE is used to obtain discrete representations. However, these discrete representations are related to human motion, not audio tokens derived from discretizing continuous audio waveforms such as speech or music, which is the specific subject of interest. The study does not discuss discrete audio tokens or their design impact on audio generation, understanding, or cross-modal modeling. Therefore, it does not meet the inclusion criteria about discrete audio tokens and their applications in audio modeling and generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on human motion generation from textual descriptions using a VQ-VAE and GPT framework, where VQ-VAE is used to obtain discrete representations. However, these discrete representations are related to human motion, not audio tokens derived from discretizing continuous audio waveforms such as speech or music, which is the specific subject of interest. The study does not discuss discrete audio tokens or their design impact on audio generation, understanding, or cross-modal modeling. Therefore, it does not meet the inclusion criteria about discrete audio tokens and their applications in audio modeling and generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
    "abstract": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
    "metadata": {
      "arxiv_id": "2301.09027",
      "title": "Cellular Network Speech Enhancement: Removing Background and Transmission Noise",
      "summary": "The primary objective of speech enhancement is to reduce background noise while preserving the target's speech. A common dilemma occurs when a speaker is confined to a noisy environment and receives a call with high background and transmission noise. To address this problem, the Deep Noise Suppression (DNS) Challenge focuses on removing the background noise with the next-generation deep learning models to enhance the target's speech; however, researchers fail to consider Voice Over IP (VoIP) applications their transmission noise. Focusing on Google Meet and its cellular application, our work achieves state-of-the-art performance on the Google Meet To Phone Track of the VoIP DNS Challenge. This paper demonstrates how to beat industrial performance and achieve 1.92 PESQ and 0.88 STOI, as well as superior acoustic fidelity, perceptual quality, and intelligibility in various metrics.",
      "authors": [
        "Amanda Shu",
        "Hamza Khalid",
        "Haohui Liu",
        "Shikhar Agnihotri",
        "Joseph Konan",
        "Ojas Bhargave"
      ],
      "published": "2023-01-22T00:18:10Z",
      "updated": "2023-01-22T00:18:10Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09027v1",
      "landing_url": "https://arxiv.org/abs/2301.09027v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.09027"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on speech enhancement and VoIP noise reduction without discussing discrete audio tokens, token generation, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on speech enhancement and VoIP noise reduction without discussing discrete audio tokens, token generation, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The given study focuses on speech enhancement to reduce background and transmission noise in cellular networks using deep learning but does not mention any discrete audio tokens, discretization processes, or the use of neural audio codecs or self-supervised models to generate token sequences, which are the core inclusion criteria. The study centers on continuous speech signal enhancement rather than on discrete tokenization or modeling sequences as tokens, nor does it provide details on tokenizers, codebooks, or quantization methods. Therefore, it doesn't meet the inclusion criteria and falls under exclusion criteria of continuous audio representation modeling without discrete token involvement.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The given study focuses on speech enhancement to reduce background and transmission noise in cellular networks using deep learning but does not mention any discrete audio tokens, discretization processes, or the use of neural audio codecs or self-supervised models to generate token sequences, which are the core inclusion criteria. The study centers on continuous speech signal enhancement rather than on discrete tokenization or modeling sequences as tokens, nor does it provide details on tokenizers, codebooks, or quantization methods. Therefore, it doesn't meet the inclusion criteria and falls under exclusion criteria of continuous audio representation modeling without discrete token involvement.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Image Super-Resolution using Efficient Striped Window Transformer",
    "abstract": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
    "metadata": {
      "arxiv_id": "2301.09869",
      "title": "Image Super-Resolution using Efficient Striped Window Transformer",
      "summary": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
      "authors": [
        "Jinpeng Shi",
        "Hui Li",
        "Tianle Liu",
        "Yulong Liu",
        "Mingjian Zhang",
        "Jinchen Zhu",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2023-01-24T09:09:35Z",
      "updated": "2023-03-14T07:03:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09869v2",
      "landing_url": "https://arxiv.org/abs/2301.09869v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.09869"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item covers image super-resolution using a transformer and does not involve discrete audio tokens, tokenizers, or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item covers image super-resolution using a transformer and does not involve discrete audio tokens, tokenizers, or token-based representations, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on image super-resolution using a transformer-based model, dealing with visual data rather than audio waveforms or discrete audio tokens, and does not mention audio tokenization, neural audio codecs, or audio-related tasks described in the inclusion criteria; thus it does not meet the inclusion criteria and falls under exclusion criteria regarding unrelated domain and absence of discrete audio token content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on image super-resolution using a transformer-based model, dealing with visual data rather than audio waveforms or discrete audio tokens, and does not mention audio tokenization, neural audio codecs, or audio-related tasks described in the inclusion criteria; thus it does not meet the inclusion criteria and falls under exclusion criteria regarding unrelated domain and absence of discrete audio token content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
    "abstract": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
    "metadata": {
      "arxiv_id": "2301.10048",
      "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
      "summary": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
      "authors": [
        "Kaidong Zhang",
        "Jialun Peng",
        "Jingjing Fu",
        "Dong Liu"
      ],
      "published": "2023-01-24T14:44:44Z",
      "updated": "2024-03-19T04:02:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.10048v2",
      "landing_url": "https://arxiv.org/abs/2301.10048v2",
      "doi": "https://doi.org/10.1109/TPAMI.2024.3361010"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on flow-guided transformer-based video inpainting and optical flow completion, with no discussion of discrete audio tokens, tokenization, or audio token design, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on flow-guided transformer-based video inpainting and optical flow completion, with no discussion of discrete audio tokens, tokenization, or audio token design, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on video inpainting using optical flows and transformer models; it does not discuss discrete audio tokens, neural audio codecs, or discrete tokenization related to audio processing as required by the inclusion criteria. It is unrelated to discrete audio token generation or usage, and deals with video rather than audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on video inpainting using optical flows and transformer models; it does not discuss discrete audio tokens, neural audio codecs, or discrete tokenization related to audio processing as required by the inclusion criteria. It is unrelated to discrete audio token generation or usage, and deals with video rather than audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
    "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
    "metadata": {
      "arxiv_id": "2301.13662",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Chao Weng",
        "Helen Meng"
      ],
      "published": "2023-01-31T14:26:52Z",
      "updated": "2023-06-25T11:42:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13662v2",
      "landing_url": "https://arxiv.org/abs/2301.13662v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.13662"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study centers on discrete audio tokens (vector-quantized tokens) and a discrete latent-space model for expressive TTS, with English-language abstract and methods, satisfying the discrete-token inclusion criteria and not falling under exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study centers on discrete audio tokens (vector-quantized tokens) and a discrete latent-space model for expressive TTS, with English-language abstract and methods, satisfying the discrete-token inclusion criteria and not falling under exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than continuous mel spectrograms, indicating it uses discrete audio tokens generated via vector quantization, which aligns with the inclusion criteria. The paper details tokenization through neural audio codec techniques and applies these tokens for expressive TTS, providing English title and abstract with methodological details. Therefore, it meets the inclusion criteria for discrete token use in audio generation and model evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than continuous mel spectrograms, indicating it uses discrete audio tokens generated via vector quantization, which aligns with the inclusion criteria. The paper details tokenization through neural audio codec techniques and applies these tokens for expressive TTS, providing English title and abstract with methodological details. Therefore, it meets the inclusion criteria for discrete token use in audio generation and model evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generating Subsurface Earth Models using Discrete Representation Learning and Deep Autoregressive Network",
    "abstract": "Subsurface earth models (referred to as geo-models) are crucial for characterizing complex subsurface systems. Multiple-point statistics are commonly used to generate geo-models. In this paper, a deep-learning-based generative method is developed as an alternative to the traditional Geomodel generation procedure. The generative method comprises two deep-learning models, namely the hierarchical vector-quantized variational autoencoder (VQ-VAE-2) and PixelSNAIL autoregressive model. Based on the principle of neural discrete representation learning, the VQ-VAE-2 learns to massively compress the Geomodels to extract the low-dimensional, discrete latent representation corresponding to each Geomodel. Following that, PixelSNAIL uses the deep autoregressive network to learn the prior distribution of the latent codes. For the purpose of Geomodel generation, PixelSNAIL samples from the newly learned prior distribution of latent codes, and then the decoder of the VQ-VAE-2 converts the newly sampled latent code to a newly constructed geo-model. PixelSNAIL can be used for unconditional or conditional geo-model generation. In an unconditional generation, the generative workflow generates an ensemble of geo-models without any constraint. On the other hand, in the conditional geo-model generation, the generative workflow generates an ensemble of geo-models similar to a user-defined source image, which ultimately facilitates the control and manipulation of the generated geo-models. To better construct the fluvial channels in the geo-models, the perceptual loss is implemented in the VQ-VAE-2 model instead of the traditional mean squared error loss. At a specific compression ratio, the quality of multi-attribute geo-model generation is better than that of single-attribute geo-model generation.",
    "metadata": {
      "arxiv_id": "2302.02594",
      "title": "Generating Subsurface Earth Models using Discrete Representation Learning and Deep Autoregressive Network",
      "summary": "Subsurface earth models (referred to as geo-models) are crucial for characterizing complex subsurface systems. Multiple-point statistics are commonly used to generate geo-models. In this paper, a deep-learning-based generative method is developed as an alternative to the traditional Geomodel generation procedure. The generative method comprises two deep-learning models, namely the hierarchical vector-quantized variational autoencoder (VQ-VAE-2) and PixelSNAIL autoregressive model. Based on the principle of neural discrete representation learning, the VQ-VAE-2 learns to massively compress the Geomodels to extract the low-dimensional, discrete latent representation corresponding to each Geomodel. Following that, PixelSNAIL uses the deep autoregressive network to learn the prior distribution of the latent codes. For the purpose of Geomodel generation, PixelSNAIL samples from the newly learned prior distribution of latent codes, and then the decoder of the VQ-VAE-2 converts the newly sampled latent code to a newly constructed geo-model. PixelSNAIL can be used for unconditional or conditional geo-model generation. In an unconditional generation, the generative workflow generates an ensemble of geo-models without any constraint. On the other hand, in the conditional geo-model generation, the generative workflow generates an ensemble of geo-models similar to a user-defined source image, which ultimately facilitates the control and manipulation of the generated geo-models. To better construct the fluvial channels in the geo-models, the perceptual loss is implemented in the VQ-VAE-2 model instead of the traditional mean squared error loss. At a specific compression ratio, the quality of multi-attribute geo-model generation is better than that of single-attribute geo-model generation.",
      "authors": [
        "Jungang Chen",
        "Chung-Kan Huang",
        "Jose F. Delgado",
        "Siddharth Misra"
      ],
      "published": "2023-02-06T07:01:21Z",
      "updated": "2023-02-06T07:01:21Z",
      "categories": [
        "physics.geo-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.02594v1",
      "landing_url": "https://arxiv.org/abs/2302.02594v1",
      "doi": "https://doi.org/10.1007/s10596-023-10243-0"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on generating subsurface geological models using discrete representations (VQ-VAE-2 and PixelSNAIL) for geoscience data, not discrete audio tokens or audio-domain modeling, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on generating subsurface geological models using discrete representations (VQ-VAE-2 and PixelSNAIL) for geoscience data, not discrete audio tokens or audio-domain modeling, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generating subsurface earth models using deep discrete representation learning, but it centers on geological modeling rather than audio signals; it does not address discretizing continuous audio waveforms into tokens for audio sequence modeling, nor does it discuss audio codec architectures or tokenizers relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on audio-related discrete token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generating subsurface earth models using deep discrete representation learning, but it centers on geological modeling rather than audio signals; it does not address discretizing continuous audio waveforms into tokens for audio sequence modeling, nor does it discuss audio codec architectures or tokenizers relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on audio-related discrete token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
    "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
    "metadata": {
      "arxiv_id": "2302.03540",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2023-02-07T15:48:31Z",
      "updated": "2023-02-07T15:48:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.03540v1",
      "landing_url": "https://arxiv.org/abs/2302.03540v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.03540"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete speech representations and a token-based decomposition of TTS into semantic and acoustic tokens, aligning with the discrete audio token theme; it’s in English and targets token-based generation/analysis without evident exclusionary aspects.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete speech representations and a token-based decomposition of TTS into semantic and acoustic tokens, aligning with the discrete audio token theme; it’s in English and targets token-based generation/analysis without evident exclusionary aspects.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a multi-speaker TTS system that explicitly utilizes two types of discrete speech representations (semantic and acoustic tokens) for modeling, which aligns with the inclusion criteria of using discrete audio tokens produced by neural audio codecs or self-supervised models; it details token usage and training methods, focuses on audio generation tasks, provides an English title and abstract, and includes empirical evaluations, thus meeting the defined inclusion while avoiding exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a multi-speaker TTS system that explicitly utilizes two types of discrete speech representations (semantic and acoustic tokens) for modeling, which aligns with the inclusion criteria of using discrete audio tokens produced by neural audio codecs or self-supervised models; it details token usage and training methods, focuses on audio generation tasks, provides an English title and abstract, and includes empirical evaluations, thus meeting the defined inclusion while avoiding exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
    "abstract": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
    "metadata": {
      "arxiv_id": "2302.04215",
      "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
      "summary": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
      "authors": [
        "Li-Wei Chen",
        "Shinji Watanabe",
        "Alexander Rudnicky"
      ],
      "published": "2023-02-08T17:34:32Z",
      "updated": "2023-02-08T17:34:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04215v1",
      "landing_url": "https://arxiv.org/abs/2302.04215v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.04215"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses learned discrete codes within multiple code groups (vector-quantized tokens) as the core representation for speech, employing discrete audio tokens for TTS built on real-world spontaneous data, and provides architecture, training details, and evaluation—satisfying the criteria for discrete audio tokens and their impact on downstream speech generation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses learned discrete codes within multiple code groups (vector-quantized tokens) as the core representation for speech, employing discrete audio tokens for TTS built on real-world spontaneous data, and provides architecture, training details, and evaluation—satisfying the criteria for discrete audio tokens and their impact on downstream speech generation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on TTS utilizing vector quantization to address issues in mel-spectrogram autoregressive models, implying the use of discrete codes as tokens and a multi-codebook structure; however, the abstract lacks explicit details on the discrete token generation process (e.g., codebook design, quantization specifics), making it hard to fully confirm compliance with all inclusion criteria related to discrete audio token definition and reproducibility; nevertheless, it fits the category of using discrete audio tokens in speech synthesis, which is central to the theme.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on TTS utilizing vector quantization to address issues in mel-spectrogram autoregressive models, implying the use of discrete codes as tokens and a multi-codebook structure; however, the abstract lacks explicit details on the discrete token generation process (e.g., codebook design, quantization specifics), making it hard to fully confirm compliance with all inclusion criteria related to discrete audio token definition and reproducibility; nevertheless, it fits the category of using discrete audio tokens in speech synthesis, which is central to the theme.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
    "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
    "metadata": {
      "arxiv_id": "2302.05406",
      "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
      "summary": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
      "authors": [
        "Pedro Colon-Hernandez",
        "Henry Lieberman",
        "Yida Xin",
        "Claire Yin",
        "Cynthia Breazeal",
        "Peter Chin"
      ],
      "published": "2023-02-10T18:21:13Z",
      "updated": "2023-02-10T18:21:13Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05406v1",
      "landing_url": "https://arxiv.org/abs/2302.05406v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05406"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on transformer-based contextual commonsense inference with prompting and GANs, evaluated over text-based knowledge graphs; there is no discussion of discrete audio tokens, tokenization, codecs, or audio-domain modeling, so it does not meet the ‘Discrete Audio Tokens’ scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on transformer-based contextual commonsense inference with prompting and GANs, evaluated over text-based knowledge graphs; there is no discussion of discrete audio tokens, tokenization, codecs, or audio-domain modeling, so it does not meet the ‘Discrete Audio Tokens’ scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a transformer model for contextual commonsense inference in natural language, using facts and knowledge graphs such as ConceptNet. It does not discuss any form of discrete audio tokens, neural audio codecs, or audio signal discretization, nor does it address any audio modeling, generation, or cross-modal alignment involving audio tokens. Therefore, it does not meet the inclusion criteria related to discrete audio tokens or audio representation, and falls under exclusion criteria for lacking discrete token methodology applied to audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a transformer model for contextual commonsense inference in natural language, using facts and knowledge graphs such as ConceptNet. It does not discuss any form of discrete audio tokens, neural audio codecs, or audio signal discretization, nor does it address any audio modeling, generation, or cross-modal alignment involving audio tokens. Therefore, it does not meet the inclusion criteria related to discrete audio tokens or audio representation, and falls under exclusion criteria for lacking discrete token methodology applied to audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
    "abstract": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
    "metadata": {
      "arxiv_id": "2302.05756",
      "title": "Improved Decoding of Attentional Selection in Multi-Talker Environments with Self-Supervised Learned Speech Representation",
      "summary": "Auditory attention decoding (AAD) is a technique used to identify and amplify the talker that a listener is focused on in a noisy environment. This is done by comparing the listener's brainwaves to a representation of all the sound sources to find the closest match. The representation is typically the waveform or spectrogram of the sounds. The effectiveness of these representations for AAD is uncertain. In this study, we examined the use of self-supervised learned speech representation in improving the accuracy and speed of AAD. We recorded the brain activity of three subjects using invasive electrocorticography (ECoG) as they listened to two conversations and focused on one. We used WavLM to extract a latent representation of each talker and trained a spatiotemporal filter to map brain activity to intermediate representations of speech. During the evaluation, the reconstructed representation is compared to each speaker's representation to determine the target speaker. Our results indicate that speech representation from WavLM provides better decoding accuracy and speed than the speech envelope and spectrogram. Our findings demonstrate the advantages of self-supervised learned speech representation for auditory attention decoding and pave the way for developing brain-controlled hearable technologies.",
      "authors": [
        "Cong Han",
        "Vishal Choudhari",
        "Yinghao Aaron Li",
        "Nima Mesgarani"
      ],
      "published": "2023-02-11T18:33:42Z",
      "updated": "2023-02-11T18:33:42Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05756v1",
      "landing_url": "https://arxiv.org/abs/2302.05756v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05756"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses self-supervised speech representations, the work does not generate or analyze discrete audio tokens or token-based representations and focuses on continuous latent representations mapped from brain activity, so it does not meet the token/discretization criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it uses self-supervised speech representations, the work does not generate or analyze discrete audio tokens or token-based representations and focuses on continuous latent representations mapped from brain activity, so it does not meet the token/discretization criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on auditory attention decoding using brain activity correlated with continuous speech representations like spectrograms and a self-supervised learned speech representation (WavLM). However, there is no indication that these speech representations are discretized into a finite vocabulary of tokens or that any form of vector quantization or discrete tokenization process has been applied or analyzed. The abstract emphasizes continuous latent representations rather than discrete token sequences, and there is no discussion of tokenizer architecture, codebooks, quantization methods, or token design choices, which are key inclusion criteria. Therefore, despite being a speech representation study, it does not meet the inclusion requirements related to discrete audio tokens as defined, nor does it satisfy the detailed English documentation of discrete token generation necessary for reproducibility evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on auditory attention decoding using brain activity correlated with continuous speech representations like spectrograms and a self-supervised learned speech representation (WavLM). However, there is no indication that these speech representations are discretized into a finite vocabulary of tokens or that any form of vector quantization or discrete tokenization process has been applied or analyzed. The abstract emphasizes continuous latent representations rather than discrete token sequences, and there is no discussion of tokenizer architecture, codebooks, quantization methods, or token design choices, which are key inclusion criteria. Therefore, despite being a speech representation study, it does not meet the inclusion requirements related to discrete audio tokens as defined, nor does it satisfy the detailed English documentation of discrete token generation necessary for reproducibility evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantized Wasserstein Auto-Encoder",
    "abstract": "Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.",
    "metadata": {
      "arxiv_id": "2302.05917",
      "title": "Vector Quantized Wasserstein Auto-Encoder",
      "summary": "Learning deep discrete latent presentations offers a promise of better symbolic and summarized abstractions that are more useful to subsequent downstream tasks. Inspired by the seminal Vector Quantized Variational Auto-Encoder (VQ-VAE), most of work in learning deep discrete representations has mainly focused on improving the original VQ-VAE form and none of them has studied learning deep discrete representations from the generative viewpoint. In this work, we study learning deep discrete representations from the generative viewpoint. Specifically, we endow discrete distributions over sequences of codewords and learn a deterministic decoder that transports the distribution over the sequences of codewords to the data distribution via minimizing a WS distance between them. We develop further theories to connect it with the clustering viewpoint of WS distance, allowing us to have a better and more controllable clustering solution. Finally, we empirically evaluate our method on several well-known benchmarks, where it achieves better qualitative and quantitative performances than the other VQ-VAE variants in terms of the codebook utilization and image reconstruction/generation.",
      "authors": [
        "Tung-Long Vuong",
        "Trung Le",
        "He Zhao",
        "Chuanxia Zheng",
        "Mehrtash Harandi",
        "Jianfei Cai",
        "Dinh Phung"
      ],
      "published": "2023-02-12T13:51:36Z",
      "updated": "2023-06-17T06:52:21Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05917v2",
      "landing_url": "https://arxiv.org/abs/2302.05917v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.05917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This article studies discrete latent representations via a vector-quantized Wasserstein auto-encoder and evaluates on image benchmarks, not on discrete audio tokens or audio-domain tokenization/generation, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This article studies discrete latent representations via a vector-quantized Wasserstein auto-encoder and evaluates on image benchmarks, not on discrete audio tokens or audio-domain tokenization/generation, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning deep discrete representations via vector quantized auto-encoders but does not explicitly describe producing discrete audio tokens from continuous audio waveforms, nor does it provide specific details on tokenizer/codec architecture or codebook settings related to neural audio codecs for discrete audio token generation; thus, it does not clearly meet the inclusion criteria centered on discrete audio tokens for audio modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning deep discrete representations via vector quantized auto-encoders but does not explicitly describe producing discrete audio tokens from continuous audio waveforms, nor does it provide specific details on tokenizer/codec architecture or codebook settings related to neural audio codecs for discrete audio token generation; thus, it does not clearly meet the inclusion criteria centered on discrete audio tokens for audio modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
    "abstract": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
    "metadata": {
      "arxiv_id": "2302.08137",
      "title": "ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations",
      "summary": "In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Jocelyn Huang",
        "Jason Li",
        "Boris Ginsburg"
      ],
      "published": "2023-02-16T08:10:41Z",
      "updated": "2023-02-16T08:10:41Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08137v1",
      "landing_url": "https://arxiv.org/abs/2302.08137v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08137"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on zero-shot voice conversion using self-supervised continuous representations without mention of discrete audio tokens, tokenization, or neural codecs, so it fails the core 'Discrete Audio Tokens' criteria and should be excluded according to the exclusion rules.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on zero-shot voice conversion using self-supervised continuous representations without mention of discrete audio tokens, tokenization, or neural codecs, so it fails the core 'Discrete Audio Tokens' criteria and should be excluded according to the exclusion rules.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a zero-shot voice conversion method using self-supervised speech representations but does not explicitly mention the use or generation of discrete audio tokens, token vocabularies, quantization, or discrete token sequences as core components; instead, it focuses on disentangled continuous representations and synthesis models. Therefore, it does not meet the inclusion criteria of using discrete audio tokens as core representations for modeling or generation, nor provides clear details on tokenizer architectures, codebooks, or quantization methods required for reproducibility and assessment.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a zero-shot voice conversion method using self-supervised speech representations but does not explicitly mention the use or generation of discrete audio tokens, token vocabularies, quantization, or discrete token sequences as core components; instead, it focuses on disentangled continuous representations and synthesis models. Therefore, it does not meet the inclusion criteria of using discrete audio tokens as core representations for modeling or generation, nor provides clear details on tokenizer architectures, codebooks, or quantization methods required for reproducibility and assessment.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speech Enhancement with Multi-granularity Vector Quantization",
    "abstract": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
    "metadata": {
      "arxiv_id": "2302.08342",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "authors": [
        "Xiao-Ying Zhao",
        "Qiu-Shi Zhu",
        "Jie Zhang"
      ],
      "published": "2023-02-16T14:53:41Z",
      "updated": "2023-02-16T14:53:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08342v1",
      "landing_url": "https://arxiv.org/abs/2302.08342v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08342"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses vector quantization to discretize speech representations into tokens (multi-codebook, multi-granularity), investigates token-level features within a speech enhancement framework, and provides English abstract with methodological details, fitting the discrete audio token criteria and not falling under exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses vector quantization to discretize speech representations into tokens (multi-codebook, multi-granularity), investigates token-level features within a speech enhancement framework, and provides English abstract with methodological details, fitting the discrete audio token criteria and not falling under exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement using multi-granularity vector quantization (VQ) modules to discretize speech representations, indicating the generation of discrete tokens from continuous speech waveforms. It discusses the use of multiple codebooks for tokenization, attention mechanisms integrating pre-trained models, and experimental evaluation. Although it emphasizes improving speech enhancement performance, the paper provides details about the discrete token generation mechanism (VQ with multiple codebooks) and aligns with the discrete audio tokens theme. It does not solely treat continuous representations, uses discrete tokens as core input, and is in English with adequate detail. Therefore, it meets inclusion criteria and lacks exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement using multi-granularity vector quantization (VQ) modules to discretize speech representations, indicating the generation of discrete tokens from continuous speech waveforms. It discusses the use of multiple codebooks for tokenization, attention mechanisms integrating pre-trained models, and experimental evaluation. Although it emphasizes improving speech enhancement performance, the paper provides details about the discrete token generation mechanism (VQ with multiple codebooks) and aligns with the discrete audio tokens theme. It does not solely treat continuous representations, uses discrete tokens as core input, and is in English with adequate detail. Therefore, it meets inclusion criteria and lacks exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
    "abstract": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
    "metadata": {
      "arxiv_id": "2302.10287",
      "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
      "summary": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
      "authors": [
        "Kavya Gupta",
        "Sagar Verma"
      ],
      "published": "2023-02-01T06:09:19Z",
      "updated": "2023-02-01T06:09:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10287v1",
      "landing_url": "https://arxiv.org/abs/2302.10287v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.10287"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on certified robustness of Vision Transformers and Lipschitz bounds, not on discrete audio tokens, tokenization, or audio-domain modeling, thus it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on certified robustness of Vision Transformers and Lipschitz bounds, not on discrete audio tokens, tokenization, or audio-domain modeling, thus it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on certified robustness of pre-trained vision transformers using Lipschitz bounding techniques and does not address discrete audio tokens or the discretization of continuous audio waveforms into tokens; therefore, it does not meet the inclusion criteria related to audio tokenization and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on certified robustness of pre-trained vision transformers using Lipschitz bounding techniques and does not address discrete audio tokens or the discretization of continuous audio waveforms into tokens; therefore, it does not meet the inclusion criteria related to audio tokenization and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
    "abstract": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
    "metadata": {
      "arxiv_id": "2302.12434",
      "title": "Catch You and I Can: Revealing Source Voiceprint Against Voice Conversion",
      "summary": "Voice conversion (VC) techniques can be abused by malicious parties to transform their audios to sound like a target speaker, making it hard for a human being or a speaker verification/identification system to trace the source speaker. In this paper, we make the first attempt to restore the source voiceprint from audios synthesized by voice conversion methods with high credit. However, unveiling the features of the source speaker from a converted audio is challenging since the voice conversion operation intends to disentangle the original features and infuse the features of the target speaker. To fulfill our goal, we develop Revelio, a representation learning model, which learns to effectively extract the voiceprint of the source speaker from converted audio samples. We equip Revelio with a carefully-designed differential rectification algorithm to eliminate the influence of the target speaker by removing the representation component that is parallel to the voiceprint of the target speaker. We have conducted extensive experiments to evaluate the capability of Revelio in restoring voiceprint from audios converted by VQVC, VQVC+, AGAIN, and BNE. The experiments verify that Revelio is able to rebuild voiceprints that can be traced to the source speaker by speaker verification and identification systems. Revelio also exhibits robust performance under inter-gender conversion, unseen languages, and telephony networks.",
      "authors": [
        "Jiangyi Deng",
        "Yanjiao Chen",
        "Yinan Zhong",
        "Qianhao Miao",
        "Xueluan Gong",
        "Wenyuan Xu"
      ],
      "published": "2023-02-24T03:33:13Z",
      "updated": "2023-02-24T03:33:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.12434v1",
      "landing_url": "https://arxiv.org/abs/2302.12434v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.12434"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets recovering source voiceprints from voice-converted audio using representation learning; it does not discuss discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets recovering source voiceprints from voice-converted audio using representation learning; it does not discuss discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on revealing the source voiceprint from voice conversion outputs using a representation learning model. It does not discuss the discretization of continuous audio waveforms into discrete tokens, neural audio codecs, vector quantization or the design/evaluation of discrete audio tokens for generative or cross-modal modeling tasks. Therefore, it does not meet the inclusion criteria relating to discrete audio token approaches and lacks description of token generation processes or vocab/codebook settings.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on revealing the source voiceprint from voice conversion outputs using a representation learning model. It does not discuss the discretization of continuous audio waveforms into discrete tokens, neural audio codecs, vector quantization or the design/evaluation of discrete audio tokens for generative or cross-modal modeling tasks. Therefore, it does not meet the inclusion criteria relating to discrete audio token approaches and lacks description of token generation processes or vocab/codebook settings.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A low latency attention module for streaming self-supervised speech representation learning",
    "abstract": "The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the latency build-up problem of other streaming attention architectures, such as the masked acausal attention (MAA), guaranteeing a latency equal to one layer even when multiple layers are stacked. We present a comparative analysis between the vanilla attention, which we will refer here as acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with automatic speech recognition as downstream task. When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%). Our implementation also reduces the inference latency from 1.92 to 0.16 seconds. The proposed low-latency module preserves many of the benefits of conventional acausal transformers, but also enables latency characteristics that make it applicable to real-time streaming applications.",
    "metadata": {
      "arxiv_id": "2302.13451",
      "title": "A low latency attention module for streaming self-supervised speech representation learning",
      "summary": "The transformer is a fundamental building block in deep learning, and the attention mechanism is the transformer's core component. Self-supervised speech representation learning (SSRL) represents a popular use-case for the transformer architecture. Due to transformers' acausal behavior, the use of transformers for SSRL has been predominantly focused on acausal applications. However, several media processing problems, such as speech processing, require real-time solutions. In this paper, we present an implementation of the attention module that enables training of SSRL architectures with low compute and memory requirements, while allowing real-time inference with low and fixed latency. The attention module proposed in this paper includes two components, streaming attention (SA) and low-latency streaming attention (LLSA). The SA represents our proposal for an efficient streaming SSRL implementation, while the LLSA solves the latency build-up problem of other streaming attention architectures, such as the masked acausal attention (MAA), guaranteeing a latency equal to one layer even when multiple layers are stacked. We present a comparative analysis between the vanilla attention, which we will refer here as acausal attention (AA), the SA, and the LLSA, by training a streaming SSRL with automatic speech recognition as downstream task. When training on librispeech-clean-100 and testing on librispeech-test-clean, our low-latency attention module has a word error rate (WER) of 5.84%, which represents a significant improvement over the MAA (WER = 13.82%). Our implementation also reduces the inference latency from 1.92 to 0.16 seconds. The proposed low-latency module preserves many of the benefits of conventional acausal transformers, but also enables latency characteristics that make it applicable to real-time streaming applications.",
      "authors": [
        "Jianbo Ma",
        "Siqi Pan",
        "Deepak Chandran",
        "Andrea Fanelli",
        "Richard Cartwright"
      ],
      "published": "2023-02-27T00:44:22Z",
      "updated": "2024-03-18T01:09:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.13451v2",
      "landing_url": "https://arxiv.org/abs/2302.13451v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.13451"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on low-latency streaming attention for self-supervised speech representation learning and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on low-latency streaming attention for self-supervised speech representation learning and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on low latency attention modules for streaming self-supervised speech representation learning but does not discuss or describe the discretization of continuous audio waveforms into discrete token sequences, nor does it provide details on tokenizers, codebooks, or quantization processes central to discrete audio tokens; rather, it concentrates on improving transformer architectures for real-time speech processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on low latency attention modules for streaming self-supervised speech representation learning but does not discuss or describe the discretization of continuous audio waveforms into discrete token sequences, nor does it provide details on tokenizers, codebooks, or quantization processes central to discrete audio tokens; rather, it concentrates on improving transformer architectures for real-time speech processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
    "abstract": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
    "metadata": {
      "arxiv_id": "2302.13458",
      "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
      "summary": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
      "authors": [
        "Yoonhyung Lee",
        "Jinhyeok Yang",
        "Kyomin Jung"
      ],
      "published": "2023-02-27T01:12:19Z",
      "updated": "2023-02-27T01:12:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.13458v1",
      "landing_url": "https://arxiv.org/abs/2302.13458v1",
      "doi": "https://doi.org/10.1109/ICASSP43922.2022.9747050"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on variance control and normalizing-flow-based TTS without introducing discrete audio tokens, tokenizers, or any discrete token analysis, thus it does not meet inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on variance control and normalizing-flow-based TTS without introducing discrete audio tokens, tokenizers, or any discrete token analysis, thus it does not meet inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a non-autoregressive text-to-speech model called VarianceFlow that improves speech quality and controllability by modeling variance information using normalizing flow. However, there is no mention or indication that the method uses discrete audio tokens such as neural audio codecs or quantized token sequences as core representations, nor does it describe any discretization process or token vocabulary design. The focus is on modeling continuous acoustic features (variance information such as pitch or energy) rather than any discrete token-based representation. Therefore, it does not satisfy the inclusion criteria requiring research on discrete audio tokens for audio generation or downstream tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a non-autoregressive text-to-speech model called VarianceFlow that improves speech quality and controllability by modeling variance information using normalizing flow. However, there is no mention or indication that the method uses discrete audio tokens such as neural audio codecs or quantized token sequences as core representations, nor does it describe any discretization process or token vocabulary design. The focus is on modeling continuous acoustic features (variance information such as pitch or energy) rather than any discrete token-based representation. Therefore, it does not satisfy the inclusion criteria requiring research on discrete audio tokens for audio generation or downstream tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Full Stack Optimization of Transformer Inference: a Survey",
    "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
    "metadata": {
      "arxiv_id": "2302.14017",
      "title": "Full Stack Optimization of Transformer Inference: a Survey",
      "summary": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
      "authors": [
        "Sehoon Kim",
        "Coleman Hooper",
        "Thanakul Wattanawong",
        "Minwoo Kang",
        "Ruohan Yan",
        "Hasan Genc",
        "Grace Dinh",
        "Qijing Huang",
        "Kurt Keutzer",
        "Michael W. Mahoney",
        "Yakun Sophia Shao",
        "Amir Gholami"
      ],
      "published": "2023-02-27T18:18:13Z",
      "updated": "2023-02-27T18:18:13Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14017v1",
      "landing_url": "https://arxiv.org/abs/2302.14017v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.14017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a survey on Transformer inference optimization and hardware deployment with no discussion of discrete audio tokens, tokenization, or related token design, so it does not meet the inclusion criteria and does not trigger any exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a survey on Transformer inference optimization and hardware deployment with no discussion of discrete audio tokens, tokenization, or related token design, so it does not meet the inclusion criteria and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper surveys optimization techniques for Transformer inference acceleration but does not address discrete audio tokens, their generation, or application in audio tokenization, modeling, or multimodal integration, which are core to the inclusion criteria; thus, it does not meet the topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper surveys optimization techniques for Transformer inference acceleration but does not address discrete audio tokens, their generation, or application in audio tokenization, modeling, or multimodal integration, which are core to the inclusion criteria; thus, it does not meet the topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "abstract": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
    "metadata": {
      "arxiv_id": "2302.14337",
      "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
      "summary": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2023-02-28T06:05:43Z",
      "updated": "2023-05-19T02:43:32Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14337v2",
      "landing_url": "https://arxiv.org/abs/2302.14337v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14337"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a unified facial landmark generator from text or speech and does not address discrete audio tokens, tokenization, or token-based modeling; it does not meet the inclusion criteria and aligns with the exclusion criteria regarding lack of discrete token discussion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a unified facial landmark generator from text or speech and does not address discrete audio tokens, tokenization, or token-based modeling; it does not meet the inclusion criteria and aligns with the exclusion criteria regarding lack of discrete token discussion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating facial landmarks from text or speech, using an end-to-end text-to-speech system that extracts latent representations common to both modalities to generate facial landmarks. However, it does not discuss or provide details about discretizing continuous audio into discrete token sequences, neural audio codecs, vector quantization, or similar discrete audio token representations, nor does it focus on using discrete tokens for audio generation, understanding, alignment, or cross-modal reasoning. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and fails to provide enough information to determine any discrete token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating facial landmarks from text or speech, using an end-to-end text-to-speech system that extracts latent representations common to both modalities to generate facial landmarks. However, it does not discuss or provide details about discretizing continuous audio into discrete token sequences, neural audio codecs, vector quantization, or similar discrete audio token representations, nor does it focus on using discrete tokens for audio generation, understanding, alignment, or cross-modal reasoning. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and fails to provide enough information to determine any discrete token design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Token-Wise Beam Search Algorithm for RNN-T",
    "abstract": "Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.",
    "metadata": {
      "arxiv_id": "2302.14357",
      "title": "A Token-Wise Beam Search Algorithm for RNN-T",
      "summary": "Standard Recurrent Neural Network Transducers (RNN-T) decoding algorithms for speech recognition are iterating over the time axis, such that one time step is decoded before moving on to the next time step. Those algorithms result in a large number of calls to the joint network, which were shown in previous work to be an important factor that reduces decoding speed. We present a decoding beam search algorithm that batches the joint network calls across a segment of time steps, which results in 20%-96% decoding speedups consistently across all models and settings experimented with. In addition, aggregating emission probabilities over a segment may be seen as a better approximation to finding the most likely model output, causing our algorithm to improve oracle word error rate by up to 11% relative as the segment size increases, and to slightly improve general word error rate.",
      "authors": [
        "Gil Keren"
      ],
      "published": "2023-02-28T07:20:49Z",
      "updated": "2023-10-05T22:34:13Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14357v2",
      "landing_url": "https://arxiv.org/abs/2302.14357v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14357"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on decoding speed and a token-wise beam search for RNN-T but does not address discrete audio token generation, tokenization, or token design (no token vocabulary/codec setup or evaluation of token discreteness), so it fails inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on decoding speed and a token-wise beam search for RNN-T but does not address discrete audio token generation, tokenization, or token design (no token vocabulary/codec setup or evaluation of token discreteness), so it fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a new beam search decoding algorithm for RNN-T in speech recognition aiming at improving decoding speed and error rates, but it does not discuss or propose any discrete audio tokenization, neural audio codec architectures, quantization, or vocabulary/codebook settings related to discrete token generation or utilization; thus, it does not meet the inclusion criteria centered around discrete audio tokens and their role in generative or multimodal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a new beam search decoding algorithm for RNN-T in speech recognition aiming at improving decoding speed and error rates, but it does not discuss or propose any discrete audio tokenization, neural audio codec architectures, quantization, or vocabulary/codebook settings related to discrete token generation or utilization; thus, it does not meet the inclusion criteria centered around discrete audio tokens and their role in generative or multimodal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
    "abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
    "metadata": {
      "arxiv_id": "2303.00957",
      "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "summary": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
      "authors": [
        "Changyeon Kim",
        "Jongjin Park",
        "Jinwoo Shin",
        "Honglak Lee",
        "Pieter Abbeel",
        "Kimin Lee"
      ],
      "published": "2023-03-02T04:24:29Z",
      "updated": "2023-03-02T04:24:29Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.00957v1",
      "landing_url": "https://arxiv.org/abs/2303.00957v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.00957"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses preference-based reinforcement learning with transformers and non-Markovian rewards, not discrete audio token generation/tokenization, thus it does not meet the ‘Discrete Audio Tokens’ inclusion criteria and fails the exclusion criteria related to lacking discrete token design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses preference-based reinforcement learning with transformers and non-Markovian rewards, not discrete audio token generation/tokenization, thus it does not meet the ‘Discrete Audio Tokens’ inclusion criteria and fails the exclusion criteria related to lacking discrete token design or usage.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on preference-based reinforcement learning using transformers for modeling human preferences and does not mention discrete audio tokens, discretization of audio waveforms, or audio token sequences as defined in the inclusion criteria. The abstract lacks any description of audio tokenization, neural audio codecs, or related audio representation techniques, and primarily addresses RL and preference modeling unrelated to audio tokenization or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on preference-based reinforcement learning using transformers for modeling human preferences and does not mention discrete audio tokens, discretization of audio waveforms, or audio token sequences as defined in the inclusion criteria. The abstract lacks any description of audio tokenization, neural audio codecs, or related audio representation techniques, and primarily addresses RL and preference modeling unrelated to audio tokenization or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations",
    "abstract": "We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models using only a fraction of paired data as latter.",
    "metadata": {
      "arxiv_id": "2303.01261",
      "title": "ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations",
      "summary": "We present ParrotTTS, a modularized text-to-speech synthesis model leveraging disentangled self-supervised speech representations. It can train a multi-speaker variant effectively using transcripts from a single speaker. ParrotTTS adapts to a new language in low resource setup and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on bilingual or parallel examples, ParrotTTS can transfer voices across languages while preserving the speaker specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results in monolingual and multi-lingual scenarios. ParrotTTS outperforms state-of-the-art multi-lingual TTS models using only a fraction of paired data as latter.",
      "authors": [
        "Neil Shah",
        "Saiteja Kosgi",
        "Vishal Tambrahalli",
        "Neha Sahipjohn",
        "Niranjan Pedanekar",
        "Vineet Gandhi"
      ],
      "published": "2023-03-01T17:23:12Z",
      "updated": "2023-12-17T00:06:16Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.01261v3",
      "landing_url": "https://arxiv.org/abs/2303.01261v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.01261"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on TTS using self-supervised representations but does not specify discrete audio tokens, tokenizers, or vocabularies, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on TTS using self-supervised representations but does not specify discrete audio tokens, tokenizers, or vocabularies, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents ParrotTTS, a text-to-speech model leveraging self-supervised speech representations, but the abstract does not explicitly describe the discretization process or the generation of discrete audio tokens such as tokenizers, vector quantization, or codebooks, making it difficult to confirm if it meets the core criteria of using discrete audio tokens as defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper presents ParrotTTS, a text-to-speech model leveraging self-supervised speech representations, but the abstract does not explicitly describe the discretization process or the generation of discrete audio tokens such as tokenizers, vector quantization, or codebooks, making it difficult to confirm if it meets the core criteria of using discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
    "abstract": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses a neural audio codec (VQ-GAN with hierarchical Codec) to extract discrete speech tokens and jointly trains a large language model to generate these tokens autoregressively for TTS/ASR customization, directly addressing discrete audio token design and downstream tasks as required by the criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses a neural audio codec (VQ-GAN with hierarchical Codec) to extract discrete speech tokens and jointly trains a large language model to generate these tokens autoregressively for TTS/ASR customization, directly addressing discrete audio token design and downstream tasks as required by the criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes FoundationTTS using a neural audio codec (vector-quantized auto-encoders with adversarial training) to extract discrete speech tokens and applies a large language model to generate those tokens autoregressively for speech synthesis and ASR customization; the paper provides clear information about discrete token generation, codec architecture, and empirical evaluations demonstrating improvements. It aligns well with the inclusion criteria focused on discrete audio tokens generated via neural audio codecs and used for downstream generation tasks (TTS, ASR customization) with sufficient methodological details in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes FoundationTTS using a neural audio codec (vector-quantized auto-encoders with adversarial training) to extract discrete speech tokens and applies a large language model to generate those tokens autoregressively for speech synthesis and ASR customization; the paper provides clear information about discrete token generation, codec architecture, and empirical evaluations demonstrating improvements. It aligns well with the inclusion criteria focused on discrete audio tokens generated via neural audio codecs and used for downstream generation tasks (TTS, ASR customization) with sufficient methodological details in English. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
    "abstract": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
    "metadata": {
      "arxiv_id": "2303.03600",
      "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
      "summary": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
      "authors": [
        "Jinjie Ni",
        "Yukun Ma",
        "Wen Wang",
        "Qian Chen",
        "Dianwen Ng",
        "Han Lei",
        "Trung Hieu Nguyen",
        "Chong Zhang",
        "Bin Ma",
        "Erik Cambria"
      ],
      "published": "2023-03-07T02:31:57Z",
      "updated": "2023-03-07T02:31:57Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03600v1",
      "landing_url": "https://arxiv.org/abs/2303.03600v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03600"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此論文聚焦於文本與語音的跨模態知識蒸餾與嵌入空間對齊，未提出以離散音訊 token（如由神經音訊編碼器/自監督模型產生的 token、碼本或字彙表）為核心的表徵設計、產生流程或評估，因此不符合「Discrete Audio Tokens」主題的加入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此論文聚焦於文本與語音的跨模態知識蒸餾與嵌入空間對齊，未提出以離散音訊 token（如由神經音訊編碼器/自監督模型產生的 token、碼本或字彙表）為核心的表徵設計、產生流程或評估，因此不符合「Discrete Audio Tokens」主題的加入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on knowledge distillation between text and speech pre-trained models and proposes a metric-based distillation method to align textual and speech embedding spaces. However, the abstract does not mention any form of discretization or conversion of continuous audio waveforms into discrete audio tokens, nor does it describe the use or design of neural audio codecs, vector quantization, or discrete token vocabularies, which are critical inclusion criteria. The work mainly addresses representation alignment and transfer learning, not the generation or use of discrete audio tokens as defined. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is better excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on knowledge distillation between text and speech pre-trained models and proposes a metric-based distillation method to align textual and speech embedding spaces. However, the abstract does not mention any form of discretization or conversion of continuous audio waveforms into discrete audio tokens, nor does it describe the use or design of neural audio codecs, vector quantization, or discrete token vocabularies, which are critical inclusion criteria. The work mainly addresses representation alignment and transfer learning, not the generation or use of discrete audio tokens as defined. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is better excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
    "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper explicitly builds a model around discrete acoustic tokens produced by a neural codec and uses those token sequences as the core input/output for cross-lingual TTS, satisfying the discrete audio token criterion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper explicitly builds a model around discrete acoustic tokens produced by a neural codec and uses those token sequences as the core input/output for cross-lingual TTS, satisfying the discrete audio token criterion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VALL-E X, a cross-lingual neural codec language model that predicts acoustic token sequences conditioned on input speech and text prompts, fitting well into the discrete audio token domain by using neural audio codec outputs as discrete tokens for speech generation and synthesis tasks. The abstract suggests the model uses discrete acoustic tokens derived from neural codec representations, supporting zero-shot TTS and speech-to-speech translation, aligning with inclusion criteria concerning token design, downstream generation applications, and providing evaluation results. Furthermore, it contains an English abstract and title with sufficient methodological detail to assess discrete token usage, and it explicitly handles token sequences, codec language modeling, and preserves speaker voice characteristics, which meets the inclusion criteria while not violating any exclusion points. Therefore, the study should be included.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VALL-E X, a cross-lingual neural codec language model that predicts acoustic token sequences conditioned on input speech and text prompts, fitting well into the discrete audio token domain by using neural audio codec outputs as discrete tokens for speech generation and synthesis tasks. The abstract suggests the model uses discrete acoustic tokens derived from neural codec representations, supporting zero-shot TTS and speech-to-speech translation, aligning with inclusion criteria concerning token design, downstream generation applications, and providing evaluation results. Furthermore, it contains an English abstract and title with sufficient methodological detail to assess discrete token usage, and it explicitly handles token sequences, codec language modeling, and preserves speaker voice characteristics, which meets the inclusion criteria while not violating any exclusion points. Therefore, the study should be included.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Self-supervised speech representation learning for keyword-spotting with light-weight transformers",
    "abstract": "Self-supervised speech representation learning (S3RL) is revolutionizing the way we leverage the ever-growing availability of data. While S3RL related studies typically use large models, we employ light-weight networks to comply with tight memory of compute-constrained devices. We demonstrate the effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers with 330k parameters and propose a mechanism to enhance utterance-wise distinction, which proves crucial for improving performance on classification tasks. On the Google speech commands v2 dataset, the proposed method applied to the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement compared to training from scratch. On an in-house KS dataset with four different keywords, it provided 6% to 23.7% relative false accept improvement at fixed false reject rate. We argue this demonstrates the applicability of S3RL approaches to light-weight models for KS and confirms S3RL is a powerful alternative to traditional supervised learning for resource-constrained applications.",
    "metadata": {
      "arxiv_id": "2303.04255",
      "title": "Self-supervised speech representation learning for keyword-spotting with light-weight transformers",
      "summary": "Self-supervised speech representation learning (S3RL) is revolutionizing the way we leverage the ever-growing availability of data. While S3RL related studies typically use large models, we employ light-weight networks to comply with tight memory of compute-constrained devices. We demonstrate the effectiveness of S3RL on a keyword-spotting (KS) problem by using transformers with 330k parameters and propose a mechanism to enhance utterance-wise distinction, which proves crucial for improving performance on classification tasks. On the Google speech commands v2 dataset, the proposed method applied to the Auto-Regressive Predictive Coding S3RL led to a 1.2% accuracy improvement compared to training from scratch. On an in-house KS dataset with four different keywords, it provided 6% to 23.7% relative false accept improvement at fixed false reject rate. We argue this demonstrates the applicability of S3RL approaches to light-weight models for KS and confirms S3RL is a powerful alternative to traditional supervised learning for resource-constrained applications.",
      "authors": [
        "Chenyang Gao",
        "Yue Gu",
        "Francesco Caliva",
        "Yuzong Liu"
      ],
      "published": "2023-03-07T21:54:35Z",
      "updated": "2023-03-07T21:54:35Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.04255v1",
      "landing_url": "https://arxiv.org/abs/2303.04255v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.04255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on self-supervised continuous speech representations for keyword spotting using small transformers; it does not discuss or utilize discrete audio tokens, tokenizers, or quantized codebooks, so it does not meet the ‘Discrete Audio Tokens’ criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on self-supervised continuous speech representations for keyword spotting using small transformers; it does not discuss or utilize discrete audio tokens, tokenizers, or quantized codebooks, so it does not meet the ‘Discrete Audio Tokens’ criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation learning using light-weight transformers for keyword spotting, but it does not mention producing or using discrete audio tokens via discretization methods like vector quantization or clustering; no evidence shows use of discrete token sequences or explicit token design and evaluation, so it does not meet the inclusion criteria focused on discrete audio tokens as core representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation learning using light-weight transformers for keyword spotting, but it does not mention producing or using discrete audio tokens via discretization methods like vector quantization or clustering; no evidence shows use of discrete token sequences or explicit token design and evaluation, so it does not meet the inclusion criteria focused on discrete audio tokens as core representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
    "abstract": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
    "metadata": {
      "arxiv_id": "2303.06424",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "authors": [
        "Jiahui Zhang",
        "Fangneng Zhan",
        "Christian Theobalt",
        "Shijian Lu"
      ],
      "published": "2023-03-11T15:20:54Z",
      "updated": "2023-10-14T06:17:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06424v2",
      "landing_url": "https://arxiv.org/abs/2303.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.06424"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete image tokens for image generation and tokenization, not on discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete image tokens for image generation and tokenization, not on discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization methods for tokenizing images, addressing issues such as codebook collapse and stochastic quantization in image synthesis. The study does not involve audio signals or discrete audio tokens derived from neural audio codecs or self-supervised audio models. Therefore, it falls outside the inclusion criteria centered on discrete audio tokens for audio waveform representation and related audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization methods for tokenizing images, addressing issues such as codebook collapse and stochastic quantization in image synthesis. The study does not involve audio signals or discrete audio tokens derived from neural audio codecs or self-supervised audio models. Therefore, it falls outside the inclusion criteria centered on discrete audio tokens for audio waveform representation and related audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
    "abstract": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
    "metadata": {
      "arxiv_id": "2303.06705",
      "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
      "summary": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
      "authors": [
        "Yuanhao Cai",
        "Hao Bian",
        "Jing Lin",
        "Haoqian Wang",
        "Radu Timofte",
        "Yulun Zhang"
      ],
      "published": "2023-03-12T16:54:08Z",
      "updated": "2023-10-26T22:19:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06705v3",
      "landing_url": "https://arxiv.org/abs/2303.06705v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06705"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on low-light image enhancement using a Retinex-based Transformer, with no discrete audio tokens or tokenization framework, thus does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on low-light image enhancement using a Retinex-based Transformer, with no discrete audio tokens or tokenization framework, thus does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on low-light image enhancement using a Retinex-based Transformer, which pertains to image processing rather than audio processing or discrete audio tokenization. It does not address discretization of continuous audio waves into discrete tokens, neural audio codecs, or audio token-related modeling and generation as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on low-light image enhancement using a Retinex-based Transformer, which pertains to image processing rather than audio processing or discrete audio tokenization. It does not address discretization of continuous audio waves into discrete tokens, neural audio codecs, or audio token-related modeling and generation as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
    "abstract": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
    "metadata": {
      "arxiv_id": "2303.06982",
      "title": "Analysing the Masked predictive coding training criterion for pre-training a Speech Representation Model",
      "summary": "Recent developments in pre-trained speech representation utilizing self-supervised learning (SSL) have yielded exceptional results on a variety of downstream tasks. One such technique, known as masked predictive coding (MPC), has been employed by some of the most high-performing models. In this study, we investigate the impact of MPC loss on the type of information learnt at various layers in the HuBERT model, using nine probing tasks. Our findings indicate that the amount of content information learned at various layers of the HuBERT model has a positive correlation to the MPC loss. Additionally, it is also observed that any speaker-related information learned at intermediate layers of the model, is an indirect consequence of the learning process, and therefore cannot be controlled using the MPC loss. These findings may serve as inspiration for further research in the speech community, specifically in the development of new pre-training tasks or the exploration of new pre-training criterion's that directly preserves both speaker and content information at various layers of a learnt model.",
      "authors": [
        "Hemant Yadav",
        "Sunayana Sitaram",
        "Rajiv Ratn Shah"
      ],
      "published": "2023-03-13T10:32:44Z",
      "updated": "2024-01-11T11:15:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06982v3",
      "landing_url": "https://arxiv.org/abs/2303.06982v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06982"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article investigates masked predictive coding loss for pre-training a speech representation model (HuBERT) and analyzes layer-wise information content, but there is no mention of discrete audio tokens, tokenization, codebooks, or token-based downstream tasks, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article investigates masked predictive coding loss for pre-training a speech representation model (HuBERT) and analyzes layer-wise information content, but there is no mention of discrete audio tokens, tokenization, codebooks, or token-based downstream tasks, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing a pre-training criterion for a speech representation model (HuBERT) using masked predictive coding loss; however, it does not describe the use or generation of discrete audio tokens from neural audio codecs or self-supervised clustering/quantization methods, nor does it mention token vocabularies, quantization, or discretization processes. Hence, it does not meet the inclusion criteria related to discrete audio token generation or usage, and more pertains to continuous speech representation learning without discrete token modeling or analysis as the core subject.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing a pre-training criterion for a speech representation model (HuBERT) using masked predictive coding loss; however, it does not describe the use or generation of discrete audio tokens from neural audio codecs or self-supervised clustering/quantization methods, nor does it mention token vocabularies, quantization, or discretization processes. Hence, it does not meet the inclusion criteria related to discrete audio token generation or usage, and more pertains to continuous speech representation learning without discrete token modeling or analysis as the core subject.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight feature encoder for wake-up word detection based on self-supervised speech representation",
    "abstract": "Self-supervised learning method that provides generalized speech representations has recently received increasing attention. Wav2vec 2.0 is the most famous example, showing remarkable performance in numerous downstream speech processing tasks. Despite its success, it is challenging to use it directly for wake-up word detection on mobile devices due to its expensive computational cost. In this work, we propose LiteFEW, a lightweight feature encoder for wake-up word detection that preserves the inherent ability of wav2vec 2.0 with a minimum scale. In the method, the knowledge of the pre-trained wav2vec 2.0 is compressed by introducing an auto-encoder-based dimensionality reduction technique and distilled to LiteFEW. Experimental results on the open-source \"Hey Snips\" dataset show that the proposed method applied to various model structures significantly improves the performance, achieving over 20% of relative improvements with only 64k parameters.",
    "metadata": {
      "arxiv_id": "2303.07592",
      "title": "Lightweight feature encoder for wake-up word detection based on self-supervised speech representation",
      "summary": "Self-supervised learning method that provides generalized speech representations has recently received increasing attention. Wav2vec 2.0 is the most famous example, showing remarkable performance in numerous downstream speech processing tasks. Despite its success, it is challenging to use it directly for wake-up word detection on mobile devices due to its expensive computational cost. In this work, we propose LiteFEW, a lightweight feature encoder for wake-up word detection that preserves the inherent ability of wav2vec 2.0 with a minimum scale. In the method, the knowledge of the pre-trained wav2vec 2.0 is compressed by introducing an auto-encoder-based dimensionality reduction technique and distilled to LiteFEW. Experimental results on the open-source \"Hey Snips\" dataset show that the proposed method applied to various model structures significantly improves the performance, achieving over 20% of relative improvements with only 64k parameters.",
      "authors": [
        "Hyungjun Lim",
        "Younggwan Kim",
        "Kiho Yeom",
        "Eunjoo Seo",
        "Hoodong Lee",
        "Stanley Jungkyu Choi",
        "Honglak Lee"
      ],
      "published": "2023-03-14T02:31:44Z",
      "updated": "2023-03-14T02:31:44Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.07592v1",
      "landing_url": "https://arxiv.org/abs/2303.07592v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.07592"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper proposes a lightweight feature encoder for wake-up word detection using wav2vec 2.0 with auto-encoder-based dimensionality reduction, it does not describe discrete audio tokens, tokenizers, or codebooks, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper proposes a lightweight feature encoder for wake-up word detection using wav2vec 2.0 with auto-encoder-based dimensionality reduction, it does not describe discrete audio tokens, tokenizers, or codebooks, and thus does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a lightweight feature encoder for wake-up word detection using self-supervised speech representation learning (specifically wav2vec 2.0), but it does not describe the production or use of discrete audio tokens via discretization or tokenization such as vector quantization or clustering; it mainly handles continuous feature compression and model distillation without detailing discrete token vocabularies or token sequences for downstream modeling, thus not meeting the inclusion criteria for discrete audio tokens as the core method or object.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a lightweight feature encoder for wake-up word detection using self-supervised speech representation learning (specifically wav2vec 2.0), but it does not describe the production or use of discrete audio tokens via discretization or tokenization such as vector quantization or clustering; it mainly handles continuous feature compression and model distillation without detailing discrete token vocabularies or token sequences for downstream modeling, thus not meeting the inclusion criteria for discrete audio tokens as the core method or object.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Vision Transformers Efficient from A Token Sparsification View",
    "abstract": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The input article focuses on vision transformers and semantic tokens for vision tasks, with no discrete audio tokenization, codecs, or audio-related discretization discussed; thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The input article focuses on vision transformers and semantic tokens for vision tasks, with no discrete audio tokenization, codecs, or audio-related discretization discussed; thus it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving the efficiency of Vision Transformers for image and video tasks by token sparsification, without any mention of discrete audio tokens, neural audio codecs, audio tokenization, or audio-related tasks. It does not address any of the inclusion criteria related to discrete audio token representation or audio tasks, and is clearly outside the scope defined by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving the efficiency of Vision Transformers for image and video tasks by token sparsification, without any mention of discrete audio tokens, neural audio codecs, audio tokenization, or audio-related tasks. It does not address any of the inclusion criteria related to discrete audio token representation or audio tasks, and is clearly outside the scope defined by the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Cross-lingual Visual Speech Representations",
    "abstract": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning.",
    "metadata": {
      "arxiv_id": "2303.09455",
      "title": "Learning Cross-lingual Visual Speech Representations",
      "summary": "Cross-lingual self-supervised learning has been a growing research topic in the last few years. However, current works only explored the use of audio signals to create representations. In this work, we study cross-lingual self-supervised visual representation learning. We use the recently-proposed Raw Audio-Visual Speech Encoders (RAVEn) framework to pre-train an audio-visual model with unlabelled multilingual data, and then fine-tune the visual model on labelled transcriptions. Our experiments show that: (1) multi-lingual models with more data outperform monolingual ones, but, when keeping the amount of data fixed, monolingual models tend to reach better performance; (2) multi-lingual outperforms English-only pre-training; (3) using languages which are more similar yields better results; and (4) fine-tuning on unseen languages is competitive to using the target language in the pre-training set. We hope our study inspires future research on non-English-only speech representation learning.",
      "authors": [
        "Andreas Zinonos",
        "Alexandros Haliassos",
        "Pingchuan Ma",
        "Stavros Petridis",
        "Maja Pantic"
      ],
      "published": "2023-03-14T17:05:08Z",
      "updated": "2023-03-14T17:05:08Z",
      "categories": [
        "cs.CL",
        "cs.CV",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.09455v1",
      "landing_url": "https://arxiv.org/abs/2303.09455v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.09455"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses cross-lingual visual speech representations using self-supervised learning with audio-visual data; it does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses cross-lingual visual speech representations using self-supervised learning with audio-visual data; it does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on cross-lingual self-supervised visual representation learning for audio-visual speech using a pre-training and fine-tuning approach. However, it does not describe the generation or utilization of discrete audio tokens—there is no mention of discretization of audio waveforms into token sequences, vector quantization, or neural audio codecs as specified in the inclusion criteria. The work seems to deal with continuous representation learning rather than discrete token modeling, thus it falls under the exclusion criteria of lacking discrete token-based methods or analysis relevant to audio token design and evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on cross-lingual self-supervised visual representation learning for audio-visual speech using a pre-training and fine-tuning approach. However, it does not describe the generation or utilization of discrete audio tokens—there is no mention of discretization of audio waveforms into token sequences, vector quantization, or neural audio codecs as specified in the inclusion criteria. The work seems to deal with continuous representation learning rather than discrete token modeling, thus it falls under the exclusion criteria of lacking discrete token-based methods or analysis relevant to audio token design and evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Representations for Singing Voice Conversion",
    "abstract": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
    "metadata": {
      "arxiv_id": "2303.12197",
      "title": "Self-Supervised Representations for Singing Voice Conversion",
      "summary": "A singing voice conversion model converts a song in the voice of an arbitrary source singer to the voice of a target singer. Recently, methods that leverage self-supervised audio representations such as HuBERT and Wav2Vec 2.0 have helped further the state-of-the-art. Though these methods produce more natural and melodic singing outputs, they often rely on confusion and disentanglement losses to render the self-supervised representations speaker and pitch-invariant. In this paper, we circumvent disentanglement training and propose a new model that leverages ASR fine-tuned self-supervised representations as inputs to a HiFi-GAN neural vocoder for singing voice conversion. We experiment with different f0 encoding schemes and show that an f0 harmonic generation module that uses a parallel bank of transposed convolutions (PBTC) alongside ASR fine-tuned Wav2Vec 2.0 features results in the best singing voice conversion quality. Additionally, the model is capable of making a spoken voice sing. We also show that a simple f0 shifting scheme during inference helps retain singer identity and bolsters the performance of our singing voice conversion model. Our results are backed up by extensive MOS studies that compare different ablations and baselines.",
      "authors": [
        "Tejas Jayashankar",
        "Jilong Wu",
        "Leda Sari",
        "David Kant",
        "Vimal Manohar",
        "Qing He"
      ],
      "published": "2023-03-21T21:04:03Z",
      "updated": "2023-03-21T21:04:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12197v1",
      "landing_url": "https://arxiv.org/abs/2303.12197v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12197"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses continuous self-supervised representations for singing voice conversion and does not present discrete audio tokens, tokenizers, or a token-based analysis, failing the core inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper uses continuous self-supervised representations for singing voice conversion and does not present discrete audio tokens, tokenizers, or a token-based analysis, failing the core inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice conversion using self-supervised audio representations (e.g., HuBERT, Wav2Vec 2.0) as features, but does not describe any discrete token generation process, tokenizer architecture, quantization, codebook or explicit discretization of audio into token sequences; rather, it uses continuous self-supervised representations as inputs without transformation into discrete tokens for sequence modeling. Thus, it does not meet the core inclusion criteria related to discrete audio tokens as core representations for modeling or generation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice conversion using self-supervised audio representations (e.g., HuBERT, Wav2Vec 2.0) as features, but does not describe any discrete token generation process, tokenizer architecture, quantization, codebook or explicit discretization of audio into token sequences; rather, it uses continuous self-supervised representations as inputs without transformation into discrete tokens for sequence modeling. Thus, it does not meet the core inclusion criteria related to discrete audio tokens as core representations for modeling or generation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
    "abstract": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
    "metadata": {
      "arxiv_id": "2303.13336",
      "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
      "summary": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
      "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Sheng Zheng",
        "Mengchun Zhang",
        "Maryam Qamar",
        "Sung-Ho Bae",
        "In So Kweon"
      ],
      "published": "2023-03-23T15:17:15Z",
      "updated": "2023-04-02T09:27:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.13336v2",
      "landing_url": "https://arxiv.org/abs/2303.13336v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.13336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on diffusion-based audio generation (TTS) and enhancement and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on diffusion-based audio generation (TTS) and enhancement and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a survey of audio diffusion models specifically for text-to-speech synthesis and speech enhancement, but do not mention or discuss discrete audio tokens derived from neural audio codecs or self-supervised learning models being used as core representations; nor do they address token design, reproduction details, or evaluation related directly to discrete token sequences. Hence, the work does not meet the key inclusion criterion of using discrete token sequences as the core audio representation for modeling or generation, and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a survey of audio diffusion models specifically for text-to-speech synthesis and speech enhancement, but do not mention or discuss discrete audio tokens derived from neural audio codecs or self-supervised learning models being used as core representations; nor do they address token design, reproduction details, or evaluation related directly to discrete token sequences. Hence, the work does not meet the key inclusion criterion of using discrete token sequences as the core audio representation for modeling or generation, and thus should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multilingual Word Error Rate Estimation: e-WER3",
    "abstract": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
    "metadata": {
      "arxiv_id": "2304.00649",
      "title": "Multilingual Word Error Rate Estimation: e-WER3",
      "summary": "The success of the multilingual automatic speech recognition systems empowered many voice-driven applications. However, measuring the performance of such systems remains a major challenge, due to its dependency on manually transcribed speech data in both mono- and multilingual scenarios. In this paper, we propose a novel multilingual framework -- eWER3 -- jointly trained on acoustic and lexical representation to estimate word error rate. We demonstrate the effectiveness of eWER3 to (i) predict WER without using any internal states from the ASR and (ii) use the multilingual shared latent space to push the performance of the close-related languages. We show our proposed multilingual model outperforms the previous monolingual word error rate estimation method (eWER2) by an absolute 9\\% increase in Pearson correlation coefficient (PCC), with better overall estimation between the predicted and reference WER.",
      "authors": [
        "Shammur Absar Chowdhury",
        "Ahmed Ali"
      ],
      "published": "2023-04-02T23:08:11Z",
      "updated": "2023-04-02T23:08:11Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00649v1",
      "landing_url": "https://arxiv.org/abs/2304.00649v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.00649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses multilingual WER estimation using a joint acoustic-lexical framework; it does not involve discrete audio tokens, tokenizers, or token-level representations, thus not meeting the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses multilingual WER estimation using a joint acoustic-lexical framework; it does not involve discrete audio tokens, tokenizers, or token-level representations, thus not meeting the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on estimating word error rate for multilingual ASR systems, involving acoustic and lexical representations but does not mention discrete audio tokens, tokenization, neural audio codec, vector quantization, or any discrete token generation mechanism relevant to the inclusion criteria. It lacks details on discrete token design, evaluation, or usage in generation/understanding tasks, and rather addresses performance estimation of ASR without discrete token modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on estimating word error rate for multilingual ASR systems, involving acoustic and lexical representations but does not mention discrete audio tokens, tokenization, neural audio codec, vector quantization, or any discrete token generation mechanism relevant to the inclusion criteria. It lacks details on discrete token design, evaluation, or usage in generation/understanding tasks, and rather addresses performance estimation of ASR without discrete token modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
    "abstract": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
    "metadata": {
      "arxiv_id": "2304.01448",
      "title": "TorchAudio-Squim: Reference-less Speech Quality and Intelligibility measures in TorchAudio",
      "summary": "Measuring quality and intelligibility of a speech signal is usually a critical step in development of speech processing systems. To enable this, a variety of metrics to measure quality and intelligibility under different assumptions have been developed. Through this paper, we introduce tools and a set of models to estimate such known metrics using deep neural networks. These models are made available in the well-established TorchAudio library, the core audio and speech processing library within the PyTorch deep learning framework. We refer to it as TorchAudio-Squim, TorchAudio-Speech QUality and Intelligibility Measures. More specifically, in the current version of TorchAudio-squim, we establish and release models for estimating PESQ, STOI and SI-SDR among objective metrics and MOS among subjective metrics. We develop a novel approach for objective metric estimation and use a recently developed approach for subjective metric estimation. These models operate in a ``reference-less\" manner, that is they do not require the corresponding clean speech as reference for speech assessment. Given the unavailability of clean speech and the effortful process of subjective evaluation in real-world situations, such easy-to-use tools would greatly benefit speech processing research and development.",
      "authors": [
        "Anurag Kumar",
        "Ke Tan",
        "Zhaoheng Ni",
        "Pranay Manocha",
        "Xiaohui Zhang",
        "Ethan Henderson",
        "Buye Xu"
      ],
      "published": "2023-04-04T01:44:24Z",
      "updated": "2023-04-04T01:44:24Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.01448v1",
      "landing_url": "https://arxiv.org/abs/2304.01448v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.01448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on reference-less speech quality and intelligibility metrics implemented in TorchAudio, with no discussion of discrete audio token generation, tokenization, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on reference-less speech quality and intelligibility metrics implemented in TorchAudio, with no discussion of discrete audio token generation, tokenization, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents models for speech quality and intelligibility estimation without addressing discrete audio tokens or their generation, design, or use in downstream audio generation or understanding tasks, thus not meeting the core inclusion criteria related to discrete token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents models for speech quality and intelligibility estimation without addressing discrete audio tokens or their generation, design, or use in downstream audio generation or understanding tasks, thus not meeting the core inclusion criteria related to discrete token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
    "abstract": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
    "metadata": {
      "arxiv_id": "2304.03635",
      "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
      "summary": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
      "authors": [
        "Changlong Jiang",
        "Yang Xiao",
        "Cunlin Wu",
        "Mingyang Zhang",
        "Jinghong Zheng",
        "Zhiguo Cao",
        "Joey Tianyi Zhou"
      ],
      "published": "2023-04-07T13:30:36Z",
      "updated": "2023-04-07T13:30:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03635v1",
      "landing_url": "https://arxiv.org/abs/2304.03635v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03635"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets 3D hand pose estimation from RGB using a Transformer; it discusses visual modality with no discrete audio tokenization or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets 3D hand pose estimation from RGB using a Transformer; it discusses visual modality with no discrete audio tokenization or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 3D hand pose estimation from RGB images using a Transformer-based model, without any mention or evidence of discrete audio token representation, discretization of audio waveforms, neural audio codecs, vector quantization, or token-based audio modeling as specified by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 3D hand pose estimation from RGB images using a Transformer-based model, without any mention or evidence of discrete audio token representation, discretization of audio waveforms, neural audio codecs, vector quantization, or token-based audio modeling as specified by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
    "abstract": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on vector quantization to produce discrete representations (tokens) for speech and evaluates their use across downstream tasks, aligning with the discrete audio tokens topic and downstream application criteria, even though it may focus more on pooling rather than full tokenizer design details.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work centers on vector quantization to produce discrete representations (tokens) for speech and evaluates their use across downstream tasks, aligning with the discrete audio tokens topic and downstream application criteria, even though it may focus more on pooling rather than full tokenizer design details.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using vector quantization, which is a form of discretization, to pool speech representations derived from large-scale self-supervised models; however, it primarily addresses the pooling problem rather than explicit tokenization into discrete audio token sequences or detailed token design and evaluation. While it involves vector quantization, the abstract lacks explicit descriptions regarding discrete token vocabulary, tokenizer architecture, codebook settings, or downstream tasks like audio generation or cross-modal alignment that are core to the inclusion criteria. Therefore, although related, the paper does not fully meet the comprehensive scope required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using vector quantization, which is a form of discretization, to pool speech representations derived from large-scale self-supervised models; however, it primarily addresses the pooling problem rather than explicit tokenization into discrete audio token sequences or detailed token design and evaluation. While it involves vector quantization, the abstract lacks explicit descriptions regarding discrete token vocabulary, tokenizer architecture, codebook settings, or downstream tasks like audio generation or cross-modal alignment that are core to the inclusion criteria. Therefore, although related, the paper does not fully meet the comprehensive scope required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano rated 4 noting the use of vector quantization aligns with discretization themes; JuniorMini rated 2 noting the paper focuses on pooling and lacks explicit tokenizer/codebook and token design details. My assessment: the work applies vector quantization to pool variable-length self-supervised speech representations and evaluates discriminative downstream tasks (keyword spotting, speaker ID, intent, emotion), but the abstract does not describe discrete token generation details (e.g., tokenizer/codec architecture, codebook/vocabulary settings, time downsampling or token sequences) nor does it target generation/LM-alignment or treat discrete tokens as the central object of study. Therefore it does not meet the inclusion requirement of clearly defined discrete audio tokens and appears chiefly concerned with pooling for classification rather than discrete-token design or use in generation/cross-modal modeling.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano rated 4 noting the use of vector quantization aligns with discretization themes; JuniorMini rated 2 noting the paper focuses on pooling and lacks explicit tokenizer/codebook and token design details. My assessment: the work applies vector quantization to pool variable-length self-supervised speech representations and evaluates discriminative downstream tasks (keyword spotting, speaker ID, intent, emotion), but the abstract does not describe discrete token generation details (e.g., tokenizer/codec architecture, codebook/vocabulary settings, time downsampling or token sequences) nor does it target generation/LM-alignment or treat discrete tokens as the central object of study. Therefore it does not meet the inclusion requirement of clearly defined discrete audio tokens and appears chiefly concerned with pooling for classification rather than discrete-token design or use in generation/cross-modal modeling.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
    "abstract": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
    "metadata": {
      "arxiv_id": "2304.06408",
      "title": "Intriguing properties of synthetic images: from generative adversarial networks to diffusion models",
      "summary": "Detecting fake images is becoming a major goal of computer vision. This need is becoming more and more pressing with the continuous improvement of synthesis methods based on Generative Adversarial Networks (GAN), and even more with the appearance of powerful methods based on Diffusion Models (DM). Towards this end, it is important to gain insight into which image features better discriminate fake images from real ones. In this paper we report on our systematic study of a large number of image generators of different families, aimed at discovering the most forensically relevant characteristics of real and generated images. Our experiments provide a number of interesting observations and shed light on some intriguing properties of synthetic images: (1) not only the GAN models but also the DM and VQ-GAN (Vector Quantized Generative Adversarial Networks) models give rise to visible artifacts in the Fourier domain and exhibit anomalous regular patterns in the autocorrelation; (2) when the dataset used to train the model lacks sufficient variety, its biases can be transferred to the generated images; (3) synthetic and real images exhibit significant differences in the mid-high frequency signal content, observable in their radial and angular spectral power distributions.",
      "authors": [
        "Riccardo Corvi",
        "Davide Cozzolino",
        "Giovanni Poggi",
        "Koki Nagano",
        "Luisa Verdoliva"
      ],
      "published": "2023-04-13T11:13:19Z",
      "updated": "2023-06-29T15:33:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.06408v2",
      "landing_url": "https://arxiv.org/abs/2304.06408v2",
      "doi": "https://doi.org/10.48550/arXiv.2304.06408"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The item focuses on forensics of synthetic images from GANs and diffusion models and discusses image-domain artifacts, not discrete audio tokens; thus it does not meet the 'Discrete Audio Tokens' criteria. Therefore exclude.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The item focuses on forensics of synthetic images from GANs and diffusion models and discusses image-domain artifacts, not discrete audio tokens; thus it does not meet the 'Discrete Audio Tokens' criteria. Therefore exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on distinguishing real and synthetic images generated by GANs and diffusion models, without any discussion or inclusion of discrete audio tokens, audio waveforms, or tokenization processes related to audio. Therefore, it does not meet the inclusion criteria about discrete audio tokens or audio generation tasks, and falls clearly outside the scope of the discrete audio token topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on distinguishing real and synthetic images generated by GANs and diffusion models, without any discussion or inclusion of discrete audio tokens, audio waveforms, or tokenization processes related to audio. Therefore, it does not meet the inclusion criteria about discrete audio tokens or audio generation tasks, and falls clearly outside the scope of the discrete audio token topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
    "abstract": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
    "metadata": {
      "arxiv_id": "2304.07240",
      "title": "GreedyGD: Enhanced Generalized Deduplication for Direct Analytics in IoT",
      "summary": "Exponential growth in the amount of data generated by the Internet of Things currently pose significant challenges for data communication, storage and analytics and leads to high costs for organisations hoping to leverage their data. Novel techniques are therefore needed to holistically improve the efficiency of data storage and analytics in IoT systems. The emerging compression technique Generalized Deduplication (GD) has been shown to deliver high compression and enable direct compressed data analytics with low storage and memory requirements. In this paper, we propose a new GD-based data compression algorithm called GreedyGD that is designed for analytics. Compared to existing versions of GD, GreedyGD enables more reliable analytics with less data, while running 11.2x faster and delivering even better compression.",
      "authors": [
        "Aaron Hurst",
        "Daniel E. Lucani",
        "Qi Zhang"
      ],
      "published": "2023-04-14T16:43:31Z",
      "updated": "2023-04-14T16:43:31Z",
      "categories": [
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.07240v1",
      "landing_url": "https://arxiv.org/abs/2304.07240v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.07240"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on GD-based data compression for IoT analytics and does not discuss discrete audio tokens, tokenization, or downstream token-based modeling, so it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on GD-based data compression for IoT analytics and does not discuss discrete audio tokens, tokenization, or downstream token-based modeling, so it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on data compression and analytics techniques for IoT systems and does not discuss discrete audio tokens, neural audio codecs, or any discreet tokenization of continuous audio signals for modeling or generation tasks, which are central to the inclusion criteria. Therefore, it falls outside the topic definition and research scope required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on data compression and analytics techniques for IoT systems and does not discuss discrete audio tokens, neural audio codecs, or any discreet tokenization of continuous audio signals for modeling or generation tasks, which are central to the inclusion criteria. Therefore, it falls outside the topic definition and research scope required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
    "abstract": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
    "metadata": {
      "arxiv_id": "2304.09226",
      "title": "Coded Speech Quality Measurement by a Non-Intrusive PESQ-DNN",
      "summary": "Wideband codecs such as AMR-WB or EVS are widely used in (mobile) speech communication. Evaluation of coded speech quality is often performed subjectively by an absolute category rating (ACR) listening test. However, the ACR test is impractical for online monitoring of speech communication networks. Perceptual evaluation of speech quality (PESQ) is one of the widely used metrics instrumentally predicting the results of an ACR test. However, the PESQ algorithm requires an original reference signal, which is usually unavailable in network monitoring, thus limiting its applicability. NISQA is a new non-intrusive neural-network-based speech quality measure, focusing on super-wideband speech signals. In this work, however, we aim at predicting the well-known PESQ metric using a non-intrusive PESQ-DNN model. We illustrate the potential of this model by predicting the PESQ scores of wideband-coded speech obtained from AMR-WB or EVS codecs operating at different bitrates in noisy, tandeming, and error-prone transmission conditions. We compare our methods with the state-of-the-art network topologies of QualityNet, WaweNet, and DNSMOS -- all applied to PESQ prediction -- by measuring the mean absolute error (MAE) and the linear correlation coefficient (LCC). The proposed PESQ-DNN offers the best total MAE and LCC of 0.11 and 0.92, respectively, in conditions without frame loss, and still is best when including frame loss. Note that our model could be similarly used to non-intrusively predict POLQA or other (intrusive) metrics. Upon article acceptance, code will be provided at GitHub.",
      "authors": [
        "Ziyi Xu",
        "Ziyue Zhao",
        "Tim Fingscheidt"
      ],
      "published": "2023-04-18T18:26:56Z",
      "updated": "2023-04-18T18:26:56Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09226v1",
      "landing_url": "https://arxiv.org/abs/2304.09226v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.09226"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on non-intrusive PESQ-DNN to predict PESQ scores for coded speech and does not involve discrete audio tokens, tokenizer/codec token design, or token-based downstream tasks, so it fails the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on non-intrusive PESQ-DNN to predict PESQ scores for coded speech and does not involve discrete audio tokens, tokenizer/codec token design, or token-based downstream tasks, so it fails the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on non-intrusive prediction of PESQ scores for wideband-coded speech using deep neural networks, which is related to speech quality evaluation rather than discretizing audio waveforms into discrete token sequences for modeling or generation; there is no mention of neural audio codecs, vector quantization, or discrete token vocabularies used for downstream generative or alignment tasks, thus it does not meet the inclusion criteria for \"Discrete Audio Tokens.\"",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on non-intrusive prediction of PESQ scores for wideband-coded speech using deep neural networks, which is related to speech quality evaluation rather than discretizing audio waveforms into discrete token sequences for modeling or generation; there is no mention of neural audio codecs, vector quantization, or discrete token vocabularies used for downstream generative or alignment tasks, thus it does not meet the inclusion criteria for \"Discrete Audio Tokens.\"",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-Based Visual Segmentation: A Survey",
    "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
    "metadata": {
      "arxiv_id": "2304.09854",
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
      "authors": [
        "Xiangtai Li",
        "Henghui Ding",
        "Haobo Yuan",
        "Wenwei Zhang",
        "Jiangmiao Pang",
        "Guangliang Cheng",
        "Kai Chen",
        "Ziwei Liu",
        "Chen Change Loy"
      ],
      "published": "2023-04-19T17:59:02Z",
      "updated": "2024-08-04T04:30:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09854v4",
      "landing_url": "https://arxiv.org/abs/2304.09854v4",
      "doi": "https://doi.org/10.48550/arXiv.2304.09854"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a survey on transformer-based visual segmentation (vision domain) and addresses visual tokens, not discrete audio tokens; it does not meet the inclusion criteria and does not trigger any exclusion criteria, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a survey on transformer-based visual segmentation (vision domain) and addresses visual tokens, not discrete audio tokens; it does not meet the inclusion criteria and does not trigger any exclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on transformer-based visual segmentation involving image, video, and 3D point cloud data, with no indication of audio-related content or discrete audio tokens. The study does not address discretization of continuous audio waveforms, neural audio codecs, or token sequences for audio modeling, generation, or cross-modal reasoning related to audio. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related audio tasks and falls under exclusion criteria for not involving discrete audio token methodology or audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on transformer-based visual segmentation involving image, video, and 3D point cloud data, with no indication of audio-related content or discrete audio tokens. The study does not address discretization of continuous audio waveforms, neural audio codecs, or token sequences for audio modeling, generation, or cross-modal reasoning related to audio. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related audio tasks and falls under exclusion criteria for not involving discrete audio token methodology or audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
    "abstract": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
    "metadata": {
      "arxiv_id": "2304.11750",
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "authors": [
        "Zhijun Liu",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2023-04-23T21:05:33Z",
      "updated": "2023-04-23T21:05:33Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11750v1",
      "landing_url": "https://arxiv.org/abs/2304.11750v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.11750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses a continuous latent representation (phoneme-rate latent space via a VAE, enhanced by adversarial training) and diffusion modeling, with no explicit discrete tokenization/codec/token vocabulary described, so it does not satisfy the Discrete Audio Tokens criterion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work uses a continuous latent representation (phoneme-rate latent space via a VAE, enhanced by adversarial training) and diffusion modeling, with no explicit discrete tokenization/codec/token vocabulary described, so it does not satisfy the Discrete Audio Tokens criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents DiffVoice, a text-to-speech model using latent diffusion and a variational autoencoder to encode speech signals into a phoneme-rate latent representation, but it does not explicitly describe discrete tokenization processes such as vector quantization, discrete codebooks, or a tokenizer producing discrete tokens; therefore, it does not clearly meet the inclusion criteria focused on discrete audio tokens, and the abstract lacks detailed information about the discrete token definitions, quantization methods, or vocabulary sets required for reliable evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents DiffVoice, a text-to-speech model using latent diffusion and a variational autoencoder to encode speech signals into a phoneme-rate latent representation, but it does not explicitly describe discrete tokenization processes such as vector quantization, discrete codebooks, or a tokenizer producing discrete tokens; therefore, it does not clearly meet the inclusion criteria focused on discrete audio tokens, and the abstract lacks detailed information about the discrete token definitions, quantization methods, or vocabulary sets required for reliable evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model",
    "abstract": "This paper proposes a zero-shot text-to-speech (TTS) conditioned by a self-supervised speech-representation model acquired through self-supervised learning (SSL). Conventional methods with embedding vectors from x-vector or global style tokens still have a gap in reproducing the speaker characteristics of unseen speakers. A novel point of the proposed method is the direct use of the SSL model to obtain embedding vectors from speech representations trained with a large amount of data. We also introduce the separate conditioning of acoustic features and a phoneme duration predictor to obtain the disentangled embeddings between rhythm-based speaker characteristics and acoustic-feature-based ones. The disentangled embeddings will enable us to achieve better reproduction performance for unseen speakers and rhythm transfer conditioned by different speeches. Objective and subjective evaluations showed that the proposed method can synthesize speech with improved similarity and achieve speech-rhythm transfer.",
    "metadata": {
      "arxiv_id": "2304.11976",
      "title": "Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model",
      "summary": "This paper proposes a zero-shot text-to-speech (TTS) conditioned by a self-supervised speech-representation model acquired through self-supervised learning (SSL). Conventional methods with embedding vectors from x-vector or global style tokens still have a gap in reproducing the speaker characteristics of unseen speakers. A novel point of the proposed method is the direct use of the SSL model to obtain embedding vectors from speech representations trained with a large amount of data. We also introduce the separate conditioning of acoustic features and a phoneme duration predictor to obtain the disentangled embeddings between rhythm-based speaker characteristics and acoustic-feature-based ones. The disentangled embeddings will enable us to achieve better reproduction performance for unseen speakers and rhythm transfer conditioned by different speeches. Objective and subjective evaluations showed that the proposed method can synthesize speech with improved similarity and achieve speech-rhythm transfer.",
      "authors": [
        "Kenichi Fujita",
        "Takanori Ashihara",
        "Hiroki Kanagawa",
        "Takafumi Moriya",
        "Yusuke Ijima"
      ],
      "published": "2023-04-24T10:15:58Z",
      "updated": "2023-04-24T10:15:58Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11976v1",
      "landing_url": "https://arxiv.org/abs/2304.11976v1",
      "doi": "https://doi.org/10.1109/ICASSPW59220.2023.10193459"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on zero-shot TTS using self-supervised speech representations and continuous embeddings, without defining or employing discrete audio tokens/tokenization; it does not meet the discrete token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on zero-shot TTS using self-supervised speech representations and continuous embeddings, without defining or employing discrete audio tokens/tokenization; it does not meet the discrete token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a zero-shot text-to-speech system conditioned by a self-supervised speech representation model, but the abstract does not describe the generation or use of discrete audio tokens obtained via discretization methods like vector quantization or clustering, nor does it specify any tokenizer architecture, codebook settings, or token vocabulary. Hence, there is no clear evidence that the core method involves discrete audio tokens as defined by the inclusion criteria, making it fall under continuous acoustic feature conditioning rather than discrete token modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a zero-shot text-to-speech system conditioned by a self-supervised speech representation model, but the abstract does not describe the generation or use of discrete audio tokens obtained via discretization methods like vector quantization or clustering, nor does it specify any tokenizer architecture, codebook settings, or token vocabulary. Hence, there is no clear evidence that the core method involves discrete audio tokens as defined by the inclusion criteria, making it fall under continuous acoustic feature conditioning rather than discrete token modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
    "abstract": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
    "metadata": {
      "arxiv_id": "2304.12404",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "authors": [
        "Sandeep Mehta",
        "Darpan Shah",
        "Ravindra Kulkarni",
        "Cornelia Caragea"
      ],
      "published": "2023-04-24T19:33:41Z",
      "updated": "2023-04-24T19:33:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12404v1",
      "landing_url": "https://arxiv.org/abs/2304.12404v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12404"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a semantic/text tokenizer for NLP and does not address discrete audio tokens or audio-token vocabulary/codec design, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a semantic/text tokenizer for NLP and does not address discrete audio tokens or audio-token vocabulary/codec design, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a semantic tokenizer for natural language processing to improve text-based tokenization and vocabulary construction, without any reference to audio signals, discretization of audio waveforms, or neural audio coding for audio token generation. It does not address discrete audio tokens, audio generation, or cross-modal audio modeling, which are core to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a semantic tokenizer for natural language processing to improve text-based tokenization and vocabulary construction, without any reference to audio signals, discretization of audio waveforms, or neural audio coding for audio token generation. It does not address discrete audio tokens, audio generation, or cross-modal audio modeling, which are core to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
    "abstract": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
    "metadata": {
      "arxiv_id": "2305.02528",
      "title": "Self-Supervised 3D Scene Flow Estimation Guided by Superpoints",
      "summary": "3D scene flow estimation aims to estimate point-wise motions between two consecutive frames of point clouds. Superpoints, i.e., points with similar geometric features, are usually employed to capture similar motions of local regions in 3D scenes for scene flow estimation. However, in existing methods, superpoints are generated with the offline clustering methods, which cannot characterize local regions with similar motions for complex 3D scenes well, leading to inaccurate scene flow estimation. To this end, we propose an iterative end-to-end superpoint based scene flow estimation framework, where the superpoints can be dynamically updated to guide the point-level flow prediction. Specifically, our framework consists of a flow guided superpoint generation module and a superpoint guided flow refinement module. In our superpoint generation module, we utilize the bidirectional flow information at the previous iteration to obtain the matching points of points and superpoint centers for soft point-to-superpoint association construction, in which the superpoints are generated for pairwise point clouds. With the generated superpoints, we first reconstruct the flow for each point by adaptively aggregating the superpoint-level flow, and then encode the consistency between the reconstructed flow of pairwise point clouds. Finally, we feed the consistency encoding along with the reconstructed flow into GRU to refine point-level flow. Extensive experiments on several different datasets show that our method can achieve promising performance.",
      "authors": [
        "Yaqi Shen",
        "Le Hui",
        "Jin Xie",
        "Jian Yang"
      ],
      "published": "2023-05-04T03:33:40Z",
      "updated": "2023-05-04T03:33:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02528v1",
      "landing_url": "https://arxiv.org/abs/2305.02528v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.02528"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on 3D scene flow estimation using superpoints in point clouds, with no discussion of discrete audio tokens, tokenizers, or audio-focused downstream tasks, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on 3D scene flow estimation using superpoints in point clouds, with no discussion of discrete audio tokens, tokenizers, or audio-focused downstream tasks, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a method for 3D scene flow estimation from point cloud data and does not involve audio data or discrete audio tokens. There is no mention of audio waveforms, tokenization, neural audio codecs, or discretization processes related to audio signals, which are central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a method for 3D scene flow estimation from point cloud data and does not involve audio data or discrete audio tokens. There is no mention of audio waveforms, tokenization, neural audio codecs, or discretization processes related to audio signals, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
    "abstract": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
    "metadata": {
      "arxiv_id": "2305.03568",
      "title": "A vector quantized masked autoencoder for audiovisual speech emotion recognition",
      "summary": "An important challenge in emotion recognition is to develop methods that can leverage unlabeled training data. In this paper, we propose the VQ-MAE-AV model, a self-supervised multimodal model that leverages masked autoencoders to learn representations of audiovisual speech without labels. The model includes vector quantized variational autoencoders that compress raw audio and visual speech data into discrete tokens. The audiovisual speech tokens are used to train a multimodal masked autoencoder that consists of an encoder-decoder architecture with attention mechanisms. The model is designed to extract both local (i.e., at the frame level) and global (i.e., at the sequence level) representations of audiovisual speech. During self-supervised pre-training, the VQ-MAE-AV model is trained on a large-scale unlabeled dataset of audiovisual speech, for the task of reconstructing randomly masked audiovisual speech tokens and with a contrastive learning strategy. During this pre-training, the encoder learns to extract a representation of audiovisual speech that can be subsequently leveraged for emotion recognition. During the supervised fine-tuning stage, a small classification model is trained on top of the VQ-MAE-AV encoder for an emotion recognition task. The proposed approach achieves state-of-the-art emotion recognition results across several datasets in both controlled and in-the-wild conditions.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:19:46Z",
      "updated": "2025-05-09T08:19:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03568v3",
      "landing_url": "https://arxiv.org/abs/2305.03568v3",
      "doi": "https://doi.org/10.1016/j.cviu.2025.104362"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a model that uses vector-quantized (discrete) tokens produced by a neural audio codec to represent audio (and visual) data, and employs these tokens in a masked autoencoder for downstream audiovisual speech emotion recognition, meeting the discrete audio token criteria and detailing token generation and downstream use.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a model that uses vector-quantized (discrete) tokens produced by a neural audio codec to represent audio (and visual) data, and employs these tokens in a masked autoencoder for downstream audiovisual speech emotion recognition, meeting the discrete audio token criteria and detailing token generation and downstream use.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised multimodal model leveraging vector quantized variational autoencoders to compress raw audio and visual speech data into discrete tokens used for downstream emotion recognition. It explicitly describes the use of vector quantized tokens (a form of neural audio codec and discrete tokenization) as a core representation for audiovisual speech, aligning with the inclusion criteria about discrete audio tokens and their usage for subsequent modeling. The abstract provides sufficient detail about the tokenization method, enabling reproducibility assessment, and the study focuses on leveraging these discrete tokens for emotion recognition, a relevant downstream task. Therefore, all inclusion criteria are met, and none of the exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised multimodal model leveraging vector quantized variational autoencoders to compress raw audio and visual speech data into discrete tokens used for downstream emotion recognition. It explicitly describes the use of vector quantized tokens (a form of neural audio codec and discrete tokenization) as a core representation for audiovisual speech, aligning with the inclusion criteria about discrete audio tokens and their usage for subsequent modeling. The abstract provides sufficient detail about the tokenization method, enabling reproducibility assessment, and the study focuses on leveraging these discrete tokens for emotion recognition, a relevant downstream task. Therefore, all inclusion criteria are met, and none of the exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
    "abstract": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
    "metadata": {
      "arxiv_id": "2305.03582",
      "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
      "summary": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Laurent Girin",
        "Xavier Alameda-Pineda",
        "Renaud Séguier"
      ],
      "published": "2023-05-05T14:37:26Z",
      "updated": "2024-02-20T16:18:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.03582v3",
      "landing_url": "https://arxiv.org/abs/2305.03582v3",
      "doi": "https://doi.org/10.1016/j.neunet.2024.106120"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses a vector-quantized VAE to generate discrete audio/visual tokens and discusses token-based representation in a multimodal setting, meeting the discrete-token coverage criterion without violating exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses a vector-quantized VAE to generate discrete audio/visual tokens and discusses token-based representation in a multimodal setting, meeting the discrete-token coverage criterion without violating exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract mentions using a vector quantized VAE (VQ-VAE) independently for each modality, which implies a neural audio codec generating discrete representations. This fits the criteria for discrete audio tokens produced by vector quantization. The study further investigates these discrete representations for tasks including audiovisual speech emotion recognition and manipulation, indicating an application in audiovisual multimodal modeling. However, the paper primarily focuses on representation learning rather than explicitly discussing token design choices, vocabulary setting, or systematic evaluation related to discrete audio tokens. Given the use of discrete tokens through VQ-VAEs and their use in generation and recognition, it meets core inclusion criteria though lacks some details on token specifics and evaluation. Therefore, it leans more towards inclusion but not absolutely certain due to limited token detail disclosure.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The abstract mentions using a vector quantized VAE (VQ-VAE) independently for each modality, which implies a neural audio codec generating discrete representations. This fits the criteria for discrete audio tokens produced by vector quantization. The study further investigates these discrete representations for tasks including audiovisual speech emotion recognition and manipulation, indicating an application in audiovisual multimodal modeling. However, the paper primarily focuses on representation learning rather than explicitly discussing token design choices, vocabulary setting, or systematic evaluation related to discrete audio tokens. Given the use of discrete tokens through VQ-VAEs and their use in generation and recognition, it meets core inclusion criteria though lacks some details on token specifics and evaluation. Therefore, it leans more towards inclusion but not absolutely certain due to limited token detail disclosure.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
    "abstract": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
    "metadata": {
      "arxiv_id": "2305.06788",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "authors": [
        "Chih Wei Ling",
        "Cheuk Ting Li"
      ],
      "published": "2023-05-11T13:23:42Z",
      "updated": "2024-01-24T13:44:44Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06788v4",
      "landing_url": "https://arxiv.org/abs/2305.06788v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.06788"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on theoretical vector quantization and entropy bounds, with no mention of discrete audio tokens, neural codecs, or downstream audio generation/understanding tasks, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on theoretical vector quantization and entropy bounds, with no mention of discrete audio tokens, neural codecs, or downstream audio generation/understanding tasks, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses vector quantization techniques focusing on error distributions in quantization lattice structures, but does not address discrete audio tokens or their application to audio waveform discretization, neural audio codecs, token sequence modeling, or related multimodal tasks. It lacks mention of token design or usage in audio generation or understanding, nor does it describe a tokenization process relevant to the inclusion criteria. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens or neural codec-based token generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses vector quantization techniques focusing on error distributions in quantization lattice structures, but does not address discrete audio tokens or their application to audio waveform discretization, neural audio codecs, token sequence modeling, or related multimodal tasks. It lacks mention of token design or usage in audio generation or understanding, nor does it describe a tokenization process relevant to the inclusion criteria. Therefore, it does not meet the inclusion criteria of focusing on discrete audio tokens or neural codec-based token generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Language Model of Java Methods with Train/Test Deduplication",
    "abstract": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
    "metadata": {
      "arxiv_id": "2305.08286",
      "title": "A Language Model of Java Methods with Train/Test Deduplication",
      "summary": "This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.",
      "authors": [
        "Chia-Yi Su",
        "Aakash Bansal",
        "Vijayanta Jain",
        "Sepideh Ghanavati",
        "Collin McMillan"
      ],
      "published": "2023-05-15T00:22:02Z",
      "updated": "2023-05-15T00:22:02Z",
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.08286v1",
      "landing_url": "https://arxiv.org/abs/2305.08286v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.08286"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns a Java code language model with deduplication, not discrete audio tokens, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns a Java code language model with deduplication, not discrete audio tokens, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper deals with a language model of Java source code and discusses deduplication and data resources for Java methods. It does not focus on audio or audio tokens, discretization of continuous audio waveforms, or any related neural audio encoders or tokenizers. Hence, it does not meet any inclusion criteria related to discrete audio tokens nor violates exclusions by focusing on audio. The content is unrelated to the stated topic on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper deals with a language model of Java source code and discusses deduplication and data resources for Java methods. It does not focus on audio or audio tokens, discretization of continuous audio waveforms, or any related neural audio encoders or tokenizers. Hence, it does not meet any inclusion criteria related to discrete audio tokens nor violates exclusions by focusing on audio. The content is unrelated to the stated topic on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SoundStorm: Efficient Parallel Audio Generation",
    "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
    "metadata": {
      "arxiv_id": "2305.09636",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "authors": [
        "Zalán Borsos",
        "Matt Sharifi",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-05-16T17:41:25Z",
      "updated": "2023-05-16T17:41:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.09636v1",
      "landing_url": "https://arxiv.org/abs/2305.09636v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.09636"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete audio tokens via a neural audio codec and semantic tokens used for non-autoregressive generation, aligning with the discrete token criteria and downstream generation tasks; it fits the inclusion criteria and does not trigger the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete audio tokens via a neural audio codec and semantic tokens used for non-autoregressive generation, aligning with the discrete token criteria and downstream generation tasks; it fits the inclusion criteria and does not trigger the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes SoundStorm, a model that generates audio from semantic tokens of AudioLM and outputs tokens of a neural audio codec, indicating the use of discrete audio tokens derived from a neural codec; it focuses on generation rather than classification and provides sufficient information about the discrete token generation process, meeting the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes SoundStorm, a model that generates audio from semantic tokens of AudioLM and outputs tokens of a neural audio codec, indicating the use of discrete audio tokens derived from a neural codec; it focuses on generation rather than classification and provides sufficient information about the discrete token generation process, meeting the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A unified front-end framework for English text-to-speech synthesis",
    "abstract": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
    "metadata": {
      "arxiv_id": "2305.10666",
      "title": "A unified front-end framework for English text-to-speech synthesis",
      "summary": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
      "authors": [
        "Zelin Ying",
        "Chen Li",
        "Yu Dong",
        "Qiuqiang Kong",
        "Qiao Tian",
        "Yuanyuan Huo",
        "Yuxuan Wang"
      ],
      "published": "2023-05-18T02:57:54Z",
      "updated": "2024-03-25T10:59:04Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10666v3",
      "landing_url": "https://arxiv.org/abs/2305.10666v3",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447144"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses a unified front-end for English TTS modules (TN, PWPP, G2P) and does not involve discrete audio tokens, tokenizers, or neural encoding, so it fails the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses a unified front-end for English TTS modules (TN, PWPP, G2P) and does not involve discrete audio tokens, tokenizers, or neural encoding, so it fails the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a unified front-end framework for English TTS, emphasizing linguistic feature extraction modules like text normalization and grapheme-to-phoneme conversion; however, there is no mention of discrete audio tokens generated by neural audio codecs or self-supervised learning models, nor details on tokenization, quantization, or vocabulary relevant to discrete audio token research, thus it does not align with the inclusion criteria focusing on discrete token-based audio representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a unified front-end framework for English TTS, emphasizing linguistic feature extraction modules like text normalization and grapheme-to-phoneme conversion; however, there is no mention of discrete audio tokens generated by neural audio codecs or self-supervised learning models, nor details on tokenization, quantization, or vocabulary relevant to discrete audio token research, thus it does not align with the inclusion criteria focusing on discrete token-based audio representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
    "abstract": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these variations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining.",
    "metadata": {
      "arxiv_id": "2305.10684",
      "title": "Data Augmentation for Diverse Voice Conversion in Noisy Environments",
      "summary": "Voice conversion (VC) models have demonstrated impressive few-shot conversion quality on the clean, native speech populations they're trained on. However, when source or target speech accents, background noise conditions, or microphone characteristics differ from training, quality voice conversion is not guaranteed. These problems are often left unexamined in VC research, giving rise to frustration in users trying to use pretrained VC models on their own data. We are interested in accent-preserving voice conversion for name pronunciation from self-recorded examples, a domain in which all three of the aforementioned conditions are present, and posit that demonstrating higher performance in this domain correlates with creating VC models that are more usable by otherwise frustrated users. We demonstrate that existing SOTA encoder-decoder VC models can be made robust to these variations and endowed with natural denoising capabilities using more diverse data and simple data augmentation techniques in pretraining.",
      "authors": [
        "Avani Tanna",
        "Michael Saxon",
        "Amr El Abbadi",
        "William Yang Wang"
      ],
      "published": "2023-05-18T03:54:10Z",
      "updated": "2023-05-18T03:54:10Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10684v1",
      "landing_url": "https://arxiv.org/abs/2305.10684v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.10684"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses robustness of voice conversion models via data augmentation in noisy/varied environments and accents, but does not discuss discrete audio tokens, tokenization, or token-based downstream tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses robustness of voice conversion models via data augmentation in noisy/varied environments and accents, but does not discuss discrete audio tokens, tokenization, or token-based downstream tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving voice conversion models for robustness to accents and noise but does not discuss the generation or use of discrete audio tokens derived via neural audio encoders or quantization, nor does it describe token vocabularies or codecs, which are essential criteria for inclusion in the \"Discrete Audio Tokens\" topic. Therefore, it lacks the core concept of discrete token-based audio representation and modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving voice conversion models for robustness to accents and noise but does not discuss the generation or use of discrete audio tokens derived via neural audio encoders or quantization, nor does it describe token vocabularies or codecs, which are essential criteria for inclusion in the \"Discrete Audio Tokens\" topic. Therefore, it lacks the core concept of discrete token-based audio representation and modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
    "abstract": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
    "metadata": {
      "arxiv_id": "2305.11403",
      "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
      "summary": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
      "authors": [
        "Ling Zheng",
        "Jinchen Zhu",
        "Jinpeng Shi",
        "Shizhuang Weng"
      ],
      "published": "2023-05-19T03:19:38Z",
      "updated": "2023-06-19T06:56:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11403v5",
      "landing_url": "https://arxiv.org/abs/2305.11403v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.11403"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about an image super-resolution transformer and does not address discrete audio tokens or tokenization; it fails the core inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about an image super-resolution transformer and does not address discrete audio tokens or tokenization; it fails the core inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an Efficient Mixed Transformer method for Single Image Super-Resolution, which pertains to image processing rather than audio processing. It does not discuss or mention discrete audio tokens, neural audio codecs, or any discretization of continuous audio waveforms into token sequences. Therefore, it does not meet the inclusion criteria focused on discrete audio token research and is unrelated to the audio domain specified.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an Efficient Mixed Transformer method for Single Image Super-Resolution, which pertains to image processing rather than audio processing. It does not discuss or mention discrete audio tokens, neural audio codecs, or any discretization of continuous audio waveforms into token sequences. Therefore, it does not meet the inclusion criteria focused on discrete audio token research and is unrelated to the audio domain specified.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
    "abstract": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
    "metadata": {
      "arxiv_id": "2305.11490",
      "title": "LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation",
      "summary": "Following the impressive development of LLMs, vision-language alignment in LLMs is actively being researched to enable multimodal reasoning and visual IO. This direction of research is particularly relevant to medical imaging because medical image analysis and generation consist of reasoning based on a combination of visual features and prior knowledge. Many recent works have focused on training adapter networks that serve as an information bridge between image processing networks and LLMs; but presumably, in order to achieve maximum reasoning potential of LLMs on visual information as well, visual and language features should be allowed to interact more freely. This is especially important in the medical domain because understanding and generating medical images such as chest X-rays (CXR) require not only accurate visual and language-based reasoning but also a more intimate mapping between the two modalities. Thus, taking inspiration from previous work on the transformer and VQ-GAN combination for bidirectional image and text generation, we build upon this approach and develop a method for instruction-tuning an LLM pre-trained only on text to gain vision-language capabilities for medical images. Specifically, we leverage a pretrained LLM's existing question-answering and instruction-following abilities to teach it to understand visual inputs by instructing it to answer questions about image inputs and, symmetrically, output both text and image responses appropriate to a given query by tuning the LLM with diverse tasks that encompass image-based text-generation and text-based image-generation. We show that our model, LLM-CXR, trained in this approach shows better image-text alignment in both CXR understanding and generation tasks while being smaller in size compared to previously developed models that perform a narrower range of tasks. The code is at https://github.com/hyn2028/llm-cxr.",
      "authors": [
        "Suhyeon Lee",
        "Won Jun Kim",
        "Jinho Chang",
        "Jong Chul Ye"
      ],
      "published": "2023-05-19T07:44:39Z",
      "updated": "2024-03-18T03:41:09Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11490v5",
      "landing_url": "https://arxiv.org/abs/2305.11490v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.11490"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns vision-language modeling for chest X-ray understanding and image generation with an instruction-tuned LLM; it does not discuss discrete audio tokens, tokenization, or audio discretization, and thus fails the core inclusion criteria focused on discrete audio tokens, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns vision-language modeling for chest X-ray understanding and image generation with an instruction-tuned LLM; it does not discuss discrete audio tokens, tokenization, or audio discretization, and thus fails the core inclusion criteria focused on discrete audio tokens, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vision-language alignment and medical image understanding in chest X-rays, without involving audio waveforms or discrete audio tokens derived from neural audio codecs or self-supervised audio models. It does not address discrete audio token design, generation, or cross-modal alignment involving audio, and lacks information on tokenization method or discrete audio token vocabulary, thus does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vision-language alignment and medical image understanding in chest X-rays, without involving audio waveforms or discrete audio tokens derived from neural audio codecs or self-supervised audio models. It does not address discrete audio token design, generation, or cross-modal alignment involving audio, and lacks information on tokenization method or discrete audio token vocabulary, thus does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images",
    "abstract": "The highly realistic image quality achieved by current image generative models has many academic and industrial applications. To limit the use of such models to benign applications, though, it is necessary that tools to conclusively detect whether an image has been generated synthetically or not are developed. For this reason, several detectors have been developed providing excellent performance in computer vision applications, however, they can not be applied as they are to multispectral satellite images, and hence new models must be trained. In general, two-class classifiers can achieve very good detection accuracies, however they are not able to generalise to image domains and generative models architectures different than those used during training. For this reason, in this paper, we propose a one-class classifier based on Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the limitations of two-class classifiers. First, we emphasize the generalization problem that binary classifiers suffer from by training and testing an EfficientNet-B4 architecture on multiple multispectral datasets. Then we show that, since the VQ-VAE 2 based classifier is trained only on pristine images, it is able to detect images belonging to different domains and generated by architectures that have not been used during training. Last, we compare the two classifiers head-to-head on the same generated datasets, highlighting the superiori generalization capabilities of the VQ-VAE 2-based detector.",
    "metadata": {
      "arxiv_id": "2305.11795",
      "title": "A One-Class Classifier for the Detection of GAN Manipulated Multi-Spectral Satellite Images",
      "summary": "The highly realistic image quality achieved by current image generative models has many academic and industrial applications. To limit the use of such models to benign applications, though, it is necessary that tools to conclusively detect whether an image has been generated synthetically or not are developed. For this reason, several detectors have been developed providing excellent performance in computer vision applications, however, they can not be applied as they are to multispectral satellite images, and hence new models must be trained. In general, two-class classifiers can achieve very good detection accuracies, however they are not able to generalise to image domains and generative models architectures different than those used during training. For this reason, in this paper, we propose a one-class classifier based on Vector Quantized Variational Autoencoder 2 (VQ-VAE 2) features to overcome the limitations of two-class classifiers. First, we emphasize the generalization problem that binary classifiers suffer from by training and testing an EfficientNet-B4 architecture on multiple multispectral datasets. Then we show that, since the VQ-VAE 2 based classifier is trained only on pristine images, it is able to detect images belonging to different domains and generated by architectures that have not been used during training. Last, we compare the two classifiers head-to-head on the same generated datasets, highlighting the superiori generalization capabilities of the VQ-VAE 2-based detector.",
      "authors": [
        "Lydia Abady",
        "Giovanna Maria Dimitri",
        "Mauro Barni"
      ],
      "published": "2023-05-19T16:30:50Z",
      "updated": "2023-05-19T16:30:50Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11795v1",
      "landing_url": "https://arxiv.org/abs/2305.11795v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11795"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns an image-forensics detector for GAN-manipulated multispectral satellite images and discusses VQ-VAE2 features, which does not address discrete audio token generation or token design as required by the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns an image-forensics detector for GAN-manipulated multispectral satellite images and discusses VQ-VAE2 features, which does not address discrete audio token generation or token design as required by the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on detecting GAN manipulated multispectral satellite images using a one-class classifier and VQ-VAE 2; it does not involve discrete audio tokens, neural audio codecs for audio discretization, or audio token sequences for audio generation, understanding, or cross-modal modeling, thus not meeting the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on detecting GAN manipulated multispectral satellite images using a one-class classifier and VQ-VAE 2; it does not involve discrete audio tokens, neural audio codecs for audio discretization, or audio token sequences for audio generation, understanding, or cross-modal modeling, thus not meeting the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting",
    "abstract": "We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts/",
    "metadata": {
      "arxiv_id": "2305.11926",
      "title": "MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting",
      "summary": "We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts/",
      "authors": [
        "Neil Shah",
        "Vishal Tambrahalli",
        "Saiteja Kosgi",
        "Niranjan Pedanekar",
        "Vineet Gandhi"
      ],
      "published": "2023-05-19T13:43:36Z",
      "updated": "2023-05-19T13:43:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11926v1",
      "landing_url": "https://arxiv.org/abs/2305.11926v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multilingual TTS using self-supervised backbones and cross-language voice transfer; it does not describe discrete audio tokens, tokenization schemes, or token-based modeling, so it fails the inclusion criteria and meets an exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multilingual TTS using self-supervised backbones and cross-language voice transfer; it does not describe discrete audio tokens, tokenization schemes, or token-based modeling, so it fails the inclusion criteria and meets an exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a multilingual, multi-speaker TTS model using self-supervised speech representations but does not mention any discretization of audio into token sequences, quantization methods, neural audio codecs, or the use of discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their design or use, and appears to rely on continuous representations rather than discrete token sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a multilingual, multi-speaker TTS model using self-supervised speech representations but does not mention any discretization of audio into token sequences, quantization methods, neural audio codecs, or the use of discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their design or use, and appears to rely on continuous representations rather than discrete token sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "abstract": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
    "metadata": {
      "arxiv_id": "2305.12200",
      "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
      "summary": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
      "authors": [
        "Yuyue Wang",
        "Huan Xiao",
        "Yihan Wu",
        "Ruihua Song"
      ],
      "published": "2023-05-20T14:24:45Z",
      "updated": "2023-05-20T14:24:45Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12200v1",
      "landing_url": "https://arxiv.org/abs/2305.12200v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12200"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on enhancing expressive TTS for stand-up comedy and uses comedian-specific tokens, but there is no discussion of discrete audio token generation, tokenizers, or neural audio codecs as core methods; it aligns with the exclusion criteria for discrete-token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on enhancing expressive TTS for stand-up comedy and uses comedian-specific tokens, but there is no discussion of discrete audio token generation, tokenizers, or neural audio codecs as core methods; it aligns with the exclusion criteria for discrete-token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on constructing a TTS system for stand-up comedies in low-resource scenarios, but it does not mention the use or design of discrete audio tokens, neural audio codec architecture, or any form of discretization mapping continuous audio waveforms into token sequences. The methods described rely on prosody encoding, rhythm modeling, and special tokens for fillers, but these do not align with the discrete audio token framework defined in the inclusion criteria, nor does it provide details about tokenization, codebooks, or vector quantization. Therefore, the paper does not meet the inclusion criteria centered around discrete audio tokens and their usage in audio generation or modeling. Hence, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on constructing a TTS system for stand-up comedies in low-resource scenarios, but it does not mention the use or design of discrete audio tokens, neural audio codec architecture, or any form of discretization mapping continuous audio waveforms into token sequences. The methods described rely on prosody encoding, rhythm modeling, and special tokens for fillers, but these do not align with the discrete audio token framework defined in the inclusion criteria, nor does it provide details about tokenization, codebooks, or vector quantization. Therefore, the paper does not meet the inclusion criteria centered around discrete audio tokens and their usage in audio generation or modeling. Hence, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
    "abstract": "Voice conversion is an increasingly popular technology, and the growing number of real-time applications requires models with streaming conversion capabilities. Unlike typical (non-streaming) voice conversion, which can leverage the entire utterance as full context, streaming voice conversion faces significant challenges due to the missing future information, resulting in degraded intelligibility, speaker similarity, and sound quality. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion. Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252.8 ms.",
    "metadata": {
      "arxiv_id": "2305.12425",
      "title": "DualVC: Dual-mode Voice Conversion using Intra-model Knowledge Distillation and Hybrid Predictive Coding",
      "summary": "Voice conversion is an increasingly popular technology, and the growing number of real-time applications requires models with streaming conversion capabilities. Unlike typical (non-streaming) voice conversion, which can leverage the entire utterance as full context, streaming voice conversion faces significant challenges due to the missing future information, resulting in degraded intelligibility, speaker similarity, and sound quality. To address this challenge, we propose DualVC, a dual-mode neural voice conversion approach that supports both streaming and non-streaming modes using jointly trained separate network parameters. Furthermore, we propose intra-model knowledge distillation and hybrid predictive coding (HPC) to enhance the performance of streaming conversion. Additionally, we incorporate data augmentation to train a noise-robust autoregressive decoder, improving the model's performance on long-form speech conversion. Experimental results demonstrate that the proposed model outperforms the baseline models in the context of streaming voice conversion, while maintaining comparable performance to the non-streaming topline system that leverages the complete context, albeit with a latency of only 252.8 ms.",
      "authors": [
        "Ziqian Ning",
        "Yuepeng Jiang",
        "Pengcheng Zhu",
        "Jixun Yao",
        "Shuai Wang",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2023-05-21T10:45:48Z",
      "updated": "2023-05-31T01:17:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12425v2",
      "landing_url": "https://arxiv.org/abs/2305.12425v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.12425"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文未提出離散音訊 token 的產生流程、碼本設定或以離散 token 為核心方法，屬於連續特徵/語音轉換研究，與本主題不符，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文未提出離散音訊 token 的產生流程、碼本設定或以離散 token 為核心方法，屬於連續特徵/語音轉換研究，與本主題不符，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion technology with streaming and non-streaming neural models and introduces techniques such as intra-model knowledge distillation and hybrid predictive coding to improve performance. However, the abstract does not mention any discrete audio tokens, tokenization, neural audio codec, vector quantization, or any process of discretizing audio into token sequences. It mainly deals with continuous audio waveform modeling for voice conversion rather than representing audio as discrete tokens or studying downstream tasks involving discrete tokens, which is the core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio token design or usage and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion technology with streaming and non-streaming neural models and introduces techniques such as intra-model knowledge distillation and hybrid predictive coding to improve performance. However, the abstract does not mention any discrete audio tokens, tokenization, neural audio codec, vector quantization, or any process of discretizing audio into token sequences. It mainly deals with continuous audio waveform modeling for voice conversion rather than representing audio as discrete tokens or studying downstream tasks involving discrete tokens, which is the core inclusion criterion. Therefore, it does not meet the inclusion criteria related to discrete audio token design or usage and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Textually Pretrained Speech Language Models",
    "abstract": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
    "metadata": {
      "arxiv_id": "2305.13009",
      "title": "Textually Pretrained Speech Language Models",
      "summary": "Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available: https://pages.cs.huji.ac.il/adiyoss-lab/twist/ .",
      "authors": [
        "Michael Hassid",
        "Tal Remez",
        "Tu Anh Nguyen",
        "Itai Gat",
        "Alexis Conneau",
        "Felix Kreuk",
        "Jade Copet",
        "Alexandre Defossez",
        "Gabriel Synnaeve",
        "Emmanuel Dupoux",
        "Roy Schwartz",
        "Yossi Adi"
      ],
      "published": "2023-05-22T13:12:16Z",
      "updated": "2024-01-30T11:52:51Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13009v3",
      "landing_url": "https://arxiv.org/abs/2305.13009v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.13009"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on training Speech Language Models with a textual warm-start and mentions a speech tokenizer, but it does not present discrete audio token generation/码本/词汇表 design or analyze the impact of discrete audio tokens, which is required by the Discrete Audio Tokens criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on training Speech Language Models with a textual warm-start and mentions a speech tokenizer, but it does not present discrete audio token generation/码本/词汇表 design or analyze the impact of discrete audio tokens, which is required by the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes TWIST, a method for training speech language models using pretrained textual language models, focusing on acoustic data processing and generating speech models. However, the abstract does not explicitly mention the use or design of discrete audio tokens, such as neural audio codec-based quantization, multi-codebooks, or discrete token vocabularies, nor does it offer details on tokenization or discrete representation used. The approach appears to be continuous acoustic representation based and focuses on model training and evaluation rather than discrete token generation or analysis. Thus, it does not meet the core inclusion criteria involving discrete audio tokens with detailed tokenization or codec mechanisms. It also does not appear to exclude based on language or study type but lacks essential aspects of the token design required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes TWIST, a method for training speech language models using pretrained textual language models, focusing on acoustic data processing and generating speech models. However, the abstract does not explicitly mention the use or design of discrete audio tokens, such as neural audio codec-based quantization, multi-codebooks, or discrete token vocabularies, nor does it offer details on tokenization or discrete representation used. The approach appears to be continuous acoustic representation based and focuses on model training and evaluation rather than discrete token generation or analysis. Thus, it does not meet the core inclusion criteria involving discrete audio tokens with detailed tokenization or codec mechanisms. It also does not appear to exclude based on language or study type but lacks essential aspects of the token design required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Adversarial Defenses via Vector Quantization",
    "abstract": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on adversarial defenses for vision using vector quantization of image patches, not on discrete audio tokens or audio tokenization; it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on adversarial defenses for vision using vector quantization of image patches, not on discrete audio tokens or audio tokenization; it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adversarial defenses for computer vision neural networks using vector quantization as a preprocessing step, without involving audio waveform discretization or discrete audio tokens representation as defined in the inclusion criteria. It lacks any discussion about generating or utilizing discrete audio tokens for audio modeling, generation, or cross-modal tasks, and it centers on image data rather than audio data, thus not meeting the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adversarial defenses for computer vision neural networks using vector quantization as a preprocessing step, without involving audio waveform discretization or discrete audio tokens representation as defined in the inclusion criteria. It lacks any discussion about generating or utilizing discrete audio tokens for audio modeling, generation, or cross-modal tasks, and it centers on image data rather than audio data, thus not meeting the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
    "abstract": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
    "metadata": {
      "arxiv_id": "2305.13686",
      "title": "MP-SENet: A Speech Enhancement Model with Parallel Denoising of Magnitude and Phase Spectra",
      "summary": "This paper proposes MP-SENet, a novel Speech Enhancement Network which directly denoises Magnitude and Phase spectra in parallel. The proposed MP-SENet adopts a codec architecture in which the encoder and decoder are bridged by convolution-augmented transformers. The encoder aims to encode time-frequency representations from the input noisy magnitude and phase spectra. The decoder is composed of parallel magnitude mask decoder and phase decoder, directly recovering clean magnitude spectra and clean-wrapped phase spectra by incorporating learnable sigmoid activation and parallel phase estimation architecture, respectively. Multi-level losses defined on magnitude spectra, phase spectra, short-time complex spectra, and time-domain waveforms are used to train the MP-SENet model jointly. Experimental results show that our proposed MP-SENet achieves a PESQ of 3.50 on the public VoiceBank+DEMAND dataset and outperforms existing advanced speech enhancement methods.",
      "authors": [
        "Ye-Xin Lu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2023-05-23T04:48:51Z",
      "updated": "2023-05-23T04:48:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13686v1",
      "landing_url": "https://arxiv.org/abs/2305.13686v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1441"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents MP-SENet for denoising magnitude and phase spectra; there is no discrete audio token generation/codec, tokenization, or analysis of token design, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper presents MP-SENet for denoising magnitude and phase spectra; there is no discrete audio token generation/codec, tokenization, or analysis of token design, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a speech enhancement model that directly denoises magnitude and phase spectra but does not mention the use or generation of discrete audio tokens through quantization, neural audio codecs, or clustering methods; therefore, it does not meet the core inclusion criteria focused on discrete token-based audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a speech enhancement model that directly denoises magnitude and phase spectra but does not mention the use or generation of discrete audio tokens through quantization, neural audio codecs, or clustering methods; therefore, it does not meet the core inclusion criteria focused on discrete token-based audio representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EfficientSpeech: An On-Device Text to Speech Model",
    "abstract": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
    "metadata": {
      "arxiv_id": "2305.13905",
      "title": "EfficientSpeech: An On-Device Text to Speech Model",
      "summary": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
      "authors": [
        "Rowel Atienza"
      ],
      "published": "2023-05-23T10:28:41Z",
      "updated": "2023-05-23T10:28:41Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13905v1",
      "landing_url": "https://arxiv.org/abs/2305.13905v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.13905"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on an on-device, efficient TTS model with a small footprint; it does not discuss discrete audio tokens, tokenizers, codecs, or token-based modeling, which are required by the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on an on-device, efficient TTS model with a small footprint; it does not discuss discrete audio tokens, tokenizers, codecs, or token-based modeling, which are required by the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a neural text-to-speech (TTS) model optimized for edge devices with low computational cost but does not mention any discrete audio token representations, their design, quantization method, or vocabulary settings, which are essential for meeting the inclusion criteria focused on discrete audio tokens. This study centers on continuous speech synthesis rather than tokenizing audio into discrete units for modeling or generation, thus failing to meet the core criteria of the review.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a neural text-to-speech (TTS) model optimized for edge devices with low computational cost but does not mention any discrete audio token representations, their design, quantization method, or vocabulary settings, which are essential for meeting the inclusion criteria focused on discrete audio tokens. This study centers on continuous speech synthesis rather than tokenizing audio into discrete units for modeling or generation, thus failing to meet the core criteria of the review.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
    "abstract": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
    "metadata": {
      "arxiv_id": "2305.14858",
      "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
      "summary": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
      "authors": [
        "Zixuan Jiang",
        "Jiaqi Gu",
        "Hanqing Zhu",
        "David Z. Pan"
      ],
      "published": "2023-05-24T08:08:26Z",
      "updated": "2023-10-26T04:32:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14858v2",
      "landing_url": "https://arxiv.org/abs/2305.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.14858"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on normalization architectures in Transformers (Pre-LN, RMSNorm, CRMSNorm) and efficiency; it does not involve discrete audio tokens, tokenizers, neural audio codecs, or analysis of token design, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on normalization architectures in Transformers (Pre-LN, RMSNorm, CRMSNorm) and efficiency; it does not involve discrete audio tokens, tokenizers, neural audio codecs, or analysis of token design, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The provided title and abstract focus on optimization and equivalence of Transformer normalization techniques aimed at computational efficiency, without discussing audio discretization, neural audio codecs, or discrete audio token generation related to speech/music/environmental sound modeling as required by the inclusion criteria. There is no description of finite token vocabularies, neural vector quantization, or downstream tasks involving discrete audio tokens, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The provided title and abstract focus on optimization and equivalence of Transformer normalization techniques aimed at computational efficiency, without discussing audio discretization, neural audio codecs, or discrete audio token generation related to speech/music/environmental sound modeling as required by the inclusion criteria. There is no description of finite token vocabularies, neural vector quantization, or downstream tasks involving discrete audio tokens, thus it does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Neural Music Generation",
    "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",
    "metadata": {
      "arxiv_id": "2305.15719",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Qiao Tian",
        "Tang Li",
        "Zongyu Yin",
        "Siyuan Feng",
        "Ming Tu",
        "Yuliang Ji",
        "Rui Xia",
        "Mingbo Ma",
        "Xuchen Song",
        "Jitong Chen",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2023-05-25T05:02:35Z",
      "updated": "2023-05-25T05:02:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15719v1",
      "landing_url": "https://arxiv.org/abs/2305.15719v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15719"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the work uses the terms 'semantic tokens' and token-based conditioning, the abstract does not specify a discrete tokenization/codec process, vocabulary size, or explicit discrete token generation (which are core to the 'Discrete Audio Tokens' topic). It centers on LM-guided diffusion with latent/semantic representations and an audio VAE-GAN for waveform decoding, which suggests continuous latent representations rather than a clearly defined discrete token framework.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the work uses the terms 'semantic tokens' and token-based conditioning, the abstract does not specify a discrete tokenization/codec process, vocabulary size, or explicit discrete token generation (which are core to the 'Discrete Audio Tokens' topic). It centers on LM-guided diffusion with latent/semantic representations and an audio VAE-GAN for waveform decoding, which suggests continuous latent representations rather than a clearly defined discrete token framework.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract presents MeLoDy, a neural music generation model using an LM-guided diffusion approach and an audio VAE-GAN, but it does not explicitly describe the use or generation of discrete audio tokens such as vector-quantized tokens or discrete vocabularies central to the inclusion criteria. While it leverages semantic tokens from MusicLM, the abstract lacks precise information on tokenization details, codebook settings, or discrete token generation mechanisms required for inclusion. Hence, it does not sufficiently demonstrate adherence to the discrete audio token framework needed for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract presents MeLoDy, a neural music generation model using an LM-guided diffusion approach and an audio VAE-GAN, but it does not explicitly describe the use or generation of discrete audio tokens such as vector-quantized tokens or discrete vocabularies central to the inclusion criteria. While it leverages semantic tokens from MusicLM, the abstract lacks precise information on tokenization details, codebook settings, or discrete token generation mechanisms required for inclusion. Hence, it does not sufficiently demonstrate adherence to the discrete audio token framework needed for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
    "abstract": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
    "metadata": {
      "arxiv_id": "2305.16753",
      "title": "ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants",
      "summary": "ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared error (MSE), linear correlation coefficient (LCC) and Spearman's rank correlation coefficient (SRCC). The ElectrodeNet-CS strategy was capable of producing N-of-M compatible electrode patterns using a modified DNN network to embed maxima selection, and to perform in similar or even slightly higher average in STOI and sentence recognition compared to ACE. The methods and findings demonstrated the feasibility and potential of using deep learning in CI coding strategy.",
      "authors": [
        "Enoch Hsin-Ho Huang",
        "Rong Chao",
        "Yu Tsao",
        "Chao-Min Wu"
      ],
      "published": "2023-05-26T09:06:04Z",
      "updated": "2023-05-26T09:06:04Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16753v1",
      "landing_url": "https://arxiv.org/abs/2305.16753v1",
      "doi": "https://doi.org/10.1109/TCDS.2023.3275587"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses deep learning-based cochlear implant coding (ElectrodeNet) but does not involve discrete audio tokens, tokenizers, or token-based modeling, thus failing to meet the inclusion criteria and aligning with the exclusion criterion against non-token discrete representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses deep learning-based cochlear implant coding (ElectrodeNet) but does not involve discrete audio tokens, tokenizers, or token-based modeling, thus failing to meet the inclusion criteria and aligning with the exclusion criterion against non-token discrete representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article presents a deep learning-based sound coding strategy for cochlear implants focusing on neural networks to replicate existing envelope detection methods, but it does not discuss discretization of continuous audio waveforms into discrete token sequences or propose a neural audio codec generating discrete tokens as core representations; thus it does not meet the inclusion criteria focused on discrete audio tokens and neural codec/tokenizer mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article presents a deep learning-based sound coding strategy for cochlear implants focusing on neural networks to replicate existing envelope detection methods, but it does not discuss discretization of continuous audio waveforms into discrete token sequences or propose a neural audio codec generating discrete tokens as core representations; thus it does not meet the inclusion criteria focused on discrete audio tokens and neural codec/tokenizer mechanisms.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
    "abstract": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
    "metadata": {
      "arxiv_id": "2305.17310",
      "title": "DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication",
      "summary": "Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time.",
      "authors": [
        "Igor Nunes",
        "Mike Heddes",
        "Pere Vergés",
        "Danny Abraham",
        "Alexander Veidenbaum",
        "Alexandru Nicolau",
        "Tony Givargis"
      ],
      "published": "2023-05-27T00:05:39Z",
      "updated": "2023-05-27T00:05:39Z",
      "categories": [
        "cs.SI",
        "cs.DS",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17310v1",
      "landing_url": "https://arxiv.org/abs/2305.17310v1",
      "doi": "https://doi.org/10.1145/3580305.3599314"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns set similarity estimators (DotHash) for link prediction/deduplication and does not address discrete audio tokens, tokenization, or audio-based modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns set similarity estimators (DotHash) for link prediction/deduplication and does not address discrete audio tokens, tokenization, or audio-based modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on developing efficient estimators for set similarity metrics used in data mining tasks like link prediction and document deduplication but does not deal with discretized audio tokens or neural audio codec-based tokenization, nor does it address audio generation, understanding, or cross-modal modeling relevant to the discrete audio token topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on developing efficient estimators for set similarity metrics used in data mining tasks like link prediction and document deduplication but does not deal with discretized audio tokens or neural audio codec-based tokenization, nor does it address audio generation, understanding, or cross-modal modeling relevant to the discrete audio token topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Geometric Algebra Transformer",
    "abstract": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
    "metadata": {
      "arxiv_id": "2305.18415",
      "title": "Geometric Algebra Transformer",
      "summary": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
      "authors": [
        "Johann Brehmer",
        "Pim de Haan",
        "Sönke Behrends",
        "Taco Cohen"
      ],
      "published": "2023-05-28T18:48:50Z",
      "updated": "2023-11-20T08:31:51Z",
      "categories": [
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18415v3",
      "landing_url": "https://arxiv.org/abs/2305.18415v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.18415"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Geometric Algebra Transformer article targets geometric data (Clifford/Geometric Algebra) and 3D E(3) equivariant transformers, not discrete audio tokens, tokenization, or audio generation/understanding; it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic and fails all relevant exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The Geometric Algebra Transformer article targets geometric data (Clifford/Geometric Algebra) and 3D E(3) equivariant transformers, not discrete audio tokens, tokenization, or audio generation/understanding; it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic and fails all relevant exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a Geometric Algebra Transformer for various geometric data types in fields like physics and robotics; however, it does not discuss discrete audio tokens, neural audio codecs, vector quantization, or any audio waveform discretization relevant to audio tokenization or audio-related sequence modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related technologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a Geometric Algebra Transformer for various geometric data types in fields like physics and robotics; however, it does not discuss discrete audio tokens, neural audio codecs, vector quantization, or any audio waveform discretization relevant to audio tokenization or audio-related sequence modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related technologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An empirical study on speech restoration guided by self supervised speech representation",
    "abstract": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
    "metadata": {
      "arxiv_id": "2305.18739",
      "title": "An empirical study on speech restoration guided by self supervised speech representation",
      "summary": "Enhancing speech quality is an indispensable yet difficult task as it is often complicated by a range of degradation factors. In addition to additive noise, reverberation, clipping, and speech attenuation can all adversely affect speech quality. Speech restoration aims to recover speech components from these distortions. This paper focuses on exploring the impact of self-supervised speech representation learning on the speech restoration task. Specifically, we employ speech representation in various speech restoration networks and evaluate their performance under complicated distortion scenarios. Our experiments demonstrate that the contextual information provided by the self-supervised speech representation can enhance speech restoration performance in various distortion scenarios, while also increasing robustness against the duration of speech attenuation and mismatched test conditions.",
      "authors": [
        "Jaeuk Byun",
        "Youna Ji",
        "Soo Whan Chung",
        "Soyeon Choe",
        "Min Seok Choi"
      ],
      "published": "2023-05-30T04:26:48Z",
      "updated": "2023-05-30T04:26:48Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18739v1",
      "landing_url": "https://arxiv.org/abs/2305.18739v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095881"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper studies self-supervised speech representations for restoration but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper studies self-supervised speech representations for restoration but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech restoration using self-supervised speech representation learning, but does not explicitly mention the use of discrete audio tokens derived via quantization or clustering, nor does it detail tokenization processes or token design choices crucial to the inclusion criteria. Since it lacks clear evidence that the core methodology revolves around discrete tokens as defined, it does not meet the required inclusion criteria nor violates the exclusion criteria explicitly, resulting in uncertainty about inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech restoration using self-supervised speech representation learning, but does not explicitly mention the use of discrete audio tokens derived via quantization or clustering, nor does it detail tokenization processes or token design choices crucial to the inclusion criteria. Since it lacks clear evidence that the core methodology revolves around discrete tokens as defined, it does not meet the required inclusion criteria nor violates the exclusion criteria explicitly, resulting in uncertainty about inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the abstract and both junior reviewers' comments: the paper investigates speech restoration using self-supervised speech representations but contains no mention of discretization, tokenization, vector quantization, codebooks, or discrete units—only continuous representations are described; this matches the exclusion criterion for studies without discrete token generation details. 1-sentence explanation: Because the manuscript lacks any description or use of discrete audio tokens (tokenizer/codec/quantization/codebook), it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed the abstract and both junior reviewers' comments: the paper investigates speech restoration using self-supervised speech representations but contains no mention of discretization, tokenization, vector quantization, codebooks, or discrete units—only continuous representations are described; this matches the exclusion criterion for studies without discrete token generation details. 1-sentence explanation: Because the manuscript lacks any description or use of discrete audio tokens (tokenizer/codec/quantization/codebook), it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "DualVAE: Controlling Colours of Generated and Real Images",
    "abstract": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity mapping that identifies structural features. The disentangled representation is obtained by two novel mechanisms: (i) a dual branch architecture that separates image colour attributes from geometric attributes, and (ii) a new ELBO that trains the combined colour and geometry representations. DualVAE can control the colour of generated images, and recolour existing images by transferring the colour latent representation obtained from an exemplar image. We demonstrate that DualVAE generates images with FID nearly two times better than VQ-GAN on a diverse collection of datasets, including animated faces, logos and artistic landscapes.",
    "metadata": {
      "arxiv_id": "2305.18769",
      "title": "DualVAE: Controlling Colours of Generated and Real Images",
      "summary": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity mapping that identifies structural features. The disentangled representation is obtained by two novel mechanisms:\n  (i) a dual branch architecture that separates image colour attributes from geometric attributes, and (ii) a new ELBO that trains the combined colour and geometry representations. DualVAE can control the colour of generated images, and recolour existing images by transferring the colour latent representation obtained from an exemplar image. We demonstrate that DualVAE generates images with FID nearly two times better than VQ-GAN on a diverse collection of datasets, including animated faces, logos and artistic landscapes.",
      "authors": [
        "Keerth Rathakumar",
        "David Liebowitz",
        "Christian Walder",
        "Kristen Moore",
        "Salil S. Kanhere"
      ],
      "published": "2023-05-30T06:04:30Z",
      "updated": "2023-05-30T06:04:30Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18769v1",
      "landing_url": "https://arxiv.org/abs/2305.18769v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.18769"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets image generation with disentangled color/geometry representations and discrete-style latent codes, not discrete audio tokens or audio tokenization workflows, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets image generation with disentangled color/geometry representations and discrete-style latent codes, not discrete audio tokens or audio tokenization workflows, so it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses DualVAE, a model for color-controllable image generation and manipulation, focusing on image representations rather than audio processing; it neither addresses discrete audio tokens nor audio tokenization methods, thus failing to meet the inclusion criteria related to audio discretization and token-based audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses DualVAE, a model for color-controllable image generation and manipulation, focusing on image representations rather than audio processing; it neither addresses discrete audio tokens nor audio tokenization methods, thus failing to meet the inclusion criteria related to audio discretization and token-based audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voice Conversion With Just Nearest Neighbors",
    "abstract": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity -- making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
    "metadata": {
      "arxiv_id": "2305.18975",
      "title": "Voice Conversion With Just Nearest Neighbors",
      "summary": "Any-to-any voice conversion aims to transform source speech into a target voice with just a few examples of the target speaker as a reference. Recent methods produce convincing conversions, but at the cost of increased complexity -- making results difficult to reproduce and build on. Instead, we keep it simple. We propose k-nearest neighbors voice conversion (kNN-VC): a straightforward yet effective method for any-to-any conversion. First, we extract self-supervised representations of the source and reference speech. To convert to the target speaker, we replace each frame of the source representation with its nearest neighbor in the reference. Finally, a pretrained vocoder synthesizes audio from the converted representation. Objective and subjective evaluations show that kNN-VC improves speaker similarity with similar intelligibility scores to existing methods. Code, samples, trained models: https://bshall.github.io/knn-vc",
      "authors": [
        "Matthew Baas",
        "Benjamin van Niekerk",
        "Herman Kamper"
      ],
      "published": "2023-05-30T12:19:07Z",
      "updated": "2023-05-30T12:19:07Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18975v1",
      "landing_url": "https://arxiv.org/abs/2305.18975v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.18975"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on kNN-based voice conversion using self-supervised frame representations and a vocoder; there is no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete-token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on kNN-based voice conversion using self-supervised frame representations and a vocoder; there is no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete-token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a voice conversion method based on k-nearest neighbors using self-supervised speech representations to replace source frames with nearest reference frames and synthesizing audio with a vocoder. However, the method does not describe any discretization of audio waveforms into discrete tokens or the use of neural audio codecs with vector quantization or similar techniques to produce discrete token sequences. The abstract lacks detail on any discrete token vocabularies, tokenization processes, or evaluation related to discrete audio token designs, making it not aligned with the inclusion criteria requiring discrete token-based representations central to the study. Hence, it falls under exclusion as it focuses on continuous self-supervised representations without explicit discrete token modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a voice conversion method based on k-nearest neighbors using self-supervised speech representations to replace source frames with nearest reference frames and synthesizing audio with a vocoder. However, the method does not describe any discretization of audio waveforms into discrete tokens or the use of neural audio codecs with vector quantization or similar techniques to produce discrete token sequences. The abstract lacks detail on any discrete token vocabularies, tokenization processes, or evaluation related to discrete audio token designs, making it not aligned with the inclusion criteria requiring discrete token-based representations central to the study. Hence, it falls under exclusion as it focuses on continuous self-supervised representations without explicit discrete token modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
    "abstract": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
    "metadata": {
      "arxiv_id": "2305.19269",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Luping Liu",
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "published": "2023-05-30T17:59:26Z",
      "updated": "2023-05-30T17:59:26Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19269v1",
      "landing_url": "https://arxiv.org/abs/2305.19269v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19269"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete audio tokens (tokenization, multi-codebooks, vector quantization) as core representations for unified voice synthesis and discusses applications to TTS, voice conversion, and singing voice synthesis with self-supervised tokens and token-based evaluation, which aligns with the inclusion criteria and does not fall under any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete audio tokens (tokenization, multi-codebooks, vector quantization) as core representations for unified voice synthesis and discusses applications to TTS, voice conversion, and singing voice synthesis with self-supervised tokens and token-based evaluation, which aligns with the inclusion criteria and does not fall under any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes Make-A-Voice, a unified voice synthesis framework that relies on discrete representations generated from self-supervised semantic tokens and acoustic tokens, which aligns with the definition of discrete audio tokens derived from neural codecs or self-supervised models as per the inclusion criteria. It details a 'coarse-to-fine' approach involving semantic and acoustic stages producing discrete tokens, with clear descriptions of modeling linguistic to acoustic transformations, and includes empirical evaluation on multiple voice synthesis tasks (TTS, VC, SVS). The abstract provides enough methodological insight to suggest the use and generation of discrete token sequences as core representations, meeting the requirement for English documentation and token design explanation, along with demonstrations of controllability and quality—hence fulfilling inclusion criteria and no exclusion criteria being met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes Make-A-Voice, a unified voice synthesis framework that relies on discrete representations generated from self-supervised semantic tokens and acoustic tokens, which aligns with the definition of discrete audio tokens derived from neural codecs or self-supervised models as per the inclusion criteria. It details a 'coarse-to-fine' approach involving semantic and acoustic stages producing discrete tokens, with clear descriptions of modeling linguistic to acoustic transformations, and includes empirical evaluation on multiple voice synthesis tasks (TTS, VC, SVS). The abstract provides enough methodological insight to suggest the use and generation of discrete token sequences as core representations, meeting the requirement for English documentation and token design explanation, along with demonstrations of controllability and quality—hence fulfilling inclusion criteria and no exclusion criteria being met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
    "abstract": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
    "metadata": {
      "arxiv_id": "2305.19533",
      "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
      "summary": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
      "authors": [
        "Hanqing Zhu",
        "Jiaqi Gu",
        "Hanrui Wang",
        "Zixuan Jiang",
        "Zhekai Zhang",
        "Rongxing Tang",
        "Chenghao Feng",
        "Song Han",
        "Ray T. Chen",
        "David Z. Pan"
      ],
      "published": "2023-05-31T03:37:11Z",
      "updated": "2023-12-31T23:22:41Z",
      "categories": [
        "cs.ET",
        "cs.AR",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19533v3",
      "landing_url": "https://arxiv.org/abs/2305.19533v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.19533"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項研究聚焦光子計算硬體加速器，與離散音訊 token 的設計、產生流程、或下游多模態建模無關，因此不符合收錄標準，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項研究聚焦光子計算硬體加速器，與離散音訊 token 的設計、產生流程、或下游多模態建模無關，因此不符合收錄標準，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on hardware acceleration for Transformer architectures using photonics and does not address discrete audio tokens derived from audio waveform discretization, nor does it discuss token design, generation processes, or evaluation related to audio signals. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens for audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on hardware acceleration for Transformer architectures using photonics and does not address discrete audio tokens derived from audio waveform discretization, nor does it discuss token design, generation processes, or evaluation related to audio signals. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens for audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
    "abstract": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
    "metadata": {
      "arxiv_id": "2305.19750",
      "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
      "summary": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
      "authors": [
        "Tobias Bollinger",
        "Jan Deriu",
        "Manfred Vogel"
      ],
      "published": "2023-05-31T11:33:18Z",
      "updated": "2023-05-31T11:33:18Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19750v1",
      "landing_url": "https://arxiv.org/abs/2305.19750v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on TTS model comparison and a discriminator-based evaluation, with no discussion of discrete audio tokens, tokenizers, or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on TTS model comparison and a discriminator-based evaluation, with no discussion of discrete audio tokens, tokenizers, or token-based representations, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a TTS pipeline for Swiss German but does not mention the use of discrete audio tokens, neural audio codec, vector quantization, or any tokenization process that maps continuous audio to discrete token sequences; the focus appears solely on model comparison and evaluation metrics without addressing discrete token representation or analysis relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a TTS pipeline for Swiss German but does not mention the use of discrete audio tokens, neural audio codec, vector quantization, or any tokenization process that maps continuous audio to discrete token sequences; the focus appears solely on model comparison and evaluation metrics without addressing discrete token representation or analysis relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
    "abstract": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
    "metadata": {
      "arxiv_id": "2305.19957",
      "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
      "summary": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
      "authors": [
        "Maoyuan Ye",
        "Jing Zhang",
        "Shanshan Zhao",
        "Juhua Liu",
        "Tongliang Liu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "published": "2023-05-31T15:44:00Z",
      "updated": "2024-03-18T13:30:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19957v2",
      "landing_url": "https://arxiv.org/abs/2305.19957v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.19957"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a DETR-based multilingual text spotting method for images, not on discrete audio tokens or tokenization for audio, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a DETR-based multilingual text spotting method for images, not on discrete audio tokens or tokenization for audio, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual text spotting using a transformer decoder for text detection and recognition, which is related to visual text processing rather than audio tokenization or discrete audio representations. It does not address discrete audio tokens, neural audio codecs, or audio token design, thus failing the inclusion criteria around discrete audio tokens entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual text spotting using a transformer decoder for text detection and recognition, which is related to visual text processing rather than audio tokenization or discrete audio representations. It does not address discrete audio tokens, neural audio codecs, or audio token design, thus failing the inclusion criteria around discrete audio tokens entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
    "abstract": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
    "metadata": {
      "arxiv_id": "2306.00331",
      "title": "A Multi-dimensional Deep Structured State Space Approach to Speech Enhancement Using Small-footprint Models",
      "summary": "We propose a multi-dimensional structured state space (S4) approach to speech enhancement. To better capture the spectral dependencies across the frequency axis, we focus on modifying the multi-dimensional S4 layer with whitening transformation to build new small-footprint models that also achieve good performance. We explore several S4-based deep architectures in time (T) and time-frequency (TF) domains. The 2-D S4 layer can be considered a particular convolutional layer with an infinite receptive field although it utilizes fewer parameters than a conventional convolutional layer. Evaluated on the VoiceBank-DEMAND data set, when compared with the conventional U-net model based on convolutional layers, the proposed TF-domain S4-based model is 78.6% smaller in size, yet it still achieves competitive results with a PESQ score of 3.15 with data augmentation. By increasing the model size, we can even reach a PESQ score of 3.18.",
      "authors": [
        "Pin-Jui Ku",
        "Chao-Han Huck Yang",
        "Sabato Marco Siniscalchi",
        "Chin-Hui Lee"
      ],
      "published": "2023-06-01T04:19:57Z",
      "updated": "2023-06-01T04:19:57Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00331v1",
      "landing_url": "https://arxiv.org/abs/2306.00331v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-1084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on S4-based speech enhancement and small-footprint models, with no mention of discrete audio tokens, tokenizers, or token-level discretization, thus it does not meet the discrete tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on S4-based speech enhancement and small-footprint models, with no mention of discrete audio tokens, tokenizers, or token-level discretization, thus it does not meet the discrete tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech enhancement using a multi-dimensional structured state space model without mentioning any discretization of audio into discrete tokens, tokenizers, neural audio codecs, or vector quantization, which are central to the inclusion criteria. The study discusses continuous representation modeling and small-footprint models but lacks any detailed information on discrete token formation, vocabulary settings, or their impact on downstream tasks related to discrete audio token research. Thus, it does not meet the inclusion criteria and falls under the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech enhancement using a multi-dimensional structured state space model without mentioning any discretization of audio into discrete tokens, tokenizers, neural audio codecs, or vector quantization, which are central to the inclusion criteria. The study discusses continuous representation modeling and small-footprint models but lacks any detailed information on discrete token formation, vocabulary settings, or their impact on downstream tasks related to discrete audio token research. Thus, it does not meet the inclusion criteria and falls under the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
    "abstract": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
    "metadata": {
      "arxiv_id": "2306.01303",
      "title": "DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model",
      "summary": "Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.",
      "authors": [
        "Haoyu Wang",
        "Siyuan Wang",
        "Wei-Qiang Zhang",
        "Jinfeng Bai"
      ],
      "published": "2023-06-02T07:03:06Z",
      "updated": "2023-06-02T07:03:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01303v1",
      "landing_url": "https://arxiv.org/abs/2306.01303v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01303"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on distilling cross-lingual speech representations and reducing linguistic information, with no discussion of discrete audio tokens, tokenizers, or token-level modeling, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on distilling cross-lingual speech representations and reducing linguistic information, with no discussion of discrete audio tokens, tokenizers, or token-level modeling, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on distilling cross-lingual speech representation models but does not mention the use or production of discrete audio tokens through discretization processes such as vector quantization or neural audio codecs; the description centers on continuous representations and model compression without detailing discrete token vocabularies, codebooks, or tokenization processes, which are key inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on distilling cross-lingual speech representation models but does not mention the use or production of discrete audio tokens through discretization processes such as vector quantization or neural audio codecs; the description centers on continuous representations and model compression without detailing discrete token vocabularies, codebooks, or tokenization processes, which are key inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Label Aware Speech Representation Learning For Language Identification",
    "abstract": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
    "metadata": {
      "arxiv_id": "2306.04374",
      "title": "Label Aware Speech Representation Learning For Language Identification",
      "summary": "Speech representation learning approaches for non-semantic tasks such as language recognition have either explored supervised embedding extraction methods using a classifier model or self-supervised representation learning approaches using raw data. In this paper, we propose a novel framework of combining self-supervised representation learning with the language label information for the pre-training task. This framework, termed as Label Aware Speech Representation (LASR) learning, uses a triplet based objective function to incorporate language labels along with the self-supervised loss function. The speech representations are further fine-tuned for the downstream task. The language recognition experiments are performed on two public datasets - FLEURS and Dhwani. In these experiments, we illustrate that the proposed LASR framework improves over the state-of-the-art systems on language identification. We also report an analysis of the robustness of LASR approach to noisy/missing labels as well as its application to multi-lingual speech recognition tasks.",
      "authors": [
        "Shikhar Vashishth",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Ankur Bapna",
        "Min Ma",
        "Wei Han",
        "Vera Axelrod",
        "Partha Talukdar"
      ],
      "published": "2023-06-07T12:14:16Z",
      "updated": "2023-06-07T12:14:16Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04374v1",
      "landing_url": "https://arxiv.org/abs/2306.04374v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.04374"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses label-aware self-supervised speech representations for language identification without mentioning discrete audio tokens, tokenization, codebooks, or discrete units, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses label-aware self-supervised speech representations for language identification without mentioning discrete audio tokens, tokenization, codebooks, or discrete units, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving language identification using a speech representation learning framework that combines self-supervised learning with label information; however, it does not mention discretizing continuous audio into discrete token sequences nor discuss neural audio codecs, vector quantization, or the creation and use of discrete audio tokens central to the inclusion criteria. It primarily targets downstream language recognition tasks rather than token design, generation, or cross-modal alignment, and does not provide technical details about tokenization or discrete token sets.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving language identification using a speech representation learning framework that combines self-supervised learning with label information; however, it does not mention discretizing continuous audio into discrete token sequences nor discuss neural audio codecs, vector quantization, or the creation and use of discrete audio tokens central to the inclusion criteria. It primarily targets downstream language recognition tasks rather than token design, generation, or cross-modal alignment, and does not provide technical details about tokenization or discrete token sets.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
    "abstract": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
    "metadata": {
      "arxiv_id": "2306.06246",
      "title": "Record Deduplication for Entity Distribution Modeling in ASR Transcripts",
      "summary": "Voice digital assistants must keep up with trending search queries. We rely on a speech recognition model using contextual biasing with a rapidly updated set of entities, instead of frequent model retraining, to keep up with trends. There are several challenges with this approach: (1) the entity set must be frequently reconstructed, (2) the entity set is of limited size due to latency and accuracy trade-offs, and (3) finding the true entity distribution for biasing is complicated by ASR misrecognition. We address these challenges and define an entity set by modeling customers true requested entity distribution from ASR output in production using record deduplication, a technique from the field of entity resolution. Record deduplication resolves or deduplicates coreferences, including misrecognitions, of the same latent entity. Our method successfully retrieves 95% of misrecognized entities and when used for contextual biasing shows an estimated 5% relative word error rate reduction.",
      "authors": [
        "Tianyu Huang",
        "Chung Hoon Hong",
        "Carl Wivagg",
        "Kanna Shimizu"
      ],
      "published": "2023-06-09T20:42:11Z",
      "updated": "2023-06-09T20:42:11Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06246v1",
      "landing_url": "https://arxiv.org/abs/2306.06246v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06246"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on using record deduplication for entity distribution modeling in ASR transcripts and misrecognitions, with no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on using record deduplication for entity distribution modeling in ASR transcripts and misrecognitions, with no discussion of discrete audio tokens, tokenizers, or token-based modeling; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving entity distribution modeling in ASR transcripts using record deduplication but does not discuss discretizing continuous audio into discrete tokens, neural audio codecs, or discrete token sequences as per the inclusion criteria; no discrete audio token generation or token-based modeling is described.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving entity distribution modeling in ASR transcripts using record deduplication but does not discuss discretizing continuous audio into discrete tokens, neural audio codecs, or discrete token sequences as per the inclusion criteria; no discrete audio token generation or token-based modeling is described.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
    "abstract": "Electrolarynx is a commonly used assistive device to help patients with removed vocal cords regain their ability to speak. Although the electrolarynx can generate excitation signals like the vocal cords, the naturalness and intelligibility of electrolaryngeal (EL) speech are very different from those of natural (NL) speech. Many deep-learning-based models have been applied to electrolaryngeal speech voice conversion (ELVC) for converting EL speech to NL speech. In this study, we propose a multimodal voice conversion (VC) model that integrates acoustic and visual information into a unified network. We compared different pre-trained models as visual feature extractors and evaluated the effectiveness of these features in the ELVC task. The experimental results demonstrate that the proposed multimodal VC model outperforms single-modal models in both objective and subjective metrics, suggesting that the integration of visual information can significantly improve the quality of ELVC.",
    "metadata": {
      "arxiv_id": "2306.06652",
      "title": "Audio-Visual Mandarin Electrolaryngeal Speech Voice Conversion",
      "summary": "Electrolarynx is a commonly used assistive device to help patients with removed vocal cords regain their ability to speak. Although the electrolarynx can generate excitation signals like the vocal cords, the naturalness and intelligibility of electrolaryngeal (EL) speech are very different from those of natural (NL) speech. Many deep-learning-based models have been applied to electrolaryngeal speech voice conversion (ELVC) for converting EL speech to NL speech. In this study, we propose a multimodal voice conversion (VC) model that integrates acoustic and visual information into a unified network. We compared different pre-trained models as visual feature extractors and evaluated the effectiveness of these features in the ELVC task. The experimental results demonstrate that the proposed multimodal VC model outperforms single-modal models in both objective and subjective metrics, suggesting that the integration of visual information can significantly improve the quality of ELVC.",
      "authors": [
        "Yung-Lun Chien",
        "Hsin-Hao Chen",
        "Ming-Chi Yen",
        "Shu-Wei Tsai",
        "Hsin-Min Wang",
        "Yu Tsao",
        "Tai-Shih Chi"
      ],
      "published": "2023-06-11T11:25:17Z",
      "updated": "2023-06-11T11:25:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06652v1",
      "landing_url": "https://arxiv.org/abs/2306.06652v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06652"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on electrolaryngeal speech voice conversion with multimodal inputs; there is no discussion of discrete audio tokens, tokenization, or token-based modeling, so it fails the inclusion criteria and does meet exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on electrolaryngeal speech voice conversion with multimodal inputs; there is no discussion of discrete audio tokens, tokenization, or token-based modeling, so it fails the inclusion criteria and does meet exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on electrolaryngeal speech voice conversion using a multimodal deep learning model integrating acoustic and visual information, but there is no mention of discretizing continuous audio waveforms into discrete token sequences, nor any description of tokenization or codec details that would meet the discrete audio tokens inclusion criteria; hence, the paper does not align with the requirement of centering on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on electrolaryngeal speech voice conversion using a multimodal deep learning model integrating acoustic and visual information, but there is no mention of discretizing continuous audio waveforms into discrete token sequences, nor any description of tokenization or codec details that would meet the discrete audio tokens inclusion criteria; hence, the paper does not align with the requirement of centering on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
    "abstract": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly centers on discrete audio tokens (semantic tokens and acoustic tokens) produced via neural tokenization (VQ-diffusion) and uses them for TTS generation/editing with empirical comparisons, satisfying the token-discrete design, downstream generation, and evaluation criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly centers on discrete audio tokens (semantic tokens and acoustic tokens) produced via neural tokenization (VQ-diffusion) and uses them for TTS generation/editing with empirical comparisons, satisfying the token-discrete design, downstream generation, and evaluation criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes UniCATS, a TTS framework utilizing semantic tokens derived from acoustic tokens via neural models; it explicitly discusses discrete tokens (semantic and acoustic), their generation, and application in speech continuation and editing, aligning with inclusion criteria emphasizing discrete audio token utilization, neural audio codec-related generation, and downstream generation tasks. The abstract describes token design and evaluation against baselines, supporting reproducibility and design insights. Therefore, it matches the inclusion criteria well and does not meet exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes UniCATS, a TTS framework utilizing semantic tokens derived from acoustic tokens via neural models; it explicitly discusses discrete tokens (semantic and acoustic), their generation, and application in speech continuation and editing, aligning with inclusion criteria emphasizing discrete audio token utilization, neural audio codec-related generation, and downstream generation tasks. The abstract describes token design and evaluation against baselines, supporting reproducibility and design insights. Therefore, it matches the inclusion criteria well and does not meet exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
    "abstract": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
    "metadata": {
      "arxiv_id": "2306.08920",
      "title": "Pushing the Limits of Unsupervised Unit Discovery for SSL Speech Representation",
      "summary": "The excellent generalization ability of self-supervised learning (SSL) for speech foundation models has garnered significant attention. HuBERT is a successful example that utilizes offline clustering to convert speech features into discrete units for a masked language modeling pretext task. However, simply clustering features as targets by k-means does not fully inspire the model's performance. In this work, we present an unsupervised method to improve SSL targets. Two models are proposed, MonoBERT and PolyBERT, which leverage context-independent and context-dependent phoneme-based units for pre-training. Our models outperform other SSL models significantly on the LibriSpeech benchmark without the need for iterative re-clustering and re-training. Furthermore, our models equipped with context-dependent units even outperform target-improvement models that use labeled data during pre-training. How we progressively improve the unit discovery process is demonstrated through experiments.",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "published": "2023-06-15T07:45:12Z",
      "updated": "2023-06-15T07:45:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.08920v1",
      "landing_url": "https://arxiv.org/abs/2306.08920v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.08920"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on unsupervised discovery of discrete audio units (phoneme-based tokens) used as targets for SSL speech pretraining, detailing token-level representations and unit discovery methods, which directly aligns with the discrete audio token criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on unsupervised discovery of discrete audio units (phoneme-based tokens) used as targets for SSL speech pretraining, detailing token-level representations and unit discovery methods, which directly aligns with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an unsupervised method to improve discrete units used as targets in SSL speech representation models, leveraging phoneme-based units derived via clustering approaches, which corresponds to discrete audio tokens from self-supervised models; it includes design details, experimental validation, and focuses on improving token discovery to enhance downstream tasks, aligning well with inclusion criteria and not contradicting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents an unsupervised method to improve discrete units used as targets in SSL speech representation models, leveraging phoneme-based units derived via clustering approaches, which corresponds to discrete audio tokens from self-supervised models; it includes design details, experimental validation, and focuses on improving token discovery to enhance downstream tasks, aligning well with inclusion criteria and not contradicting any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
    "abstract": "Second-pass rescoring is employed in most state-of-the-art speech recognition systems. Recently, BERT based models have gained popularity for re-ranking the n-best hypothesis by exploiting the knowledge from masked language model pre-training. Further, fine-tuning with discriminative loss such as minimum word error rate (MWER) has shown to perform better than likelihood-based loss. Streaming applications with low latency requirements impose significant constraints on the size of the models, thereby limiting the word error rate (WER) performance gains. In this paper, we propose effective strategies for distilling from large models discriminatively trained with the MWER objective. We experiment on Librispeech and production scale internal dataset for voice-assistant. Our results demonstrate relative improvements of upto 7% WER over student models trained with MWER. We also show that the proposed distillation can reduce the WER gap between the student and the teacher by 62% upto 100%.",
    "metadata": {
      "arxiv_id": "2306.09452",
      "title": "Distillation Strategies for Discriminative Speech Recognition Rescoring",
      "summary": "Second-pass rescoring is employed in most state-of-the-art speech recognition systems. Recently, BERT based models have gained popularity for re-ranking the n-best hypothesis by exploiting the knowledge from masked language model pre-training. Further, fine-tuning with discriminative loss such as minimum word error rate (MWER) has shown to perform better than likelihood-based loss. Streaming applications with low latency requirements impose significant constraints on the size of the models, thereby limiting the word error rate (WER) performance gains. In this paper, we propose effective strategies for distilling from large models discriminatively trained with the MWER objective. We experiment on Librispeech and production scale internal dataset for voice-assistant. Our results demonstrate relative improvements of upto 7% WER over student models trained with MWER. We also show that the proposed distillation can reduce the WER gap between the student and the teacher by 62% upto 100%.",
      "authors": [
        "Prashanth Gurunath Shivakumar",
        "Jari Kolehmainen",
        "Yile Gu",
        "Ankur Gandhe",
        "Ariya Rastrow",
        "Ivan Bulyko"
      ],
      "published": "2023-06-15T19:15:14Z",
      "updated": "2023-06-15T19:15:14Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.09452v1",
      "landing_url": "https://arxiv.org/abs/2306.09452v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.09452"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on distillation strategies for discriminative speech recognition rescoring and MWER, with no discussion of discrete audio tokens, tokenizers, or token-based representations, thus failing the core inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on distillation strategies for discriminative speech recognition rescoring and MWER, with no discussion of discrete audio tokens, tokenizers, or token-based representations, thus failing the core inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on distillation strategies for discriminative speech recognition rescoring using BERT-based models with MWER objective, which involves continuous speech recognition and model training improvements rather than the discretization of audio into discrete token sequences or related discrete audio token design, generation, or analysis. There is no mention or evidence of discrete audio tokenization, neural audio codec usage, vector quantization, or discrete token generation and usage as core methods or analysis objects, thus it falls outside the defined inclusion scope for 'Discrete Audio Tokens'.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on distillation strategies for discriminative speech recognition rescoring using BERT-based models with MWER objective, which involves continuous speech recognition and model training improvements rather than the discretization of audio into discrete token sequences or related discrete audio token design, generation, or analysis. There is no mention or evidence of discrete audio tokenization, neural audio codec usage, vector quantization, or discrete token generation and usage as core methods or analysis objects, thus it falls outside the defined inclusion scope for 'Discrete Audio Tokens'.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
    "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses tokenizing speech into discrete semantic tokens (HuBERT) and acoustic tokens (SoundStream) and uses these discrete units as the core representation for zero-shot voice conversion, aligning with the discrete audio tokens criteria and providing implementation details that support reproducibility.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses tokenizing speech into discrete semantic tokens (HuBERT) and acoustic tokens (SoundStream) and uses these discrete units as the core representation for zero-shot voice conversion, aligning with the discrete audio tokens criteria and providing implementation details that support reproducibility.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper clearly involves discrete audio tokens derived from neural audio encoders (HuBERT and SoundStream) with semantic and acoustic tokens used as core representations for zero-shot voice conversion, addressing token design influence on quality and controllability, and provides detailed modeling strategies indicating reproducibility; thus it fits the inclusion criteria well and does not meet any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper clearly involves discrete audio tokens derived from neural audio encoders (HuBERT and SoundStream) with semantic and acoustic tokens used as core representations for zero-shot voice conversion, addressing token design influence on quality and controllability, and provides detailed modeling strategies indicating reproducibility; thus it fits the inclusion criteria well and does not meet any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Visual-Aware Text-to-Speech",
    "abstract": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
    "metadata": {
      "arxiv_id": "2306.12020",
      "title": "Visual-Aware Text-to-Speech",
      "summary": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
      "authors": [
        "Mohan Zhou",
        "Yalong Bai",
        "Wei Zhang",
        "Ting Yao",
        "Tiejun Zhao",
        "Tao Mei"
      ],
      "published": "2023-06-21T05:11:39Z",
      "updated": "2023-06-21T05:11:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12020v1",
      "landing_url": "https://arxiv.org/abs/2306.12020v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095084"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on visual-aware TTS conditioned on textual (phoneme) input and listener visual feedback, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on visual-aware TTS conditioned on textual (phoneme) input and listener visual feedback, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract introduces a visual-aware text-to-speech (VA-TTS) system that synthesizes speech combining text and visual feedback from a listener, focusing on prosody and naturalness of audio. However, it does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of audio waveforms into token sequences. There is no description of tokenizer architecture, quantization details, or discrete token vocabularies, which are primary inclusion criteria. The work appears to be continuous speech synthesis influenced by visual signals without employing discrete token representations or analyzing their design or impact. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and fails the necessary condition for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract introduces a visual-aware text-to-speech (VA-TTS) system that synthesizes speech combining text and visual feedback from a listener, focusing on prosody and naturalness of audio. However, it does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of audio waveforms into token sequences. There is no description of tokenizer architecture, quantization details, or discrete token vocabularies, which are primary inclusion criteria. The work appears to be continuous speech synthesis influenced by visual signals without employing discrete token representations or analyzing their design or impact. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and fails the necessary condition for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
    "abstract": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
    "metadata": {
      "arxiv_id": "2306.12785",
      "title": "MFCCGAN: A Novel MFCC-Based Speech Synthesizer Using Adversarial Learning",
      "summary": "In this paper, we introduce MFCCGAN as a novel speech synthesizer based on adversarial learning that adopts MFCCs as input and generates raw speech waveforms. Benefiting the GAN model capabilities, it produces speech with higher intelligibility than a rule-based MFCC-based speech synthesizer WORLD. We evaluated the model based on a popular intrusive objective speech intelligibility measure (STOI) and quality (NISQA score). Experimental results show that our proposed system outperforms Librosa MFCC- inversion (by an increase of about 26% up to 53% in STOI and 16% up to 78% in NISQA score) and a rise of about 10% in intelligibility and about 4% in naturalness in comparison with conventional rule-based vocoder WORLD that used in the CycleGAN-VC family. However, WORLD needs additional data like F0. Finally, using perceptual loss in discriminators based on STOI could improve the quality more. WebMUSHRA-based subjective tests also show the quality of the proposed approach.",
      "authors": [
        "Mohammad Reza Hasanabadi Majid Behdad Davood Gharavian"
      ],
      "published": "2023-06-22T10:29:24Z",
      "updated": "2023-06-22T10:29:24Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12785v1",
      "landing_url": "https://arxiv.org/abs/2306.12785v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦以 MFCC 為輸入，透過 GAN 生成原始語音訊號，且未公開離散化 token/碼本/字彙表的設計或以離散 token 為核心的生成或分析，因此不符合離散語音 token 主題的包含條件，屬於連續音訊表徵建模的研究，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦以 MFCC 為輸入，透過 GAN 生成原始語音訊號，且未公開離散化 token/碼本/字彙表的設計或以離散 token 為核心的生成或分析，因此不符合離散語音 token 主題的包含條件，屬於連續音訊表徵建模的研究，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes MFCCGAN, which generates raw speech waveforms from MFCC features using adversarial learning, but it does not mention discretizing audio waveforms into discrete token sequences or employing neural audio codecs/vector quantization methods to produce discrete audio tokens; thus, it does not meet the inclusion criteria focused on discrete audio tokens as core representations for modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes MFCCGAN, which generates raw speech waveforms from MFCC features using adversarial learning, but it does not mention discretizing audio waveforms into discrete token sequences or employing neural audio codecs/vector quantization methods to produce discrete audio tokens; thus, it does not meet the inclusion criteria focused on discrete audio tokens as core representations for modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Singing Voice Conversion Challenge 2023",
    "abstract": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
    "metadata": {
      "arxiv_id": "2306.14422",
      "title": "The Singing Voice Conversion Challenge 2023",
      "summary": "We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of them could reach a significant correlation.",
      "authors": [
        "Wen-Chin Huang",
        "Lester Phillip Violeta",
        "Songxiang Liu",
        "Jiatong Shi",
        "Tomoki Toda"
      ],
      "published": "2023-06-26T05:04:58Z",
      "updated": "2023-07-06T08:17:31Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.14422v2",
      "landing_url": "https://arxiv.org/abs/2306.14422v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.14422"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The SVCC 2023 paper describes a singing voice conversion challenge and its perceptual/evaluation results, with no discussion of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criteria for continuous representations without discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The SVCC 2023 paper describes a singing voice conversion challenge and its perceptual/evaluation results, with no discussion of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the inclusion criteria and aligns with the exclusion criteria for continuous representations without discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes the Singing Voice Conversion Challenge 2023, which focuses on comparing singing voice conversion systems but does not mention any discrete audio tokenization methods, neural audio codecs, vector quantization, or the production and utilization of discrete token sequences central to the 'Discrete Audio Tokens' theme; therefore, it does not meet the inclusion criteria related to discrete token generation and usage, nor does it provide sufficient methodological detail about discrete token design or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes the Singing Voice Conversion Challenge 2023, which focuses on comparing singing voice conversion systems but does not mention any discrete audio tokenization methods, neural audio codecs, vector quantization, or the production and utilization of discrete token sequences central to the 'Discrete Audio Tokens' theme; therefore, it does not meet the inclusion criteria related to discrete token generation and usage, nor does it provide sufficient methodological detail about discrete token design or evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
    "abstract": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
    "metadata": {
      "arxiv_id": "2306.15354",
      "title": "3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement",
      "summary": "Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/",
      "authors": [
        "Siqi Zheng",
        "Luyao Cheng",
        "Yafeng Chen",
        "Hui Wang",
        "Qian Chen"
      ],
      "published": "2023-06-27T10:09:43Z",
      "updated": "2023-09-25T02:36:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15354v3",
      "landing_url": "https://arxiv.org/abs/2306.15354v3",
      "doi": "https://doi.org/10.48550/arXiv.2306.15354"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a corpus for disentangling speech representations and cross-domain evaluation, but it does not address discrete audio tokens, tokenization methods, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a corpus for disentangling speech representations and cross-domain evaluation, but it does not address discrete audio tokens, tokenization methods, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a large-scale speech corpus focused on disentangling speech representations recorded under multiple devices, distances, and dialects, but does not describe any discrete audio tokenization process, neural audio codec usage, or discretization into token sequences as required by the inclusion criteria; it mainly provides a dataset resource rather than advancing or analyzing discrete audio tokens or their design, thus failing to meet the inclusion requirements and fitting the exclusion criteria of lacking discrete token information or methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a large-scale speech corpus focused on disentangling speech representations recorded under multiple devices, distances, and dialects, but does not describe any discrete audio tokenization process, neural audio codec usage, or discretization into token sequences as required by the inclusion criteria; it mainly provides a dataset resource rather than advancing or analyzing discrete audio tokens or their design, thus failing to meet the inclusion requirements and fitting the exclusion criteria of lacking discrete token information or methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
    "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
    "metadata": {
      "arxiv_id": "2306.15687",
      "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
      "summary": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
      "authors": [
        "Matthew Le",
        "Apoorv Vyas",
        "Bowen Shi",
        "Brian Karrer",
        "Leda Sari",
        "Rashel Moritz",
        "Mary Williamson",
        "Vimal Manohar",
        "Yossi Adi",
        "Jay Mahadeokar",
        "Wei-Ning Hsu"
      ],
      "published": "2023-06-23T16:23:24Z",
      "updated": "2023-10-19T13:23:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15687v2",
      "landing_url": "https://arxiv.org/abs/2306.15687v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15687"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a text-guided speech generation model without any discussion of discrete audio tokens, tokenizers, or token-based representations, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a text-guided speech generation model without any discussion of discrete audio tokens, tokenizers, or token-based representations, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents \"Voicebox,\" a text-guided speech generative model trained on continuous speech data, focusing on speech infilling, synthesis, and style conversion, but the abstract does not describe any discrete audio tokenization process or neural audio codec that produces discrete token sequences used as core inputs for modeling, nor details about tokenization such as codebooks or quantization, which are essential to meet the inclusion criteria of discrete audio token-based research. Therefore, it does not meet the necessary criteria to be included as it revolves around continuous speech modeling rather than discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents \"Voicebox,\" a text-guided speech generative model trained on continuous speech data, focusing on speech infilling, synthesis, and style conversion, but the abstract does not describe any discrete audio tokenization process or neural audio codec that produces discrete token sequences used as core inputs for modeling, nor details about tokenization such as codebooks or quantization, which are essential to meet the inclusion criteria of discrete audio token-based research. Therefore, it does not meet the necessary criteria to be included as it revolves around continuous speech modeling rather than discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
    "abstract": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model. In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences: 1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these. 2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism. 3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
    "metadata": {
      "arxiv_id": "2306.16317",
      "title": "On the complexity of isomorphism problems for tensors, groups, and polynomials IV: linear-length reductions and their applications",
      "summary": "Many isomorphism problems for tensors, groups, algebras, and polynomials were recently shown to be equivalent to one another under polynomial-time reductions, prompting the introduction of the complexity class TI (Grochow & Qiao, ITCS '21; SIAM J. Comp., '23). Using the tensorial viewpoint, Grochow & Qiao (CCC '21) then gave moderately exponential-time search- and counting-to-decision reductions for a class of $p$-groups. A significant issue was that the reductions usually incurred a quadratic increase in the length of the tensors involved. When the tensors represent $p$-groups, this corresponds to an increase in the order of the group of the form $|G|^{Θ(\\log |G|)}$, negating any asymptotic gains in the Cayley table model.\n  In this paper, we present a new kind of tensor gadget that allows us to replace those quadratic-length reductions with linear-length ones, yielding the following consequences:\n  1. If Graph Isomorphism is in P, then testing equivalence of cubic forms in $n$ variables over $F_q$, and testing isomorphism of $n$-dimensional algebras over $F_q$, can both be solved in time $q^{O(n)}$, improving from the brute-force upper bound $q^{O(n^2)}$ for both of these.\n  2. Combined with the $|G|^{O((\\log |G|)^{5/6})}$-time isomorphism-test for $p$-groups of class 2 and exponent $p$ (Sun, STOC '23), our reductions extend this runtime to $p$-groups of class $c$ and exponent $p$ where $c<p$, and yield algorithms in time $q^{O(n^{1.8}\\cdot \\log q)}$ for cubic form equivalence and algebra isomorphism.\n  3. Polynomial-time search- and counting-to-decision reduction for testing isomorphism of $p$-groups of class $2$ and exponent $p$ when Cayley tables are given. This answers questions of Arvind and Tóran (Bull. EATCS, 2005) for this group class, thought to be one of the hardest cases of Group Isomorphism.",
      "authors": [
        "Joshua A. Grochow",
        "Youming Qiao"
      ],
      "published": "2023-06-28T15:49:20Z",
      "updated": "2024-04-12T13:10:08Z",
      "categories": [
        "cs.CC",
        "cs.DS",
        "math.AG",
        "math.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.16317v2",
      "landing_url": "https://arxiv.org/abs/2306.16317v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.16317"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about computational isomorphism problems (tensors, groups, polynomials) and reductions, not about discrete audio tokens or tokenization, so it fails the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about computational isomorphism problems (tensors, groups, polynomials) and reductions, not about discrete audio tokens or tokenization, so it fails the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe theoretical advances in isomorphism problems in algebra and tensor complexity, with no mention of audio signals, discrete audio tokens, or neural audio codecs. The work does not relate to discrete tokenization of continuous audio waveforms or their modeling, generation, or alignment, nor does it provide any information about tokenizers, vocabularies, or codebooks relevant to audio representation. Therefore, it does not meet any inclusion criteria related to discrete audio tokens and falls under the exclusion criteria for lacking discrete token representations and relevance to audio-based tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe theoretical advances in isomorphism problems in algebra and tensor complexity, with no mention of audio signals, discrete audio tokens, or neural audio codecs. The work does not relate to discrete tokenization of continuous audio waveforms or their modeling, generation, or alignment, nor does it provide any information about tokenizers, vocabularies, or codebooks relevant to audio representation. Therefore, it does not meet any inclusion criteria related to discrete audio tokens and falls under the exclusion criteria for lacking discrete token representations and relevance to audio-based tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
    "abstract": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
    "metadata": {
      "arxiv_id": "2307.00024",
      "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
      "summary": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
      "authors": [
        "Daria Diatlova",
        "Vitaly Shutov"
      ],
      "published": "2023-06-28T19:34:16Z",
      "updated": "2023-06-28T19:34:16Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00024v1",
      "landing_url": "https://arxiv.org/abs/2307.00024v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00024"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although EmoSpeech improves emotional TTS by modifying FastSpeech2, it does not address discrete audio tokens or tokenization schemes, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although EmoSpeech improves emotional TTS by modifying FastSpeech2, it does not address discrete audio tokens or tokenization schemes, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving emotional text-to-speech synthesis using FastSpeech2 but does not mention any discrete audio token representation, neural audio codecs, vector quantization, or any process of discretizing continuous audio into token sequences as required by the inclusion criteria. Additionally, the abstract lacks detail on tokenizers, codebooks, quantization methods, or discrete units used as core modeling input, making it unlikely to meet the criteria for discrete audio tokens or providing necessary details for reproducibility assessment.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving emotional text-to-speech synthesis using FastSpeech2 but does not mention any discrete audio token representation, neural audio codecs, vector quantization, or any process of discretizing continuous audio into token sequences as required by the inclusion criteria. Additionally, the abstract lacks detail on tokenizers, codebooks, quantization methods, or discrete units used as core modeling input, making it unlikely to meet the criteria for discrete audio tokens or providing necessary details for reproducibility assessment.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
    "abstract": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
    "metadata": {
      "arxiv_id": "2307.00393",
      "title": "Using joint training speaker encoder with consistency loss to achieve cross-lingual voice conversion and expressive voice conversion",
      "summary": "Voice conversion systems have made significant advancements in terms of naturalness and similarity in common voice conversion tasks. However, their performance in more complex tasks such as cross-lingual voice conversion and expressive voice conversion remains imperfect. In this study, we propose a novel approach that combines a jointly trained speaker encoder and content features extracted from the cross-lingual speech recognition model Whisper to achieve high-quality cross-lingual voice conversion. Additionally, we introduce a speaker consistency loss to the joint encoder, which improves the similarity between the converted speech and the reference speech. To further explore the capabilities of the joint speaker encoder, we use the phonetic posteriorgram as the content feature, which enables the model to effectively reproduce both the speaker characteristics and the emotional aspects of the reference speech.",
      "authors": [
        "Houjian Guo",
        "Chaoran Liu",
        "Carlos Toshinori Ishi",
        "Hiroshi Ishiguro"
      ],
      "published": "2023-07-01T17:44:18Z",
      "updated": "2023-07-01T17:44:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00393v1",
      "landing_url": "https://arxiv.org/abs/2307.00393v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00393"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and content features (Whisper phonetic posteriorgrams) and does not present discrete audio tokens, tokenization, or a token-based discrete representation as core methodology.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on cross-lingual and expressive voice conversion using a joint speaker encoder and content features (Whisper phonetic posteriorgrams) and does not present discrete audio tokens, tokenization, or a token-based discrete representation as core methodology.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on voice conversion using a joint speaker encoder and content features from a speech recognition model, aiming to improve cross-lingual and expressive voice conversion. However, there is no mention of discretizing continuous audio waveforms into discrete token sequences, nor detailed description of any tokenizer, codebook, quantization, or discrete audio token generation process, which are essential inclusion criteria for this review. The work seems based on continuous feature representations rather than discrete token sequences, and does not address issues such as token design, codec architectures, or token-based downstream tasks relevant to the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on voice conversion using a joint speaker encoder and content features from a speech recognition model, aiming to improve cross-lingual and expressive voice conversion. However, there is no mention of discretizing continuous audio waveforms into discrete token sequences, nor detailed description of any tokenizer, codebook, quantization, or discrete audio token generation process, which are essential inclusion criteria for this review. The work seems based on continuous feature representations rather than discrete token sequences, and does not address issues such as token design, codec architectures, or token-based downstream tasks relevant to the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
    "abstract": "Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.",
    "metadata": {
      "arxiv_id": "2307.00453",
      "title": "Don't Stop Self-Supervision: Accent Adaptation of Speech Representations via Residual Adapters",
      "summary": "Speech representations learned in a self-supervised fashion from massive unlabeled speech corpora have been adapted successfully toward several downstream tasks. However, such representations may be skewed toward canonical data characteristics of such corpora and perform poorly on atypical, non-native accented speaker populations. With the state-of-the-art HuBERT model as a baseline, we propose and investigate self-supervised adaptation of speech representations to such populations in a parameter-efficient way via training accent-specific residual adapters. We experiment with 4 accents and choose automatic speech recognition (ASR) as the downstream task of interest. We obtain strong word error rate reductions (WERR) over HuBERT-large for all 4 accents, with a mean WERR of 22.7% with accent-specific adapters and a mean WERR of 25.1% if the entire encoder is accent-adapted. While our experiments utilize HuBERT and ASR as the downstream task, our proposed approach is both model and task-agnostic.",
      "authors": [
        "Anshu Bhatia",
        "Sanchit Sinha",
        "Saket Dingliwal",
        "Karthik Gopalakrishnan",
        "Sravan Bodapati",
        "Katrin Kirchhoff"
      ],
      "published": "2023-07-02T02:21:29Z",
      "updated": "2023-07-02T02:21:29Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00453v1",
      "landing_url": "https://arxiv.org/abs/2307.00453v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00453"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on accent adaptation of self-supervised speech representations (HuBERT) using residual adapters, not on discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria and violates the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on accent adaptation of self-supervised speech representations (HuBERT) using residual adapters, not on discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria and violates the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised adaptation of speech representations for accent adaptation, using models like HuBERT and residual adapters, but does not mention discretizing continuous audio waveforms into discrete token sequences, nor details about tokenizers, codebooks, or explicit discrete audio token generation processes, which are core to the inclusion criteria. It primarily deals with continuous representation adaptation and downstream ASR tasks rather than discrete audio token design or use in generation or cross-modal tasks. Thus, it does not meet the inclusion criteria centered on discrete audio tokens and fails the exclusion criteria regarding absence of discrete token details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised adaptation of speech representations for accent adaptation, using models like HuBERT and residual adapters, but does not mention discretizing continuous audio waveforms into discrete token sequences, nor details about tokenizers, codebooks, or explicit discrete audio token generation processes, which are core to the inclusion criteria. It primarily deals with continuous representation adaptation and downstream ASR tasks rather than discrete audio token design or use in generation or cross-modal tasks. Thus, it does not meet the inclusion criteria centered on discrete audio tokens and fails the exclusion criteria regarding absence of discrete token details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
    "abstract": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
    "metadata": {
      "arxiv_id": "2307.00729",
      "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
      "summary": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
      "authors": [
        "Sheng Zhao",
        "Qilong Yuan",
        "Yibo Duan",
        "Zhuoyue Chen"
      ],
      "published": "2023-07-03T03:21:23Z",
      "updated": "2023-07-03T03:21:23Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00729v1",
      "landing_url": "https://arxiv.org/abs/2307.00729v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00729"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper describes an end-to-end multi-module neural TTS/deepfake system (speaker encoder, Tacotron2, WaveRNN) without discussing discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the token-based inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper describes an end-to-end multi-module neural TTS/deepfake system (speaker encoder, Tacotron2, WaveRNN) without discussing discrete audio tokens, tokenizers, or neural audio codecs; it does not meet the token-based inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes an end-to-end synthetic speech generation system focusing on speech synthesis quality and success in a challenge; however, it does not explicitly mention the use or design of discrete audio tokens, neural audio codecs with quantization, or any form of discretization mapping continuous audio waveforms to token sequences. The abstract lacks details on tokenizer architecture, codebook settings, quantization methods, or token-based modeling characteristic to the inclusion criteria. Therefore, it does not meet the critical requirement of focusing on discrete audio tokens as core representations for modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes an end-to-end synthetic speech generation system focusing on speech synthesis quality and success in a challenge; however, it does not explicitly mention the use or design of discrete audio tokens, neural audio codecs with quantization, or any form of discretization mapping continuous audio waveforms to token sequences. The abstract lacks details on tokenizer architecture, codebook settings, quantization methods, or token-based modeling characteristic to the inclusion criteria. Therefore, it does not meet the critical requirement of focusing on discrete audio tokens as core representations for modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
    "metadata": {
      "arxiv_id": "2307.00782",
      "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
      "summary": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
      "authors": [
        "Yujia Xiao",
        "Shaofei Zhang",
        "Xi Wang",
        "Xu Tan",
        "Lei He",
        "Sheng Zhao",
        "Frank K. Soong",
        "Tan Lee"
      ],
      "published": "2023-07-03T06:55:03Z",
      "updated": "2023-10-07T08:32:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00782v2",
      "landing_url": "https://arxiv.org/abs/2307.00782v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-122"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "該論文聚焦於 Paragraph 讀長文本的語音合成的上下文建模與成本效益，未討論離散音訊 token 的產生、碼本/字彙表設定或以 token 為核心的建模/評估，因此不符合「離散音訊 token」主題的納入條件。",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "該論文聚焦於 Paragraph 讀長文本的語音合成的上下文建模與成本效益，未討論離散音訊 token 的產生、碼本/字彙表設定或以 token 為核心的建模/評估，因此不符合「離散音訊 token」主題的納入條件。",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving Text-to-Speech (TTS) for paragraph reading by leveraging context and efficiency improvements, but it does not mention the use or design of discrete audio tokens or neural audio codecs for discretization, nor does it provide details on token vocabularies, quantization, or token-based modeling central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving Text-to-Speech (TTS) for paragraph reading by leveraging context and efficiency improvements, but it does not mention the use or design of discrete audio tokens or neural audio codecs for discretization, nor does it provide details on token vocabularies, quantization, or token-based modeling central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Semantic enrichment towards efficient speech representations",
    "abstract": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
    "metadata": {
      "arxiv_id": "2307.01323",
      "title": "Semantic enrichment towards efficient speech representations",
      "summary": "Over the past few years, self-supervised learned speech representations have emerged as fruitful replacements for conventional surface representations when solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual models trained on massive textual data were introduced to encode language agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make profit from such textual models to enrich multilingual speech representations with language agnostic semantics. By aiming for better semantic extraction on a challenging Spoken Language Understanding task and in consideration with computation costs, this study investigates a specific in-domain semantic enrichment of the SAMU-XLSR model by specializing it on a small amount of transcribed data from the downstream task. In addition, we show the benefits of the use of same-domain French and Italian benchmarks for low-resource language portability and explore cross-domain capacities of the enriched SAMU-XLSR.",
      "authors": [
        "Gaëlle Laperrière",
        "Ha Nguyen",
        "Sahar Ghannay",
        "Bassam Jabaian",
        "Yannick Estève"
      ],
      "published": "2023-07-03T19:52:56Z",
      "updated": "2023-07-03T19:52:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.01323v1",
      "landing_url": "https://arxiv.org/abs/2307.01323v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-2234"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses semantic enrichment of multilingual speech representations but does not describe discrete audio tokens, tokenizers, or codebooks as core components, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses semantic enrichment of multilingual speech representations but does not describe discrete audio tokens, tokenizers, or codebooks as core components, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semantic enrichment of self-supervised speech representations for spoken language understanding but does not explicitly mention the generation or use of discrete audio tokens derived from neural audio codecs or vector quantization; it emphasizes semantic representations rather than discretized token sequences and lacks details on any tokenizer, codebook, or quantization process that would confirm conformity with the discrete audio token definition. Therefore, it does not meet the inclusion criteria related to explicit use and description of discrete audio tokens and appears to focus on continuous embeddings or representations rather than discretized token sequences.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semantic enrichment of self-supervised speech representations for spoken language understanding but does not explicitly mention the generation or use of discrete audio tokens derived from neural audio codecs or vector quantization; it emphasizes semantic representations rather than discretized token sequences and lacks details on any tokenizer, codebook, or quantization process that would confirm conformity with the discrete audio token definition. Therefore, it does not meet the inclusion criteria related to explicit use and description of discrete audio tokens and appears to focus on continuous embeddings or representations rather than discretized token sequences.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
    "abstract": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
    "metadata": {
      "arxiv_id": "2307.02720",
      "title": "On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation",
      "summary": "Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",
      "authors": [
        "Gene-Ping Yang",
        "Yue Gu",
        "Qingming Tang",
        "Dongsu Du",
        "Yuzong Liu"
      ],
      "published": "2023-07-06T02:03:31Z",
      "updated": "2023-07-06T02:03:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02720v1",
      "landing_url": "https://arxiv.org/abs/2307.02720v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.02720"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on on-device self-supervised speech representation learning with knowledge distillation for keyword spotting and does not present discrete audio tokens, tokenization methods, or codebooks as core components, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on on-device self-supervised speech representation learning with knowledge distillation for keyword spotting and does not present discrete audio tokens, tokenization methods, or codebooks as core components, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised speech representation learning for keyword spotting using knowledge distillation but does not describe any discretization process converting continuous audio waveforms into discrete tokens or codebooks, nor does it elaborate on neural audio codec architectures or quantization methods that generate discrete audio token sequences, which is essential per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised speech representation learning for keyword spotting using knowledge distillation but does not describe any discretization process converting continuous audio waveforms into discrete tokens or codebooks, nor does it elaborate on neural audio codec architectures or quantization methods that generate discrete audio token sequences, which is essential per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
    "abstract": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
    "metadata": {
      "arxiv_id": "2307.04179",
      "title": "IANS: Intelligibility-aware Null-steering Beamforming for Dual-Microphone Arrays",
      "summary": "Beamforming techniques are popular in speech-related applications due to their effective spatial filtering capabilities. Nonetheless, conventional beamforming techniques generally depend heavily on either the target's direction-of-arrival (DOA), relative transfer function (RTF) or covariance matrix. This paper presents a new approach, the intelligibility-aware null-steering (IANS) beamforming framework, which uses the STOI-Net intelligibility prediction model to improve speech intelligibility without prior knowledge of the speech signal parameters mentioned earlier. The IANS framework combines a null-steering beamformer (NSBF) to generate a set of beamformed outputs, and STOI-Net, to determine the optimal result. Experimental results indicate that IANS can produce intelligibility-enhanced signals using a small dual-microphone array. The results are comparable to those obtained by null-steering beamformers with given knowledge of DOAs.",
      "authors": [
        "Wen-Yuan Ting",
        "Syu-Siang Wang",
        "Yu Tsao",
        "Borching Su"
      ],
      "published": "2023-07-09T14:04:58Z",
      "updated": "2023-07-09T14:04:58Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04179v1",
      "landing_url": "https://arxiv.org/abs/2307.04179v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.04179"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on intelligibility-enhanced beamforming for dual-microphone arrays and uses STOI-Net for intelligibility prediction; it does not involve discrete audio token generation or token-based representations, so it does not meet the token-centric inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on intelligibility-enhanced beamforming for dual-microphone arrays and uses STOI-Net for intelligibility prediction; it does not involve discrete audio token generation or token-based representations, so it does not meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on beamforming techniques to enhance speech intelligibility using dual-microphone arrays without mentioning any discrete audio tokenization, neural audio codecs, or the generation of discrete token sequences for modeling, aligning, or cross-modal reasoning of audio data. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls under studies dealing with continuous audio processing and spatial filtering, which are excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on beamforming techniques to enhance speech intelligibility using dual-microphone arrays without mentioning any discrete audio tokenization, neural audio codecs, or the generation of discrete token sequences for modeling, aligning, or cross-modal reasoning of audio data. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls under studies dealing with continuous audio processing and spatial filtering, which are excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
    "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes VampNet as a non-autoregressive model using masked acoustic token modeling for music generation and related tasks, indicating discrete token-based modeling, which aligns with the topic of Discrete Audio Tokens, though it does not explicitly detail the tokenization/codec architecture.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes VampNet as a non-autoregressive model using masked acoustic token modeling for music generation and related tasks, indicating discrete token-based modeling, which aligns with the topic of Discrete Audio Tokens, though it does not explicitly detail the tokenization/codec architecture.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes VampNet as a non-autoregressive masked acoustic token modeling approach for music generation and related tasks. It implies using discrete token sequences for acoustic representation and generation, aligning with the inclusion focus on discrete audio tokens produced by neural audio encoding methods. The mention of variable masking and transformer-based modeling further supports the involvement of discrete tokens for music synthesis. The abstract, however, does not explicitly detail token discretization mechanisms such as quantization or vocab size, but the method and application strongly indicate adherence to the themes of discrete acoustic token modeling, music generation, and neural codec-based token use, thus meeting the criteria better than excluding it.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The abstract describes VampNet as a non-autoregressive masked acoustic token modeling approach for music generation and related tasks. It implies using discrete token sequences for acoustic representation and generation, aligning with the inclusion focus on discrete audio tokens produced by neural audio encoding methods. The mention of variable masking and transformer-based modeling further supports the involvement of discrete tokens for music synthesis. The abstract, however, does not explicitly detail token discretization mechanisms such as quantization or vocab size, but the method and application strongly indicate adherence to the themes of discrete acoustic token modeling, music generation, and neural codec-based token use, thus meeting the criteria better than excluding it.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Rhythm Modeling for Voice Conversion",
    "abstract": "Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.",
    "metadata": {
      "arxiv_id": "2307.06040",
      "title": "Rhythm Modeling for Voice Conversion",
      "summary": "Voice conversion aims to transform source speech into a different target voice. However, typical voice conversion systems do not account for rhythm, which is an important factor in the perception of speaker identity. To bridge this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion that does not require parallel data or text transcriptions. Using self-supervised representations, we first divide source audio into segments approximating sonorants, obstruents, and silences. Then we model rhythm by estimating speaking rate or the duration distribution of each segment type. Finally, we match the target speaking rate or rhythm by time-stretching the speech segments. Experiments show that Urhythmic outperforms existing unsupervised methods in terms of quality and prosody. Code and checkpoints: https://github.com/bshall/urhythmic. Audio demo page: https://ubisoft-laforge.github.io/speech/urhythmic.",
      "authors": [
        "Benjamin van Niekerk",
        "Marc-André Carbonneau",
        "Herman Kamper"
      ],
      "published": "2023-07-12T09:35:16Z",
      "updated": "2023-07-12T09:35:16Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06040v1",
      "landing_url": "https://arxiv.org/abs/2307.06040v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06040"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses rhythm modeling and self-supervised representations for time-stretching in voice conversion but does not introduce or rely on discrete audio tokens/码本/字彙表; hence it fails to meet the discrete-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses rhythm modeling and self-supervised representations for time-stretching in voice conversion but does not introduce or rely on discrete audio tokens/码本/字彙表; hence it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a voice conversion method focusing on rhythm modeling but does not mention discretizing continuous audio into discrete token sequences, neural audio codecs, or quantized token representations as defined in the inclusion criteria. It mainly addresses prosody and rhythm but lacks details on discrete token generation or usage in modeling, which are core to the theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a voice conversion method focusing on rhythm modeling but does not mention discretizing continuous audio into discrete token sequences, neural audio codecs, or quantized token representations as defined in the inclusion criteria. It mainly addresses prosody and rhythm but lacks details on discrete token generation or usage in modeling, which are core to the theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
    "abstract": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
    "metadata": {
      "arxiv_id": "2307.06832",
      "title": "Personalization for BERT-based Discriminative Speech Recognition Rescoring",
      "summary": "Recognition of personalized content remains a challenge in end-to-end speech recognition. We explore three novel approaches that use personalized content in a neural rescoring step to improve recognition: gazetteers, prompting, and a cross-attention based encoder-decoder model. We use internal de-identified en-US data from interactions with a virtual voice assistant supplemented with personalized named entities to compare these approaches. On a test set with personalized named entities, we show that each of these approaches improves word error rate by over 10%, against a neural rescoring baseline. We also show that on this test set, natural language prompts can improve word error rate by 7% without any training and with a marginal loss in generalization. Overall, gazetteers were found to perform the best with a 10% improvement in word error rate (WER), while also improving WER on a general test set by 1%.",
      "authors": [
        "Jari Kolehmainen",
        "Yile Gu",
        "Aditya Gourav",
        "Prashanth Gurunath Shivakumar",
        "Ankur Gandhe",
        "Ariya Rastrow",
        "Ivan Bulyko"
      ],
      "published": "2023-07-13T15:54:32Z",
      "updated": "2023-07-13T15:54:32Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.06832v1",
      "landing_url": "https://arxiv.org/abs/2307.06832v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.06832"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on personalization in speech recognition and rescoring methods (gazetteers, prompting, cross-attention) but does not describe any discrete audio token generation/encoding (tokenizers, vocabularies, quantization, or token-level representations) or use of discrete tokens as the core input—thus it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on personalization in speech recognition and rescoring methods (gazetteers, prompting, cross-attention) but does not describe any discrete audio token generation/encoding (tokenizers, vocabularies, quantization, or token-level representations) or use of discrete tokens as the core input—thus it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving speech recognition through personalized neural rescoring approaches, but there is no indication of using discrete audio tokens generated via neural audio codecs or self-supervised learning models as core representations; the work is centered on discriminative rescoring rather than tokenization or discrete audio representation for generation or multimodal modeling, and no discrete token design or quantization details are provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving speech recognition through personalized neural rescoring approaches, but there is no indication of using discrete audio tokens generated via neural audio codecs or self-supervised learning models as core representations; the work is centered on discriminative rescoring rather than tokenization or discrete audio representation for generation or multimodal modeling, and no discrete token design or quantization details are provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable Emphasis with zero data for text-to-speech",
    "abstract": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
    "metadata": {
      "arxiv_id": "2307.07062",
      "title": "Controllable Emphasis with zero data for text-to-speech",
      "summary": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
      "authors": [
        "Arnaud Joly",
        "Marco Nicolis",
        "Ekaterina Peterova",
        "Alessandro Lombardi",
        "Ammar Abbas",
        "Arent van Korlaar",
        "Aman Hussain",
        "Parul Sharma",
        "Alexis Moinet",
        "Mateusz Lajszczak",
        "Penny Karanasou",
        "Antonio Bonafonte",
        "Thomas Drugman",
        "Elena Sokolova"
      ],
      "published": "2023-07-13T21:06:23Z",
      "updated": "2023-07-13T21:06:23Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07062v1",
      "landing_url": "https://arxiv.org/abs/2307.07062v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.07062"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on duration-based emphasis in TTS without any discussion of discrete audio tokens or tokenization, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on duration-based emphasis in TTS without any discussion of discrete audio tokens or tokenization, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a method for controllable emphasis in TTS without using recordings or annotations, emphasizing duration modification rather than generating or modeling discrete audio tokens through neural audio codecs or vector quantization. The abstract does not describe producing discrete token sequences, their design choices, or usage relevant to the discrete audio token research domain, nor does it mention any tokenizer, codec, quantization, or vocabulary/codebook setup, which are essential per the inclusion criteria. Therefore, it does not meet the core inclusion criteria concerning discrete token representation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a method for controllable emphasis in TTS without using recordings or annotations, emphasizing duration modification rather than generating or modeling discrete audio tokens through neural audio codecs or vector quantization. The abstract does not describe producing discrete token sequences, their design choices, or usage relevant to the discrete audio token research domain, nor does it mention any tokenizer, codec, quantization, or vocabulary/codebook setup, which are essential per the inclusion criteria. Therefore, it does not meet the core inclusion criteria concerning discrete token representation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
    "abstract": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
    "metadata": {
      "arxiv_id": "2307.07940",
      "title": "Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions",
      "summary": "Referring to solution programs written by other users is helpful for learners in programming education. However, current online judge systems just list all solution programs submitted by users for references, and the programs are sorted based on the submission date and time, execution time, or user rating, ignoring to what extent the programs can be helpful to be referenced. In addition, users struggle to refer to a variety of solution approaches since there are too many duplicated and near-duplicated programs. To motivate learners to refer to various solutions to learn better solution approaches, in this paper, we propose an approach to deduplicate and rank common solution programs in each programming problem. Inspired by the nature that the many-duplicated program adopts a more common approach and can be a general reference, we remove the near-duplicated solution programs and rank the unique programs based on the duplicate count. The experiments on the solution programs submitted to a real-world online judge system demonstrate that the number of programs is reduced by 60.20%, whereas the baseline only reduces by 29.59% after the deduplication, meaning that users only need to refer to 39.80% of programs on average. Furthermore, our analysis shows that top-10 ranked programs cover 29.95% of programs on average, indicating that users can grasp 29.95% of solution approaches by referring to only 10 programs. The proposed approach shows the potential of reducing the learners' burden of referring to too many solutions and motivating them to learn a variety of solution approaches.",
      "authors": [
        "Atsushi Shirafuji",
        "Yutaka Watanobe"
      ],
      "published": "2023-07-16T04:20:26Z",
      "updated": "2023-09-11T09:42:37Z",
      "categories": [
        "cs.SE",
        "cs.CL",
        "cs.PL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07940v2",
      "landing_url": "https://arxiv.org/abs/2307.07940v2",
      "doi": "https://doi.org/10.1145/3634814.3634828"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe deduplicating and ranking code solution programs in an online-judge context, which has no relation to discrete audio tokens or tokenization methods, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe deduplicating and ranking code solution programs in an online-judge context, which has no relation to discrete audio tokens or tokenization methods, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deduplication and ranking of programming solution programs to aid learners, which is unrelated to the core theme of \"Discrete Audio Tokens\" that involves discretizing continuous audio waveforms into token sequences for modeling and generation. It lacks any mention of audio, neural audio codec, tokenization of audio, or related downstream audio tasks, thus failing to meet inclusion criteria and matching exclusion criteria regarding the absence of discrete audio tokens or relevant methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deduplication and ranking of programming solution programs to aid learners, which is unrelated to the core theme of \"Discrete Audio Tokens\" that involves discretizing continuous audio waveforms into token sequences for modeling and generation. It lacks any mention of audio, neural audio codec, tokenization of audio, or related downstream audio tasks, thus failing to meet inclusion criteria and matching exclusion criteria regarding the absence of discrete audio tokens or relevant methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs",
    "abstract": "In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.",
    "metadata": {
      "arxiv_id": "2307.09435",
      "title": "SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs",
      "summary": "In recent years, large-scale pre-trained speech language models (SLMs) have demonstrated remarkable advancements in various generative speech modeling applications, such as text-to-speech synthesis, voice conversion, and speech enhancement. These applications typically involve mapping text or speech inputs to pre-trained SLM representations, from which target speech is decoded. This paper introduces a new approach, SLMGAN, to leverage SLM representations for discriminative tasks within the generative adversarial network (GAN) framework, specifically for voice conversion. Building upon StarGANv2-VC, we add our novel SLM-based WavLM discriminators on top of the mel-based discriminators along with our newly designed SLM feature matching loss function, resulting in an unsupervised zero-shot voice conversion system that does not require text labels during training. Subjective evaluation results show that SLMGAN outperforms existing state-of-the-art zero-shot voice conversion models in terms of naturalness and achieves comparable similarity, highlighting the potential of SLM-based discriminators for related applications.",
      "authors": [
        "Yinghao Aaron Li",
        "Cong Han",
        "Nima Mesgarani"
      ],
      "published": "2023-07-18T17:09:15Z",
      "updated": "2023-07-18T17:09:15Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09435v1",
      "landing_url": "https://arxiv.org/abs/2307.09435v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09435"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on SLM representations and SLMGAN discriminators for unsupervised zero-shot voice conversion and does not discuss discrete audio tokens or any tokenization strategy.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on SLM representations and SLMGAN discriminators for unsupervised zero-shot voice conversion and does not discuss discrete audio tokens or any tokenization strategy.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on leveraging pre-trained speech language model representations as discriminators in a GAN-based voice conversion task, without mentioning the use of discrete audio tokens derived via neural audio codecs or clustering methods, nor providing specifics on token generation or vocabulary settings; hence, it does not meet the inclusion criteria centered on discrete token representations and mainly addresses continuous speech features and discriminative modeling rather than discrete token-based generative or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on leveraging pre-trained speech language model representations as discriminators in a GAN-based voice conversion task, without mentioning the use of discrete audio tokens derived via neural audio codecs or clustering methods, nor providing specifics on token generation or vocabulary settings; hence, it does not meet the inclusion criteria centered on discrete token representations and mainly addresses continuous speech features and discriminative modeling rather than discrete token-based generative or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reduction of the secret key length in the perfect cipher by data compression and randomisation",
    "abstract": "Perfect ciphers have been a very attractive cryptographic tool ever since C. Shannon described them. Note that, by definition, if a perfect cipher is used, no one can get any information about the encrypted message without knowing the secret key. We consider the problem of reducing the key length of perfect ciphers, because in many applications the length of the secret key is a crucial parameter. This paper describes a simple method of key length reduction. This method gives a perfect cipher and is based on the use of data compression and randomisation, and the average key length can be made close to Shannon entropy (which is the key length limit). It should be noted that the method can effectively use readily available data compressors (archivers).",
    "metadata": {
      "arxiv_id": "2307.09735",
      "title": "Reduction of the secret key length in the perfect cipher by data compression and randomisation",
      "summary": "Perfect ciphers have been a very attractive cryptographic tool ever since C. Shannon described them. Note that, by definition, if a perfect cipher is used, no one can get any information about the encrypted message without knowing the secret key. We consider the problem of reducing the key length of perfect ciphers, because in many applications the length of the secret key is a crucial parameter. This paper describes a simple method of key length reduction. This method gives a perfect cipher and is based on the use of data compression and randomisation, and the average key length can be made close to Shannon entropy (which is the key length limit). It should be noted that the method can effectively use readily available data compressors (archivers).",
      "authors": [
        "Boris Ryabko"
      ],
      "published": "2023-07-19T03:08:44Z",
      "updated": "2023-07-19T03:08:44Z",
      "categories": [
        "cs.CR",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09735v1",
      "landing_url": "https://arxiv.org/abs/2307.09735v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.09735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns reducing secret-key length in perfect ciphers using data compression, which is a cryptography topic and does not involve discrete audio tokens, tokenization methods, or downstream audio modeling tasks, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns reducing secret-key length in perfect ciphers using data compression, which is a cryptography topic and does not involve discrete audio tokens, tokenization methods, or downstream audio modeling tasks, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on reducing secret key length in perfect ciphers through data compression and randomisation, which is related to cryptography and information theory rather than discrete audio token representation, neural audio codecs, or modeling of audio signals with discrete tokens for generative or multimodal tasks. It does not discuss discrete token creation, vector quantization, or audio-related methodologies, nor does it fit any inclusion criteria or address topics related to audio tokenization or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on reducing secret key length in perfect ciphers through data compression and randomisation, which is related to cryptography and information theory rather than discrete audio token representation, neural audio codecs, or modeling of audio signals with discrete tokens for generative or multimodal tasks. It does not discuss discrete token creation, vector quantization, or audio-related methodologies, nor does it fit any inclusion criteria or address topics related to audio tokenization or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MASR: Multi-label Aware Speech Representation",
    "abstract": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
    "metadata": {
      "arxiv_id": "2307.10982",
      "title": "MASR: Multi-label Aware Speech Representation",
      "summary": "In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant performance improvements for the MASR over other established benchmarks. We perform a detailed analysis on the language identification task to provide insights on how the proposed loss function enables the representations to separate closely related languages.",
      "authors": [
        "Anjali Raj",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Min Ma",
        "Shikhar Vashishth"
      ],
      "published": "2023-07-20T16:09:57Z",
      "updated": "2023-09-25T12:49:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.10982v2",
      "landing_url": "https://arxiv.org/abs/2307.10982v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.10982"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on continuous self-supervised speech representations and external knowledge integration, with no mention of discrete token generation, tokenizers, codebooks, or token-based modeling, thus it does not meet the discrete audio tokens criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on continuous self-supervised speech representations and external knowledge integration, with no mention of discrete token generation, tokenizers, codebooks, or token-based modeling, thus it does not meet the discrete audio tokens criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a multi-label aware speech representation learning framework (MASR) that integrates multiple external knowledge sources to improve speech representations for tasks such as language identification, speech recognition, speaker and emotion recognition. However, the abstract does not mention any discrete audio tokens, discretization processes, tokenizers, codecs, quantization methods, vocabulary settings, or the production of token sequences that serve as core representations for generative or cross-modal modeling. The focus is on continuous speech representation learning with no explicit reference to discrete tokens or their design, which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria of analyzing or utilizing discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a multi-label aware speech representation learning framework (MASR) that integrates multiple external knowledge sources to improve speech representations for tasks such as language identification, speech recognition, speaker and emotion recognition. However, the abstract does not mention any discrete audio tokens, discretization processes, tokenizers, codecs, quantization methods, vocabulary settings, or the production of token sequences that serve as core representations for generative or cross-modal modeling. The focus is on continuous speech representation learning with no explicit reference to discrete tokens or their design, which are central to the inclusion criteria. Therefore, the study does not meet the inclusion criteria of analyzing or utilizing discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
    "abstract": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
    "metadata": {
      "arxiv_id": "2307.11394",
      "title": "MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems",
      "summary": "MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC-WER and MIMO-WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the additional overhead caused by processing the time stamps.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-07-21T07:22:18Z",
      "updated": "2024-01-25T19:48:36Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11394v3",
      "landing_url": "https://arxiv.org/abs/2307.11394v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.11394"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於會議轉錄的字錯誤率（WER）計算與時間對齊等評估工具，未涉及離散音訊 token、tokenization 設計、或以離散 token 為核心的生成/解析任務，屬於排除範疇。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於會議轉錄的字錯誤率（WER）計算與時間對齊等評估工具，未涉及離散音訊 token、tokenization 設計、或以離散 token 為核心的生成/解析任務，屬於排除範疇。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a toolkit for computing word error rates in meeting transcription systems and does not discuss the generation or use of discrete audio tokens derived from neural audio codec or self-supervised models. It lacks mention of discretization of continuous audio into token sequences or any relevant token design, generation details, or application in audio generation, understanding, or cross-modal modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a toolkit for computing word error rates in meeting transcription systems and does not discuss the generation or use of discrete audio tokens derived from neural audio codec or self-supervised models. It lacks mention of discretization of continuous audio into token sequences or any relevant token design, generation details, or application in audio generation, understanding, or cross-modal modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
    "abstract": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
    "metadata": {
      "arxiv_id": "2307.11794",
      "title": "Artificial Intelligence-Generated Terahertz Multi-Resonant Metasurfaces via Improved Transformer and CGAN Neural Networks",
      "summary": "It is well known that the inverse design of terahertz (THz) multi-resonant graphene metasurfaces by using traditional deep neural networks (DNNs) has limited generalization ability. In this paper, we propose improved Transformer and conditional generative adversarial neural networks (CGAN) for the inverse design of graphene metasurfaces based upon THz multi-resonant absorption spectra. The improved Transformer can obtain higher accuracy and generalization performance in the StoV (Spectrum to Vector) design compared to traditional multilayer perceptron (MLP) neural networks, while the StoI (Spectrum to Image) design achieved through CGAN can provide more comprehensive information and higher accuracy than the StoV design obtained by MLP. Moreover, the improved CGAN can achieve the inverse design of graphene metasurface images directly from the desired multi-resonant absorption spectra. It is turned out that this work can finish facilitating the design process of artificial intelligence-generated metasurfaces (AIGM), and even provide a useful guide for developing complex THz metasurfaces based on 2D materials using generative neural networks.",
      "authors": [
        "Yangpeng Huang",
        "Naixing Feng",
        "Yijun Cai"
      ],
      "published": "2023-07-21T02:49:03Z",
      "updated": "2023-07-21T02:49:03Z",
      "categories": [
        "physics.optics",
        "cs.LG",
        "physics.app-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.11794v1",
      "landing_url": "https://arxiv.org/abs/2307.11794v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.11794"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper concerns inverse design of terahertz graphene metasurfaces using Transformer and CGAN networks; there is no mention of discrete audio tokens, tokenizers, or audio-token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper concerns inverse design of terahertz graphene metasurfaces using Transformer and CGAN networks; there is no mention of discrete audio tokens, tokenizers, or audio-token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the inverse design of terahertz graphene metasurfaces using neural networks and does not discuss audio signals, discrete audio tokens, or any form of discretization of audio waveforms for sequence modeling; therefore, it does not meet the inclusion criteria related to discrete audio tokens and related framework.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the inverse design of terahertz graphene metasurfaces using neural networks and does not discuss audio signals, discrete audio tokens, or any form of discretization of audio waveforms for sequence modeling; therefore, it does not meet the inclusion criteria related to discrete audio tokens and related framework.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
    "abstract": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism. As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
    "metadata": {
      "arxiv_id": "2307.12052",
      "title": "Blockchain-based Cloud Data Deduplication Scheme with Fair Incentives",
      "summary": "With the rapid development of cloud computing, vast amounts of duplicated data are being uploaded to the cloud, wasting storage resources. Deduplication (dedup) is an efficient solution to save storage costs of cloud storage providers (CSPs) by storing only one copy of the uploaded data. However, cloud users do not benefit directly from dedup and may be reluctant to dedup their data. To motivate the cloud users towards dedup, CSPs offer incentives on storage fees. The problems with the existing dedup schemes are that they do not consider: (1) correctness - the incentive offered to a cloud user should be computed correctly without any prejudice. (2) fairness - the cloud user receives the file link and access rights of the uploaded data if and only if the CSP receives the storage fee. Meeting these requirements without a trusted party is non-trivial, and most of the existing dedup schemes do not apply. Another drawback is that most of the existing schemes emphasize incentives to cloud users but failed to provide a reliable incentive mechanism.\n  As public Blockchain networks emulate the properties of trusted parties, in this paper, we propose a new Blockchain-based dedup scheme to meet the above requirements. In our scheme, a smart contract computes the incentives on storage fee, and the fairness rules are encoded into the smart contract for facilitating fair payments between the CSPs and cloud users. We prove the correctness and fairness of the proposed scheme. We also design a new incentive mechanism and show that the scheme is individually rational and incentive compatible. Furthermore, we conduct experiments by implementing the designed smart contract on Ethereum local Blockchain network and list the transactional and financial costs of interacting with the designed smart contract.",
      "authors": [
        "Mallikarjun Reddy Dorsala",
        "V. N. Sastry",
        "Sudhakar Chapram"
      ],
      "published": "2023-07-22T11:27:05Z",
      "updated": "2023-07-22T11:27:05Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.12052v1",
      "landing_url": "https://arxiv.org/abs/2307.12052v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.12052"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies blockchain-based data deduplication and incentives, with no discrete audio tokens, tokenization methods, or audio modeling discussed, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies blockchain-based data deduplication and incentives, with no discrete audio tokens, tokenization methods, or audio modeling discussed, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses blockchain-based cloud data deduplication and incentive mechanisms related to cloud storage, without any mention or indication of discrete audio tokens, audio waveform discretization, neural audio codecs, or any related audio token mechanisms as specified by the inclusion criteria. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is unrelated to the domain of audio tokenization or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses blockchain-based cloud data deduplication and incentive mechanisms related to cloud storage, without any mention or indication of discrete audio tokens, audio waveform discretization, neural audio codecs, or any related audio token mechanisms as specified by the inclusion criteria. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is unrelated to the domain of audio tokenization or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Noisy k-means++ Revisited",
    "abstract": "The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem. While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation. In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact. Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$. In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.",
    "metadata": {
      "arxiv_id": "2307.13685",
      "title": "Noisy k-means++ Revisited",
      "summary": "The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem. While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation.\n  In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact.\n  Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$. In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.",
      "authors": [
        "Christoph Grunau",
        "Ahmet Alper Özüdoğru",
        "Václav Rozhoň"
      ],
      "published": "2023-07-25T17:45:41Z",
      "updated": "2023-07-25T17:45:41Z",
      "categories": [
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.13685v1",
      "landing_url": "https://arxiv.org/abs/2307.13685v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.13685"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a theoretical CS paper on noisy k-means++ and does not discuss discrete audio tokens, tokenization, or downstream audio modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a theoretical CS paper on noisy k-means++ and does not discuss discrete audio tokens, tokenization, or downstream audio modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus exclusively on the k-means++ algorithm and its theoretical guarantees under noisy conditions, which is a clustering algorithm topic unrelated to discrete audio token generation, modeling, or application as specified in the inclusion criteria. There is no mention of audio, token discretization, neural audio codecs, or any audio-related downstream tasks. Hence, it does not meet the inclusion criteria and falls under the exclusion criteria for not involving discrete audio tokens or related systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus exclusively on the k-means++ algorithm and its theoretical guarantees under noisy conditions, which is a clustering algorithm topic unrelated to discrete audio token generation, modeling, or application as specified in the inclusion criteria. There is no mention of audio, token discretization, neural audio codecs, or any audio-related downstream tasks. Hence, it does not meet the inclusion criteria and falls under the exclusion criteria for not involving discrete audio tokens or related systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Online Clustered Codebook",
    "abstract": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
    "metadata": {
      "arxiv_id": "2307.15139",
      "title": "Online Clustered Codebook",
      "summary": "Vector Quantisation (VQ) is experiencing a comeback in machine learning, where it is increasingly used in representation learning. However, optimizing the codevectors in existing VQ-VAE is not entirely trivial. A problem is codebook collapse, where only a small subset of codevectors receive gradients useful for their optimisation, whereas a majority of them simply ``dies off'' and is never updated or used. This limits the effectiveness of VQ for learning larger codebooks in complex computer vision tasks that require high-capacity representations. In this paper, we present a simple alternative method for online codebook learning, Clustering VQ-VAE (CVQ-VAE). Our approach selects encoded features as anchors to update the ``dead'' codevectors, while optimising the codebooks which are alive via the original loss. This strategy brings unused codevectors closer in distribution to the encoded features, increasing the likelihood of being chosen and optimized. We extensively validate the generalization capability of our quantiser on various datasets, tasks (e.g. reconstruction and generation), and architectures (e.g. VQ-VAE, VQGAN, LDM). Our CVQ-VAE can be easily integrated into the existing models with just a few lines of code.",
      "authors": [
        "Chuanxia Zheng",
        "Andrea Vedaldi"
      ],
      "published": "2023-07-27T18:31:04Z",
      "updated": "2023-07-27T18:31:04Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15139v1",
      "landing_url": "https://arxiv.org/abs/2307.15139v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.15139"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on online codebook learning for VQ-VAE in computer vision; no explicit reference to discrete audio tokens or audio tokenization, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on online codebook learning for VQ-VAE in computer vision; no explicit reference to discrete audio tokens or audio tokenization, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving vector quantization techniques for representation learning in computer vision tasks and does not discuss discretizing continuous audio waveforms into tokens for audio generation or cross-modal modeling, nor does it provide details on discrete audio token generation processes relevant to the inclusion criteria. Therefore, it does not meet the topic definition of \"Discrete Audio Tokens\" focused on audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving vector quantization techniques for representation learning in computer vision tasks and does not discuss discretizing continuous audio waveforms into tokens for audio generation or cross-modal modeling, nor does it provide details on discrete audio token generation processes relevant to the inclusion criteria. Therefore, it does not meet the topic definition of \"Discrete Audio Tokens\" focused on audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text",
    "abstract": "In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.",
    "metadata": {
      "arxiv_id": "2307.16332",
      "title": "Pre-training End-to-end ASR Models with Augmented Speech Samples Queried by Text",
      "summary": "In end-to-end automatic speech recognition system, one of the difficulties for language expansion is the limited paired speech and text training data. In this paper, we propose a novel method to generate augmented samples with unpaired speech feature segments and text data for model pre-training, which has the advantage of low cost without using additional speech data. When mixing 20,000 hours augmented speech data generated by our method with 12,500 hours original transcribed speech data for Italian Transformer transducer model pre-training, we achieve 8.7% relative word error rate reduction. The pre-trained model achieves similar performance as the model pre-trained with multilingual transcribed 75,000 hours raw speech data. When merging the augmented speech data with the multilingual data to pre-train a new model, we achieve even more relative word error rate reduction of 12.2% over the baseline, which further verifies the effectiveness of our method for speech data augmentation.",
      "authors": [
        "Eric Sun",
        "Jinyu Li",
        "Jian Xue",
        "Yifan Gong"
      ],
      "published": "2023-07-30T22:36:22Z",
      "updated": "2023-07-30T22:36:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16332v1",
      "landing_url": "https://arxiv.org/abs/2307.16332v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.16332"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe data augmentation for ASR pre-training using unpaired speech segments and text, without any discussion of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe data augmentation for ASR pre-training using unpaired speech segments and text, without any discussion of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on pre-training end-to-end ASR models with augmented speech samples but does not describe any process of discretizing continuous audio into discrete tokens or using neural audio codecs or quantization methods to produce discrete token sequences. It lacks details about tokenizers, codebooks, or any discrete token design, which fails to meet the essential inclusion criteria related to \"Discrete Audio Tokens.\" Therefore, it is not aligned with the defined topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on pre-training end-to-end ASR models with augmented speech samples but does not describe any process of discretizing continuous audio into discrete tokens or using neural audio codecs or quantization methods to produce discrete token sequences. It lacks details about tokenizers, codebooks, or any discrete token design, which fails to meet the essential inclusion criteria related to \"Discrete Audio Tokens.\" Therefore, it is not aligned with the defined topic and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
    "metadata": {
      "arxiv_id": "2307.16679",
      "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
      "summary": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
      "authors": [
        "Guangyan Zhang",
        "Thomas Merritt",
        "Manuel Sam Ribeiro",
        "Biel Tura-Vecino",
        "Kayoko Yanagisawa",
        "Kamil Pokora",
        "Abdelhamid Ezzerg",
        "Sebastian Cygert",
        "Ammar Abbas",
        "Piotr Bilinski",
        "Roberto Barra-Chicote",
        "Daniel Korzekwa",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2023-07-31T13:57:04Z",
      "updated": "2023-07-31T13:57:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16679v1",
      "landing_url": "https://arxiv.org/abs/2307.16679v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.16679"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Focuses on continuous representations (log-f0, duration, mel-spectrogram) and compares flow/diffusion models without any discrete tokenization or token-based analysis, thus not meeting the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Focuses on continuous representations (log-f0, duration, mel-spectrogram) and compares flow/diffusion models without any discrete tokenization or token-based analysis, thus not meeting the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper compares normalizing flows and diffusion models for prosody and acoustic modeling in text-to-speech synthesis, focusing on L1/L2, diffusion, and flow-based approaches to predict prosody features and mel-spectrograms. However, it does not mention the use of discrete audio tokens or any discretization process that maps continuous audio waveforms into discrete token sequences, nor does it describe any tokenizer or codec architecture or any vocabulary/codebook. Since the core topic requires discrete token sequences as a central representation and this paper exclusively addresses continuous acoustic modeling without discrete tokenization details, it does not meet the inclusion criteria and falls under exclusion due to continuous representation focus without discrete token modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper compares normalizing flows and diffusion models for prosody and acoustic modeling in text-to-speech synthesis, focusing on L1/L2, diffusion, and flow-based approaches to predict prosody features and mel-spectrograms. However, it does not mention the use of discrete audio tokens or any discretization process that maps continuous audio waveforms into discrete token sequences, nor does it describe any tokenizer or codec architecture or any vocabulary/codebook. Since the core topic requires discrete token sequences as a central representation and this paper exclusively addresses continuous acoustic modeling without discrete tokenization details, it does not meet the inclusion criteria and falls under exclusion due to continuous representation focus without discrete token modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods",
    "abstract": "This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.",
    "metadata": {
      "arxiv_id": "2308.00129",
      "title": "Speech representation learning: Learning bidirectional encoders with single-view, multi-view, and multi-task methods",
      "summary": "This thesis focuses on representation learning for sequence data over time or space, aiming to improve downstream sequence prediction tasks by using the learned representations. Supervised learning has been the most dominant approach for training deep neural networks for learning good sequential representations. However, one limiting factor to scale supervised learning is the lack of enough annotated data. Motivated by this challenge, it is natural to explore representation learning methods that can utilize large amounts of unlabeled and weakly labeled data, as well as an additional data modality. I describe my broad study of representation learning for speech data. Unlike most other works that focus on a single learning setting, this thesis studies multiple settings: supervised learning with auxiliary losses, unsupervised learning, semi-supervised learning, and multi-view learning. Besides different learning problems, I also explore multiple approaches for representation learning. Though I focus on speech data, the methods described in this thesis can also be applied to other domains. Overall, the field of representation learning is developing rapidly. State-of-the-art results on speech related tasks are typically based on Transformers pre-trained with large-scale self-supervised learning, which aims to learn generic representations that can benefit multiple downstream tasks. Since 2020, large-scale pre-training has been the de facto choice to achieve good performance. This delayed thesis does not attempt to summarize and compare with the latest results on speech representation learning; instead, it presents a unique study on speech representation learning before the Transformer era, that covers multiple learning settings. Some of the findings in this thesis can still be useful today.",
      "authors": [
        "Qingming Tang"
      ],
      "published": "2023-07-25T20:38:55Z",
      "updated": "2023-07-25T20:38:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00129v1",
      "landing_url": "https://arxiv.org/abs/2308.00129v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00129"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on general speech representation learning (including supervised/unsupervised and pre-Transformer-era methods) without discussing discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on general speech representation learning (including supervised/unsupervised and pre-Transformer-era methods) without discussing discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The thesis discusses representation learning for speech data across multiple learning settings but does not mention the discretization of continuous audio waveforms into discrete tokens or the use of neural audio codecs or vector quantization to obtain token sequences, which is central to the inclusion criteria. It also lacks specific details about tokenization processes, codebooks, or vocabulary settings and does not focus on discrete audio tokens as core representations or in downstream generative or cross-modal tasks. Therefore, it does not meet the core inclusion requirements related to discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The thesis discusses representation learning for speech data across multiple learning settings but does not mention the discretization of continuous audio waveforms into discrete tokens or the use of neural audio codecs or vector quantization to obtain token sequences, which is central to the inclusion criteria. It also lacks specific details about tokenization processes, codebooks, or vocabulary settings and does not focus on discrete audio tokens as core representations or in downstream generative or cross-modal tasks. Therefore, it does not meet the core inclusion requirements related to discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Pre-trained Data Deduplication Model based on Active Learning",
    "abstract": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
    "metadata": {
      "arxiv_id": "2308.00721",
      "title": "A Pre-trained Data Deduplication Model based on Active Learning",
      "summary": "In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These \"dirty data\" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve the model's performance. Experimental results demonstrate that our proposed model outperforms previous state-of-the-art (SOTA) for deduplicated data identification, achieving up to a 28% improvement in Recall score on benchmark datasets.",
      "authors": [
        "Haochen Shi",
        "Xinyao Liu",
        "Fengmao Lv",
        "Hongtao Xue",
        "Jie Hu",
        "Shengdong Du",
        "Tianrui Li"
      ],
      "published": "2023-07-31T03:56:46Z",
      "updated": "2025-01-10T09:35:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00721v4",
      "landing_url": "https://arxiv.org/abs/2308.00721v4",
      "doi": "https://doi.org/10.48550/arXiv.2308.00721"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or tokenization; it's a data deduplication model in NLP/ML, thus not relevant to the 'Discrete Audio Tokens' topic.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or tokenization; it's a data deduplication model in NLP/ML, thus not relevant to the 'Discrete Audio Tokens' topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on data deduplication using a pre-trained Transformer model enhanced with active learning, targeting semantic-level duplicate data identification in big data contexts. It does not discuss audio signals, discrete audio tokens, neural audio codecs, or discretization of continuous audio into token sequences as per the given inclusion criteria. Therefore, it does not meet the thematic requirements related to discrete audio token research and is outside the scope defined by the inclusion/exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on data deduplication using a pre-trained Transformer model enhanced with active learning, targeting semantic-level duplicate data identification in big data contexts. It does not discuss audio signals, discrete audio tokens, neural audio codecs, or discretization of continuous audio into token sequences as per the given inclusion criteria. Therefore, it does not meet the thematic requirements related to discrete audio token research and is outside the scope defined by the inclusion/exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Are Easy Data Easy (for K-Means)",
    "abstract": "This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.",
    "metadata": {
      "arxiv_id": "2308.01926",
      "title": "Are Easy Data Easy (for K-Means)",
      "summary": "This paper investigates the capability of correctly recovering well-separated clusters by various brands of the $k$-means algorithm. The concept of well-separatedness used here is derived directly from the common definition of clusters, which imposes an interplay between the requirements of within-cluster-homogenicity and between-clusters-diversity. Conditions are derived for a special case of well-separated clusters such that the global minimum of $k$-means cost function coincides with the well-separatedness. An experimental investigation is performed to find out whether or no various brands of $k$-means are actually capable of discovering well separated clusters. It turns out that they are not. A new algorithm is proposed that is a variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The new algorithm outperforms four other algorithms from $k$-means family on the task.",
      "authors": [
        "Mieczysław A. Kłopotek"
      ],
      "published": "2023-08-02T09:40:19Z",
      "updated": "2023-08-02T09:40:19Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.01926v1",
      "landing_url": "https://arxiv.org/abs/2308.01926v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.01926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on k-means clustering and well-separated clusters, with no discussion of discrete audio tokens, tokenizers, or related token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on k-means clustering and well-separated clusters, with no discussion of discrete audio tokens, tokenizers, or related token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the evaluation and improvement of k-means clustering algorithms, specifically investigating cluster separability and proposing a variation of k-means++; it does not discuss discrete audio tokens, discretization of continuous audio waveforms, neural audio codec, tokenization, or related representations relevant to audio modeling or generation, nor does it address any tasks related to audio generation, understanding, or cross-modal alignment. Therefore, it does not meet any inclusion criteria and falls under an exclusion criterion regarding lack of discrete audio token content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the evaluation and improvement of k-means clustering algorithms, specifically investigating cluster separability and proposing a variation of k-means++; it does not discuss discrete audio tokens, discretization of continuous audio waveforms, neural audio codec, tokenization, or related representations relevant to audio modeling or generation, nor does it address any tasks related to audio generation, understanding, or cross-modal alignment. Therefore, it does not meet any inclusion criteria and falls under an exclusion criterion regarding lack of discrete audio token content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
    "abstract": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
    "metadata": {
      "arxiv_id": "2308.03332",
      "title": "Improving Deep Attractor Network by BGRU and GMM for Speech Separation",
      "summary": "Deep Attractor Network (DANet) is the state-of-the-art technique in speech separation field, which uses Bidirectional Long Short-Term Memory (BLSTM), but the complexity of the DANet model is very high. In this paper, a simplified and powerful DANet model is proposed using Bidirectional Gated neural network (BGRU) instead of BLSTM. The Gaussian Mixture Model (GMM) other than the k-means was applied in DANet as a clustering algorithm to reduce the complexity and increase the learning speed and accuracy. The metrics used in this paper are Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), Signal to Artifact Ratio (SAR), and Perceptual Evaluation Speech Quality (PESQ) score. Two speaker mixture datasets from TIMIT corpus were prepared to evaluate the proposed model, and the system achieved 12.3 dB and 2.94 for SDR and PESQ scores respectively, which were better than the original DANet model. Other improvements were 20.7% and 17.9% in the number of parameters and time training, respectively. The model was applied on mixed Arabic speech signals and the results were better than that in English.",
      "authors": [
        "Rawad Melhem",
        "Assef Jafar",
        "Riad Hamadeh"
      ],
      "published": "2023-08-07T06:26:53Z",
      "updated": "2023-08-07T06:26:53Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.03332v1",
      "landing_url": "https://arxiv.org/abs/2308.03332v1",
      "doi": "https://doi.org/10.11916/j.issn.1005-9113.2019044"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses DANet with BGRU and GMM for speech separation, but there is no mention of discrete audio tokens, tokenizers, or codebooks, so it does not meet the discrete token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses DANet with BGRU and GMM for speech separation, but there is no mention of discrete audio tokens, tokenizers, or codebooks, so it does not meet the discrete token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving a Deep Attractor Network for speech separation using BGRU and GMM methods but does not discuss or mention the generation or use of discrete audio tokens through neural audio codecs, vector quantization, or any tokenizer that discretizes continuous audio into token sequences; thus, it does not meet the core inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving a Deep Attractor Network for speech separation using BGRU and GMM methods but does not discuss or mention the generation or use of discrete audio tokens through neural audio codecs, vector quantization, or any tokenizer that discretizes continuous audio into token sequences; thus, it does not meet the core inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
    "abstract": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
    "metadata": {
      "arxiv_id": "2308.05502",
      "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
      "summary": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
      "authors": [
        "Candida M. Greco",
        "Andrea Tagarelli"
      ],
      "published": "2023-08-10T11:14:22Z",
      "updated": "2024-02-03T09:54:51Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.NE",
        "physics.soc-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05502v2",
      "landing_url": "https://arxiv.org/abs/2308.05502v2",
      "doi": "https://doi.org/10.1007/s10506-023-09374-7"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Transformer-based language models applied to AI and law and provides a systemic overview of the legal domain, with no discussion of discrete audio tokens, tokenization schemes, or neural audio codecs, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Transformer-based language models applied to AI and law and provides a systemic overview of the legal domain, with no discussion of discrete audio tokens, tokenization schemes, or neural audio codecs, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article is a systematic overview of Transformer-based language models for AI in law, focusing on legal textual processing, without mentioning any discretization of continuous audio signals into discrete tokens or neural audio codecs; thus, it does not meet the inclusion criteria specifying discrete audio tokens or related methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article is a systematic overview of Transformer-based language models for AI in law, focusing on legal textual processing, without mentioning any discretization of continuous audio signals into discrete tokens or neural audio codecs; thus, it does not meet the inclusion criteria specifying discrete audio tokens or related methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it proposes neural codec language modeling for speech tasks, the abstract does not explicitly describe discrete audio tokens, tokenizer/codec settings, or token-based downstream evaluations, so it does not meet the key inclusion criteria for discrete-token representation.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it proposes neural codec language modeling for speech tasks, the abstract does not explicitly describe discrete audio tokens, tokenizer/codec settings, or token-based downstream evaluations, so it does not meet the key inclusion criteria for discrete-token representation.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study SpeechX introduces a neural codec language model for speech generation and transformation tasks, which suggests the use of neural audio codec-based discrete tokens. However, the abstract lacks explicit detail on the discrete token generation process (such as codec architecture, quantization method, codebook settings, or time downsampling) necessary to verify compliance with the discrete audio tokens inclusion criteria. While the model handles varied speech tasks using codec language modeling, without clear documentation of discrete token design, settings, and evaluation, it cannot be confidently included under the strict criteria. Therefore, it is borderline but leans towards exclusion due to insufficient information on critical discrete token specifics.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study SpeechX introduces a neural codec language model for speech generation and transformation tasks, which suggests the use of neural audio codec-based discrete tokens. However, the abstract lacks explicit detail on the discrete token generation process (such as codec architecture, quantization method, codebook settings, or time downsampling) necessary to verify compliance with the discrete audio tokens inclusion criteria. While the model handles varied speech tasks using codec language modeling, without clear documentation of discrete token design, settings, and evaluation, it cannot be confidently included under the strict criteria. Therefore, it is borderline but leans towards exclusion due to insufficient information on critical discrete token specifics.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommends exclusion (2) noting absence of discrete-token details; the other considered it borderline (3) but leaned exclusion for the same reason. My assessment: the title/abstract (“Neural Codec Language Model”) strongly suggests the use of neural audio-codec-derived discrete tokens and downstream token-based modeling, which aligns with the Discrete Audio Tokens topic, but the abstract fails to specify any tokenizer/codec architecture, quantization/codebook settings, bitrate or time-downsampling—information that the inclusion criteria require to judge a work as centered on discrete audio tokens and reproducible. Without the full paper’s methods, we cannot confirm that discrete-token generation and settings are provided; therefore this item is uncertain for inclusion under the strict criteria.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommends exclusion (2) noting absence of discrete-token details; the other considered it borderline (3) but leaned exclusion for the same reason. My assessment: the title/abstract (“Neural Codec Language Model”) strongly suggests the use of neural audio-codec-derived discrete tokens and downstream token-based modeling, which aligns with the Discrete Audio Tokens topic, but the abstract fails to specify any tokenizer/codec architecture, quantization/codebook settings, bitrate or time-downsampling—information that the inclusion criteria require to judge a work as centered on discrete audio tokens and reproducible. Without the full paper’s methods, we cannot confirm that discrete-token generation and settings are provided; therefore this item is uncertain for inclusion under the strict criteria.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion",
    "abstract": "Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.",
    "metadata": {
      "arxiv_id": "2308.11084",
      "title": "PMVC: Data Augmentation-Based Prosody Modeling for Expressive Voice Conversion",
      "summary": "Voice conversion as the style transfer task applied to speech, refers to converting one person's speech into a new speech that sounds like another person's. Up to now, there has been a lot of research devoted to better implementation of VC tasks. However, a good voice conversion model should not only match the timbre information of the target speaker, but also expressive information such as prosody, pace, pause, etc. In this context, prosody modeling is crucial for achieving expressive voice conversion that sounds natural and convincing. Unfortunately, prosody modeling is important but challenging, especially without text transcriptions. In this paper, we firstly propose a novel voice conversion framework named 'PMVC', which effectively separates and models the content, timbre, and prosodic information from the speech without text transcriptions. Specially, we introduce a new speech augmentation algorithm for robust prosody extraction. And building upon this, mask and predict mechanism is applied in the disentanglement of prosody and content information. The experimental results on the AIShell-3 corpus supports our improvement of naturalness and similarity of converted speech.",
      "authors": [
        "Yimin Deng",
        "Huaizhen Tang",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-08-21T23:37:45Z",
      "updated": "2023-08-21T23:37:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.11084v1",
      "landing_url": "https://arxiv.org/abs/2308.11084v1",
      "doi": "https://doi.org/10.1145/3581783.3613800"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on prosody modeling and data augmentation for expressive voice conversion, with no mention of discrete audio tokens, tokenization, or token-based representation, thus failing the inclusion criteria and meeting the exclusion criterion for lacking discrete-token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on prosody modeling and data augmentation for expressive voice conversion, with no mention of discrete audio tokens, tokenization, or token-based representation, thus failing the inclusion criteria and meeting the exclusion criterion for lacking discrete-token analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on expressive voice conversion with prosody modeling, without mention of discretizing continuous audio waveforms into discrete tokens or using neural audio codecs/vector quantization to generate token sequences. It lacks clear description about discrete audio token generation, their vocabulary or codebook settings, or employing these tokens as core elements in the modeling, which are essential inclusion criteria. Therefore, it does not meet the theme of \"Discrete Audio Tokens\" as defined, making it outside the inclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on expressive voice conversion with prosody modeling, without mention of discretizing continuous audio waveforms into discrete tokens or using neural audio codecs/vector quantization to generate token sequences. It lacks clear description about discrete audio token generation, their vocabulary or codebook settings, or employing these tokens as core elements in the modeling, which are essential inclusion criteria. Therefore, it does not meet the theme of \"Discrete Audio Tokens\" as defined, making it outside the inclusion scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Burnt area extraction from high-resolution satellite images based on anomaly detection",
    "abstract": "Wildfire detection using satellite images is a widely studied task in remote sensing with many applications to fire delineation and mapping. Recently, deep learning methods have become a scalable solution to automate this task, especially in the field of unsupervised learning where no training data is available. This is particularly important in the context of emergency risk monitoring where fast and effective detection is needed, generally based on high-resolution satellite data. Among various approaches, Anomaly Detection (AD) appears to be highly potential thanks to its broad applications in computer vision, medical imaging, as well as remote sensing. In this work, we build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE), a popular reconstruction-based AD method with discrete latent spaces, to perform unsupervised burnt area extraction. We integrate VQ-VAE into an end-to-end framework with an intensive post-processing step using dedicated vegetation, water and brightness indexes. Our experiments conducted on high-resolution SPOT-6/7 images provide promising results of the proposed technique, showing its high potential in future research on unsupervised burnt area extraction.",
    "metadata": {
      "arxiv_id": "2308.13367",
      "title": "Burnt area extraction from high-resolution satellite images based on anomaly detection",
      "summary": "Wildfire detection using satellite images is a widely studied task in remote sensing with many applications to fire delineation and mapping. Recently, deep learning methods have become a scalable solution to automate this task, especially in the field of unsupervised learning where no training data is available. This is particularly important in the context of emergency risk monitoring where fast and effective detection is needed, generally based on high-resolution satellite data. Among various approaches, Anomaly Detection (AD) appears to be highly potential thanks to its broad applications in computer vision, medical imaging, as well as remote sensing. In this work, we build upon the framework of Vector Quantized Variational Autoencoder (VQ-VAE), a popular reconstruction-based AD method with discrete latent spaces, to perform unsupervised burnt area extraction. We integrate VQ-VAE into an end-to-end framework with an intensive post-processing step using dedicated vegetation, water and brightness indexes. Our experiments conducted on high-resolution SPOT-6/7 images provide promising results of the proposed technique, showing its high potential in future research on unsupervised burnt area extraction.",
      "authors": [
        "Oscar David Rafael Narvaez Luces",
        "Minh-Tan Pham",
        "Quentin Poterek",
        "Rémi Braun"
      ],
      "published": "2023-08-25T13:25:27Z",
      "updated": "2023-08-25T13:25:27Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.13367v1",
      "landing_url": "https://arxiv.org/abs/2308.13367v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.13367"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on burnt area extraction from high-resolution satellite images and uses a VQ-VAE framework, but it does not discuss discrete audio tokens or audio tokenization, so it fails the required inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on burnt area extraction from high-resolution satellite images and uses a VQ-VAE framework, but it does not discuss discrete audio tokens or audio tokenization, so it fails the required inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on burnt area extraction from high-resolution satellite images using anomaly detection and vector quantized variational autoencoders (VQ-VAE) in a remote sensing context, which is unrelated to discrete audio token research involving discretization of audio waveforms for sequence modeling, generation, or multimodal alignment; thus, it does not meet the inclusion criteria nor relate to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on burnt area extraction from high-resolution satellite images using anomaly detection and vector quantized variational autoencoders (VQ-VAE) in a remote sensing context, which is unrelated to discrete audio token research involving discretization of audio waveforms for sequence modeling, generation, or multimodal alignment; thus, it does not meet the inclusion criteria nor relate to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voice Conversion with Denoising Diffusion Probabilistic GAN Models",
    "abstract": "Voice conversion is a method that allows for the transformation of speaking style while maintaining the integrity of linguistic information. There are many researchers using deep generative models for voice conversion tasks. Generative Adversarial Networks (GANs) can quickly generate high-quality samples, but the generated samples lack diversity. The samples generated by the Denoising Diffusion Probabilistic Models (DDPMs) are better than GANs in terms of mode coverage and sample diversity. But the DDPMs have high computational costs and the inference speed is slower than GANs. In order to make GANs and DDPMs more practical we proposes DiffGAN-VC, a variant of GANs and DDPMS, to achieve non-parallel many-to-many voice conversion (VC). We use large steps to achieve denoising, and also introduce a multimodal conditional GANs to model the denoising diffusion generative adversarial network. According to both objective and subjective evaluation experiments, DiffGAN-VC has been shown to achieve high voice quality on non-parallel data sets. Compared with the CycleGAN-VC method, DiffGAN-VC achieves speaker similarity, naturalness and higher sound quality.",
    "metadata": {
      "arxiv_id": "2308.14319",
      "title": "Voice Conversion with Denoising Diffusion Probabilistic GAN Models",
      "summary": "Voice conversion is a method that allows for the transformation of speaking style while maintaining the integrity of linguistic information. There are many researchers using deep generative models for voice conversion tasks. Generative Adversarial Networks (GANs) can quickly generate high-quality samples, but the generated samples lack diversity. The samples generated by the Denoising Diffusion Probabilistic Models (DDPMs) are better than GANs in terms of mode coverage and sample diversity. But the DDPMs have high computational costs and the inference speed is slower than GANs. In order to make GANs and DDPMs more practical we proposes DiffGAN-VC, a variant of GANs and DDPMS, to achieve non-parallel many-to-many voice conversion (VC). We use large steps to achieve denoising, and also introduce a multimodal conditional GANs to model the denoising diffusion generative adversarial network. According to both objective and subjective evaluation experiments, DiffGAN-VC has been shown to achieve high voice quality on non-parallel data sets. Compared with the CycleGAN-VC method, DiffGAN-VC achieves speaker similarity, naturalness and higher sound quality.",
      "authors": [
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-08-28T05:53:06Z",
      "updated": "2023-08-28T05:53:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.14319v1",
      "landing_url": "https://arxiv.org/abs/2308.14319v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.14319"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract focuses on diffusion/GAN-based voice conversion without any discussion of discrete audio tokens, tokenization, or neural audio codecs, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract focuses on diffusion/GAN-based voice conversion without any discussion of discrete audio tokens, tokenization, or neural audio codecs, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion using GAN and diffusion models but does not mention the use or generation of discrete audio tokens, neural audio codecs, or vector quantization for discretization; it mainly discusses continuous generative modeling approaches without detailing any discrete token representation or tokenizer mechanisms, making it outside the scope defined by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion using GAN and diffusion models but does not mention the use or generation of discrete audio tokens, neural audio codecs, or vector quantization for discretization; it mainly discusses continuous generative modeling approaches without detailing any discrete token representation or tokenizer mechanisms, making it outside the scope defined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
    "abstract": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
    "metadata": {
      "arxiv_id": "2308.16552",
      "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
      "summary": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
      "authors": [
        "Yang Liu",
        "Xiaoyun Zhong",
        "Shiyao Zhai",
        "Zhicheng Du",
        "Zhenyuan Gao",
        "Qiming Huang",
        "Canyang Zhang",
        "Bin Jiang",
        "Vijay Kumar Pandey",
        "Sanyang Han",
        "Runming Wang",
        "Yuxing Han",
        "Peiwu Qin"
      ],
      "published": "2023-08-31T08:43:52Z",
      "updated": "2023-08-31T08:43:52Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16552v1",
      "landing_url": "https://arxiv.org/abs/2308.16552v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.16552"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on video-based temporal action segmentation for CPR training and does not address discrete audio tokens, token design, or audio-token-based modeling, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on video-based temporal action segmentation for CPR training and does not address discrete audio tokens, token design, or audio-token-based modeling, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using deep learning and temporal action segmentation to improve CPR instruction quality from video data and does not involve discrete audio tokens or the generation, modeling, or analysis of audio token sequences derived from continuous audio waveforms. There is no mention of neural audio codec, vector quantization, or discrete token representations related to audio, so it does not meet the inclusion criteria centered around discrete audio tokens and is unrelated to the core topic required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using deep learning and temporal action segmentation to improve CPR instruction quality from video data and does not involve discrete audio tokens or the generation, modeling, or analysis of audio token sequences derived from continuous audio waveforms. There is no mention of neural audio codec, vector quantization, or discrete token representations related to audio, so it does not meet the inclusion criteria centered around discrete audio tokens and is unrelated to the core topic required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
    "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly introduces a discrete speech tokenizer (RVQ with multi-codebooks) and treats tokens as core representations for downstream speech-language modeling, with English title/abstract and evaluative experiments comparing token-based methods, satisfying all inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly introduces a discrete speech tokenizer (RVQ with multi-codebooks) and treats tokens as core representations for downstream speech-language modeling, with English title/abstract and evaluative experiments comparing token-based methods, satisfying all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a unified speech tokenizer using residual vector quantization, combining semantic and acoustic discrete tokens, which clearly aligns with the discrete audio token topic focusing on neural audio codec-based tokenization suitable for speech large language models, with code and details for reproducibility; thus it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a unified speech tokenizer using residual vector quantization, combining semantic and acoustic discrete tokens, which clearly aligns with the discrete audio token topic focusing on neural audio codec-based tokenization suitable for speech large language models, with code and details for reproducibility; thus it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
    "abstract": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
    "metadata": {
      "arxiv_id": "2309.00126",
      "title": "QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning",
      "summary": "This paper proposes a novel semi-supervised TTS framework, QS-TTS, to improve TTS quality with lower supervised data requirements via Vector-Quantized Self-Supervised Speech Representation Learning (VQ-S3RL) utilizing more unlabeled speech audio. This framework comprises two VQ-S3R learners: first, the principal learner aims to provide a generative Multi-Stage Multi-Codebook (MSMC) VQ-S3R via the MSMC-VQ-GAN combined with the contrastive S3RL, while decoding it back to the high-quality audio; then, the associate learner further abstracts the MSMC representation into a highly-compact VQ representation through a VQ-VAE. These two generative VQ-S3R learners provide profitable speech representations and pre-trained models for TTS, significantly improving synthesis quality with the lower requirement for supervised data. QS-TTS is evaluated comprehensively under various scenarios via subjective and objective tests in experiments. The results powerfully demonstrate the superior performance of QS-TTS, winning the highest MOS over supervised or semi-supervised baseline TTS approaches, especially in low-resource scenarios. Moreover, comparing various speech representations and transfer learning methods in TTS further validates the notable improvement of the proposed VQ-S3RL to TTS, showing the best audio quality and intelligibility metrics. The trend of slower decay in the synthesis quality of QS-TTS with decreasing supervised data further highlights its lower requirements for supervised data, indicating its great potential in low-resource scenarios.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Jiawen Kang",
        "Yujia Xiao",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-08-31T20:25:44Z",
      "updated": "2023-08-31T20:25:44Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00126v1",
      "landing_url": "https://arxiv.org/abs/2309.00126v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.00126"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It explicitly uses discrete audio tokens via vector quantization (VQ-S3RL, MSMC-VQ-GAN, VQ-VAE) for TTS, addressing token design and evaluation, thus meeting the inclusion criteria and should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It explicitly uses discrete audio tokens via vector quantization (VQ-S3RL, MSMC-VQ-GAN, VQ-VAE) for TTS, addressing token design and evaluation, thus meeting the inclusion criteria and should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a semi-supervised text-to-speech (TTS) framework using vector-quantized self-supervised speech representation learning, involving multi-codebook vector quantization to produce discrete speech representations used for TTS. This fits the theme of discrete audio tokens derived via neural audio encoding (vector quantization) utilized as core representations for speech synthesis, with detailed design about multi-stage multi-codebook VQ and evaluations on speech quality. Therefore, it meets the inclusion criteria of focusing on discrete audio tokens generated by neural discrete representation methods with application to TTS generation and empirical evaluation. No exclusion criteria apply since discrete tokens and tokenization details are central to the work and the abstract provides sufficient English information to assess these aspects. Hence, it is suitable for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents a semi-supervised text-to-speech (TTS) framework using vector-quantized self-supervised speech representation learning, involving multi-codebook vector quantization to produce discrete speech representations used for TTS. This fits the theme of discrete audio tokens derived via neural audio encoding (vector quantization) utilized as core representations for speech synthesis, with detailed design about multi-stage multi-codebook VQ and evaluations on speech quality. Therefore, it meets the inclusion criteria of focusing on discrete audio tokens generated by neural discrete representation methods with application to TTS generation and empirical evaluation. No exclusion criteria apply since discrete tokens and tokenization details are central to the work and the abstract provides sufficient English information to assess these aspects. Hence, it is suitable for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "RepCodec uses a vector-quantized codebook to convert speech representations into discrete tokens for downstream modeling, clearly aligning with the discrete audio tokens concept and providing English abstract details, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "RepCodec uses a vector-quantized codebook to convert speech representations into discrete tokens for downstream modeling, clearly aligning with the discrete audio tokens concept and providing English abstract details, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces RepCodec, a neural speech representation codec that produces discrete semantic speech tokens through vector quantization, aiming to improve speech understanding and generation with large language models. The paper clearly presents the discrete token generation process, leveraging neural encoders and vector quantization codebooks, which fits the inclusion criteria of discrete audio tokens for subsequent modeling and generation tasks. The work emphasizes token design improving fidelity and usability in downstream tasks, with detailed methods and evaluation, all in English. Therefore, it meets the inclusion standards and does not fall into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces RepCodec, a neural speech representation codec that produces discrete semantic speech tokens through vector quantization, aiming to improve speech understanding and generation with large language models. The paper clearly presents the discrete token generation process, leveraging neural encoders and vector quantization codebooks, which fits the inclusion criteria of discrete audio tokens for subsequent modeling and generation tasks. The work emphasizes token design improving fidelity and usability in downstream tasks, with detailed methods and evaluation, all in English. Therefore, it meets the inclusion standards and does not fall into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
    "abstract": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
    "metadata": {
      "arxiv_id": "2309.01692",
      "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
      "summary": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
      "authors": [
        "Xin Lai",
        "Yuhui Yuan",
        "Ruihang Chu",
        "Yukang Chen",
        "Han Hu",
        "Jiaya Jia"
      ],
      "published": "2023-09-04T16:09:28Z",
      "updated": "2023-09-04T16:09:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01692v1",
      "landing_url": "https://arxiv.org/abs/2309.01692v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.01692"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a 3D instance segmentation paper in computer vision, not about discrete audio tokens, tokenization, or audio-related downstream tasks, hence it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a 3D instance segmentation paper in computer vision, not about discrete audio tokens, tokenization, or audio-related downstream tasks, hence it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on 3D instance segmentation using a transformer architecture without any indication of involvement in audio token discretization, neural audio codecs, or related audio token generation and modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens for audio generation, understanding, or cross-modal reasoning, and instead is irrelevant to the given topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on 3D instance segmentation using a transformer architecture without any indication of involvement in audio token discretization, neural audio codecs, or related audio token generation and modeling. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens for audio generation, understanding, or cross-modal reasoning, and instead is irrelevant to the given topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Employing Real Training Data for Deep Noise Suppression",
    "abstract": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
    "metadata": {
      "arxiv_id": "2309.02432",
      "title": "Employing Real Training Data for Deep Noise Suppression",
      "summary": "Most deep noise suppression (DNS) models are trained with reference-based losses requiring access to clean speech. However, sometimes an additive microphone model is insufficient for real-world applications. Accordingly, ways to use real training data in supervised learning for DNS models promise to reduce a potential training/inference mismatch. Employing real data for DNS training requires either generative approaches or a reference-free loss without access to the corresponding clean speech. In this work, we propose to employ an end-to-end non-intrusive deep neural network (DNN), named PESQ-DNN, to estimate perceptual evaluation of speech quality (PESQ) scores of enhanced real data. It provides a reference-free perceptual loss for employing real data during DNS training, maximizing the PESQ scores. Furthermore, we use an epoch-wise alternating training protocol, updating the DNS model on real data, followed by PESQ-DNN updating on synthetic data. The DNS model trained with the PESQ-DNN employing real data outperforms all reference methods employing only synthetic training data. On synthetic test data, our proposed method excels the Interspeech 2021 DNS Challenge baseline by a significant 0.32 PESQ points. Both on synthetic and real test data, the proposed method beats the baseline by 0.05 DNSMOS points - although PESQ-DNN optimizes for a different perceptual metric.",
      "authors": [
        "Ziyi Xu",
        "Marvin Sach",
        "Jan Pirklbauer",
        "Tim Fingscheidt"
      ],
      "published": "2023-09-05T17:58:58Z",
      "updated": "2023-09-05T17:58:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02432v1",
      "landing_url": "https://arxiv.org/abs/2309.02432v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article centers on using real training data and a PESQ-DNN perceptual loss for deep noise suppression, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article centers on using real training data and a PESQ-DNN perceptual loss for deep noise suppression, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training deep noise suppression models using real data and introduces a DNN model for reference-free perceptual loss, but it does not mention any discrete audio tokens, discretization of audio waveforms into token sequences, or neural audio codecs producing such tokens; thus, it does not meet the core inclusion criteria regarding discrete audio tokens and their use for audio generation or multimodal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training deep noise suppression models using real data and introduces a DNN model for reference-free perceptual loss, but it does not mention any discrete audio tokens, discretization of audio waveforms into token sequences, or neural audio codecs producing such tokens; thus, it does not meet the core inclusion criteria regarding discrete audio tokens and their use for audio generation or multimodal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improved Outlier Robust Seeding for k-means",
    "abstract": "The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold. Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee. Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \\cite{Charikar}, \\cite{KrishnaswamyLS18} and \\textit{robust $k$-means++} \\cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\\cite{tkmeanspp}, and robust $k$-means++~\\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \\cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \\cite{feldman2007ptas,langberg2010universal,feldman2011unified}.",
    "metadata": {
      "arxiv_id": "2309.02710",
      "title": "Improved Outlier Robust Seeding for k-means",
      "summary": "The $k$-means is a popular clustering objective, although it is inherently non-robust and sensitive to outliers. Its popular seeding or initialization called $k$-means++ uses $D^{2}$ sampling and comes with a provable $O(\\log k)$ approximation guarantee \\cite{AV2007}. However, in the presence of adversarial noise or outliers, $D^{2}$ sampling is more likely to pick centers from distant outliers instead of inlier clusters, and therefore its approximation guarantees \\textit{w.r.t.} $k$-means solution on inliers, does not hold.\n  Assuming that the outliers constitute a constant fraction of the given data, we propose a simple variant in the $D^2$ sampling distribution, which makes it robust to the outliers. Our algorithm runs in $O(ndk)$ time, outputs $O(k)$ clusters, discards marginally more points than the optimal number of outliers, and comes with a provable $O(1)$ approximation guarantee.\n  Our algorithm can also be modified to output exactly $k$ clusters instead of $O(k)$ clusters, while keeping its running time linear in $n$ and $d$. This is an improvement over previous results for robust $k$-means based on LP relaxation and rounding \\cite{Charikar}, \\cite{KrishnaswamyLS18} and \\textit{robust $k$-means++} \\cite{DeshpandeKP20}. Our empirical results show the advantage of our algorithm over $k$-means++~\\cite{AV2007}, uniform random seeding, greedy sampling for $k$ means~\\cite{tkmeanspp}, and robust $k$-means++~\\cite{DeshpandeKP20}, on standard real-world and synthetic data sets used in previous work. Our proposal is easily amenable to scalable, faster, parallel implementations of $k$-means++ \\cite{Bahmani,BachemL017} and is of independent interest for coreset constructions in the presence of outliers \\cite{feldman2007ptas,langberg2010universal,feldman2011unified}.",
      "authors": [
        "Amit Deshpande",
        "Rameshwar Pratap"
      ],
      "published": "2023-09-06T04:46:01Z",
      "updated": "2023-09-06T04:46:01Z",
      "categories": [
        "cs.LG",
        "cs.CG",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.02710v1",
      "landing_url": "https://arxiv.org/abs/2309.02710v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.02710"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses k-means seeding robustness and outlier handling; there is no mention of discrete audio tokens, tokenizers, or audio token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses k-means seeding robustness and outlier handling; there is no mention of discrete audio tokens, tokenizers, or audio token-based modeling, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving k-means clustering seeding with robustness to outliers, which is a machine learning optimization topic unrelated to discrete audio tokenization. It does not discuss discrete token generation from continuous audio waveforms, neural audio codecs, vector quantization, or any audio-specific tokenization or generation tasks relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving k-means clustering seeding with robustness to outliers, which is a machine learning optimization topic unrelated to discrete audio tokenization. It does not discuss discrete token generation from continuous audio waveforms, neural audio codecs, vector quantization, or any audio-specific tokenization or generation tasks relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-Utterance Conditioned VAE for Speech Generation",
    "abstract": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
    "metadata": {
      "arxiv_id": "2309.04156",
      "title": "Cross-Utterance Conditioned VAE for Speech Generation",
      "summary": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
      "authors": [
        "Yang Li",
        "Cheng Yu",
        "Guangzhi Sun",
        "Weiqin Zu",
        "Zheng Tian",
        "Ying Wen",
        "Wei Pan",
        "Chao Zhang",
        "Jun Wang",
        "Yang Yang",
        "Fanglei Sun"
      ],
      "published": "2023-09-08T06:48:41Z",
      "updated": "2024-09-19T13:41:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04156v2",
      "landing_url": "https://arxiv.org/abs/2309.04156v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.04156"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a CVAE-based speech synthesis framework using continuous mel-spectrogram features and context, with no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and aligns with the exclusion criteria for continuous representations without tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a CVAE-based speech synthesis framework using continuous mel-spectrogram features and context, with no discussion of discrete audio tokens, tokenizers, codebooks, or token-based modeling; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and aligns with the exclusion criteria for continuous representations without tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a speech synthesis framework focusing on context-aware prosody generation using VAEs and pre-trained language models but does not describe any discretization of audio into discrete token sequences or the use of neural audio codecs generating discrete tokens. There is no mention of tokenizer architecture, vector quantization, codebook settings, or discrete token usage integral to the modeling approach, which are essential per the inclusion criteria. Hence, it does not meet the core inclusion criteria related to discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a speech synthesis framework focusing on context-aware prosody generation using VAEs and pre-trained language models but does not describe any discretization of audio into discrete token sequences or the use of neural audio codecs generating discrete tokens. There is no mention of tokenizer architecture, vector quantization, codebook settings, or discrete token usage integral to the modeling approach, which are essential per the inclusion criteria. Hence, it does not meet the core inclusion criteria related to discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A multiple k-means cluster ensemble framework for clustering citation trajectories",
    "abstract": "Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles from the Microsoft Academic Graph data are considered for clustering short term (10 year) and long term (30 year) trajectories, respectively. It has linear run time. Four distinct trajectories are obtained Early Rise Rapid Decline (2.2%), Early Rise Slow Decline (45%), Delayed Rise No Decline (53%), and Delayed Rise Slow Decline (0.8%). Individual trajectory differences for two different spans are studied. Most papers exhibit Early Rise Slow Decline and Delayed Rise No Decline patterns. The growth and decay times, cumulative citation distribution, and peak characteristics of individual trajectories are redefined empirically. A detailed comparative study reveals our proposed methodology can detect all distinct trajectory classes.",
    "metadata": {
      "arxiv_id": "2309.04949",
      "title": "A multiple k-means cluster ensemble framework for clustering citation trajectories",
      "summary": "Citation maturity time varies for different articles. However, the impact of all articles is measured in a fixed window. Clustering their citation trajectories helps understand the knowledge diffusion process and reveals that not all articles gain immediate success after publication. Moreover, clustering trajectories is necessary for paper impact recommendation algorithms. It is a challenging problem because citation time series exhibit significant variability due to non linear and non stationary characteristics. Prior works propose a set of arbitrary thresholds and a fixed rule based approach. All methods are primarily parameter dependent. Consequently, it leads to inconsistencies while defining similar trajectories and ambiguities regarding their specific number. Most studies only capture extreme trajectories. Thus, a generalised clustering framework is required. This paper proposes a feature based multiple k means cluster ensemble framework. 1,95,783 and 41,732 well cited articles from the Microsoft Academic Graph data are considered for clustering short term (10 year) and long term (30 year) trajectories, respectively. It has linear run time. Four distinct trajectories are obtained Early Rise Rapid Decline (2.2%), Early Rise Slow Decline (45%), Delayed Rise No Decline (53%), and Delayed Rise Slow Decline (0.8%). Individual trajectory differences for two different spans are studied. Most papers exhibit Early Rise Slow Decline and Delayed Rise No Decline patterns. The growth and decay times, cumulative citation distribution, and peak characteristics of individual trajectories are redefined empirically. A detailed comparative study reveals our proposed methodology can detect all distinct trajectory classes.",
      "authors": [
        "Joyita Chakraborty",
        "Dinesh K. Pradhan",
        "Subrata Nandi"
      ],
      "published": "2023-09-10T07:10:31Z",
      "updated": "2023-09-10T07:10:31Z",
      "categories": [
        "cs.SI",
        "cs.DB",
        "cs.DL",
        "cs.IR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04949v1",
      "landing_url": "https://arxiv.org/abs/2309.04949v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.04949"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is about clustering citation trajectories and does not address discrete audio tokens or tokenization, thus it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is about clustering citation trajectories and does not address discrete audio tokens or tokenization, thus it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on clustering citation trajectories of academic articles using a multiple k-means cluster ensemble framework, which is unrelated to discrete audio tokens or audio waveform discretization, neural audio codecs, or related topics in audio token design and usage as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on clustering citation trajectories of academic articles using a multiple k-means cluster ensemble framework, which is unrelated to discrete audio tokens or audio waveform discretization, neural audio codecs, or related topics in audio token design and usage as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
    "abstract": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
    "metadata": {
      "arxiv_id": "2309.05027",
      "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Ziyang Ma",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-09-10T13:47:39Z",
      "updated": "2024-09-01T14:57:31Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05027v3",
      "landing_url": "https://arxiv.org/abs/2309.05027v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.05027"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets diffusion/flow-based TTS and mel-spectrogram synthesis without any discrete audio tokenization or token/codec design described, so it does not meet the discrete audio tokens criteria and falls under the exclusion set.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets diffusion/flow-based TTS and mel-spectrogram synthesis without any discrete audio tokenization or token/codec design described, so it does not meet the discrete audio tokens criteria and falls under the exclusion set.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses VoiceFlow, a text-to-speech model using a rectified flow matching algorithm for efficient mel-spectrogram generation. However, it does not mention discretization of audio into token sequences, neural audio codec or vector quantization, nor does it describe any discrete audio token vocabulary or tokenization process. Instead, it focuses on continuous acoustic modeling via differential equations, which aligns with continuous audio representations rather than discrete tokens. Therefore, it does not meet the inclusion criteria which require discrete audio tokens as the core method or analysis focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses VoiceFlow, a text-to-speech model using a rectified flow matching algorithm for efficient mel-spectrogram generation. However, it does not mention discretization of audio into token sequences, neural audio codec or vector quantization, nor does it describe any discrete audio token vocabulary or tokenization process. Instead, it focuses on continuous acoustic modeling via differential equations, which aligns with continuous audio representations rather than discrete tokens. Therefore, it does not meet the inclusion criteria which require discrete audio tokens as the core method or analysis focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
    "abstract": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
    "metadata": {
      "arxiv_id": "2309.05224",
      "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
      "summary": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
      "authors": [
        "Krisna Pinasthika",
        "Blessius Sheldo Putra Laksono",
        "Riyandi Banovbi Putera Irsal",
        "Syifa Hukma Shabiyya",
        "Novanto Yudistira"
      ],
      "published": "2023-09-11T04:03:43Z",
      "updated": "2023-09-11T04:03:43Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05224v1",
      "landing_url": "https://arxiv.org/abs/2309.05224v1",
      "doi": "https://doi.org/10.1016/j.neucom.2024.127433"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets image transformers (SparseSwin) with sparse tokens; there is no discrete audio tokenization/codec or audio-specific token design, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets image transformers (SparseSwin) with sparse tokens; there is no discrete audio tokenization/codec or audio-specific token design, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving transformer efficiency for image classification by reducing token quantities in visual tokens; it does not involve discrete audio tokens, nor does it address audio signal discretization, neural audio coders, or audio-related token designs, thus not meeting any inclusion criteria centered on audio discrete token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving transformer efficiency for image classification by reducing token quantities in visual tokens; it does not involve discrete audio tokens, nor does it address audio signal discretization, neural audio coders, or audio-related token designs, thus not meeting any inclusion criteria centered on audio discrete token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
    "abstract": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
    "metadata": {
      "arxiv_id": "2309.06787",
      "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
      "summary": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
      "authors": [
        "Zhichao Wu",
        "Qiulin Li",
        "Sixing Liu",
        "Qun Yang"
      ],
      "published": "2023-09-13T08:22:38Z",
      "updated": "2023-09-13T08:22:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06787v1",
      "landing_url": "https://arxiv.org/abs/2309.06787v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06787"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract indicate a discrete diffusion model in discrete space for TTS, implying discrete tokens, but the abstract does not explicitly describe how discrete tokens are produced (tokenizer/codec architecture, codebooks, quantization, or explicit token-level inputs), making it unclear whether it fully satisfies the discrete audio token criteria.\n",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The title/abstract indicate a discrete diffusion model in discrete space for TTS, implying discrete tokens, but the abstract does not explicitly describe how discrete tokens are produced (tokenizer/codec architecture, codebooks, quantization, or explicit token-level inputs), making it unclear whether it fully satisfies the discrete audio token criteria.\n",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a discrete diffusion model for text-to-speech generation, but the abstract does not explicitly describe the process of generating discrete audio tokens by neural audio codec or self-supervised representation learning, nor details about tokenizer, codebook, quantization, or vocabulary settings required to confirm it fully meets the inclusion criteria involving discrete audio token representations as core models; it focuses more on model efficiency and contrastive learning rather than token discretization specifics.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a discrete diffusion model for text-to-speech generation, but the abstract does not explicitly describe the process of generating discrete audio tokens by neural audio codec or self-supervised representation learning, nor details about tokenizer, codebook, quantization, or vocabulary settings required to confirm it fully meets the inclusion criteria involving discrete audio token representations as core models; it focuses more on model efficiency and contrastive learning rather than token discretization specifics.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract and title state a “discrete diffusion model” and refer to operating in “discrete space,” but they do not describe any concrete discrete-token generation process (no tokenizer/codec architecture, codebook/quantization details, vocabulary/bitrate or time-downsampling information) required by the inclusion criteria; both junior reviewers flagged this same omission, so from the provided abstract we cannot verify that discrete audio tokens (as defined) are the core representation. Brief explanation for score: the submission should be excluded (or at best provisionally excluded) because essential tokenization/codec details are missing, preventing confirmation that it meets the Discrete Audio Tokens scope.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract and title state a “discrete diffusion model” and refer to operating in “discrete space,” but they do not describe any concrete discrete-token generation process (no tokenizer/codec architecture, codebook/quantization details, vocabulary/bitrate or time-downsampling information) required by the inclusion criteria; both junior reviewers flagged this same omission, so from the provided abstract we cannot verify that discrete audio tokens (as defined) are the core representation. Brief explanation for score: the submission should be excluded (or at best provisionally excluded) because essential tokenization/codec details are missing, preventing confirmation that it meets the Discrete Audio Tokens scope.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Direct Text to Speech Translation System using Acoustic Units",
    "abstract": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
    "metadata": {
      "arxiv_id": "2309.07478",
      "title": "Direct Text to Speech Translation System using Acoustic Units",
      "summary": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
      "authors": [
        "Victoria Mingote",
        "Pablo Gimeno",
        "Luis Vicente",
        "Sameer Khurana",
        "Antoine Laurent",
        "Jarod Duret"
      ],
      "published": "2023-09-14T07:35:14Z",
      "updated": "2023-09-14T07:35:14Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07478v1",
      "landing_url": "https://arxiv.org/abs/2309.07478v1",
      "doi": "https://doi.org/10.1109/LSP.2023.3313513"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses discrete acoustic units produced by a speech encoder plus clustering, forming a discrete token representation used to drive a downstream encoder–decoder and vocoder for direct text-to-speech translation, aligning with the discrete audio tokens criterion; English title/abstract provided and describe token production and downstream generation, with no exclusion indicators present.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses discrete acoustic units produced by a speech encoder plus clustering, forming a discrete token representation used to drive a downstream encoder–decoder and vocoder for direct text-to-speech translation, aligning with the discrete audio tokens criterion; English title/abstract provided and describe token production and downstream generation, with no exclusion indicators present.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper clearly uses discrete acoustic units obtained via a speech encoder combined with clustering, effectively creating a discrete token sequence representing audio units for text-to-speech translation, aligning with the discrete audio tokens definition. The system applies these tokens in a downstream generation task (direct text-to-speech translation), includes experimental validation, and provides sufficient methodological detail, fulfilling inclusion criteria and no exclusion criteria are met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper clearly uses discrete acoustic units obtained via a speech encoder combined with clustering, effectively creating a discrete token sequence representing audio units for text-to-speech translation, aligning with the discrete audio tokens definition. The system applies these tokens in a downstream generation task (direct text-to-speech translation), includes experimental validation, and provides sufficient methodological detail, fulfilling inclusion criteria and no exclusion criteria are met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper VoxtLM explicitly uses discrete speech tokens from self-supervised features as input tokens alongside text tokens in a unified decoder-only model for multiple tasks (ASR, TTS, text/speech continuation), aligning with the Discrete Audio Tokens criteria and providing empirical evaluations, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper VoxtLM explicitly uses discrete speech tokens from self-supervised features as input tokens alongside text tokens in a unified decoder-only model for multiple tasks (ASR, TTS, text/speech continuation), aligning with the Discrete Audio Tokens criteria and providing empirical evaluations, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes VoxtLM, a decoder-only model integrating text vocabulary with discrete speech tokens derived from self-supervised speech features, which align with the inclusion criterion of using discrete audio tokens from self-supervised models; moreover, it addresses tasks related to speech recognition and synthesis, indicating usage of discrete token sequences for audio generation and understanding, and open-sourced training details support reproducibility, satisfying evaluation requirements; therefore, it meets inclusion criteria and no exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes VoxtLM, a decoder-only model integrating text vocabulary with discrete speech tokens derived from self-supervised speech features, which align with the inclusion criterion of using discrete audio tokens from self-supervised models; moreover, it addresses tasks related to speech recognition and synthesis, indicating usage of discrete token sequences for audio generation and understanding, and open-sourced training details support reproducibility, satisfying evaluation requirements; therefore, it meets inclusion criteria and no exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Residual Speaker Representation for One-Shot Voice Conversion",
    "abstract": "Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach that leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. Introducing multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available.",
    "metadata": {
      "arxiv_id": "2309.08166",
      "title": "Residual Speaker Representation for One-Shot Voice Conversion",
      "summary": "Recently, there have been significant advancements in voice conversion, resulting in high-quality performance. However, there are still two critical challenges in this field. Firstly, current voice conversion methods have limited robustness when encountering unseen speakers. Secondly, they also have limited ability to control timbre representation. To address these challenges, this paper presents a novel approach that leverages tokens of multi-layer residual approximations to enhance robustness when dealing with unseen speakers, called the residual speaker module. Introducing multi-layer approximations facilitates the separation of information from the timbre, enabling effective control over timbre in voice conversion. The proposed method outperforms baselines in subjective and objective evaluations, demonstrating superior performance and increased robustness. Our demo page is publicly available.",
      "authors": [
        "Le Xu",
        "Jiangyan Yi",
        "Tao Wang",
        "Yong Ren",
        "Rongxiu Zhong",
        "Zhengqi Wen",
        "Jianhua Tao"
      ],
      "published": "2023-09-15T05:27:21Z",
      "updated": "2024-08-11T16:40:07Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08166v2",
      "landing_url": "https://arxiv.org/abs/2309.08166v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.08166"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'tokens' and a residual module, but it does not clearly define how discrete audio tokens are generated (tokenizer/codec) or used as the core representation, so it is not clearly meeting the discrete-token criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'tokens' and a residual module, but it does not clearly define how discrete audio tokens are generated (tokenizer/codec) or used as the core representation, so it is not clearly meeting the discrete-token criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on voice conversion enhancement using a residual speaker module to improve robustness and control over timbre, but it does not mention the use or generation of discrete audio tokens through discretization, neural audio codecs, or self-supervised representation learning models. No details on tokenizer architecture, quantization methods, vocabularies, or discrete token sequences are provided, making it unclear if the proposed method involves discrete audio tokenization as defined by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete token generation and usage, nor does it provide sufficient English method details to validate discrete tokens. As a result, it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on voice conversion enhancement using a residual speaker module to improve robustness and control over timbre, but it does not mention the use or generation of discrete audio tokens through discretization, neural audio codecs, or self-supervised representation learning models. No details on tokenizer architecture, quantization methods, vocabularies, or discrete token sequences are provided, making it unclear if the proposed method involves discrete audio tokenization as defined by the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete token generation and usage, nor does it provide sufficient English method details to validate discrete tokens. As a result, it should be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment",
    "abstract": "This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.",
    "metadata": {
      "arxiv_id": "2309.09470",
      "title": "Face-Driven Zero-Shot Voice Conversion with Memory-based Face-Voice Alignment",
      "summary": "This paper presents a novel task, zero-shot voice conversion based on face images (zero-shot FaceVC), which aims at converting the voice characteristics of an utterance from any source speaker to a newly coming target speaker, solely relying on a single face image of the target speaker. To address this task, we propose a face-voice memory-based zero-shot FaceVC method. This method leverages a memory-based face-voice alignment module, in which slots act as the bridge to align these two modalities, allowing for the capture of voice characteristics from face images. A mixed supervision strategy is also introduced to mitigate the long-standing issue of the inconsistency between training and inference phases for voice conversion tasks. To obtain speaker-independent content-related representations, we transfer the knowledge from a pretrained zero-shot voice conversion model to our zero-shot FaceVC model. Considering the differences between FaceVC and traditional voice conversion tasks, systematic subjective and objective metrics are designed to thoroughly evaluate the homogeneity, diversity and consistency of voice characteristics controlled by face images. Through extensive experiments, we demonstrate the superiority of our proposed method on the zero-shot FaceVC task. Samples are presented on our demo website.",
      "authors": [
        "Zheng-Yan Sheng",
        "Yang Ai",
        "Yan-Nian Chen",
        "Zhen-Hua Ling"
      ],
      "published": "2023-09-18T04:08:02Z",
      "updated": "2023-09-18T04:08:02Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09470v1",
      "landing_url": "https://arxiv.org/abs/2309.09470v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.09470"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: This item describes face-driven zero-shot voice conversion with memory-based face-voice alignment; it does not involve discrete audio tokens, tokenizers, or token-based modeling as required by the 'Discrete Audio Tokens' topic, hence it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This item describes face-driven zero-shot voice conversion with memory-based face-voice alignment; it does not involve discrete audio tokens, tokenizers, or token-based modeling as required by the 'Discrete Audio Tokens' topic, hence it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a zero-shot voice conversion system driven by face images, aiming to convert voice characteristics based on a single face image. However, the abstract does not mention any discretization or mapping of audio waveforms into discrete tokens, nor does it describe the use or design of discrete audio tokens, neural audio codecs, or vector quantization methods. Therefore, the content does not align with the core inclusion criteria regarding discrete audio tokens as a central representation or modeling focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a zero-shot voice conversion system driven by face images, aiming to convert voice characteristics based on a single face image. However, the abstract does not mention any discretization or mapping of audio waveforms into discrete tokens, nor does it describe the use or design of discrete audio tokens, neural audio codecs, or vector quantization methods. Therefore, the content does not align with the core inclusion criteria regarding discrete audio tokens as a central representation or modeling focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
    "abstract": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
    "metadata": {
      "arxiv_id": "2309.09630",
      "title": "Refining DNN-based Mask Estimation using CGMM-based EM Algorithm for Multi-channel Noise Reduction",
      "summary": "In this paper, we present a method that allows to further improve speech enhancement obtained with recently introduced Deep Neural Network (DNN) models. We propose a multi-channel refinement method of time-frequency masks obtained with single-channel DNNs, which consists of an iterative Complex Gaussian Mixture Model (CGMM) based algorithm, followed by optimum spatial filtration. We validate our approach on time-frequency masks estimated with three recent deep learning models, namely DCUnet, DCCRN, and FullSubNet. We show that our method with the proposed mask refinement procedure allows to improve the accuracy of estimated masks, in terms of the Area Under the ROC Curve (AUC) measure, and as a consequence the overall speech quality of the enhanced speech signal, as measured by PESQ improvement, and that the improvement is consistent across all three DNN models.",
      "authors": [
        "Julitta Bartolewska",
        "Stanisław Kacprzak",
        "Konrad Kowalczyk"
      ],
      "published": "2023-09-18T10:05:41Z",
      "updated": "2023-09-18T10:05:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09630v1",
      "landing_url": "https://arxiv.org/abs/2309.09630v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10632"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on continuous time-frequency mask refinement for multi-channel noise reduction and does not define or use discrete audio tokens or a tokenization pipeline, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item centers on continuous time-frequency mask refinement for multi-channel noise reduction and does not define or use discrete audio tokens or a tokenization pipeline, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving speech enhancement by refining time-frequency masks using a CGMM-based algorithm and spatial filtration, but it does not mention the generation or use of discrete audio tokens, neural audio codecs, quantization strategies, vocabularies, or tokenizers, nor does it relate to modeling audio as discrete token sequences for downstream tasks like generation or alignment as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving speech enhancement by refining time-frequency masks using a CGMM-based algorithm and spatial filtration, but it does not mention the generation or use of discrete audio tokens, neural audio codecs, quantization strategies, vocabularies, or tokenizers, nor does it relate to modeling audio as discrete token sequences for downstream tasks like generation or alignment as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
    "abstract": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
    "metadata": {
      "arxiv_id": "2309.10379",
      "title": "PDPCRN: Parallel Dual-Path CRN with Bi-directional Inter-Branch Interactions for Multi-Channel Speech Enhancement",
      "summary": "Multi-channel speech enhancement seeks to utilize spatial information to distinguish target speech from interfering signals. While deep learning approaches like the dual-path convolutional recurrent network (DPCRN) have made strides, challenges persist in effectively modeling inter-channel correlations and amalgamating multi-level information. In response, we introduce the Parallel Dual-Path Convolutional Recurrent Network (PDPCRN). This acoustic modeling architecture has two key innovations. First, a parallel design with separate branches extracts complementary features. Second, bi-directional modules enable cross-branch communication. Together, these facilitate diverse representation fusion and enhanced modeling. Experimental validation on TIMIT datasets underscores the prowess of PDPCRN. Notably, against baseline models like the standard DPCRN, PDPCRN not only outperforms in PESQ and STOI metrics but also boasts a leaner computational footprint with reduced parameters.",
      "authors": [
        "Jiahui Pan",
        "Shulin He",
        "Tianci Wu",
        "Hui Zhang",
        "Xueliang Zhang"
      ],
      "published": "2023-09-19T07:27:38Z",
      "updated": "2023-09-19T07:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10379v1",
      "landing_url": "https://arxiv.org/abs/2309.10379v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a neural network architecture for multi-channel speech enhancement and evaluation metrics, with no mention of discrete audio tokens, tokenization, or token-based methodologies, thus it does not meet the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a neural network architecture for multi-channel speech enhancement and evaluation metrics, with no mention of discrete audio tokens, tokenization, or token-based methodologies, thus it does not meet the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network architecture (PDPCRN) for multi-channel speech enhancement, which involves continuous audio signal processing rather than discretizing audio into token sequences. There is no mention of discrete token generation, token vocabularies, vector quantization, or related mechanisms required by the inclusion criteria. Thus, it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network architecture (PDPCRN) for multi-channel speech enhancement, which involves continuous audio signal processing rather than discretizing audio into token sequences. There is no mention of discrete token generation, token vocabularies, vector quantization, or related mechanisms required by the inclusion criteria. Thus, it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
    "abstract": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
    "metadata": {
      "arxiv_id": "2309.10818",
      "title": "SlimPajama-DC: Understanding Data Combinations for LLM Training",
      "summary": "This paper aims to understand the impacts of various data combinations (e.g., web text, Wikipedia, GitHub, books) on the pretraining of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T token RedPajama dataset contributed by Together. We have termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations on SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our SlimPajama-DC models are available at: https://huggingface.co/MBZUAI-LLM/SlimPajama-DC and the separate SlimPajama-DC datasets are available at: https://huggingface.co/datasets/MBZUAI-LLM/SlimPajama-627B-DC.",
      "authors": [
        "Zhiqiang Shen",
        "Tianhua Tao",
        "Liqun Ma",
        "Willie Neiswanger",
        "Zhengzhong Liu",
        "Hongyi Wang",
        "Bowen Tan",
        "Joel Hestness",
        "Natalia Vassilieva",
        "Daria Soboleva",
        "Eric Xing"
      ],
      "published": "2023-09-19T17:59:54Z",
      "updated": "2024-05-09T13:56:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10818v3",
      "landing_url": "https://arxiv.org/abs/2309.10818v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.10818"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項研究聚焦於 SlimPajama-DC 資料集的來源、去重與訓練配置對 LLM 性能的影響，未涉及離散音訊 token 的產生、編碼或對下游任務的分析，因此不符合『Discrete Audio Tokens』主題的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項研究聚焦於 SlimPajama-DC 資料集的來源、去重與訓練配置對 LLM 性能的影響，未涉及離散音訊 token 的產生、編碼或對下游任務的分析，因此不符合『Discrete Audio Tokens』主題的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on understanding the impact of various data combinations on training large language models using a curated text dataset called SlimPajama-DC, with no mention of discrete audio tokens or audio waveform discretization. It does not address neural audio codecs, vector quantization of audio, or tokens derived from audio signals, nor does it discuss audio generation, understanding, or cross-modal reasoning involving audio tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and instead falls outside the scope defined by the task.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on understanding the impact of various data combinations on training large language models using a curated text dataset called SlimPajama-DC, with no mention of discrete audio tokens or audio waveform discretization. It does not address neural audio codecs, vector quantization of audio, or tokens derived from audio signals, nor does it discuss audio generation, understanding, or cross-modal reasoning involving audio tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and instead falls outside the scope defined by the task.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attentive VQ-VAE",
    "abstract": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
    "metadata": {
      "arxiv_id": "2309.11641",
      "title": "Attentive VQ-VAE",
      "summary": "We present a novel approach to enhance the capabilities of VQ-VAE models through the integration of a Residual Encoder and a Residual Pixel Attention layer, named Attentive Residual Encoder (AREN). The objective of our research is to improve the performance of VQ-VAE while maintaining practical parameter levels. The AREN encoder is designed to operate effectively at multiple levels, accommodating diverse architectural complexities. The key innovation is the integration of an inter-pixel auto-attention mechanism into the AREN encoder. This approach allows us to efficiently capture and utilize contextual information across latent vectors. Additionally, our models uses additional encoding levels to further enhance the model's representational power. Our attention layer employs a minimal parameter approach, ensuring that latent vectors are modified only when pertinent information from other pixels is available. Experimental results demonstrate that our proposed modifications lead to significant improvements in data representation and generation, making VQ-VAEs even more suitable for a wide range of applications as the presented.",
      "authors": [
        "Angello Hoyos",
        "Mariano Rivera"
      ],
      "published": "2023-09-20T21:11:36Z",
      "updated": "2024-02-08T20:52:25Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11641v2",
      "landing_url": "https://arxiv.org/abs/2309.11641v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.11641"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on improving VQ-VAE via an Attentive Residual Encoder, with no explicit mention of discrete audio tokens, tokenization, or audio-specific discrete latent codes, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on improving VQ-VAE via an Attentive Residual Encoder, with no explicit mention of discrete audio tokens, tokenization, or audio-specific discrete latent codes, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses advancements in VQ-VAE models with a focus on attention mechanisms to improve representational power, but it does not explicitly mention applying discretized audio tokens or neural audio codecs for discrete token generation as defined in the inclusion criteria. The abstract lacks detail on discretization processes, token vocabularies, or discrete token use in downstream audio generation or understanding tasks, which are key to inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses advancements in VQ-VAE models with a focus on attention mechanisms to improve representational power, but it does not explicitly mention applying discretized audio tokens or neural audio codecs for discrete token generation as defined in the inclusion criteria. The abstract lacks detail on discretization processes, token vocabularies, or discrete token use in downstream audio generation or understanding tasks, which are key to inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
    "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions quantizing speech into discrete acoustic tokens and modeling these tokens with a language model for TTS, aligning with the discrete-audio-token criteria and downstream generation/understanding tasks.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions quantizing speech into discrete acoustic tokens and modeling these tokens with a language model for TTS, aligning with the discrete-audio-token criteria and downstream generation/understanding tasks.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses zero-shot TTS using discrete acoustic tokens obtained by quantizing speech waveforms, which are then modeled by a neural codec language model (VALL-E). It clearly involves discrete audio tokens generated via neural audio codec and uses these tokens for downstream speech generation, meeting the inclusion criteria. The abstract describes the tokenization and modeling approach sufficiently in English, supporting reproducibility assessment. There is no indication of exclusion criteria such as only continuous representations or lack of discrete token details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study discusses zero-shot TTS using discrete acoustic tokens obtained by quantizing speech waveforms, which are then modeled by a neural codec language model (VALL-E). It clearly involves discrete audio tokens generated via neural audio codec and uses these tokens for downstream speech generation, meeting the inclusion criteria. The abstract describes the tokenization and modeling approach sufficiently in English, supporting reproducibility assessment. There is no indication of exclusion criteria such as only continuous representations or lack of discrete token details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
    "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Uses a neural audio codec to produce a sequence of discrete codes (tokens) for voice conversion and employs text-instruction prompts to modulate prosody, aligning with the discrete-token topic and downstream generative modeling.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Uses a neural audio codec to produce a sequence of discrete codes (tokens) for voice conversion and employs text-instruction prompts to modulate prosody, aligning with the discrete-token topic and downstream generative modeling.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a voice conversion model that processes speech as a sequence of discrete codes generated by a neural codec language model, effectively utilizing discrete audio tokens as core representations for speech transformation, aligning with the inclusion criteria focused on discrete audio tokens and their usage in downstream tasks, and it provides enough technical details in English for evaluation, with no evidence of exclusion criteria being met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a voice conversion model that processes speech as a sequence of discrete codes generated by a neural codec language model, effectively utilizing discrete audio tokens as core representations for speech transformation, aligning with the inclusion criteria focused on discrete audio tokens and their usage in downstream tasks, and it provides enough technical details in English for evaluation, with no evidence of exclusion criteria being met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
    "abstract": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
    "metadata": {
      "arxiv_id": "2309.15505",
      "title": "Finite Scalar Quantization: VQ-VAE Made Simple",
      "summary": "We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.",
      "authors": [
        "Fabian Mentzer",
        "David Minnen",
        "Eirikur Agustsson",
        "Michael Tschannen"
      ],
      "published": "2023-09-27T09:13:40Z",
      "updated": "2023-10-12T07:55:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15505v2",
      "landing_url": "https://arxiv.org/abs/2309.15505v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.15505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper proposes discrete latent representations via finite scalar quantization in VQ-VAE and shows results on vision tasks, it does not address discrete audio tokens or audio-specific tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although the paper proposes discrete latent representations via finite scalar quantization in VQ-VAE and shows results on vision tasks, it does not address discrete audio tokens or audio-specific tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on finite scalar quantization applied in VQ-VAE frameworks, targeting image generation and computer vision tasks rather than audio signal discretization; it does not discuss discrete audio tokens, neural audio codecs, or audio token generation processes, thus falling outside the scope of the defined inclusion criteria related to discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on finite scalar quantization applied in VQ-VAE frameworks, targeting image generation and computer vision tasks rather than audio signal discretization; it does not discuss discrete audio tokens, neural audio codecs, or audio token generation processes, thus falling outside the scope of the defined inclusion criteria related to discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multi-Swap $k$-Means++",
    "abstract": "The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.",
    "metadata": {
      "arxiv_id": "2309.16384",
      "title": "Multi-Swap $k$-Means++",
      "summary": "The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \\log \\log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \\varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler (ICML 2019) on several datasets.",
      "authors": [
        "Lorenzo Beretta",
        "Vincent Cohen-Addad",
        "Silvio Lattanzi",
        "Nikos Parotsidis"
      ],
      "published": "2023-09-28T12:31:35Z",
      "updated": "2024-10-25T18:14:44Z",
      "categories": [
        "cs.CG",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16384v2",
      "landing_url": "https://arxiv.org/abs/2309.16384v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16384"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses multi-swap k-means and local search for k-means clustering, not about discrete audio tokens or tokenization; thus it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses multi-swap k-means and local search for k-means clustering, not about discrete audio tokens or tokenization; thus it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improvements to the k-means++ clustering algorithm, which is a general machine learning method for clustering data points. There is no mention of audio data, discretization of audio waveforms, discrete audio tokens, neural audio codecs, or any audio-related tasks such as TTS, speech or music generation, cross-modal modeling, or representation learning. Thus, the paper does not meet any of the inclusion criteria related to discrete audio token generation or usage, nor does it involve audio as its research area. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improvements to the k-means++ clustering algorithm, which is a general machine learning method for clustering data points. There is no mention of audio data, discretization of audio waveforms, discrete audio tokens, neural audio codecs, or any audio-related tasks such as TTS, speech or music generation, cross-modal modeling, or representation learning. Thus, the paper does not meet any of the inclusion criteria related to discrete audio token generation or usage, nor does it involve audio as its research area. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization",
    "abstract": "We propose a modular pipeline for the single-channel separation, recognition, and diarization of meeting-style recordings and evaluate it on the Libri-CSS dataset. Using a Continuous Speech Separation (CSS) system with a TF-GridNet separation architecture, followed by a speaker-agnostic speech recognizer, we achieve state-of-the-art recognition performance in terms of Optimal Reference Combination Word Error Rate (ORC WER). Then, a d-vector-based diarization module is employed to extract speaker embeddings from the enhanced signals and to assign the CSS outputs to the correct speaker. Here, we propose a syntactically informed diarization using sentence- and word-level boundaries of the ASR module to support speaker turn detection. This results in a state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) for the full meeting recognition pipeline.",
    "metadata": {
      "arxiv_id": "2309.16482",
      "title": "Meeting Recognition with Continuous Speech Separation and Transcription-Supported Diarization",
      "summary": "We propose a modular pipeline for the single-channel separation, recognition, and diarization of meeting-style recordings and evaluate it on the Libri-CSS dataset. Using a Continuous Speech Separation (CSS) system with a TF-GridNet separation architecture, followed by a speaker-agnostic speech recognizer, we achieve state-of-the-art recognition performance in terms of Optimal Reference Combination Word Error Rate (ORC WER). Then, a d-vector-based diarization module is employed to extract speaker embeddings from the enhanced signals and to assign the CSS outputs to the correct speaker. Here, we propose a syntactically informed diarization using sentence- and word-level boundaries of the ASR module to support speaker turn detection. This results in a state-of-the-art Concatenated minimum-Permutation Word Error Rate (cpWER) for the full meeting recognition pipeline.",
      "authors": [
        "Thilo von Neumann",
        "Christoph Boeddeker",
        "Tobias Cord-Landwehr",
        "Marc Delcroix",
        "Reinhold Haeb-Umbach"
      ],
      "published": "2023-09-28T14:45:46Z",
      "updated": "2024-05-06T07:09:31Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.16482v2",
      "landing_url": "https://arxiv.org/abs/2309.16482v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.16482"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe CSS-based meeting recognition and diarization without discussing discrete audio tokens, tokenizers, or neural-codec tokenization; hence it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe CSS-based meeting recognition and diarization without discussing discrete audio tokens, tokenizers, or neural-codec tokenization; hence it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on continuous speech separation, diarization, and recognition without discussing or employing discrete audio tokens derived via discretization methods such as vector quantization or neural audio encoding. There is no mention of token vocabulary design, tokenization processes, or the use of discrete token sequences as core modeling units, which are essential for inclusion based on the criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token representations for audio modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on continuous speech separation, diarization, and recognition without discussing or employing discrete audio tokens derived via discretization methods such as vector quantization or neural audio encoding. There is no mention of token vocabulary design, tokenization processes, or the use of discrete token sequences as core modeling units, which are essential for inclusion based on the criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token representations for audio modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
    "abstract": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
    "metadata": {
      "arxiv_id": "2310.01381",
      "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
      "summary": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
      "authors": [
        "Roi Benita",
        "Michael Elad",
        "Joseph Keshet"
      ],
      "published": "2023-10-02T17:42:22Z",
      "updated": "2024-03-10T22:31:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01381v3",
      "landing_url": "https://arxiv.org/abs/2310.01381v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.01381"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on end-to-end diffusion for raw speech waveform generation without proposing or evaluating discrete audio tokens, tokenizers, or codebooks, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on end-to-end diffusion for raw speech waveform generation without proposing or evaluating discrete audio tokens, tokenizers, or codebooks, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a diffusion probabilistic model generating raw speech waveforms directly without evidence of discretizing continuous audio into discrete token sequences as required by the inclusion criteria. There is no description of neural audio codec architectures, vector quantization, vocabulary/codebook settings, or tokenization process that turns audio into discrete token sequences. Hence, it does not meet the core inclusion topic of discrete audio tokens, but rather focuses on continuous waveform modeling. It also lacks explicit mention of discrete tokens or downstream use of such tokens, failing to fulfill the required criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a diffusion probabilistic model generating raw speech waveforms directly without evidence of discretizing continuous audio into discrete token sequences as required by the inclusion criteria. There is no description of neural audio codec architectures, vector quantization, vocabulary/codebook settings, or tokenization process that turns audio into discrete token sequences. Hence, it does not meet the core inclusion topic of discrete audio tokens, but rather focuses on continuous waveform modeling. It also lacks explicit mention of discrete tokens or downstream use of such tokens, failing to fulfill the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages",
    "abstract": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
    "metadata": {
      "arxiv_id": "2310.03639",
      "title": "Evaluating Self-Supervised Speech Representations for Indigenous American Languages",
      "summary": "The application of self-supervision to speech representation learning has garnered significant interest in recent years, due to its scalability to large amounts of unlabeled data. However, much progress, both in terms of pre-training and downstream evaluation, has remained concentrated in monolingual models that only consider English. Few models consider other languages, and even fewer consider indigenous ones. In our submission to the New Language Track of the ASRU 2023 ML-SUPERB Challenge, we present an ASR corpus for Quechua, an indigenous South American Language. We benchmark the efficacy of large SSL models on Quechua, along with 6 other indigenous languages such as Guarani and Bribri, on low-resource ASR. Our results show surprisingly strong performance by state-of-the-art SSL models, showing the potential generalizability of large-scale models to real-world data.",
      "authors": [
        "Chih-Chen Chen",
        "William Chen",
        "Rodolfo Zevallos",
        "John E. Ortega"
      ],
      "published": "2023-10-05T16:11:14Z",
      "updated": "2023-10-08T23:28:50Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03639v2",
      "landing_url": "https://arxiv.org/abs/2310.03639v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.03639"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on evaluating self-supervised speech representations for indigenous languages without describing discrete audio tokens, tokenizers, or token-based downstream modeling, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on evaluating self-supervised speech representations for indigenous languages without describing discrete audio tokens, tokenizers, or token-based downstream modeling, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating self-supervised speech representations for indigenous languages, but it does not specifically describe the generation of discrete audio tokens through discretization processes such as tokenizers, codecs, or quantization, nor does it detail vocabulary or codebook settings. While it uses self-supervised learning, the core method and evaluation seem centered on continuous speech representations used for ASR rather than discrete token sequences as defined by the inclusion criteria. Therefore, it does not meet the core requirements about discrete audio tokens and token design or evaluation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating self-supervised speech representations for indigenous languages, but it does not specifically describe the generation of discrete audio tokens through discretization processes such as tokenizers, codecs, or quantization, nor does it detail vocabulary or codebook settings. While it uses self-supervised learning, the core method and evaluation seem centered on continuous speech representations used for ASR rather than discrete token sequences as defined by the inclusion criteria. Therefore, it does not meet the core requirements about discrete audio tokens and token design or evaluation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
    "abstract": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
    "metadata": {
      "arxiv_id": "2310.04358",
      "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
      "summary": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
      "authors": [
        "Ziyun Cui",
        "Wen Wu",
        "Wei-Qiang Zhang",
        "Ji Wu",
        "Chao Zhang"
      ],
      "published": "2023-10-06T16:28:07Z",
      "updated": "2023-10-06T16:28:07Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04358v1",
      "landing_url": "https://arxiv.org/abs/2310.04358v1",
      "doi": "https://doi.org/10.1109/ASRU57964.2023.10389785"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The manuscript focuses on transferring speech/generic and depression-specific knowledge for Alzheimer's disease detection and does not discuss discrete audio tokens, tokenization schemes, or token-based modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The manuscript focuses on transferring speech/generic and depression-specific knowledge for Alzheimer's disease detection and does not discuss discrete audio tokens, tokenization schemes, or token-based modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Alzheimer's disease detection using knowledge transfer from speech-generic and depression-specific models, but it does not mention the use or design of discrete audio tokens, such as neural audio codecs or quantized token sequences. The methods described are not centered around discretizing audio into token sequences or analyzing token design choices for audio generation or cross-modal tasks, which are core to the inclusion criteria. Therefore, it fails to meet the necessary inclusion criteria related to discrete audio token representation and usage and aligns with the exclusion criteria for continuous audio representation without discrete token analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Alzheimer's disease detection using knowledge transfer from speech-generic and depression-specific models, but it does not mention the use or design of discrete audio tokens, such as neural audio codecs or quantized token sequences. The methods described are not centered around discretizing audio into token sequences or analyzing token design choices for audio generation or cross-modal tasks, which are core to the inclusion criteria. Therefore, it fails to meet the necessary inclusion criteria related to discrete audio token representation and usage and aligns with the exclusion criteria for continuous audio representation without discrete token analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023",
    "abstract": "This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.",
    "metadata": {
      "arxiv_id": "2310.05203",
      "title": "A Comparative Study of Voice Conversion Models with Large-Scale Speech and Singing Data: The T13 Systems for the Singing Voice Conversion Challenge 2023",
      "summary": "This paper presents our systems (denoted as T13) for the singing voice conversion challenge (SVCC) 2023. For both in-domain and cross-domain English singing voice conversion (SVC) tasks (Task 1 and Task 2), we adopt a recognition-synthesis approach with self-supervised learning-based representation. To achieve data-efficient SVC with a limited amount of target singer/speaker's data (150 to 160 utterances for SVCC 2023), we first train a diffusion-based any-to-any voice conversion model using publicly available large-scale 750 hours of speech and singing data. Then, we finetune the model for each target singer/speaker of Task 1 and Task 2. Large-scale listening tests conducted by SVCC 2023 show that our T13 system achieves competitive naturalness and speaker similarity for the harder cross-domain SVC (Task 2), which implies the generalization ability of our proposed method. Our objective evaluation results show that using large datasets is particularly beneficial for cross-domain SVC.",
      "authors": [
        "Ryuichi Yamamoto",
        "Reo Yoneyama",
        "Lester Phillip Violeta",
        "Wen-Chin Huang",
        "Tomoki Toda"
      ],
      "published": "2023-10-08T15:30:44Z",
      "updated": "2023-10-08T15:30:44Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05203v1",
      "landing_url": "https://arxiv.org/abs/2310.05203v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.05203"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on diffusion-based voice conversion with self-supervised representations and large-scale data; it does not discuss discrete audio tokens, tokenizers, or token-based discretization, thus failing the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on diffusion-based voice conversion with self-supervised representations and large-scale data; it does not discuss discrete audio tokens, tokenizers, or token-based discretization, thus failing the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on singing voice conversion using a recognition-synthesis approach and self-supervised representation, but it does not explicitly describe the use or generation of discrete audio tokens via neural audio codecs or vector quantization, nor does it provide details on tokenization or discrete vocabularies that are core to the inclusion criteria; the emphasis is on voice conversion modeling rather than on discrete token generation or their design implications, thus it does not meet the criteria for including discrete audio token-focused research.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on singing voice conversion using a recognition-synthesis approach and self-supervised representation, but it does not explicitly describe the use or generation of discrete audio tokens via neural audio codecs or vector quantization, nor does it provide details on tokenization or discrete vocabularies that are core to the inclusion criteria; the emphasis is on voice conversion modeling rather than on discrete token generation or their design implications, thus it does not meet the criteria for including discrete audio token-focused research.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
    "abstract": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
    "metadata": {
      "arxiv_id": "2310.07161",
      "title": "Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms",
      "summary": "Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were used to explain of perceptual quality and intelligibility. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry.",
      "authors": [
        "Joseph Konan",
        "Shikhar Agnihotri",
        "Ojas Bhargave",
        "Shuo Han",
        "Yunyang Zeng",
        "Ankit Shah",
        "Bhiksha Raj"
      ],
      "published": "2023-10-11T03:19:22Z",
      "updated": "2024-08-01T11:37:16Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07161v3",
      "landing_url": "https://arxiv.org/abs/2310.07161v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.07161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on psychoacoustic effects and denoising on VoIP platforms and uses metrics like PESQ/STOI, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on psychoacoustic effects and denoising on VoIP platforms and uses metrics like PESQ/STOI, with no discussion of discrete audio tokens, tokenizers, neural codecs, or token-based downstream tasks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on psychoacoustic challenges and speech enhancement in VoIP platforms, using deep noise suppression datasets and various acoustic metrics, without any mention of discretization of audio waveforms into discrete tokens or neural audio codecs generating token sequences; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their generation or application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on psychoacoustic challenges and speech enhancement in VoIP platforms, using deep noise suppression datasets and various acoustic metrics, without any mention of discretization of audio waveforms into discrete tokens or neural audio codecs generating token sequences; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their generation or application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
    "abstract": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
    "metadata": {
      "arxiv_id": "2310.07246",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "authors": [
        "Xinfa Zhu",
        "Yuanjun Lv",
        "Yi Lei",
        "Tao Li",
        "Wendi He",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-10-11T07:23:27Z",
      "updated": "2023-10-12T05:49:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07246v2",
      "landing_url": "https://arxiv.org/abs/2310.07246v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.07246"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents a neural speech codec that produces discrete tokens (speech vectors and semantic tokens) and uses BPE to create token sequences for LM-based generation across multiple tasks, addressing token design, fidelity, and downstream applicability, with English title/abstract and explicit tokenization/codec details.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract presents a neural speech codec that produces discrete tokens (speech vectors and semantic tokens) and uses BPE to create token sequences for LM-based generation across multiple tasks, addressing token design, fidelity, and downstream applicability, with English title/abstract and explicit tokenization/codec details.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a novel neural speech codec that uses speech vectors and semantic tokens for speech generation, detailing the use of discrete tokens derived from neural audio encoding for various downstream tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens and their application in speech generation and understanding. It provides an English abstract, discusses tokenizer/codecs, and demonstrates downstream applications, making it a suitable inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a novel neural speech codec that uses speech vectors and semantic tokens for speech generation, detailing the use of discrete tokens derived from neural audio encoding for various downstream tasks, which aligns well with the inclusion criteria focusing on discrete audio tokens and their application in speech generation and understanding. It provides an English abstract, discusses tokenizer/codecs, and demonstrates downstream applications, making it a suitable inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
    "abstract": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
    "metadata": {
      "arxiv_id": "2310.08104",
      "title": "Voice Conversion for Stuttered Speech, Instruments, Unseen Languages and Textually Described Voices",
      "summary": "Voice conversion aims to convert source speech into a target voice using recordings of the target speaker as a reference. Newer models are producing increasingly realistic output. But what happens when models are fed with non-standard data, such as speech from a user with a speech impairment? We investigate how a recent voice conversion model performs on non-standard downstream voice conversion tasks. We use a simple but robust approach called k-nearest neighbors voice conversion (kNN-VC). We look at four non-standard applications: stuttered voice conversion, cross-lingual voice conversion, musical instrument conversion, and text-to-voice conversion. The latter involves converting to a target voice specified through a text description, e.g. \"a young man with a high-pitched voice\". Compared to an established baseline, we find that kNN-VC retains high performance in stuttered and cross-lingual voice conversion. Results are more mixed for the musical instrument and text-to-voice conversion tasks. E.g., kNN-VC works well on some instruments like drums but not on others. Nevertheless, this shows that voice conversion models - and kNN-VC in particular - are increasingly applicable in a range of non-standard downstream tasks. But there are still limitations when samples are very far from the training distribution. Code, samples, trained models: https://rf5.github.io/sacair2023-knnvc-demo/.",
      "authors": [
        "Matthew Baas",
        "Herman Kamper"
      ],
      "published": "2023-10-12T08:00:25Z",
      "updated": "2023-10-12T08:00:25Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08104v1",
      "landing_url": "https://arxiv.org/abs/2310.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08104"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on general voice conversion across non-standard data but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on general voice conversion across non-standard data but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper investigates voice conversion tasks on non-standard datasets but does not mention the use or generation of discrete audio tokens or tokenization mechanisms as defined in the inclusion criteria. It focuses on voice conversion models and their performance rather than discrete token representations or related analyses, which does not align with the core topic of \"Discrete Audio Tokens.\" Thus, it does not meet the required criteria for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper investigates voice conversion tasks on non-standard datasets but does not mention the use or generation of discrete audio tokens or tokenization mechanisms as defined in the inclusion criteria. It focuses on voice conversion models and their performance rather than discrete token representations or related analyses, which does not align with the core topic of \"Discrete Audio Tokens.\" Thus, it does not meet the required criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
    "abstract": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
    "metadata": {
      "arxiv_id": "2310.08225",
      "title": "Fast Word Error Rate Estimation Using Self-Supervised Representations for Speech and Text",
      "summary": "Word error rate (WER) estimation aims to evaluate the quality of an automatic speech recognition (ASR) system's output without requiring ground-truth labels. This task has gained increasing attention as advanced ASR systems are trained on large amounts of data. In this context, the computational efficiency of a WER estimator becomes essential in practice. However, previous works have not prioritised this aspect. In this paper, a Fast estimator for WER (Fe-WER) is introduced, utilizing average pooling over self-supervised learning representations for speech and text. Our results demonstrate that Fe-WER outperformed a baseline relatively by 14.10% in root mean square error and 1.22% in Pearson correlation coefficient on Ted-Lium3. Moreover, a comparative analysis of the distributions of target WER and WER estimates was conducted, including an examination of the average values per speaker. Lastly, the inference speed was approximately 3.4 times faster in the real-time factor.",
      "authors": [
        "Chanho Park",
        "Chengsong Lu",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2023-10-12T11:17:40Z",
      "updated": "2025-01-29T11:28:34Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08225v2",
      "landing_url": "https://arxiv.org/abs/2310.08225v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.08225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets fast WER estimation using self-supervised representations and does not discuss discrete audio tokens, tokenization, or vocabulary/codec-based token generation, thus it does not meet the inclusion criteria and fits the exclusion criteria for continuous representations without discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets fast WER estimation using self-supervised representations and does not discuss discrete audio tokens, tokenization, or vocabulary/codec-based token generation, thus it does not meet the inclusion criteria and fits the exclusion criteria for continuous representations without discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on estimating word error rate using self-supervised representations for speech and text but does not describe or emphasize discrete audio tokens generated through discretization or neural audio codecs; thus, it does not meet the inclusion criteria related to discrete token use or design. The paper lacks details on discrete token generation, vocabulary settings, or quantization methods necessary for the survey's focus, and it primarily deals with continuous speech representations for error estimation rather than token-based modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on estimating word error rate using self-supervised representations for speech and text but does not describe or emphasize discrete audio tokens generated through discretization or neural audio codecs; thus, it does not meet the inclusion criteria related to discrete token use or design. The paper lacks details on discrete token generation, vocabulary settings, or quantization methods necessary for the survey's focus, and it primarily deals with continuous speech representations for error estimation rather than token-based modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
    "abstract": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
    "metadata": {
      "arxiv_id": "2310.08696",
      "title": "End-to-end Online Speaker Diarization with Target Speaker Tracking",
      "summary": "This paper proposes an online target speaker voice activity detection system for speaker diarization tasks, which does not require a priori knowledge from the clustering-based diarization system to obtain the target speaker embeddings. By adapting the conventional target speaker voice activity detection for real-time operation, this framework can identify speaker activities using self-generated embeddings, resulting in consistent performance without permutation inconsistencies in the inference phase. During the inference process, we employ a front-end model to extract the frame-level speaker embeddings for each coming block of a signal. Next, we predict the detection state of each speaker based on these frame-level speaker embeddings and the previously estimated target speaker embedding. Then, the target speaker embeddings are updated by aggregating these frame-level speaker embeddings according to the predictions in the current block. Our model predicts the results for each block and updates the target speakers' embeddings until reaching the end of the signal. Experimental results show that the proposed method outperforms the offline clustering-based diarization system on the DIHARD III and AliMeeting datasets. The proposed method is further extended to multi-channel data, which achieves similar performance with the state-of-the-art offline diarization systems.",
      "authors": [
        "Weiqing Wang",
        "Ming Li"
      ],
      "published": "2023-10-12T20:02:07Z",
      "updated": "2023-10-12T20:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08696v1",
      "landing_url": "https://arxiv.org/abs/2310.08696v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.08696"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article targets online speaker diarization with target speaker tracking using embeddings and does not mention discrete audio tokens, tokenizer/codec design, or token-based modeling essential to the discrete audio token criteria; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article targets online speaker diarization with target speaker tracking using embeddings and does not mention discrete audio tokens, tokenizer/codec design, or token-based modeling essential to the discrete audio token criteria; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an online speaker diarization system using target speaker voice activity detection, without mentioning the use or creation of discrete audio tokens, vector quantization, or any form of discrete tokenization of continuous audio signals, which are key inclusion criteria; instead, it deals with speaker embeddings and diarization tasks, which rely on continuous representations rather than discrete tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens nor does it discuss relevant token generation or codec architectures.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an online speaker diarization system using target speaker voice activity detection, without mentioning the use or creation of discrete audio tokens, vector quantization, or any form of discrete tokenization of continuous audio signals, which are key inclusion criteria; instead, it deals with speaker embeddings and diarization tasks, which rely on continuous representations rather than discrete tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens nor does it discuss relevant token generation or codec architectures.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low-latency Speech Enhancement via Speech Token Generation",
    "abstract": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
    "metadata": {
      "arxiv_id": "2310.08981",
      "title": "Low-latency Speech Enhancement via Speech Token Generation",
      "summary": "Existing deep learning based speech enhancement mainly employ a data-driven approach, which leverage large amounts of data with a variety of noise types to achieve noise removal from noisy signal. However, the high dependence on the data limits its generalization on the unseen complex noises in real-life environment. In this paper, we focus on the low-latency scenario and regard speech enhancement as a speech generation problem conditioned on the noisy signal, where we generate clean speech instead of identifying and removing noises. Specifically, we propose a conditional generative framework for speech enhancement, which models clean speech by acoustic codes of a neural speech codec and generates the speech codes conditioned on past noisy frames in an auto-regressive way. Moreover, we propose an explicit-alignment approach to align noisy frames with the generated speech tokens to improve the robustness and scalability to different input lengths. Different from other methods that leverage multiple stages to generate speech codes, we leverage a single-stage speech generation approach based on the TF-Codec neural codec to achieve high speech quality with low latency. Extensive results on both synthetic and real-recorded test set show its superiority over data-driven approaches in terms of noise robustness and temporal speech coherence.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Yan Lu"
      ],
      "published": "2023-10-13T09:57:09Z",
      "updated": "2024-01-23T06:13:04Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08981v3",
      "landing_url": "https://arxiv.org/abs/2310.08981v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.08981"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract mentions 'speech tokens' and a neural codec, it does not clearly specify discrete token generation/quantization with a token vocabulary or codebook, so it's unclear whether it fulfills the discrete audio token criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "Although the abstract mentions 'speech tokens' and a neural codec, it does not clearly specify discrete token generation/quantization with a token vocabulary or codebook, so it's unclear whether it fulfills the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a speech enhancement method that uses acoustic codes from a neural speech codec as generated speech tokens, conditioned on noisy input. It clearly involves discrete token sequences derived from neural codec quantization (acoustic codes), aligning with the inclusion criteria of research using neural audio codecs to produce discrete tokens for generation tasks. The abstract mentions single-stage speech generation based on TF-Codec neural codec, indicating design details related to token generation. Therefore, it meets the requirements of using discrete audio tokens and discusses their generation and usage in a speech generation task, justifying inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper describes a speech enhancement method that uses acoustic codes from a neural speech codec as generated speech tokens, conditioned on noisy input. It clearly involves discrete token sequences derived from neural codec quantization (acoustic codes), aligning with the inclusion criteria of research using neural audio codecs to produce discrete tokens for generation tasks. The abstract mentions single-stage speech generation based on TF-Codec neural codec, indicating design details related to token generation. Therefore, it meets the requirements of using discrete audio tokens and discusses their generation and usage in a speech generation task, justifying inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano flagged uncertainty due to lack of explicit mention of quantization/vocabulary/codebook in the abstract, while JuniorMini judged it clearly uses neural-codec-derived acoustic codes as discrete tokens (TF-Codec) for autoregressive speech-code generation and recommended inclusion. My judgment: the abstract explicitly states the method models clean speech by \"acoustic codes of a neural speech codec\" and generates those speech codes autoregressively (single-stage TF-Codec), which indicates the use of discrete codec-derived tokens as the core representation for generation—matching the token-source and downstream-generation inclusion criteria; although the abstract does not list full tokenizer/codebook hyperparameters, it sufficiently demonstrates that discrete audio tokens from a neural codec are central to the method and the paper targets generation tasks rather than purely continuous representation or discriminative tasks. Therefore, it should be included. (Score rationale: the study uses neural-audio-codec acoustic codes as discrete tokens for generation, satisfying inclusion.)",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano flagged uncertainty due to lack of explicit mention of quantization/vocabulary/codebook in the abstract, while JuniorMini judged it clearly uses neural-codec-derived acoustic codes as discrete tokens (TF-Codec) for autoregressive speech-code generation and recommended inclusion. My judgment: the abstract explicitly states the method models clean speech by \"acoustic codes of a neural speech codec\" and generates those speech codes autoregressively (single-stage TF-Codec), which indicates the use of discrete codec-derived tokens as the core representation for generation—matching the token-source and downstream-generation inclusion criteria; although the abstract does not list full tokenizer/codebook hyperparameters, it sufficiently demonstrates that discrete audio tokens from a neural codec are central to the method and the paper targets generation tasks rather than purely continuous representation or discriminative tasks. Therefore, it should be included. (Score rationale: the study uses neural-audio-codec acoustic codes as discrete tokens for generation, satisfying inclusion.)",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "abstract": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on learnable lattice vector quantization for discrete embeddings, primarily tested on image datasets (FFHQ-1024, FashionMNIST, Celeb-A) and does not address discrete audio tokens, tokenization methods for audio, or audio-domain evaluation, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on learnable lattice vector quantization for discrete embeddings, primarily tested on image datasets (FFHQ-1024, FashionMNIST, Celeb-A) and does not address discrete audio tokens, tokenization methods for audio, or audio-domain evaluation, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a vector quantization method (LL-VQ-VAE) for learning discrete representations but does not mention any application to audio signals, nor does it describe tokenization or discretization of audio waveforms into token sequences for modeling tasks such as speech or music generation or multimodal alignment. The examples are datasets like FFHQ, FashionMNIST, and Celeb-A, which are image datasets, indicating the method is applied in image domain rather than audio. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens for audio-related tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a vector quantization method (LL-VQ-VAE) for learning discrete representations but does not mention any application to audio signals, nor does it describe tokenization or discretization of audio waveforms into token sequences for modeling tasks such as speech or music generation or multimodal alignment. The examples are datasets like FFHQ, FashionMNIST, and Celeb-A, which are image datasets, indicating the method is applied in image domain rather than audio. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens for audio-related tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
    "abstract": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
    "metadata": {
      "arxiv_id": "2310.09653",
      "title": "SelfVC: Voice Conversion With Iterative Refinement using Self Transformations",
      "summary": "We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Rafael Valle",
        "Boris Ginsburg",
        "Rishabh Ranjan",
        "Shlomo Dubnov",
        "Farinaz Koushanfar",
        "Julian McAuley"
      ],
      "published": "2023-10-14T19:51:17Z",
      "updated": "2024-05-03T16:45:39Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09653v2",
      "landing_url": "https://arxiv.org/abs/2310.09653v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.09653"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on continuous voice conversion using SSL-derived representations and self-synthesized data, with no discretization into tokens or tokenizers, thus not meeting the 'Discrete Audio Tokens' inclusion criteria and aligning with the exclusion criteria for non-token-based continuous representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on continuous voice conversion using SSL-derived representations and self-synthesized data, with no discretization into tokens or tokenizers, thus not meeting the 'Discrete Audio Tokens' inclusion criteria and aligning with the exclusion criteria for non-token-based continuous representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a voice conversion model trained on entangled speech representations derived from self-supervised learning and speaker verification models, improving speaker similarity without explicitly disentangling speech attributes. However, the description lacks any mention of discrete tokenization or discretization of continuous audio waveforms into finite token sequences or codebooks, which is central to the inclusion criteria regarding \"Discrete Audio Tokens.\" The work focuses primarily on voice conversion using self-transformations rather than using or analyzing discrete audio tokens for modeling or generation. Therefore, it does not meet the core inclusion requirements and falls into the category of continuous audio representation modeling without clearly defined discrete tokens or codebooks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a voice conversion model trained on entangled speech representations derived from self-supervised learning and speaker verification models, improving speaker similarity without explicitly disentangling speech attributes. However, the description lacks any mention of discrete tokenization or discretization of continuous audio waveforms into finite token sequences or codebooks, which is central to the inclusion criteria regarding \"Discrete Audio Tokens.\" The work focuses primarily on voice conversion using self-transformations rather than using or analyzing discrete audio tokens for modeling or generation. Therefore, it does not meet the core inclusion requirements and falls into the category of continuous audio representation modeling without clearly defined discrete tokens or codebooks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
    "abstract": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
    "metadata": {
      "arxiv_id": "2310.10922",
      "title": "Spatial HuBERT: Self-supervised Spatial Speech Representation Learning for a Single Talker from Multi-channel Audio",
      "summary": "Self-supervised learning has been used to leverage unlabelled data, improving accuracy and generalisation of speech systems through the training of representation models. While many recent works have sought to produce effective representations across a variety of acoustic domains, languages, modalities and even simultaneous speakers, these studies have all been limited to single-channel audio recordings. This paper presents Spatial HuBERT, a self-supervised speech representation model that learns both acoustic and spatial information pertaining to a single speaker in a potentially noisy environment by using multi-channel audio inputs. Spatial HuBERT learns representations that outperform state-of-the-art single-channel speech representations on a variety of spatial downstream tasks, particularly in reverberant and noisy environments. We also demonstrate the utility of the representations learned by Spatial HuBERT on a speech localisation downstream task. Along with this paper, we publicly release a new dataset of 100 000 simulated first-order ambisonics room impulse responses.",
      "authors": [
        "Antoni Dimitriadis",
        "Siqi Pan",
        "Vidhyasaharan Sethu",
        "Beena Ahmed"
      ],
      "published": "2023-10-17T01:31:59Z",
      "updated": "2023-10-17T01:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10922v1",
      "landing_url": "https://arxiv.org/abs/2310.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10922"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper focuses on self-supervised spatial speech representation learning with multi-channel inputs and does not mention discrete audio tokens, tokenization, codebooks, or token-based downstream tasks, hence it does not satisfy the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on self-supervised spatial speech representation learning with multi-channel inputs and does not mention discrete audio tokens, tokenization, codebooks, or token-based downstream tasks, hence it does not satisfy the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised learning for speech representation using multi-channel audio inputs to capture spatial information, but from the abstract, it does not mention the use of discrete audio tokens or tokenization processes such as vector quantization or clustering to convert continuous audio waveforms into discrete token sequences as defined by the inclusion criteria. The abstract emphasizes representation learning, particularly for spatial speech signals, but lacks details on discrete token generation, vocabularies, or codec architectures necessary to confirm it meets the discrete audio token criteria. Therefore, it does not satisfy the key inclusion requirements related to discrete audio tokens, and should likely be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised learning for speech representation using multi-channel audio inputs to capture spatial information, but from the abstract, it does not mention the use of discrete audio tokens or tokenization processes such as vector quantization or clustering to convert continuous audio waveforms into discrete token sequences as defined by the inclusion criteria. The abstract emphasizes representation learning, particularly for spatial speech signals, but lacks details on discrete token generation, vocabularies, or codec architectures necessary to confirm it meets the discrete audio token criteria. Therefore, it does not satisfy the key inclusion requirements related to discrete audio tokens, and should likely be excluded.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
    "abstract": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
    "metadata": {
      "arxiv_id": "2310.11541",
      "title": "MUST&P-SRL: Multi-lingual and Unified Syllabification in Text and Phonetic Domains for Speech Representation Learning",
      "summary": "In this paper, we present a methodology for linguistic feature extraction, focusing particularly on automatically syllabifying words in multiple languages, with a design to be compatible with a forced-alignment tool, the Montreal Forced Aligner (MFA). In both the textual and phonetic domains, our method focuses on the extraction of phonetic transcriptions from text, stress marks, and a unified automatic syllabification (in text and phonetic domains). The system was built with open-source components and resources. Through an ablation study, we demonstrate the efficacy of our approach in automatically syllabifying words from several languages (English, French and Spanish). Additionally, we apply the technique to the transcriptions of the CMU ARCTIC dataset, generating valuable annotations available online\\footnote{\\url{https://github.com/noetits/MUST_P-SRL}} that are ideal for speech representation learning, speech unit discovery, and disentanglement of speech factors in several speech-related fields.",
      "authors": [
        "Noé Tits"
      ],
      "published": "2023-10-17T19:27:23Z",
      "updated": "2023-10-17T19:27:23Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.11541v1",
      "landing_url": "https://arxiv.org/abs/2310.11541v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.11541"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on automatic syllabification and phonetic transcription without defining or evaluating discrete audio tokens or a token vocabulary, so it does not meet the discrete-token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on automatic syllabification and phonetic transcription without defining or evaluating discrete audio tokens or a token vocabulary, so it does not meet the discrete-token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multi-lingual syllabification in text and phonetic domains to support speech representation learning, but it does not describe the generation or use of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it detail tokenization, quantization, or codebook settings related to discretization of continuous audio signals into tokens for subsequent modeling or multimodal tasks. Hence, it does not meet the core inclusion criteria about discrete audio token generation and usage, and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multi-lingual syllabification in text and phonetic domains to support speech representation learning, but it does not describe the generation or use of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it detail tokenization, quantization, or codebook settings related to discretization of continuous audio signals into tokens for subsequent modeling or multimodal tasks. Hence, it does not meet the core inclusion criteria about discrete audio token generation and usage, and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models",
    "abstract": "Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion.",
    "metadata": {
      "arxiv_id": "2310.14044",
      "title": "Composer Style-specific Symbolic Music Generation Using Vector Quantized Discrete Diffusion Models",
      "summary": "Emerging Denoising Diffusion Probabilistic Models (DDPM) have become increasingly utilised because of promising results they have achieved in diverse generative tasks with continuous data, such as image and sound synthesis. Nonetheless, the success of diffusion models has not been fully extended to discrete symbolic music. We propose to combine a vector quantized variational autoencoder (VQ-VAE) and discrete diffusion models for the generation of symbolic music with desired composer styles. The trained VQ-VAE can represent symbolic music as a sequence of indexes that correspond to specific entries in a learned codebook. Subsequently, a discrete diffusion model is used to model the VQ-VAE's discrete latent space. The diffusion model is trained to generate intermediate music sequences consisting of codebook indexes, which are then decoded to symbolic music using the VQ-VAE's decoder. The evaluation results demonstrate our model can generate symbolic music with target composer styles that meet the given conditions with a high accuracy of 72.36%. Our code is available at https://github.com/jinchengzhanggg/VQVAE-Diffusion.",
      "authors": [
        "Jincheng Zhang",
        "György Fazekas",
        "Charalampos Saitis"
      ],
      "published": "2023-10-21T15:41:50Z",
      "updated": "2024-09-03T19:14:25Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14044v2",
      "landing_url": "https://arxiv.org/abs/2310.14044v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14044"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item uses a VQ-VAE to produce discrete token sequences (codebook indices) and trains a discrete diffusion model to generate symbolic music in that discrete latent space, which directly aligns with the 'Discrete Audio Tokens' theme and criteria; English title/abstract are provided, and it clearly targets discrete token-based generation of music.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item uses a VQ-VAE to produce discrete token sequences (codebook indices) and trains a discrete diffusion model to generate symbolic music in that discrete latent space, which directly aligns with the 'Discrete Audio Tokens' theme and criteria; English title/abstract are provided, and it clearly targets discrete token-based generation of music.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study utilizes a vector quantized variational autoencoder (VQ-VAE) to convert symbolic music into discrete token sequences (indexes in a learned codebook), which aligns with the 'Discrete Audio Tokens' topic requiring discrete token sequences derived via neural audio codecs such as vector quantization. It also applies a discrete diffusion model on these tokens for generation, with clear mention of the tokenizer structure, codebook, and latent space modeling. The study focuses on music generation, a downstream task involving discrete token-based audio generative models, and provides evaluation results and code for reproducibility, fulfilling inclusion criteria without evident exclusion points.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study utilizes a vector quantized variational autoencoder (VQ-VAE) to convert symbolic music into discrete token sequences (indexes in a learned codebook), which aligns with the 'Discrete Audio Tokens' topic requiring discrete token sequences derived via neural audio codecs such as vector quantization. It also applies a discrete diffusion model on these tokens for generation, with clear mention of the tokenizer structure, codebook, and latent space modeling. The study focuses on music generation, a downstream task involving discrete token-based audio generative models, and provides evaluation results and code for reproducibility, fulfilling inclusion criteria without evident exclusion points.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
    "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses discrete audio tokens produced by neural/self-supervised methods, uses acoustic BPE to compress token sequences, and demonstrates downstream speech generation/TTS tasks, aligning with the discrete token design and downstream evaluation criteria without falling into the exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item discusses discrete audio tokens produced by neural/self-supervised methods, uses acoustic BPE to compress token sequences, and demonstrates downstream speech generation/TTS tasks, aligning with the discrete token design and downstream evaluation criteria without falling into the exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an approach for discrete audio token pattern encoding derived from self-supervised learning models, explicitly focusing on discrete audio tokens for speech generation and providing details on token sequence length reduction and modeling improvements; the abstract specifies that discrete tokens are derived from self-supervised learning models and used as the core representation for speech generation tasks, aligning well with the inclusion criteria specifying use of discrete tokens derived from neural or self-supervised models and applied in speech generation, with sufficient methodological description to judge token design and application. Hence, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes an approach for discrete audio token pattern encoding derived from self-supervised learning models, explicitly focusing on discrete audio tokens for speech generation and providing details on token sequence length reduction and modeling improvements; the abstract specifies that discrete tokens are derived from self-supervised learning models and used as the core representation for speech generation tasks, aligning well with the inclusion criteria specifying use of discrete tokens derived from neural or self-supervised models and applied in speech generation, with sufficient methodological description to judge token design and application. Hence, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
    "abstract": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
    "metadata": {
      "arxiv_id": "2310.14837",
      "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
      "summary": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
      "authors": [
        "Daniel Biermann",
        "Fabrizio Palumbo",
        "Morten Goodwin",
        "Ole-Christoffer Granmo"
      ],
      "published": "2023-10-23T11:57:44Z",
      "updated": "2023-10-23T11:57:44Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14837v1",
      "landing_url": "https://arxiv.org/abs/2310.14837v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.14837"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses sequence length reduction via attention-based autoencoders and does not mention generating or using discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses sequence length reduction via attention-based autoencoders and does not mention generating or using discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sequence length reduction using an attention-based autoencoder without explicitly discussing discrete audio tokens, tokenization, neural audio codecs, or related quantization processes relevant to discrete audio token representation for audio tasks. It does not meet the core inclusion criteria involving discrete token sequences for audio, nor does it provide detailed information on token generation or codebook settings, making it unrelated to the review topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sequence length reduction using an attention-based autoencoder without explicitly discussing discrete audio tokens, tokenization, neural audio codecs, or related quantization processes relevant to discrete audio token representation for audio tasks. It does not meet the core inclusion criteria involving discrete token sequences for audio, nor does it provide detailed information on token generation or codebook settings, making it unrelated to the review topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamically Weighted Federated k-Means",
    "abstract": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
    "metadata": {
      "arxiv_id": "2310.14858",
      "title": "Dynamically Weighted Federated k-Means",
      "summary": "Federated clustering, an integral aspect of federated machine learning, enables multiple data sources to collaboratively cluster their data, maintaining decentralization and preserving privacy. In this paper, we introduce a novel federated clustering algorithm named Dynamically Weighted Federated k-means (DWF k-means) based on Lloyd's method for k-means clustering, to address the challenges associated with distributed data sources and heterogeneous data. Our proposed algorithm combines the benefits of traditional clustering techniques with the privacy and scalability benefits offered by federated learning. The algorithm facilitates collaborative clustering among multiple data owners, allowing them to cluster their local data collectively while exchanging minimal information with the central coordinator. The algorithm optimizes the clustering process by adaptively aggregating cluster assignments and centroids from each data source, thereby learning a global clustering solution that reflects the collective knowledge of the entire federated network. We address the issue of empty clusters, which commonly arises in the context of federated clustering. We conduct experiments on multiple datasets and data distribution settings to evaluate the performance of our algorithm in terms of clustering score, accuracy, and v-measure. The results demonstrate that our approach can match the performance of the centralized classical k-means baseline, and outperform existing federated clustering methods like k-FED in realistic scenarios.",
      "authors": [
        "Patrick Holzer",
        "Tania Jacob",
        "Shubham Kavane"
      ],
      "published": "2023-10-23T12:28:21Z",
      "updated": "2023-11-17T10:35:48Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14858v2",
      "landing_url": "https://arxiv.org/abs/2310.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.14858"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not relevant to the discrete audio tokens topic; the paper focuses on federated clustering (DWF k-means) with no discrete audio token generation, tokenizers, or token-based modeling aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not relevant to the discrete audio tokens topic; the paper focuses on federated clustering (DWF k-means) with no discrete audio token generation, tokenizers, or token-based modeling aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The reviewed article introduces a novel federated k-means clustering algorithm focusing on collaborative clustering over distributed data while preserving privacy. However, the content and focus do not pertain to discrete audio tokens, nor do they discuss discrete tokenization of audio waveforms or neural audio codecs, which are core to the inclusion criteria. The study is unrelated to audio token generation, modeling, or evaluation, and therefore does not meet the inclusion criteria nor is relevant to the defined topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The reviewed article introduces a novel federated k-means clustering algorithm focusing on collaborative clustering over distributed data while preserving privacy. However, the content and focus do not pertain to discrete audio tokens, nor do they discuss discrete tokenization of audio waveforms or neural audio codecs, which are core to the inclusion criteria. The study is unrelated to audio token generation, modeling, or evaluation, and therefore does not meet the inclusion criteria nor is relevant to the defined topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
    "abstract": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
    "metadata": {
      "arxiv_id": "2310.15399",
      "title": "GESI: Gammachirp Envelope Similarity Index for Predicting Intelligibility of Simulated Hearing Loss Sounds",
      "summary": "We propose an objective intelligibility measure (OIM), called the Gammachirp Envelope Similarity Index (GESI), which can predict the speech intelligibility (SI) of simulated hearing loss (HL) sounds for normal hearing (NH) listeners. GESI is an intrusive method that computes the SI metric using the gammachirp filterbank (GCFB), the modulation filterbank, and the extended cosine similarity measure. The unique features of GESI are that i) it reflects the hearing impaired (HI) listener's HL that appears in the audiogram and is caused by active and passive cochlear dysfunction, ii) it provides a single goodness metric, as in the widely used STOI and ESTOI, that can be used immediately to evaluate SE algorithms, and iii) it provides a simple control parameter to accept the level asymmetry of the reference and test sounds and to deal with individual listening conditions and environments. We evaluated GESI and the conventional OIMs, STOI, ESTOI, MBSTOI, and HASPI versions 1 and 2 by using four SI experiments on words of male and female speech sounds in both laboratory and remote environments. GESI was shown to outperform the other OIMs in the evaluations. GESI could be used to improve SE algorithms in assistive listening devices for individual HI listeners.",
      "authors": [
        "Ayako Yamamoto",
        "Toshio Irino",
        "Fuki Miyazaki",
        "Honoka Tamaru"
      ],
      "published": "2023-10-23T23:01:33Z",
      "updated": "2024-03-14T02:14:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15399v3",
      "landing_url": "https://arxiv.org/abs/2310.15399v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.15399"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "GESI is an objective intelligibility measure based on continuous audio processing (gammachirp filterbank, modulation filters, cosine similarity) and does not introduce, generate, or analyze discrete audio tokens/tokenization, so it does not meet the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "GESI is an objective intelligibility measure based on continuous audio processing (gammachirp filterbank, modulation filters, cosine similarity) and does not introduce, generate, or analyze discrete audio tokens/tokenization, so it does not meet the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes an objective intelligibility measure for simulated hearing loss sounds using the gammachirp filterbank and related signal processing techniques, but it does not mention any discrete audio tokens, neural audio codecs, or quantization methods to represent audio as discrete token sequences; therefore, it does not meet the inclusion criteria focusing on discrete audio tokens and their design or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes an objective intelligibility measure for simulated hearing loss sounds using the gammachirp filterbank and related signal processing techniques, but it does not mention any discrete audio tokens, neural audio codecs, or quantization methods to represent audio as discrete token sequences; therefore, it does not meet the inclusion criteria focusing on discrete audio tokens and their design or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
    "abstract": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
    "metadata": {
      "arxiv_id": "2310.16550",
      "title": "Dynamic Processing Neural Network Architecture For Hearing Loss Compensation",
      "summary": "This paper proposes neural networks for compensating sensorineural hearing loss. The aim of the hearing loss compensation task is to transform a speech signal to increase speech intelligibility after further processing by a person with a hearing impairment, which is modeled by a hearing loss model. We propose an interpretable model called dynamic processing network, which has a structure similar to band-wise dynamic compressor. The network is differentiable, and therefore allows to learn its parameters to maximize speech intelligibility. More generic models based on convolutional layers were tested as well. The performance of the tested architectures was assessed using spectro-temporal objective index (STOI) with hearing-threshold noise and hearing aid speech intelligibility (HASPI) metrics. The dynamic processing network gave a significant improvement of STOI and HASPI in comparison to popular compressive gain prescription rule Camfit. A large enough convolutional network could outperform the interpretable model with the cost of larger computational load. Finally, a combination of the dynamic processing network with convolutional neural network gave the best results in terms of STOI and HASPI.",
      "authors": [
        "Szymon Drgas",
        "Lars Bramsløw",
        "Archontis Politis",
        "Gaurav Naithani",
        "Tuomas Virtanen"
      ],
      "published": "2023-10-25T11:04:32Z",
      "updated": "2023-10-25T11:04:32Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16550v1",
      "landing_url": "https://arxiv.org/abs/2310.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a dynamic processing network for hearing loss compensation with no mention of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a dynamic processing network for hearing loss compensation with no mention of discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neural networks for hearing loss compensation and does not describe discretization of continuous audio waveforms into discrete audio tokens or the use of neural audio codecs/vector quantization methods to generate token sequences. It lacks any mention of discrete token design, vocabulary, or downstream modeling of audio as token sequences, which is a core inclusion criterion. Therefore, it does not meet the defined subject matter for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neural networks for hearing loss compensation and does not describe discretization of continuous audio waveforms into discrete audio tokens or the use of neural audio codecs/vector quantization methods to generate token sequences. It lacks any mention of discrete token design, vocabulary, or downstream modeling of audio as token sequences, which is a core inclusion criterion. Therefore, it does not meet the defined subject matter for discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ArTST: Arabic Text and Speech Transformer",
    "abstract": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
    "metadata": {
      "arxiv_id": "2310.16621",
      "title": "ArTST: Arabic Text and Speech Transformer",
      "summary": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
      "authors": [
        "Hawau Olamide Toyin",
        "Amirbek Djanibekov",
        "Ajinkya Kulkarni",
        "Hanan Aldarmaki"
      ],
      "published": "2023-10-25T13:20:54Z",
      "updated": "2023-10-25T13:20:54Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16621v1",
      "landing_url": "https://arxiv.org/abs/2310.16621v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16621"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not involve discrete audio tokens or a tokenization/codec design; it describes a unified-modal Arabic text and speech transformer for ASR/TTS/dialect ID, thus failing the discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not involve discrete audio tokens or a tokenization/codec design; it describes a unified-modal Arabic text and speech transformer for ASR/TTS/dialect ID, thus failing the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an Arabic text and speech transformer pre-trained for ASR, TTS, and dialect identification, but the abstract does not mention the use or design of discrete audio tokens through discretization or neural audio codecs, nor does it describe tokenization or quantization schemes. Therefore, it does not appear to meet the core criteria of using discrete audio tokens as central representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an Arabic text and speech transformer pre-trained for ASR, TTS, and dialect identification, but the abstract does not mention the use or design of discrete audio tokens through discretization or neural audio codecs, nor does it describe tokenization or quantization schemes. Therefore, it does not appear to meet the core criteria of using discrete audio tokens as central representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Single channel speech enhancement by colored spectrograms",
    "abstract": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
    "metadata": {
      "arxiv_id": "2310.17142",
      "title": "Single channel speech enhancement by colored spectrograms",
      "summary": "Speech enhancement concerns the processes required to remove unwanted background sounds from the target speech to improve its quality and intelligibility. In this paper, a novel approach for single-channel speech enhancement is presented, using colored spectrograms. We propose the use of a deep neural network (DNN) architecture adapted from the pix2pix generative adversarial network (GAN) and train it over colored spectrograms of speech to denoise them. After denoising, the colors of spectrograms are translated to magnitudes of short-time Fourier transform (STFT) using a shallow regression neural network. These estimated STFT magnitudes are later combined with the noisy phases to obtain an enhanced speech. The results show an improvement of almost 0.84 points in the perceptual evaluation of speech quality (PESQ) and 1% in the short-term objective intelligibility (STOI) over the unprocessed noisy data. The gain in quality and intelligibility over the unprocessed signal is almost equal to the gain achieved by the baseline methods used for comparison with the proposed model, but at a much reduced computational cost. The proposed solution offers a comparative PESQ score at almost 10 times reduced computational cost than a similar baseline model that has generated the highest PESQ score trained on grayscaled spectrograms, while it provides only a 1% deficit in STOI at 28 times reduced computational cost when compared to another baseline system based on convolutional neural network-GAN (CNN-GAN) that produces the most intelligible speech.",
      "authors": [
        "Sania Gul",
        "Muhammad Salman Khan",
        "Muhammad Fazeel"
      ],
      "published": "2023-10-26T04:29:27Z",
      "updated": "2023-10-26T04:29:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.17142v1",
      "landing_url": "https://arxiv.org/abs/2310.17142v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.17142"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on single-channel speech enhancement using colored spectrograms and GAN-based denoising without describing discrete audio tokens, tokenization, or token-based representations, so it does not meet the discrete-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on single-channel speech enhancement using colored spectrograms and GAN-based denoising without describing discrete audio tokens, tokenization, or token-based representations, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on single-channel speech enhancement using colored spectrograms and deep neural networks but does not mention discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes; thus, it does not meet the inclusion criteria related to discrete token generation and use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on single-channel speech enhancement using colored spectrograms and deep neural networks but does not mention discrete audio tokens, neural audio codecs, vector quantization, or tokenization processes; thus, it does not meet the inclusion criteria related to discrete token generation and use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pressure influence on excitonic luminescence of CsPbBr3 perovskite",
    "abstract": "This study investigates the effect of hydrostatic pressure on the luminescence properties of CsPbBr3 single crystals at 12 K. The luminescence at the edge of the band gap reveals a structure attributed to free excitons, phonon replica of the free excitons, and Rashba excitons. Changes in the relative intensity of the free and Rashba excitons were observed with increasing pressure, caused by changes in the probability of nonradiative deexcitation. At pressures around 3 GPa, luminescence completely fades away. The red shift of the energy position of the maximum luminescence of free and Rashba excitons in pressure ranges of 0-1.3 GPa is attributed to the length reduction of Pb-Br bonds in [PbBr6]4- octahedra, while the high-energy shift of the Rashba excitons at pressures above 1.3 GPa is due to [PbBr6]4- octahedra rotation and changes in the Pb-Br_Pb angle.",
    "metadata": {
      "arxiv_id": "2310.18967",
      "title": "Pressure influence on excitonic luminescence of CsPbBr3 perovskite",
      "summary": "This study investigates the effect of hydrostatic pressure on the luminescence properties of CsPbBr3 single crystals at 12 K. The luminescence at the edge of the band gap reveals a structure attributed to free excitons, phonon replica of the free excitons, and Rashba excitons. Changes in the relative intensity of the free and Rashba excitons were observed with increasing pressure, caused by changes in the probability of nonradiative deexcitation. At pressures around 3 GPa, luminescence completely fades away. The red shift of the energy position of the maximum luminescence of free and Rashba excitons in pressure ranges of 0-1.3 GPa is attributed to the length reduction of Pb-Br bonds in [PbBr6]4- octahedra, while the high-energy shift of the Rashba excitons at pressures above 1.3 GPa is due to [PbBr6]4- octahedra rotation and changes in the Pb-Br_Pb angle.",
      "authors": [
        "Lev Ivan Bulyk",
        "Taras Demkiv",
        "Oleh Antonyak",
        "Yaroslav M. Chornodolskyy",
        "Roman Gamernyk",
        "Andrzej Suchocki",
        "Anatolii Voloshinovskii"
      ],
      "published": "2023-10-29T10:17:41Z",
      "updated": "2023-10-29T10:17:41Z",
      "categories": [
        "cond-mat.mtrl-sci"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.18967v1",
      "landing_url": "https://arxiv.org/abs/2310.18967v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.18967"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究與離散音訊 token 主題無關，未描述離散 token 的生成、碼本/字彙表設定或以離散 token 作為核心分析，因此不符合包含條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究與離散音訊 token 主題無關，未描述離散 token 的生成、碼本/字彙表設定或以離散 token 作為核心分析，因此不符合包含條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the luminescence properties of CsPbBr3 perovskite crystals under pressure and does not mention any discrete audio tokens, neural audio encoding, or tokenization relevant to audio waveforms or their discretization. It thus does not meet any inclusion criterion related to discrete audio tokens or their applications, and is unrelated to audio tokenization or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the luminescence properties of CsPbBr3 perovskite crystals under pressure and does not mention any discrete audio tokens, neural audio encoding, or tokenization relevant to audio waveforms or their discretization. It thus does not meet any inclusion criterion related to discrete audio tokens or their applications, and is unrelated to audio tokenization or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low-latency Real-time Voice Conversion on CPU",
    "abstract": "We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\\textbf{L}$ow-latency $\\textbf{L}$ow-resource $\\textbf{V}$oice $\\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.",
    "metadata": {
      "arxiv_id": "2311.00873",
      "title": "Low-latency Real-time Voice Conversion on CPU",
      "summary": "We adapt the architectures of previous audio manipulation and generation neural networks to the task of real-time any-to-one voice conversion. Our resulting model, LLVC ($\\textbf{L}$ow-latency $\\textbf{L}$ow-resource $\\textbf{V}$oice $\\textbf{C}$onversion), has a latency of under 20ms at a bitrate of 16kHz and runs nearly 2.8x faster than real-time on a consumer CPU. LLVC uses both a generative adversarial architecture as well as knowledge distillation in order to attain this performance. To our knowledge LLVC achieves both the lowest resource usage as well as the lowest latency of any open-source voice conversion model. We provide open-source samples, code, and pretrained model weights at https://github.com/KoeAI/LLVC.",
      "authors": [
        "Konstantine Sadov",
        "Matthew Hutter",
        "Asara Near"
      ],
      "published": "2023-11-01T21:57:52Z",
      "updated": "2023-11-01T21:57:52Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00873v1",
      "landing_url": "https://arxiv.org/abs/2311.00873v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00873"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on real-time voice conversion on CPU and does not involve discrete audio tokens or tokenizer/codec-based token representations, so it fails to meet the token-centric inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work focuses on real-time voice conversion on CPU and does not involve discrete audio tokens or tokenizer/codec-based token representations, so it fails to meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a low-latency real-time voice conversion model that runs efficiently on CPU, but they do not mention any discretization process or the use of discrete audio tokens as core input representations; the work instead focuses on real-time voice conversion performance with generative adversarial networks and knowledge distillation. There is no information about producing or utilizing discrete token sequences as defined in the inclusion criteria, nor about neural audio codec architectures or vector quantization for token generation. Therefore, the study does not meet the inclusion criteria related to discrete audio token design or application, and falls under the exclusion criteria since the core method is continuous audio modeling without discrete token definitions or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a low-latency real-time voice conversion model that runs efficiently on CPU, but they do not mention any discretization process or the use of discrete audio tokens as core input representations; the work instead focuses on real-time voice conversion performance with generative adversarial networks and knowledge distillation. There is no information about producing or utilizing discrete token sequences as defined in the inclusion criteria, nor about neural audio codec architectures or vector quantization for token generation. Therefore, the study does not meet the inclusion criteria related to discrete audio token design or application, and falls under the exclusion criteria since the core method is continuous audio modeling without discrete token definitions or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
    "abstract": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
    "metadata": {
      "arxiv_id": "2311.00945",
      "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
      "summary": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "authors": [
        "Yuan Gao",
        "Nobuyuki Morioka",
        "Yu Zhang",
        "Nanxin Chen"
      ],
      "published": "2023-11-02T02:22:21Z",
      "updated": "2023-11-02T02:22:21Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00945v1",
      "landing_url": "https://arxiv.org/abs/2311.00945v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00945"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper introduces a diffusion-based TTS system, it generates waveform directly from text without any discrete audio tokens, tokenizers, or discretization, so it does not meet the 'Discrete Audio Tokens' criteria and falls under the exclusion scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper introduces a diffusion-based TTS system, it generates waveform directly from text without any discrete audio tokens, tokenizers, or discretization, so it does not meet the 'Discrete Audio Tokens' criteria and falls under the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents an end-to-end diffusion-based TTS system that directly generates audio waveforms from text without relying on intermediate discrete token representations or any discrete audio tokenization process. Since there is no indication that discrete audio tokens or their quantization, codebooks, or vocabularies are used or proposed, the study does not fit the inclusion criteria focused on discrete audio tokens and their design or usage. It instead focuses on continuous waveform modeling via diffusion models, placing it under the exclusion criteria for continuous audio representations without discrete token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents an end-to-end diffusion-based TTS system that directly generates audio waveforms from text without relying on intermediate discrete token representations or any discrete audio tokenization process. Since there is no indication that discrete audio tokens or their quantization, codebooks, or vocabularies are used or proposed, the study does not fit the inclusion criteria focused on discrete audio tokens and their design or usage. It instead focuses on continuous waveform modeling via diffusion models, placing it under the exclusion criteria for continuous audio representations without discrete token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
    "abstract": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
    "metadata": {
      "arxiv_id": "2311.01635",
      "title": "RTP: Rethinking Tensor Parallelism with Memory Deduplication",
      "summary": "In the evolving landscape of neural network models, one prominent challenge stand out: the significant memory overheads associated with training expansive models. Addressing this challenge, this study delves deep into the Rotated Tensor Parallelism (RTP). RTP is an innovative approach that strategically focuses on memory deduplication in distributed training environments. It boasts of unique features like a customized communication primitive and the Flyweight Pattern initialization. Furthermore, RTP ensures a seamless overlap between partition computation and partition weight communication, optimizing the training process. Our empirical evaluations underscore RTP's efficiency, revealing that its memory consumption during distributed system training is remarkably close to the optimal - distributing the memory overhead of a single machine equitably among multiple machines. The experimental results demonstrate that RTP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of memory. Code of RTP is available at https://github.com/wdlctc/rtp.",
      "authors": [
        "Cheng Luo",
        "Tianle Zhong",
        "Geoffrey Fox"
      ],
      "published": "2023-11-02T23:12:42Z",
      "updated": "2023-11-02T23:12:42Z",
      "categories": [
        "cs.DC",
        "cs.AI",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.01635v1",
      "landing_url": "https://arxiv.org/abs/2311.01635v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.01635"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about memory-efficient tensor parallelism for distributed training and does not discuss discrete audio tokens, tokenizers, or any discrete token design/analysis, failing all inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about memory-efficient tensor parallelism for distributed training and does not discuss discrete audio tokens, tokenizers, or any discrete token design/analysis, failing all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving tensor parallelism and memory deduplication techniques in distributed training of large neural networks, with no mention of discrete audio tokens, neural audio codecs, discretization of audio signals, or tokenization related to audio modeling or generation tasks, thus failing the inclusion criteria and fitting the exclusion criteria related to lack of discrete audio tokens and core audio-related content.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving tensor parallelism and memory deduplication techniques in distributed training of large neural networks, with no mention of discrete audio tokens, neural audio codecs, discretization of audio signals, or tokenization related to audio modeling or generation tasks, thus failing the inclusion criteria and fitting the exclusion criteria related to lack of discrete audio tokens and core audio-related content.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
    "abstract": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discretized semantic tokens from a self-supervised audio representation (wav2vec2.0) to drive a neural transducer for TTS and a separate non-autoregressive generator, addressing token design/usage and downstream generation, which aligns with the ‘Discrete Audio Tokens’ inclusion criteria and does not match the exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discretized semantic tokens from a self-supervised audio representation (wav2vec2.0) to drive a neural transducer for TTS and a separate non-autoregressive generator, addressing token design/usage and downstream generation, which aligns with the ‘Discrete Audio Tokens’ inclusion criteria and does not match the exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a text-to-speech framework that generates aligned semantic tokens derived from wav2vec2.0 embeddings, which are discretized semantic tokens; this fits the definition of discrete audio tokens used as core representations in audio generation tasks. The method details the use of such tokens, demonstrating empirical evaluation on TTS quality, which aligns with the inclusion criteria emphasizing discrete token-based downstream generation and evaluation. The abstract is in English and provides sufficient methodological context on the discrete token generation, supporting reproducibility assessment. Hence, it meets all inclusion criteria and does not appear to violate exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a text-to-speech framework that generates aligned semantic tokens derived from wav2vec2.0 embeddings, which are discretized semantic tokens; this fits the definition of discrete audio tokens used as core representations in audio generation tasks. The method details the use of such tokens, demonstrating empirical evaluation on TTS quality, which aligns with the inclusion criteria emphasizing discrete token-based downstream generation and evaluation. The abstract is in English and provides sufficient methodological context on the discrete token generation, supporting reproducibility assessment. Hence, it meets all inclusion criteria and does not appear to violate exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Disentangled Speech Representations",
    "abstract": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity. In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
    "metadata": {
      "arxiv_id": "2311.03389",
      "title": "Learning Disentangled Speech Representations",
      "summary": "Disentangled representation learning in speech processing has lagged behind other domains, largely due to the lack of datasets with annotated generative factors for robust evaluation. To address this, we propose SynSpeech, a novel large-scale synthetic speech dataset specifically designed to enable research on disentangled speech representations. SynSpeech includes controlled variations in speaker identity, spoken text, and speaking style, with three dataset versions to support experimentation at different levels of complexity.\n  In this study, we present a comprehensive framework to evaluate disentangled representation learning techniques, applying both linear probing and established supervised disentanglement metrics to assess the modularity, compactness, and informativeness of the representations learned by a state-of-the-art model. Using the RAVE model as a test case, we find that SynSpeech facilitates benchmarking across a range of factors, achieving promising disentanglement of simpler features like gender and speaking style, while highlighting challenges in isolating complex attributes like speaker identity. This benchmark dataset and evaluation framework fills a critical gap, supporting the development of more robust and interpretable speech representation learning methods.",
      "authors": [
        "Yusuf Brima",
        "Ulf Krumnack",
        "Simone Pika",
        "Gunther Heidemann"
      ],
      "published": "2023-11-04T04:54:17Z",
      "updated": "2025-01-11T06:05:41Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03389v4",
      "landing_url": "https://arxiv.org/abs/2311.03389v4",
      "doi": "https://doi.org/10.48550/arXiv.2311.03389"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract focuses on disentangled speech representations and a synthetic benchmark for evaluation, with no discussion of discrete audio tokens, tokenization, or neural codecs, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on disentangled speech representations and a synthetic benchmark for evaluation, with no discussion of discrete audio tokens, tokenization, or neural codecs, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on learning disentangled speech representations using a synthetic dataset, but it does not describe the creation or use of discrete audio tokens via discretization methods (e.g., neural audio codecs or vector quantization) mapping continuous waveforms into token sequences for modeling or generation. There is no mention of token vocabularies, tokenizers, quantization specifics, or downstream tasks involving discrete tokens as required by the inclusion criteria. Therefore, it falls outside the scope of discrete audio tokens research and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on learning disentangled speech representations using a synthetic dataset, but it does not describe the creation or use of discrete audio tokens via discretization methods (e.g., neural audio codecs or vector quantization) mapping continuous waveforms into token sequences for modeling or generation. There is no mention of token vocabularies, tokenizers, quantization specifics, or downstream tasks involving discrete tokens as required by the inclusion criteria. Therefore, it falls outside the scope of discrete audio tokens research and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
    "abstract": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
    "metadata": {
      "arxiv_id": "2311.03792",
      "title": "Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment",
      "summary": "The International Phonetic Alphabet (IPA) is indispensable in language learning and understanding, aiding users in accurate pronunciation and comprehension. Additionally, it plays a pivotal role in speech therapy, linguistic research, accurate transliteration, and the development of text-to-speech systems, making it an essential tool across diverse fields. Bangla being 7th as one of the widely used languages, gives rise to the need for IPA in its domain. Its IPA mapping is too diverse to be captured manually giving the need for Artificial Intelligence and Machine Learning in this field. In this study, we have utilized a transformer-based sequence-to-sequence model at the letter and symbol level to get the IPA of each Bangla word as the variation of IPA in association of different words is almost null. Our transformer model only consisted of 8.5 million parameters with only a single decoder and encoder layer. Additionally, to handle the punctuation marks and the occurrence of foreign languages in the text, we have utilized manual mapping as the model won't be able to learn to separate them from Bangla words while decreasing our required computational resources. Finally, maintaining the relative position of the sentence component IPAs and generation of the combined IPA has led us to achieve the top position with a word error rate of 0.10582 in the public ranking of DataVerse Challenge - ITVerse 2023 (https://www.kaggle.com/competitions/dataverse_2023/).",
      "authors": [
        "Jakir Hasan",
        "Shrestha Datta",
        "Ameya Debnath"
      ],
      "published": "2023-11-07T08:20:06Z",
      "updated": "2023-11-07T08:20:06Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.03792v1",
      "landing_url": "https://arxiv.org/abs/2311.03792v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.03792"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Bangla text-to-IPA transcription using a transformer and does not discuss discrete audio tokenization, neural audio codecs, or token-based downstream modeling, so it does not meet the discrete audio tokens criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Bangla text-to-IPA transcription using a transformer and does not discuss discrete audio tokenization, neural audio codecs, or token-based downstream modeling, so it does not meet the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Bangla text-to-IPA transcription using a transformer-based sequence-to-sequence model at the letter and symbol level, which is primarily textual-to-textual mapping rather than discretization of continuous audio waveforms into discrete audio tokens. It does not mention neural audio codecs, vector quantization, nor any discretization of continuous audio into token sequences for generative or multimodal tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead is more about linguistic transcription from text to IPA, a symbolic phonetic representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Bangla text-to-IPA transcription using a transformer-based sequence-to-sequence model at the letter and symbol level, which is primarily textual-to-textual mapping rather than discretization of continuous audio waveforms into discrete audio tokens. It does not mention neural audio codecs, vector quantization, nor any discretization of continuous audio into token sequences for generative or multimodal tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead is more about linguistic transcription from text to IPA, a symbolic phonetic representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
    "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
    "metadata": {
      "arxiv_id": "2311.04534",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "published": "2023-11-08T08:45:14Z",
      "updated": "2024-02-05T02:42:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04534v2",
      "landing_url": "https://arxiv.org/abs/2311.04534v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.04534"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly studies discrete-token-based ASR, detailing speech discretization into tokens and their use in a decoder-only Transformer with a token-based loss (SLD), thus addressing discrete audio tokens and their downstream impact, meeting the inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly studies discrete-token-based ASR, detailing speech discretization into tokens and their use in a decoder-only Transformer with a token-based loss (SLD), thus addressing discrete audio tokens and their downstream impact, meeting the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on modeling speech tokens derived from discrete-token-based ASR using decoder-only Transformers and proposes a novel method (Smoothed Label Distillation) for better modeling speech tokens, which indicates use of discrete token sequences obtained through discretization; however, the abstract does not clearly describe the tokenizer/codec architecture, quantization method, vocabulary/codebook settings, or other details about token generation to verify reproducibility and confirm the discrete token design and their characteristics as per inclusion criteria. The study addresses downstream ASR tasks using discrete tokens, aligning well with the topic, but lacks full details on discrete token generation and system-level evaluation, leaving some ambiguity about full compliance to inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on modeling speech tokens derived from discrete-token-based ASR using decoder-only Transformers and proposes a novel method (Smoothed Label Distillation) for better modeling speech tokens, which indicates use of discrete token sequences obtained through discretization; however, the abstract does not clearly describe the tokenizer/codec architecture, quantization method, vocabulary/codebook settings, or other details about token generation to verify reproducibility and confirm the discrete token design and their characteristics as per inclusion criteria. The study addresses downstream ASR tasks using discrete tokens, aligning well with the topic, but lacks full details on discrete token generation and system-level evaluation, leaving some ambiguity about full compliance to inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano judged this clearly relevant because the paper explicitly studies discrete-token-based ASR and proposes a token-level loss; JuniorMini raised a valid concern that the abstract omits low-level tokenizer/codec and codebook details. Countering that, the abstract explicitly states speech discretization is used (and evaluated across different discretization methods) and the authors provide a code link, indicating discrete audio tokens are the core representation and reproducibility details are likely available in the full paper/repo—therefore it meets the inclusion groups (use of discretized tokens from codecs/quantization for downstream ASR). Score justification (1-sentence): 5 — include, because the work centers on discrete audio tokens as primary inputs for decoder-only Transformers (with experiments across discretization methods and accompanying code), satisfying the topic and inclusion criteria.",
      "evaluation": 5
    },
    "round-B_SeniorLead_reasoning": "JuniorNano judged this clearly relevant because the paper explicitly studies discrete-token-based ASR and proposes a token-level loss; JuniorMini raised a valid concern that the abstract omits low-level tokenizer/codec and codebook details. Countering that, the abstract explicitly states speech discretization is used (and evaluated across different discretization methods) and the authors provide a code link, indicating discrete audio tokens are the core representation and reproducibility details are likely available in the full paper/repo—therefore it meets the inclusion groups (use of discretized tokens from codecs/quantization for downstream ASR). Score justification (1-sentence): 5 — include, because the work centers on discrete audio tokens as primary inputs for decoder-only Transformers (with experiments across discretization methods and accompanying code), satisfying the topic and inclusion criteria.",
    "round-B_SeniorLead_evaluation": 5,
    "final_verdict": "include (senior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
    "abstract": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
    "metadata": {
      "arxiv_id": "2311.08104",
      "title": "Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion",
      "summary": "Research on deep learning-powered voice conversion (VC) in speech-to-speech scenarios is getting increasingly popular. Although many of the works in the field of voice conversion share a common global pipeline, there is a considerable diversity in the underlying structures, methods, and neural sub-blocks used across research efforts. Thus, obtaining a comprehensive understanding of the reasons behind the choice of the different methods in the voice conversion pipeline can be challenging, and the actual hurdles in the proposed solutions are often unclear. To shed light on these aspects, this paper presents a scoping review that explores the use of deep learning in speech analysis, synthesis, and disentangled speech representation learning within modern voice conversion systems. We screened 621 publications from more than 38 different venues between the years 2017 and 2023, followed by an in-depth review of a final database consisting of 123 eligible studies. Based on the review, we summarise the most frequently used approaches to voice conversion based on deep learning and highlight common pitfalls within the community. Lastly, we condense the knowledge gathered, identify main challenges and provide recommendations for future research directions.",
      "authors": [
        "Anders R. Bargum",
        "Stefania Serafin",
        "Cumhur Erkut"
      ],
      "published": "2023-11-14T12:03:46Z",
      "updated": "2023-11-14T12:03:46Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08104v1",
      "landing_url": "https://arxiv.org/abs/2311.08104v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08104"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a scoping review on deep learning-powered voice conversion and does not discuss discrete audio tokens or tokenization designs, thus it does not meet the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is a scoping review on deep learning-powered voice conversion and does not discuss discrete audio tokens or tokenization designs, thus it does not meet the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a broad scoping review of deep learning-powered voice conversion but does not explicitly mention the use or analysis of discrete audio tokens or neural audio codecs with quantization mechanisms, nor does it describe tokenization processes or vocabulary/codebook settings necessary to meet the inclusion criteria focused on discrete audio token representations.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper is a broad scoping review of deep learning-powered voice conversion but does not explicitly mention the use or analysis of discrete audio tokens or neural audio codecs with quantization mechanisms, nor does it describe tokenization processes or vocabulary/codebook settings necessary to meet the inclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation",
    "abstract": "Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.",
    "metadata": {
      "arxiv_id": "2311.08670",
      "title": "CLN-VC: Text-Free Voice Conversion Based on Fine-Grained Style Control and Contrastive Learning with Negative Samples Augmentation",
      "summary": "Better disentanglement of speech representation is essential to improve the quality of voice conversion. Recently contrastive learning is applied to voice conversion successfully based on speaker labels. However, the performance of model will reduce in conversion between similar speakers. Hence, we propose an augmented negative sample selection to address the issue. Specifically, we create hard negative samples based on the proposed speaker fusion module to improve learning ability of speaker encoder. Furthermore, considering the fine-grain modeling of speaker style, we employ a reference encoder to extract fine-grained style and conduct the augmented contrastive learning on global style. The experimental results show that the proposed method outperforms previous work in voice conversion tasks.",
      "authors": [
        "Yimin Deng",
        "Xulong Zhang",
        "Jianzong Wang",
        "Ning Cheng",
        "Jing Xiao"
      ],
      "published": "2023-11-15T03:29:31Z",
      "updated": "2023-11-15T03:29:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08670v1",
      "landing_url": "https://arxiv.org/abs/2311.08670v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08670"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio tokens or tokenization; it focuses on text-free voice conversion using continuous representations and contrastive learning, thus not meeting the discrete-token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio tokens or tokenization; it focuses on text-free voice conversion using continuous representations and contrastive learning, thus not meeting the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using contrastive learning and style control but does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of continuous audio signals into token sequences as required by the inclusion criteria. The abstract lacks details on token generation processes, discrete vocabularies, or codec architectures, and the core approach seems to operate on continuous speech representations rather than discrete tokens, thus not meeting the defined inclusion criteria and falling into the exclusion category of continuous audio representation modeling without discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using contrastive learning and style control but does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any form of discretization of continuous audio signals into token sequences as required by the inclusion criteria. The abstract lacks details on token generation processes, discrete vocabularies, or codec architectures, and the core approach seems to operate on continuous speech representations rather than discrete tokens, thus not meeting the defined inclusion criteria and falling into the exclusion category of continuous audio representation modeling without discrete tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
    "abstract": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
    "metadata": {
      "arxiv_id": "2311.10525",
      "title": "Utilizing VQ-VAE for End-to-End Health Indicator Generation in Predicting Rolling Bearing RUL",
      "summary": "The prediction of the remaining useful life (RUL) of rolling bearings is a pivotal issue in industrial production. A crucial approach to tackling this issue involves transforming vibration signals into health indicators (HI) to aid model training. This paper presents an end-to-end HI construction method, vector quantised variational autoencoder (VQ-VAE), which addresses the need for dimensionality reduction of latent variables in traditional unsupervised learning methods such as autoencoder. Moreover, concerning the inadequacy of traditional statistical metrics in reflecting curve fluctuations accurately, two novel statistical metrics, mean absolute distance (MAD) and mean variance (MV), are introduced. These metrics accurately depict the fluctuation patterns in the curves, thereby indicating the model's accuracy in discerning similar features. On the PMH2012 dataset, methods employing VQ-VAE for label construction achieved lower values for MAD and MV. Furthermore, the ASTCN prediction model trained with VQ-VAE labels demonstrated commendable performance, attaining the lowest values for MAD and MV.",
      "authors": [
        "Junliang Wang",
        "Qinghua Zhang",
        "Guanhua Zhu",
        "Guoxi Sun"
      ],
      "published": "2023-11-17T13:45:31Z",
      "updated": "2023-11-17T13:45:31Z",
      "categories": [
        "cs.LG",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.10525v1",
      "landing_url": "https://arxiv.org/abs/2311.10525v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.10525"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work applies VQ-VAE to vibration-based health indicator generation for rolling bearing RUL and does not discuss discrete audio tokens, tokenizers, or audio-domain discretization, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work applies VQ-VAE to vibration-based health indicator generation for rolling bearing RUL and does not discuss discrete audio tokens, tokenizers, or audio-domain discretization, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using VQ-VAE, a vector quantized variational autoencoder, for health indicator generation in predicting bearing remaining useful life, which involves vector quantization. However, the application is on industrial vibration signal analysis for RUL prediction, not on audio waveform discretization or tokenization for audio generation, understanding, or cross-modal modeling. The tokens generated here are not audio tokens from speech, music, or environmental sounds used for sequence modeling of audio, but latent features useful for machinery health monitoring. Therefore, it does not meet the inclusion criteria of discrete audio tokens from neural audio codecs or self-supervised speech/audio models used as token sequences for audio modeling and generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using VQ-VAE, a vector quantized variational autoencoder, for health indicator generation in predicting bearing remaining useful life, which involves vector quantization. However, the application is on industrial vibration signal analysis for RUL prediction, not on audio waveform discretization or tokenization for audio generation, understanding, or cross-modal modeling. The tokens generated here are not audio tokens from speech, music, or environmental sounds used for sequence modeling of audio, but latent features useful for machinery health monitoring. Therefore, it does not meet the inclusion criteria of discrete audio tokens from neural audio codecs or self-supervised speech/audio models used as token sequences for audio modeling and generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
    "abstract": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
    "metadata": {
      "arxiv_id": "2311.13588",
      "title": "User-guided Page Merging for Memory Deduplication in Serverless Systems",
      "summary": "Serverless computing is an emerging cloud paradigm that offers an elastic and scalable allocation of computing resources with pay-as-you-go billing. In the Function-as-a-Service (FaaS) programming model, applications comprise short-lived and stateless serverless functions executed in isolated containers or microVMs, which can quickly scale to thousands of instances and process terabytes of data. This flexibility comes at the cost of duplicated runtimes, libraries, and user data spread across many function instances, and cloud providers do not utilize this redundancy. The memory footprint of serverless forces removing idle containers to make space for new ones, which decreases performance through more cold starts and fewer data caching opportunities. We address this issue by proposing deduplicating memory pages of serverless workers with identical content, based on the content-based page-sharing concept of Linux Kernel Same-page Merging (KSM). We replace the background memory scanning process of KSM, as it is too slow to locate sharing candidates in short-lived functions. Instead, we design User-Guided Page Merging (UPM), a built-in Linux kernel module that leverages the madvise system call: we enable users to advise the kernel of memory areas that can be shared with others. We show that UPM reduces memory consumption by up to 55% on 16 concurrent containers executing a typical image recognition function, more than doubling the density for containers of the same function that can run on a system.",
      "authors": [
        "Wei Qiu",
        "Marcin Copik",
        "Yun Wang",
        "Alexandru Calotoiu",
        "Torsten Hoefler"
      ],
      "published": "2023-11-22T18:49:00Z",
      "updated": "2023-11-22T18:49:00Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.13588v1",
      "landing_url": "https://arxiv.org/abs/2311.13588v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.13588"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This title/abstract discusses memory deduplication in serverless computing and does not involve discrete audio tokens, tokenization, or related evaluation; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This title/abstract discusses memory deduplication in serverless computing and does not involve discrete audio tokens, tokenization, or related evaluation; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on memory deduplication in serverless systems through page merging, which is unrelated to the discrete audio token topic that requires the use of discrete token sequences derived from audio waveforms for modeling or generation. There is no mention of audio, tokenization of audio, or related neural audio codec or discrete audio token mechanisms in the abstract or title.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on memory deduplication in serverless systems through page merging, which is unrelated to the discrete audio token topic that requires the use of discrete token sequences derived from audio waveforms for modeling or generation. There is no mention of audio, tokenization of audio, or related neural audio codec or discrete audio token mechanisms in the abstract or title.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RETSim: Resilient and Efficient Text Similarity",
    "abstract": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
    "metadata": {
      "arxiv_id": "2311.17264",
      "title": "RETSim: Resilient and Efficient Text Similarity",
      "summary": "This paper introduces RETSim (Resilient and Efficient Text Similarity), a lightweight, multilingual deep learning model trained to produce robust metric embeddings for near-duplicate text retrieval, clustering, and dataset deduplication tasks. We demonstrate that RETSim is significantly more robust and accurate than MinHash and neural text embeddings, achieving new state-of-the-art performance on dataset deduplication, adversarial text retrieval benchmarks, and spam clustering tasks. We also introduce the W4NT3D benchmark (Wiki-40B 4dversarial Near-T3xt Dataset) for evaluating multilingual, near-duplicate text retrieval capabilities under adversarial settings. RETSim and the W4NT3D benchmark are open-sourced under the MIT License at https://github.com/google/unisim.",
      "authors": [
        "Marina Zhang",
        "Owen Vallis",
        "Aysegul Bumin",
        "Tanay Vakharia",
        "Elie Bursztein"
      ],
      "published": "2023-11-28T22:54:33Z",
      "updated": "2023-11-28T22:54:33Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.17264v1",
      "landing_url": "https://arxiv.org/abs/2311.17264v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.17264"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item investigates multilingual text similarity and dataset deduplication, not discrete audio tokens or neural audio codecs; it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item investigates multilingual text similarity and dataset deduplication, not discrete audio tokens or neural audio codecs; it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on text similarity using a deep learning model for near-duplicate text retrieval, clustering, and dataset deduplication, which does not involve discrete audio tokens, neural audio codecs, or any audio-related tokenization processes as required by the inclusion criteria. Therefore, it does not meet the main thematic requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on text similarity using a deep learning model for near-duplicate text retrieval, clustering, and dataset deduplication, which does not involve discrete audio tokens, neural audio codecs, or any audio-related tokenization processes as required by the inclusion criteria. Therefore, it does not meet the main thematic requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GIVT: Generative Infinite-Vocabulary Transformers",
    "abstract": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
    "metadata": {
      "arxiv_id": "2312.02116",
      "title": "GIVT: Generative Infinite-Vocabulary Transformers",
      "summary": "We introduce Generative Infinite-Vocabulary Transformers (GIVT) which generate vector sequences with real-valued entries, instead of discrete tokens from a finite vocabulary. To this end, we propose two surprisingly simple modifications to decoder-only transformers: 1) at the input, we replace the finite-vocabulary lookup table with a linear projection of the input vectors; and 2) at the output, we replace the logits prediction (usually mapped to a categorical distribution) with the parameters of a multivariate Gaussian mixture model. Inspired by the image-generation paradigm of VQ-GAN and MaskGIT, where transformers are used to model the discrete latent sequences of a VQ-VAE, we use GIVT to model the unquantized real-valued latent sequences of a $β$-VAE. In class-conditional image generation GIVT outperforms VQ-GAN (and improved variants thereof) as well as MaskGIT, and achieves performance competitive with recent latent diffusion models. Finally, we obtain strong results outside of image generation when applying GIVT to panoptic segmentation and depth estimation with a VAE variant of the UViM framework.",
      "authors": [
        "Michael Tschannen",
        "Cian Eastwood",
        "Fabian Mentzer"
      ],
      "published": "2023-12-04T18:48:02Z",
      "updated": "2024-07-17T16:32:09Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02116v4",
      "landing_url": "https://arxiv.org/abs/2312.02116v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.02116"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses Generative Infinite-Vocabulary Transformers generating real-valued latent sequences for images (not discrete audio tokens or tokenization/quantization schemes), so it does not meet the discrete audio token inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses Generative Infinite-Vocabulary Transformers generating real-valued latent sequences for images (not discrete audio tokens or tokenization/quantization schemes), so it does not meet the discrete audio token inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Generative Infinite-Vocabulary Transformers generating sequences with real-valued, continuous vectors instead of discrete tokens from a finite vocabulary. It applies the method mainly to image generation, panoptic segmentation, and depth estimation within a VAE framework, with no mention of discretized audio tokens or audio waveform discretization into token sequences for audio modeling or generation. Hence, it does not meet the inclusion criteria regarding discrete audio token usage or analysis for audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Generative Infinite-Vocabulary Transformers generating sequences with real-valued, continuous vectors instead of discrete tokens from a finite vocabulary. It applies the method mainly to image generation, panoptic segmentation, and depth estimation within a VAE framework, with no mention of discretized audio tokens or audio waveform discretization into token sequences for audio modeling or generation. Hence, it does not meet the inclusion criteria regarding discrete audio token usage or analysis for audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
    "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
    "metadata": {
      "arxiv_id": "2312.02147",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "authors": [
        "Sucheng Ren",
        "Zeyu Wang",
        "Hongru Zhu",
        "Junfei Xiao",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "published": "2023-12-04T18:59:20Z",
      "updated": "2024-07-05T05:07:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02147v2",
      "landing_url": "https://arxiv.org/abs/2312.02147v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.02147"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on image-GPT and visual representation learning with image/semantic tokens, not on discrete audio tokens or audio tokenization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on image-GPT and visual representation learning with image/semantic tokens, not on discrete audio tokens or audio tokenization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on visual representation learning using image-GPT and semantic token prediction for images, without addressing discrete audio tokens, neural audio codecs, or any form of discretized audio representation as defined in the inclusion criteria. It concerns discrete tokens of visual data rather than audio data, thus not meeting the thematic requirements for discrete audio tokens or related audio tokenization methods and applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on visual representation learning using image-GPT and semantic token prediction for images, without addressing discrete audio tokens, neural audio codecs, or any form of discretized audio representation as defined in the inclusion criteria. It concerns discrete tokens of visual data rather than audio data, thus not meeting the thematic requirements for discrete audio tokens or related audio tokenization methods and applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
    "abstract": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
    "metadata": {
      "arxiv_id": "2312.03406",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "authors": [
        "Chao Chen",
        "Tian Zhou",
        "Yanjun Zhao",
        "Hui Liu",
        "Liang Sun",
        "Rong Jin"
      ],
      "published": "2023-12-06T10:42:40Z",
      "updated": "2025-05-18T09:11:15Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03406v4",
      "landing_url": "https://arxiv.org/abs/2312.03406v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.03406"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on differentiable vector quantization for spatio-temporal forecasting, not on discrete audio tokens or tokenization/design of audio representations, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and fails the domain-specific requirements.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on differentiable vector quantization for spatio-temporal forecasting, not on discrete audio tokens or tokenization/design of audio representations, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and fails the domain-specific requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization methods for spatio-temporal forecasting and does not address audio data or the generation of discrete audio tokens from continuous audio waveforms. It does not discuss neural audio codecs, audio tokenization, or any discrete audio token design or evaluation related to audio tasks (such as speech, music, or environment sounds). Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and falls under exclusion criteria regarding research topic and lack of discrete audio token details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization methods for spatio-temporal forecasting and does not address audio data or the generation of discrete audio tokens from continuous audio waveforms. It does not discuss neural audio codecs, audio tokenization, or any discrete audio token design or evaluation related to audio tasks (such as speech, music, or environment sounds). Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and falls under exclusion criteria regarding research topic and lack of discrete audio token details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Concatenative Singing Voice Conversion: Rethinking Concatenation-Based Approach for One-Shot Singing Voice Conversion",
    "abstract": "Any-to-any singing voice conversion (SVC) is confronted with the challenge of ``timbre leakage'' issue caused by inadequate disentanglement between the content and the speaker timbre. To address this issue, this study introduces NeuCoSVC, a novel neural concatenative SVC framework. It consists of a self-supervised learning (SSL) representation extractor, a neural harmonic signal generator, and a waveform synthesizer. The SSL extractor condenses audio into fixed-dimensional SSL features, while the harmonic signal generator leverages linear time-varying filters to produce both raw and filtered harmonic signals for pitch information. The synthesizer reconstructs waveforms using SSL features, harmonic signals, and loudness information. During inference, voice conversion is performed by substituting source SSL features with their nearest counterparts from a matching pool which comprises SSL features extracted from the reference audio, while preserving raw harmonic signals and loudness from the source audio. By directly utilizing SSL features from the reference audio, the proposed framework effectively resolves the ``timbre leakage\" issue caused by previous disentanglement-based approaches. Experimental results demonstrate that the proposed NeuCoSVC system outperforms the disentanglement-based speaker embedding approach in one-shot SVC across intra-language, cross-language, and cross-domain evaluations.",
    "metadata": {
      "arxiv_id": "2312.04919",
      "title": "Neural Concatenative Singing Voice Conversion: Rethinking Concatenation-Based Approach for One-Shot Singing Voice Conversion",
      "summary": "Any-to-any singing voice conversion (SVC) is confronted with the challenge of ``timbre leakage'' issue caused by inadequate disentanglement between the content and the speaker timbre. To address this issue, this study introduces NeuCoSVC, a novel neural concatenative SVC framework. It consists of a self-supervised learning (SSL) representation extractor, a neural harmonic signal generator, and a waveform synthesizer. The SSL extractor condenses audio into fixed-dimensional SSL features, while the harmonic signal generator leverages linear time-varying filters to produce both raw and filtered harmonic signals for pitch information. The synthesizer reconstructs waveforms using SSL features, harmonic signals, and loudness information. During inference, voice conversion is performed by substituting source SSL features with their nearest counterparts from a matching pool which comprises SSL features extracted from the reference audio, while preserving raw harmonic signals and loudness from the source audio. By directly utilizing SSL features from the reference audio, the proposed framework effectively resolves the ``timbre leakage\" issue caused by previous disentanglement-based approaches. Experimental results demonstrate that the proposed NeuCoSVC system outperforms the disentanglement-based speaker embedding approach in one-shot SVC across intra-language, cross-language, and cross-domain evaluations.",
      "authors": [
        "Binzhu Sha",
        "Xu Li",
        "Zhiyong Wu",
        "Ying Shan",
        "Helen Meng"
      ],
      "published": "2023-12-08T09:35:08Z",
      "updated": "2024-01-08T13:36:36Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04919v2",
      "landing_url": "https://arxiv.org/abs/2312.04919v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.04919"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a neural concatenative SVC framework using SSL features and harmonic signal generation, with no mention of discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural concatenative SVC framework using SSL features and harmonic signal generation, with no mention of discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural concatenative singing voice conversion framework that utilizes self-supervised learning (SSL) features to perform voice conversion. However, the abstract does not explicitly describe the discretization process, tokenization, or the formation of discrete audio tokens, nor does it discuss vocabularies, codebooks, or quantization details that would satisfy the discrete audio token inclusion criteria. Therefore, it primarily deals with continuous audio feature representations rather than discrete token sequences, which does not meet the core inclusion requirements.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural concatenative singing voice conversion framework that utilizes self-supervised learning (SSL) features to perform voice conversion. However, the abstract does not explicitly describe the discretization process, tokenization, or the formation of discrete audio tokens, nor does it discuss vocabularies, codebooks, or quantization details that would satisfy the discrete audio token inclusion criteria. Therefore, it primarily deals with continuous audio feature representations rather than discrete token sequences, which does not meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable k-Means",
    "abstract": "Compressing large neural networks with minimal performance loss is crucial to enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight quantization method that uses an attention-based clustering algorithm called differentiable $k$-means (DKM). Despite achieving state-of-the-art results, DKM's performance is constrained by its heavy memory dependency. We propose an implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$ be the number of weight-vectors, and $b$ be the number of bits per cluster address. IDKM reduces the overall memory complexity of a single $k$-means layer from $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for which the time complexity of the gradient calculation is independent of $t$ as well. We provide a proof of concept of our methods by showing that, under the same settings, IDKM achieves comparable performance to DKM with less compute time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural network, Resnet18, on hardware where DKM cannot train at all.",
    "metadata": {
      "arxiv_id": "2312.07759",
      "title": "IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable k-Means",
      "summary": "Compressing large neural networks with minimal performance loss is crucial to enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight quantization method that uses an attention-based clustering algorithm called differentiable $k$-means (DKM). Despite achieving state-of-the-art results, DKM's performance is constrained by its heavy memory dependency. We propose an implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$ be the number of weight-vectors, and $b$ be the number of bits per cluster address. IDKM reduces the overall memory complexity of a single $k$-means layer from $\\mathcal{O}(t \\cdot m \\cdot 2^b)$ to $\\mathcal{O}( m \\cdot 2^b)$. We also introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for which the time complexity of the gradient calculation is independent of $t$ as well. We provide a proof of concept of our methods by showing that, under the same settings, IDKM achieves comparable performance to DKM with less compute time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural network, Resnet18, on hardware where DKM cannot train at all.",
      "authors": [
        "Sean Jaffe",
        "Ambuj K. Singh",
        "Francesco Bullo"
      ],
      "published": "2023-12-12T22:02:57Z",
      "updated": "2023-12-15T21:46:10Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07759v2",
      "landing_url": "https://arxiv.org/abs/2312.07759v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.07759"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The input article addresses memory-efficient neural network quantization via implicit differentiable k-means and does not discuss discrete audio tokens, neural audio codecs, or tokenization for audio; it does not meet the inclusion criteria and aligns with the exclusion criteria (no discrete audio token focus).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The input article addresses memory-efficient neural network quantization via implicit differentiable k-means and does not discuss discrete audio tokens, neural audio codecs, or tokenization for audio; it does not meet the inclusion criteria and aligns with the exclusion criteria (no discrete audio token focus).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a neural network quantization method for compressing weights to reduce memory usage and computational cost, without mentioning any discrete audio tokens, tokenization of audio waveforms, or applications related to audio generation, understanding, or cross-modal modeling. It does not provide details on discrete token generation processes or address any audio-specific token design or evaluation aspects as required by the inclusion criteria, nor does it relate to audio domain or discrete audio tokens usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a neural network quantization method for compressing weights to reduce memory usage and computational cost, without mentioning any discrete audio tokens, tokenization of audio waveforms, or applications related to audio generation, understanding, or cross-modal modeling. It does not provide details on discrete token generation processes or address any audio-specific token design or evaluation aspects as required by the inclusion criteria, nor does it relate to audio domain or discrete audio tokens usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
    "abstract": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
    "metadata": {
      "arxiv_id": "2312.08309",
      "title": "FASTEN: Towards a FAult-tolerant and STorage EfficieNt Cloud: Balancing Between Replication and Deduplication",
      "summary": "With the surge in cloud storage adoption, enterprises face challenges managing data duplication and exponential data growth. Deduplication mitigates redundancy, yet maintaining redundancy ensures high availability, incurring storage costs. Balancing these aspects is a significant research concern. We propose FASTEN, a distributed cloud storage scheme ensuring efficiency, security, and high availability. FASTEN achieves fault tolerance by dispersing data subsets optimally across servers and maintains redundancy for high availability. Experimental results show FASTEN's effectiveness in fault tolerance, cost reduction, batch auditing, and file and block-level deduplication. It outperforms existing systems with low time complexity, strong fault tolerance, and commendable deduplication performance.",
      "authors": [
        "Sabbir Ahmed",
        "Md Nahiduzzaman",
        "Tariqul Islam",
        "Faisal Haque Bappy",
        "Tarannum Shaila Zaman",
        "Raiful Hasan"
      ],
      "published": "2023-12-13T17:27:17Z",
      "updated": "2023-12-13T17:27:17Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08309v1",
      "landing_url": "https://arxiv.org/abs/2312.08309v1",
      "doi": "https://doi.org/10.1109/CCNC51664.2024.10454894"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about fault-tolerant cloud storage and deduplication; it does not involve discrete audio tokens or tokenization, hence doesn't meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about fault-tolerant cloud storage and deduplication; it does not involve discrete audio tokens or tokenization, hence doesn't meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cloud storage systems balancing replication and deduplication for fault tolerance and efficiency, without discussing discrete audio tokens, neural audio codecs, or any discretization of continuous audio waveforms into token sequences; thus, it does not meet the inclusion criteria and falls under the exclusion criteria of non-audio-token related research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cloud storage systems balancing replication and deduplication for fault tolerance and efficiency, without discussing discrete audio tokens, neural audio codecs, or any discretization of continuous audio waveforms into token sequences; thus, it does not meet the inclusion criteria and falls under the exclusion criteria of non-audio-token related research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses HuBERT semantic tokens (discrete tokens) as the core representation for zero-shot voice conversion, reconstructing waveforms from token sequences and employing token-based modeling, which aligns with the discrete audio token theme and provides methodological details in English.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses HuBERT semantic tokens (discrete tokens) as the core representation for zero-shot voice conversion, reconstructing waveforms from token sequences and employing token-based modeling, which aligns with the discrete audio token theme and provides methodological details in English.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a zero-shot voice conversion model that reconstructs waveform from HuBERT semantic tokens, implying the use of discrete audio tokens derived from a self-supervised speech model (HuBERT), which aligns with the inclusion criteria of leveraging discrete tokens in audio generation; however, the abstract lacks explicit details on the tokenization process, vocabulary/token set, or quantization strategy, making it unclear if the token design and reproducibility criteria are fully met.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a zero-shot voice conversion model that reconstructs waveform from HuBERT semantic tokens, implying the use of discrete audio tokens derived from a self-supervised speech model (HuBERT), which aligns with the inclusion criteria of leveraging discrete tokens in audio generation; however, the abstract lacks explicit details on the tokenization process, vocabulary/token set, or quantization strategy, making it unclear if the token design and reproducibility criteria are fully met.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly states waveform reconstruction from \"HuBERT semantic tokens,\" which indicates the use of discrete units derived from a self-supervised speech model and that these tokens are used as the core representation for generation—this satisfies the inclusion group criterion for self-supervised derived discrete tokens and application to audio generation. However, the abstract does not describe how those tokens are produced (tokenizer/quantization/vocabulary/codebook, rates or downsampling), so I cannot confirm the English reproducibility requirement that the paper clearly describes token generation details; the two junior reviews reflect this split (one leaned to include, the other was unsure). Therefore inclusion is uncertain based on the information given. 1-sentence explanation: The paper appears to use HuBERT-derived discrete semantic tokens for zero-shot voice conversion (matching the topic) but the abstract lacks explicit tokenizer/quantization details needed to verify full compliance with the reproducibility/detail criteria, so inclusion is unclear.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly states waveform reconstruction from \"HuBERT semantic tokens,\" which indicates the use of discrete units derived from a self-supervised speech model and that these tokens are used as the core representation for generation—this satisfies the inclusion group criterion for self-supervised derived discrete tokens and application to audio generation. However, the abstract does not describe how those tokens are produced (tokenizer/quantization/vocabulary/codebook, rates or downsampling), so I cannot confirm the English reproducibility requirement that the paper clearly describes token generation details; the two junior reviews reflect this split (one leaned to include, the other was unsure). Therefore inclusion is uncertain based on the information given. 1-sentence explanation: The paper appears to use HuBERT-derived discrete semantic tokens for zero-shot voice conversion (matching the topic) but the abstract lacks explicit tokenizer/quantization details needed to verify full compliance with the reproducibility/detail criteria, so inclusion is unclear.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Tokenize Anything via Prompting",
    "abstract": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
    "metadata": {
      "arxiv_id": "2312.09128",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "authors": [
        "Ting Pan",
        "Lulu Tang",
        "Xinlong Wang",
        "Shiguang Shan"
      ],
      "published": "2023-12-14T17:01:02Z",
      "updated": "2024-07-17T04:34:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09128v2",
      "landing_url": "https://arxiv.org/abs/2312.09128v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vision tokens and promptable region representations for images rather than discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' topic criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vision tokens and promptable region representations for images rather than discrete audio tokens, so it does not meet the 'Discrete Audio Tokens' topic criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a promptable image tokenizer model for segmenting, recognizing, and captioning visual regions, without any mention of audio tokenization, discrete audio tokens, or audio waveform discretization; thus, it does not meet the inclusion criteria related to discrete audio tokens and is unrelated to audio processing.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a promptable image tokenizer model for segmenting, recognizing, and captioning visual regions, without any mention of audio tokenization, discrete audio tokens, or audio waveform discretization; thus, it does not meet the inclusion criteria related to discrete audio tokens and is unrelated to audio processing.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
    "abstract": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
    "metadata": {
      "arxiv_id": "2312.09469",
      "title": "Clinical Text Deduplication Practices for Efficient Pretraining and Improved Clinical Tasks",
      "summary": "Despite being a unique source of information on patients' status and disease progression, clinical notes are characterized by high levels of duplication and information redundancy. In general domain text, it has been shown that deduplication does not harm language model (LM) pretraining, thus helping reduce the training cost. Although large LMs have proven to learn medical knowledge, they still require specialized domain adaptation for improved downstream clinical tasks. By leveraging large real-world clinical corpora, we first provided a fine-grained characterization of duplicates stemming from common writing practices and clinical relevancy. Second, we demonstrated that deduplicating clinical text can help clinical LMs encode less redundant information in a more efficient manner and do not harm classification tasks via prompt-based learning.",
      "authors": [
        "Isotta Landi",
        "Eugenia Alleva",
        "Alissa A. Valentine",
        "Lauren A. Lepow",
        "Alexander W. Charney"
      ],
      "published": "2023-09-29T18:35:52Z",
      "updated": "2023-09-29T18:35:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09469v1",
      "landing_url": "https://arxiv.org/abs/2312.09469v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09469"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe deduplication of clinical text for LM pretraining, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe deduplication of clinical text for LM pretraining, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deduplication of clinical text for language model pretraining in the medical domain, with no mention or indication of discrete audio tokens, neural audio codecs, or any form of discrete tokenization of audio data—which is the core inclusion criteria. The topic is irrelevant to audio token discretization and sequence modeling of audio waveforms, hence does not meet any of the inclusion criteria and falls under exclusion due to subject mismatch.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deduplication of clinical text for language model pretraining in the medical domain, with no mention or indication of discrete audio tokens, neural audio codecs, or any form of discrete tokenization of audio data—which is the core inclusion criteria. The topic is irrelevant to audio token discretization and sequence modeling of audio waveforms, hence does not meet any of the inclusion criteria and falls under exclusion due to subject mismatch.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LiteVSR: Efficient Visual Speech Recognition by Learning from Speech Representations of Unlabeled Data",
    "abstract": "This paper proposes a novel, resource-efficient approach to Visual Speech Recognition (VSR) leveraging speech representations produced by any trained Automatic Speech Recognition (ASR) model. Moving away from the resource-intensive trends prevalent in recent literature, our method distills knowledge from a trained Conformer-based ASR model, achieving competitive performance on standard VSR benchmarks with significantly less resource utilization. Using unlabeled audio-visual data only, our baseline model achieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test benchmarks, respectively. After fine-tuning the model with limited labeled data, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can be trained on a single consumer-grade GPU within a few days and is capable of performing real-time end-to-end VSR on dated hardware, suggesting a path towards more accessible and resource-efficient VSR methodologies.",
    "metadata": {
      "arxiv_id": "2312.09727",
      "title": "LiteVSR: Efficient Visual Speech Recognition by Learning from Speech Representations of Unlabeled Data",
      "summary": "This paper proposes a novel, resource-efficient approach to Visual Speech Recognition (VSR) leveraging speech representations produced by any trained Automatic Speech Recognition (ASR) model. Moving away from the resource-intensive trends prevalent in recent literature, our method distills knowledge from a trained Conformer-based ASR model, achieving competitive performance on standard VSR benchmarks with significantly less resource utilization. Using unlabeled audio-visual data only, our baseline model achieves a word error rate (WER) of 47.4% and 54.7% on the LRS2 and LRS3 test benchmarks, respectively. After fine-tuning the model with limited labeled data, the word error rate reduces to 35% (LRS2) and 45.7% (LRS3). Our model can be trained on a single consumer-grade GPU within a few days and is capable of performing real-time end-to-end VSR on dated hardware, suggesting a path towards more accessible and resource-efficient VSR methodologies.",
      "authors": [
        "Hendrik Laux",
        "Emil Mededovic",
        "Ahmed Hallawa",
        "Lukas Martin",
        "Arne Peine",
        "Anke Schmeink"
      ],
      "published": "2023-12-15T12:04:24Z",
      "updated": "2023-12-15T12:04:24Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09727v1",
      "landing_url": "https://arxiv.org/abs/2312.09727v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.09727"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on visual speech recognition using ASR representations but does not discuss discrete audio tokens, tokenization methods, or token-based modeling, so it does not satisfy the required token/discrete-audio criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on visual speech recognition using ASR representations but does not discuss discrete audio tokens, tokenization methods, or token-based modeling, so it does not satisfy the required token/discrete-audio criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on visual speech recognition by distilling knowledge from an ASR model without detailing any discrete audio tokenization process or providing specific information about discrete token design, vocabulary, or quantization methods, thus not meeting the essential inclusion criteria requiring explicit discrete audio token representation and its design impact; it rather works with continuous or representation-based speech recognition models.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on visual speech recognition by distilling knowledge from an ASR model without detailing any discrete audio tokenization process or providing specific information about discrete token design, vocabulary, or quantization methods, thus not meeting the essential inclusion criteria requiring explicit discrete audio token representation and its design impact; it rather works with continuous or representation-based speech recognition models.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
    "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
    "metadata": {
      "arxiv_id": "2312.09747",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "YuanJun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "published": "2023-12-15T12:36:05Z",
      "updated": "2024-01-07T09:02:52Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09747v2",
      "landing_url": "https://arxiv.org/abs/2312.09747v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09747"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly uses discrete audio tokens generated from SSL representations and a tokenizer, models them with language models, and reconstructs enhanced speech, thereby satisfying the discrete-token criterion and downstream modeling goal without obvious exclusionary issues.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly uses discrete audio tokens generated from SSL representations and a tokenizer, models them with language models, and reconstructs enhanced speech, thereby satisfying the discrete-token criterion and downstream modeling goal without obvious exclusionary issues.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly involves transforming continuous speech waveforms into discrete tokens using self-supervised learning and k-means tokenization, aligning with the inclusion criteria of discrete audio tokens produced by neural audio encoders or self-supervised models. It further uses these tokens as input for language models to perform speech enhancement, addressing downstream generation tasks with empirical evaluation, meeting the inclusion requirements. The abstract provides sufficient methodological detail in English, enabling assessability. There are no indicators in the abstract suggesting exclusion criteria such as focus on continuous audio representations or lack of discrete token use.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly involves transforming continuous speech waveforms into discrete tokens using self-supervised learning and k-means tokenization, aligning with the inclusion criteria of discrete audio tokens produced by neural audio encoders or self-supervised models. It further uses these tokens as input for language models to perform speech enhancement, addressing downstream generation tasks with empirical evaluation, meeting the inclusion requirements. The abstract provides sufficient methodological detail in English, enabling assessability. There are no indicators in the abstract suggesting exclusion criteria such as focus on continuous audio representations or lack of discrete token use.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
    "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
    "metadata": {
      "arxiv_id": "2312.09911",
      "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
      "summary": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
      "authors": [
        "Xueyao Zhang",
        "Liumeng Xue",
        "Yicheng Gu",
        "Yuancheng Wang",
        "Jiaqi Li",
        "Haorui He",
        "Chaoren Wang",
        "Songting Liu",
        "Xi Chen",
        "Junan Zhang",
        "Zihao Fang",
        "Haopeng Chen",
        "Tze Ying Tang",
        "Lexiao Zou",
        "Mingxuan Wang",
        "Jun Han",
        "Kai Chen",
        "Haizhou Li",
        "Zhizheng Wu"
      ],
      "published": "2023-12-15T16:23:21Z",
      "updated": "2024-09-16T11:35:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09911v3",
      "landing_url": "https://arxiv.org/abs/2312.09911v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.09911"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The Amphion paper is a high-level toolkit overview about Audio/Music/Speech generation and preprocessing; it does not discuss discrete audio tokens, tokenization architectures, or token-based representations required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The Amphion paper is a high-level toolkit overview about Audio/Music/Speech generation and preprocessing; it does not discuss discrete audio tokens, tokenization architectures, or token-based representations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces Amphion, an open-source toolkit for audio, music, and speech generation with beginner-friendly workflows and pre-trained models supporting tasks like TTS, TTA, and Singing Voice Conversion. However, the abstract does not provide any details on the generation or use of discrete audio tokens, their neural audio codec architectures, quantization methods, codebook configurations, or any discrete token-based modeling or evaluation. Thus, it does not meet the inclusion criteria requiring explicit description and use of discrete audio tokens as core representations for generation or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces Amphion, an open-source toolkit for audio, music, and speech generation with beginner-friendly workflows and pre-trained models supporting tasks like TTS, TTA, and Singing Voice Conversion. However, the abstract does not provide any details on the generation or use of discrete audio tokens, their neural audio codec architectures, quantization methods, codebook configurations, or any discrete token-based modeling or evaluation. Thus, it does not meet the inclusion criteria requiring explicit description and use of discrete audio tokens as core representations for generation or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
    "abstract": "This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clovaai/TVQ-VAE.",
    "metadata": {
      "arxiv_id": "2312.11532",
      "title": "Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided Document Generation",
      "summary": "This paper introduces a novel approach for topic modeling utilizing latent codebooks from Vector-Quantized Variational Auto-Encoder~(VQ-VAE), discretely encapsulating the rich information of the pre-trained embeddings such as the pre-trained language model. From the novel interpretation of the latent codebooks and embeddings as conceptual bag-of-words, we propose a new generative topic model called Topic-VQ-VAE~(TVQ-VAE) which inversely generates the original documents related to the respective latent codebook. The TVQ-VAE can visualize the topics with various generative distributions including the traditional BoW distribution and the autoregressive image generation. Our experimental results on document analysis and image generation demonstrate that TVQ-VAE effectively captures the topic context which reveals the underlying structures of the dataset and supports flexible forms of document generation. Official implementation of the proposed TVQ-VAE is available at https://github.com/clovaai/TVQ-VAE.",
      "authors": [
        "YoungJoon Yoo",
        "Jongwon Choi"
      ],
      "published": "2023-12-15T15:01:10Z",
      "updated": "2024-01-21T09:30:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.11532v2",
      "landing_url": "https://arxiv.org/abs/2312.11532v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.11532"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on topic modeling for documents via latent codebooks from VQ-VAE and does not involve discrete audio tokens or audio-focused tokenization, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on topic modeling for documents via latent codebooks from VQ-VAE and does not involve discrete audio tokens or audio-focused tokenization, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on topic modeling and document generation using discrete latent codebooks from VQ-VAE, which relates to latent discrete representations, but entirely in the textual/document domain rather than audio waveforms or audio tokens. It does not discuss discretization of continuous audio signals, neural audio codecs, or discrete audio tokens for audio generation or understanding, thus not meeting the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on topic modeling and document generation using discrete latent codebooks from VQ-VAE, which relates to latent discrete representations, but entirely in the textual/document domain rather than audio waveforms or audio tokens. It does not discuss discretization of continuous audio signals, neural audio codecs, or discrete audio tokens for audio generation or understanding, thus not meeting the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based Pre-training for Expressive Audiobook Speech Synthesis",
    "abstract": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios.",
    "metadata": {
      "arxiv_id": "2312.12181",
      "title": "StyleSpeech: Self-supervised Style Enhancing with VQ-VAE-based Pre-training for Expressive Audiobook Speech Synthesis",
      "summary": "The expressive quality of synthesized speech for audiobooks is limited by generalized model architecture and unbalanced style distribution in the training data. To address these issues, in this paper, we propose a self-supervised style enhancing method with VQ-VAE-based pre-training for expressive audiobook speech synthesis. Firstly, a text style encoder is pre-trained with a large amount of unlabeled text-only data. Secondly, a spectrogram style extractor based on VQ-VAE is pre-trained in a self-supervised manner, with plenty of audio data that covers complex style variations. Then a novel architecture with two encoder-decoder paths is specially designed to model the pronunciation and high-level style expressiveness respectively, with the guidance of the style extractor. Both objective and subjective evaluations demonstrate that our proposed method can effectively improve the naturalness and expressiveness of the synthesized speech in audiobook synthesis especially for the role and out-of-domain scenarios.",
      "authors": [
        "Xueyuan Chen",
        "Xi Wang",
        "Shaofei Zhang",
        "Lei He",
        "Zhiyong Wu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2023-12-19T14:13:26Z",
      "updated": "2023-12-19T14:13:26Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12181v1",
      "landing_url": "https://arxiv.org/abs/2312.12181v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.12181"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract mentions VQ-VAE-based pre-training implying discrete latent codes, but the work centers on expressive audiobook synthesis and style modeling without explicitly treating discrete tokens or tokenizers as the core representation or providing token-level evaluation, making its inclusion under the discrete audio tokens criteria uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "Abstract mentions VQ-VAE-based pre-training implying discrete latent codes, but the work centers on expressive audiobook synthesis and style modeling without explicitly treating discrete tokens or tokenizers as the core representation or providing token-level evaluation, making its inclusion under the discrete audio tokens criteria uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving expressive audiobook speech synthesis using a VQ-VAE based self-supervised style enhancing method; although VQ-VAE involves vector quantization which could imply discrete tokens, the abstract does not explicitly describe the generation or use of discrete audio tokens as core representations, nor does it detail the tokenization process, token vocabulary, or evaluation related to discrete token design and application as defined in the inclusion criteria. Therefore, it is unclear if the study fully meets the requirement of using discrete audio tokens as a core modeling element.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving expressive audiobook speech synthesis using a VQ-VAE based self-supervised style enhancing method; although VQ-VAE involves vector quantization which could imply discrete tokens, the abstract does not explicitly describe the generation or use of discrete audio tokens as core representations, nor does it detail the tokenization process, token vocabulary, or evaluation related to discrete token design and application as defined in the inclusion criteria. Therefore, it is unclear if the study fully meets the requirement of using discrete audio tokens as a core modeling element.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses a VQ-VAE-based component (which typically yields discrete codes) but flagged insufficient evidence in the abstract that discrete token sequences, tokenizer/codebook settings, or token-level usage are explicitly defined or used as the core representation for downstream sequence modeling; the title/abstract emphasize style extraction and expressive TTS improvements rather than presenting design details or evaluation of discrete audio tokens, so inclusion is uncertain based on the provided information.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses a VQ-VAE-based component (which typically yields discrete codes) but flagged insufficient evidence in the abstract that discrete token sequences, tokenizer/codebook settings, or token-level usage are explicitly defined or used as the core representation for downstream sequence modeling; the title/abstract emphasize style extraction and expressive TTS improvements rather than presenting design details or evaluation of discrete audio tokens, so inclusion is uncertain based on the provided information.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Sign Language Production with Latent Motion Transformer",
    "abstract": "Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we've developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video's hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL.",
    "metadata": {
      "arxiv_id": "2312.12917",
      "title": "Sign Language Production with Latent Motion Transformer",
      "summary": "Sign Language Production (SLP) is the tough task of turning sign language into sign videos. The main goal of SLP is to create these videos using a sign gloss. In this research, we've developed a new method to make high-quality sign videos without using human poses as a middle step. Our model works in two main parts: first, it learns from a generator and the video's hidden features, and next, it uses another model to understand the order of these hidden features. To make this method even better for sign videos, we make several significant improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn downsampled latent representations. (ii) In the second stage, we introduce sequence-to-sequence attention to better leverage conditional information. (iii) The separated two-stage training discards the realistic visual semantic of the latent codes in the second stage. To endow the latent sequences semantic information, we extend the token-level autoregressive latent codes learning with perceptual loss and reconstruction loss for the prior model with visual perception. Compared with previous state-of-the-art approaches, our model performs consistently better on two word-level sign language datasets, i.e., WLASL and NMFs-CSL.",
      "authors": [
        "Pan Xie",
        "Taiyi Peng",
        "Yao Du",
        "Qipeng Zhang"
      ],
      "published": "2023-12-20T10:53:06Z",
      "updated": "2023-12-20T10:53:06Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.12917v1",
      "landing_url": "https://arxiv.org/abs/2312.12917v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.12917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on sign language video generation and uses discrete latent tokens from a 3D VQ-GAN, which are visual/video tokens rather than discrete audio tokens; it does not meet the 'Discrete Audio Tokens' inclusion criteria (no audio token design, generation, or audio-token-based modeling).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on sign language video generation and uses discrete latent tokens from a 3D VQ-GAN, which are visual/video tokens rather than discrete audio tokens; it does not meet the 'Discrete Audio Tokens' inclusion criteria (no audio token design, generation, or audio-token-based modeling).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language video synthesis using a latent motion transformer and improved 3D VQ-GAN to generate high-quality sign language videos; however, it does not address discrete audio tokens or audio discretization techniques involving neural audio encoders, vector quantization for audio signals, or token sequences derived from audio waveforms, and thus does not meet the inclusion criteria related to discrete audio token methodology.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language video synthesis using a latent motion transformer and improved 3D VQ-GAN to generate high-quality sign language videos; however, it does not address discrete audio tokens or audio discretization techniques involving neural audio encoders, vector quantization for audio signals, or token sequences derived from audio waveforms, and thus does not meet the inclusion criteria related to discrete audio token methodology.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Single-channel speech enhancement using learnable loss mixup",
    "abstract": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
    "metadata": {
      "arxiv_id": "2312.17255",
      "title": "Single-channel speech enhancement using learnable loss mixup",
      "summary": "Generalization remains a major problem in supervised learning of single-channel speech enhancement. In this work, we propose learnable loss mixup (LLM), a simple and effortless training diagram, to improve the generalization of deep learning-based speech enhancement models. Loss mixup, of which learnable loss mixup is a special variant, optimizes a mixture of the loss functions of random sample pairs to train a model on virtual training data constructed from these pairs of samples. In learnable loss mixup, by conditioning on the mixed data, the loss functions are mixed using a non-linear mixing function automatically learned via neural parameterization. Our experimental results on the VCTK benchmark show that learnable loss mixup achieves 3.26 PESQ, outperforming the state-of-the-art.",
      "authors": [
        "Oscar Chang",
        "Dung N. Tran",
        "Kazuhito Koishida"
      ],
      "published": "2023-12-20T00:25:55Z",
      "updated": "2023-12-20T00:25:55Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.17255v1",
      "landing_url": "https://arxiv.org/abs/2312.17255v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.17255"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a training technique for generalization in single-channel speech enhancement (learnable loss mixup) with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a training technique for generalization in single-channel speech enhancement (learnable loss mixup) with no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving generalization in single-channel speech enhancement using a novel loss function mixup, but it does not mention the use of discrete audio tokens or any discretization process mapping continuous audio waveforms to token sequences. It also lacks details on tokenization, codecs, or vocabulary settings related to discrete audio tokens, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving generalization in single-channel speech enhancement using a novel loss function mixup, but it does not mention the use of discrete audio tokens or any discretization process mapping continuous audio waveforms to token sequences. It also lacks details on tokenization, codecs, or vocabulary settings related to discrete audio tokens, which are core to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio token representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
    "abstract": "Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances codebook usage and improves reconstruction performance. We also validated HQ-VAE in terms of its applicability to a different modality with an audio dataset.",
    "metadata": {
      "arxiv_id": "2401.00365",
      "title": "HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes",
      "summary": "Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on image datasets show that HQ-VAE enhances codebook usage and improves reconstruction performance. We also validated HQ-VAE in terms of its applicability to a different modality with an audio dataset.",
      "authors": [
        "Yuhta Takida",
        "Yukara Ikemiya",
        "Takashi Shibuya",
        "Kazuki Shimada",
        "Woosung Choi",
        "Chieh-Hsin Lai",
        "Naoki Murata",
        "Toshimitsu Uesaka",
        "Kengo Uchida",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2023-12-31T01:39:38Z",
      "updated": "2024-03-28T06:38:55Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.00365v2",
      "landing_url": "https://arxiv.org/abs/2401.00365v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.00365"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a hierarchical discrete representation learning framework (HQ-VAE) based on vector quantization with discrete codebooks, and it explicitly notes applicability to an audio dataset, aligning with discrete audio token generation/representation and downstream modeling considerations.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item discusses a hierarchical discrete representation learning framework (HQ-VAE) based on vector quantization with discrete codebooks, and it explicitly notes applicability to an audio dataset, aligning with discrete audio token generation/representation and downstream modeling considerations.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes HQ-VAE, a hierarchical discrete representation learning method based on variational Bayes, and demonstrates its applicability on image and audio datasets; however, the abstract lacks clear details on discrete audio token generation mechanisms, such as codebook/tokenizer settings, neural audio codec specifics, or explicit use of discrete tokens for audio modeling, which are core to the inclusion criteria focused on \"Discrete Audio Tokens\" in audio waveforms for downstream modeling and alignment tasks. Therefore, it does not sufficiently meet the inclusion criteria concerning discrete audio token design and usage in audio generation or understanding.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes HQ-VAE, a hierarchical discrete representation learning method based on variational Bayes, and demonstrates its applicability on image and audio datasets; however, the abstract lacks clear details on discrete audio token generation mechanisms, such as codebook/tokenizer settings, neural audio codec specifics, or explicit use of discrete tokens for audio modeling, which are core to the inclusion criteria focused on \"Discrete Audio Tokens\" in audio waveforms for downstream modeling and alignment tasks. Therefore, it does not sufficiently meet the inclusion criteria concerning discrete audio token design and usage in audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed: one noted the paper aligns with discrete representation learning and mentions audio applicability, while the other highlighted the abstract’s lack of concrete details about discrete audio token generation. The title/abstract indicate HQ-VAE learns hierarchical discrete representations (generalizing VQ-VAE variants) and was validated on an audio dataset, but the abstract does not describe critical information required by the inclusion criteria — e.g., tokenizer/codec architecture, quantization/codebook settings, time downsampling or bitrate, nor explicit use of discrete tokens as sequences for downstream modeling/alignments. Because the inclusion rules require clear, reproducible description of how discrete audio tokens are produced and used (or that the paper’s contribution is explicitly about discrete audio token design/use), the provided abstract is insufficient to confirm eligibility.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed: one noted the paper aligns with discrete representation learning and mentions audio applicability, while the other highlighted the abstract’s lack of concrete details about discrete audio token generation. The title/abstract indicate HQ-VAE learns hierarchical discrete representations (generalizing VQ-VAE variants) and was validated on an audio dataset, but the abstract does not describe critical information required by the inclusion criteria — e.g., tokenizer/codec architecture, quantization/codebook settings, time downsampling or bitrate, nor explicit use of discrete tokens as sequences for downstream modeling/alignments. Because the inclusion rules require clear, reproducible description of how discrete audio tokens are produced and used (or that the paper’s contribution is explicitly about discrete audio token design/use), the provided abstract is insufficient to confirm eligibility.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
    "abstract": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on a discrete semantic token approach for TTS, using a token transducer to predict tokens derived from neural embeddings and a non-autoregressive generator to synthesize speech, clearly addressing discrete audio tokens as core representation and including English abstract content.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on a discrete semantic token approach for TTS, using a token transducer to predict tokens derived from neural embeddings and a non-autoregressive generator to synthesize speech, clearly addressing discrete audio tokens as core representation and including English abstract content.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly uses discrete semantic tokens derived from self-supervised representation learning (wav2vec2.0 embeddings), applies a neural transducer for semantic token prediction in a TTS pipeline, and provides empirical evaluations on speech quality and controllability, all aligned with the inclusion criteria focusing on discrete audio tokens and their application in TTS models. The abstract also describes the token generation process (discrete semantic tokens) and its use in downstream generation tasks, satisfying requirements for reproducibility and analysis. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly uses discrete semantic tokens derived from self-supervised representation learning (wav2vec2.0 embeddings), applies a neural transducer for semantic token prediction in a TTS pipeline, and provides empirical evaluations on speech quality and controllability, all aligned with the inclusion criteria focusing on discrete audio tokens and their application in TTS models. The abstract also describes the token generation process (discrete semantic tokens) and its use in downstream generation tasks, satisfying requirements for reproducibility and analysis. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
    "abstract": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
    "metadata": {
      "arxiv_id": "2401.01755",
      "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
      "summary": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
      "authors": [
        "Muyang Du",
        "Chuan Liu",
        "Junjie Lai"
      ],
      "published": "2024-01-03T14:17:35Z",
      "updated": "2024-01-03T14:17:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01755v1",
      "landing_url": "https://arxiv.org/abs/2401.01755v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01755"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study targets incremental text-to-speech architecture and latency reduction, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not fulfill the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study targets incremental text-to-speech architecture and latency reduction, with no discussion of discrete audio tokens, tokenizers, or token-based representations; thus it does not fulfill the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a text-to-speech model (Incremental FastPitch) that incrementally produces high-quality Mel spectrogram chunks for speech synthesis. However, it does not mention discretization of audio into tokens, nor details about neural audio codecs, vector quantization, or discrete token vocabularies as core representations. The focus is on parallel and incremental speech synthesis using continuous representations (Mel spectrograms), without explicit use of discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens for modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a text-to-speech model (Incremental FastPitch) that incrementally produces high-quality Mel spectrogram chunks for speech synthesis. However, it does not mention discretization of audio into tokens, nor details about neural audio codecs, vector quantization, or discrete token vocabularies as core representations. The focus is on parallel and incremental speech synthesis using continuous representations (Mel spectrograms), without explicit use of discrete audio tokens. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens for modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pheme: Efficient and Conversational Speech Generation",
    "abstract": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
    "metadata": {
      "arxiv_id": "2401.02839",
      "title": "Pheme: Efficient and Conversational Speech Generation",
      "summary": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
      "authors": [
        "Paweł Budzianowski",
        "Taras Sereda",
        "Tomasz Cichy",
        "Ivan Vulić"
      ],
      "published": "2024-01-05T14:47:20Z",
      "updated": "2024-01-05T14:47:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02839v1",
      "landing_url": "https://arxiv.org/abs/2401.02839v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.02839"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on compact TTS models and distillation for real-time speech generation but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on compact TTS models and distillation for real-time speech generation but does not discuss discrete audio tokens, tokenization, or token-based modeling, so it does not meet the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract focuses on efficient and conversational speech generation models, mentioning neural audio codecs and teacher-student distillation. However, it does not explicitly describe the use or design of discrete audio tokens, such as vector quantization, multi-codebooks, or a tokenizer process, nor does it clarify how audio is discretized into token sequences for modeling. Without clear evidence of discrete token definition or usage as a core method, the paper does not fully meet the inclusion criteria for discrete audio tokens, though it is close to relevance. Therefore, inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract focuses on efficient and conversational speech generation models, mentioning neural audio codecs and teacher-student distillation. However, it does not explicitly describe the use or design of discrete audio tokens, such as vector quantization, multi-codebooks, or a tokenizer process, nor does it clarify how audio is discretized into token sequences for modeling. Without clear evidence of discrete token definition or usage as a core method, the paper does not fully meet the inclusion criteria for discrete audio tokens, though it is close to relevance. Therefore, inclusion is uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: JuniorNano recommended exclusion (1); JuniorMini was uncertain (3). The abstract describes compact, parallel conversational TTS and references hierarchical neural audio codecs and distillation but provides no explicit description or evidence of discrete audio-token generation (e.g., vector quantization/codebook settings, tokenizer design, token sequences or bitrate/downsampling) nor treats discrete tokens as the core representation, so it does not meet the inclusion criteria for the “Discrete Audio Tokens” topic.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: JuniorNano recommended exclusion (1); JuniorMini was uncertain (3). The abstract describes compact, parallel conversational TTS and references hierarchical neural audio codecs and distillation but provides no explicit description or evidence of discrete audio-token generation (e.g., vector quantization/codebook settings, tokenizer design, token sequences or bitrate/downsampling) nor treats discrete tokens as the core representation, so it does not meet the inclusion criteria for the “Discrete Audio Tokens” topic.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Learning-Augmented K-Means Clustering Using Dimensional Reduction",
    "abstract": "Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower cost results compared to running it without PCA. \"Principal component analysis (PCA) is the problem of fitting a low-dimensional affine subspace to a set of data points in a high-dimensional space. PCA is well-established in the literature and has become one of the most useful tools for data modeling, compression, and visualization.\"",
    "metadata": {
      "arxiv_id": "2401.03198",
      "title": "Learning-Augmented K-Means Clustering Using Dimensional Reduction",
      "summary": "Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower cost results compared to running it without PCA. \"Principal component analysis (PCA) is the problem of fitting a low-dimensional affine subspace to a set of data points in a high-dimensional space. PCA is well-established in the literature and has become one of the most useful tools for data modeling, compression, and visualization.\"",
      "authors": [
        "Issam K. O Jabari",
        "Shofiyah",
        "Pradiptya Kahvi S",
        "Novi Nur Putriwijaya",
        "Novanto Yudistira"
      ],
      "published": "2024-01-06T12:02:33Z",
      "updated": "2024-01-06T12:02:33Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03198v1",
      "landing_url": "https://arxiv.org/abs/2401.03198v1",
      "doi": "https://doi.org/10.1145/3626641.3627239"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss learning-augmented k-means with PCA; there is no discrete audio token generation or tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss learning-augmented k-means with PCA; there is no discrete audio token generation or tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving k-means clustering using dimensionality reduction techniques like PCA, with no mention of discrete audio tokens, neural audio codec-based discretization, or tokenization for audio generation or understanding; thus, it does not meet the inclusion criteria focused on discrete audio token representations for audio modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving k-means clustering using dimensionality reduction techniques like PCA, with no mention of discrete audio tokens, neural audio codec-based discretization, or tokenization for audio generation or understanding; thus, it does not meet the inclusion criteria focused on discrete audio token representations for audio modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation",
    "abstract": "Self-supervised speech pre-training methods have developed rapidly in recent years, which show to be very effective for many near-field single-channel speech tasks. However, far-field multichannel speech processing is suffering from the scarcity of labeled multichannel data and complex ambient noises. The efficacy of self-supervised learning for far-field multichannel and multi-modal speech processing has not been well explored. Considering that visual information helps to improve speech recognition performance in noisy scenes, in this work we propose a multichannel multi-modal speech self-supervised learning framework AV-wav2vec2, which utilizes video and multichannel audio data as inputs. First, we propose a multi-path structure to process multichannel audio streams and a visual stream in parallel, with intra- and inter-channel contrastive losses as training targets to fully exploit the spatiotemporal information in multichannel speech data. Second, based on contrastive learning, we use additional single-channel audio data, which is trained jointly to improve the performance of speech representation. Finally, we use a Chinese multichannel multi-modal dataset in real scenarios to validate the effectiveness of the proposed method on audio-visual speech recognition (AVSR), automatic speech recognition (ASR), visual speech recognition (VSR) and audio-visual speaker diarization (AVSD) tasks.",
    "metadata": {
      "arxiv_id": "2401.03468",
      "title": "Multichannel AV-wav2vec2: A Framework for Learning Multichannel Multi-Modal Speech Representation",
      "summary": "Self-supervised speech pre-training methods have developed rapidly in recent years, which show to be very effective for many near-field single-channel speech tasks. However, far-field multichannel speech processing is suffering from the scarcity of labeled multichannel data and complex ambient noises. The efficacy of self-supervised learning for far-field multichannel and multi-modal speech processing has not been well explored. Considering that visual information helps to improve speech recognition performance in noisy scenes, in this work we propose a multichannel multi-modal speech self-supervised learning framework AV-wav2vec2, which utilizes video and multichannel audio data as inputs. First, we propose a multi-path structure to process multichannel audio streams and a visual stream in parallel, with intra- and inter-channel contrastive losses as training targets to fully exploit the spatiotemporal information in multichannel speech data. Second, based on contrastive learning, we use additional single-channel audio data, which is trained jointly to improve the performance of speech representation. Finally, we use a Chinese multichannel multi-modal dataset in real scenarios to validate the effectiveness of the proposed method on audio-visual speech recognition (AVSR), automatic speech recognition (ASR), visual speech recognition (VSR) and audio-visual speaker diarization (AVSD) tasks.",
      "authors": [
        "Qiushi Zhu",
        "Jie Zhang",
        "Yu Gu",
        "Yuchen Hu",
        "Lirong Dai"
      ],
      "published": "2024-01-07T12:27:18Z",
      "updated": "2024-01-07T12:27:18Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03468v1",
      "landing_url": "https://arxiv.org/abs/2401.03468v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03468"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract presents multichannel AV speech self-supervised learning (AV-wav2vec2) with contrastive losses and multimodal tasks, but it does not describe discrete audio tokens, tokenizers, codebooks, or any discretization design—key criteria for inclusion are not met.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract presents multichannel AV speech self-supervised learning (AV-wav2vec2) with contrastive losses and multimodal tasks, but it does not describe discrete audio tokens, tokenizers, codebooks, or any discretization design—key criteria for inclusion are not met.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a self-supervised learning framework for multichannel multi-modal speech representation using audio and video inputs, focusing on speech recognition tasks; however, it does not mention any discretization process to convert continuous audio waveforms into discrete tokens, nor does it detail any tokenization, quantization, or vocabulary design relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria related to discrete audio token generation and usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a self-supervised learning framework for multichannel multi-modal speech representation using audio and video inputs, focusing on speech recognition tasks; however, it does not mention any discretization process to convert continuous audio waveforms into discrete tokens, nor does it detail any tokenization, quantization, or vocabulary design relevant to discrete audio tokens. Therefore, it does not meet the inclusion criteria related to discrete audio token generation and usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Classification of circular polarization Stokes profiles in a sunspot using k-means clustering",
    "abstract": "The magnetic and velocity fields in sunspots are highly structured on small spatial scales which are encoded in the Stokes profiles. Our aim is to identify Stokes profiles in a sunspot which exhibit spectral characteristics that deviate from those associated with the Evershed flow and their spatial distribution. We employ a k-means clustering routine to classify Stokes V spectra in the penumbra of a sunspot. 75% of the penumbral region is dominated by profiles comprising two, nearly anti-symmetric lobes, while 21% of the area is occupied by three-lobed profiles that represent the Evershed flow returning to the photosphere. 4% of the area is dominated by four profile groups - Group 1: three-lobed profiles in which both the rest and strong downflowing component have the same polarity as the sunspot and seen exclusively in the light bridge. Group 2: single, red-lobed profiles over an area of about 2% seen at the outer penumbra in discrete patches that possibly signify the downflowing leg of an Omega-loop. Group 3: three-lobed/highly asymmetric profiles, where the rest and strong downflowing component have a polarity opposite the sunspot. These occupy 1.4% of the penumbral area over conspicuous, elongated structures or isolated patches in the outer penumbra and penumbra-QS boundary. Group 4: three lobed-profiles, in which the rest component has the same polarity as the sunspot and a weaker, upflowing component with an opposite polarity. These profiles are located near the entrance of the light bridge and are found in only 0.12% of the penumbral area. These minority groups of profiles could be related to dynamic phenomena that could affect the overlying chromosphere. The simplicity and speed of k-means can be utilized to identify such anomalous profiles in larger data sets to ascertain their temporal evolution and the physical processes responsible for these inhomogeneities.",
    "metadata": {
      "arxiv_id": "2401.03908",
      "title": "Classification of circular polarization Stokes profiles in a sunspot using k-means clustering",
      "summary": "The magnetic and velocity fields in sunspots are highly structured on small spatial scales which are encoded in the Stokes profiles. Our aim is to identify Stokes profiles in a sunspot which exhibit spectral characteristics that deviate from those associated with the Evershed flow and their spatial distribution. We employ a k-means clustering routine to classify Stokes V spectra in the penumbra of a sunspot. 75% of the penumbral region is dominated by profiles comprising two, nearly anti-symmetric lobes, while 21% of the area is occupied by three-lobed profiles that represent the Evershed flow returning to the photosphere. 4% of the area is dominated by four profile groups - Group 1: three-lobed profiles in which both the rest and strong downflowing component have the same polarity as the sunspot and seen exclusively in the light bridge. Group 2: single, red-lobed profiles over an area of about 2% seen at the outer penumbra in discrete patches that possibly signify the downflowing leg of an Omega-loop. Group 3: three-lobed/highly asymmetric profiles, where the rest and strong downflowing component have a polarity opposite the sunspot. These occupy 1.4% of the penumbral area over conspicuous, elongated structures or isolated patches in the outer penumbra and penumbra-QS boundary. Group 4: three lobed-profiles, in which the rest component has the same polarity as the sunspot and a weaker, upflowing component with an opposite polarity. These profiles are located near the entrance of the light bridge and are found in only 0.12% of the penumbral area. These minority groups of profiles could be related to dynamic phenomena that could affect the overlying chromosphere. The simplicity and speed of k-means can be utilized to identify such anomalous profiles in larger data sets to ascertain their temporal evolution and the physical processes responsible for these inhomogeneities.",
      "authors": [
        "Rohan Eugene Louis",
        "Shibu K. Mathew",
        "A. Raja Bayanna"
      ],
      "published": "2024-01-08T14:10:43Z",
      "updated": "2024-01-08T14:10:43Z",
      "categories": [
        "astro-ph.SR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03908v1",
      "landing_url": "https://arxiv.org/abs/2401.03908v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03908"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about solar physics and classification of Stokes profiles using clustering, not about discrete audio tokens or token-based modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about solar physics and classification of Stokes profiles using clustering, not about discrete audio tokens or token-based modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analysis of Stokes profiles in sunspots using k-means clustering and does not involve discrete audio tokens, neural audio encoders/codecs, or any discretization of continuous audio waveforms into token sequences. It is centered on astrophysical spectral data and magnetic field analysis, which is outside the scope of the specified inclusion criteria focusing on discrete audio token generation and usage in audio generation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analysis of Stokes profiles in sunspots using k-means clustering and does not involve discrete audio tokens, neural audio encoders/codecs, or any discretization of continuous audio waveforms into token sequences. It is centered on astrophysical spectral data and magnetic field analysis, which is outside the scope of the specified inclusion criteria focusing on discrete audio token generation and usage in audio generation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
    "abstract": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
    "metadata": {
      "arxiv_id": "2401.04511",
      "title": "Zero Shot Audio to Audio Emotion Transfer With Speaker Disentanglement",
      "summary": "The problem of audio-to-audio (A2A) style transfer involves replacing the style features of the source audio with those from the target audio while preserving the content related attributes of the source audio. In this paper, we propose an efficient approach, termed as Zero-shot Emotion Style Transfer (ZEST), that allows the transfer of emotional content present in the given source audio with the one embedded in the target audio while retaining the speaker and speech content from the source. The proposed system builds upon decomposing speech into semantic tokens, speaker representations and emotion embeddings. Using these factors, we propose a framework to reconstruct the pitch contour of the given speech signal and train a decoder that reconstructs the speech signal. The model is trained using a self-supervision based reconstruction loss. During conversion, the emotion embedding is alone derived from the target audio, while rest of the factors are derived from the source audio. In our experiments, we show that, even without using parallel training data or labels from the source or target audio, we illustrate zero shot emotion transfer capabilities of the proposed ZEST model using objective and subjective quality evaluations.",
      "authors": [
        "Soumya Dutta",
        "Sriram Ganapathy"
      ],
      "published": "2024-01-09T12:10:04Z",
      "updated": "2024-01-09T12:10:04Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04511v1",
      "landing_url": "https://arxiv.org/abs/2401.04511v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04511"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it mentions 'semantic tokens' and a token-based decomposition of speech, the abstract does not specify a discrete tokenization/codec/token vocabulary or treat discrete audio tokens as the core representation, so it likely does not meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Although it mentions 'semantic tokens' and a token-based decomposition of speech, the abstract does not specify a discrete tokenization/codec/token vocabulary or treat discrete audio tokens as the core representation, so it likely does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses a zero-shot emotion style transfer system that decomposes speech into semantic tokens, speaker representations, and emotion embeddings. However, the abstract does not clarify whether the semantic tokens are discretized audio tokens produced via neural audio codec or self-supervised models, nor does it describe details about token quantization, vocabulary size, or discrete representation explicit enough to confirm the use of discrete audio tokens as defined in the inclusion criteria. Moreover, it does not specify design choices or experimental analysis related to discrete token representations, which makes it difficult to ascertain if the paper aligns with the core focus on discrete audio tokens for audio generation or cross-modal modeling. Therefore, due to insufficient evidence about discrete token usage and detailed description, the study likely does not meet all inclusion criteria and falls under exclusion reason of lacking clear discrete token detail.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study discusses a zero-shot emotion style transfer system that decomposes speech into semantic tokens, speaker representations, and emotion embeddings. However, the abstract does not clarify whether the semantic tokens are discretized audio tokens produced via neural audio codec or self-supervised models, nor does it describe details about token quantization, vocabulary size, or discrete representation explicit enough to confirm the use of discrete audio tokens as defined in the inclusion criteria. Moreover, it does not specify design choices or experimental analysis related to discrete token representations, which makes it difficult to ascertain if the paper aligns with the core focus on discrete audio tokens for audio generation or cross-modal modeling. Therefore, due to insufficient evidence about discrete token usage and detailed description, the study likely does not meet all inclusion criteria and falls under exclusion reason of lacking clear discrete token detail.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
    "abstract": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
    "metadata": {
      "arxiv_id": "2401.04964",
      "title": "Self-supervised speech representation and contextual text embedding for match-mismatch classification with EEG recording",
      "summary": "Relating speech to EEG holds considerable importance but is challenging. In this study, a deep convolutional network was employed to extract spatiotemporal features from EEG data. Self-supervised speech representation and contextual text embedding were used as speech features. Contrastive learning was used to relate EEG features to speech features. The experimental results demonstrate the benefits of using self-supervised speech representation and contextual text embedding. Through feature fusion and model ensemble, an accuracy of 60.29% was achieved, and the performance was ranked as No.2 in Task 1 of the Auditory EEG Challenge (ICASSP 2024). The code to implement our work is available on Github: https://github.com/bobwangPKU/EEG-Stimulus-Match-Mismatch.",
      "authors": [
        "Bo Wang",
        "Xiran Xu",
        "Zechen Zhang",
        "Haolin Zhu",
        "YuJie Yan",
        "Xihong Wu",
        "Jing Chen"
      ],
      "published": "2024-01-10T07:11:36Z",
      "updated": "2024-02-01T04:51:22Z",
      "categories": [
        "eess.SP",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04964v2",
      "landing_url": "https://arxiv.org/abs/2401.04964v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04964"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes continuous speech representations for EEG without any discretized tokens or tokenization/codec design, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes continuous speech representations for EEG without any discretized tokens or tokenization/codec design, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study employs self-supervised speech representation and contextual text embedding to relate speech features to EEG signals, focusing on match-mismatch classification; however, it does not describe the discretization of continuous audio into discrete tokens or the use of discrete audio token vocabularies, codebooks, or quantization mechanisms as required by the inclusion criteria. Therefore, it does not meet the core inclusion criterion of using discrete audio tokens as central representations and seems to focus primarily on a neural network classification task rather than audio token design or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study employs self-supervised speech representation and contextual text embedding to relate speech features to EEG signals, focusing on match-mismatch classification; however, it does not describe the discretization of continuous audio into discrete tokens or the use of discrete audio token vocabularies, codebooks, or quantization mechanisms as required by the inclusion criteria. Therefore, it does not meet the core inclusion criterion of using discrete audio tokens as central representations and seems to focus primarily on a neural network classification task rather than audio token design or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters",
    "abstract": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
    "metadata": {
      "arxiv_id": "2401.05111",
      "title": "Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters",
      "summary": "The zero-shot text-to-speech (TTS) method, based on speaker embeddings extracted from reference speech using self-supervised learning (SSL) speech representations, can reproduce speaker characteristics very accurately. However, this approach suffers from degradation in speech synthesis quality when the reference speech contains noise. In this paper, we propose a noise-robust zero-shot TTS method. We incorporated adapters into the SSL model, which we fine-tuned with the TTS model using noisy reference speech. In addition, to further improve performance, we adopted a speech enhancement (SE) front-end. With these improvements, our proposed SSL-based zero-shot TTS achieved high-quality speech synthesis with noisy reference speech. Through the objective and subjective evaluations, we confirmed that the proposed method is highly robust to noise in reference speech, and effectively works in combination with SE.",
      "authors": [
        "Kenichi Fujita",
        "Hiroshi Sato",
        "Takanori Ashihara",
        "Hiroki Kanagawa",
        "Marc Delcroix",
        "Takafumi Moriya",
        "Yusuke Ijima"
      ],
      "published": "2024-01-10T12:21:21Z",
      "updated": "2024-01-10T12:21:21Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05111v1",
      "landing_url": "https://arxiv.org/abs/2401.05111v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.05111"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses noise-robust zero-shot TTS using SSL speaker embeddings and adapters; it does not introduce or analyze discrete audio tokens or tokenization schemes, hence fails the token-discrete criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses noise-robust zero-shot TTS using SSL speaker embeddings and adapters; it does not introduce or analyze discrete audio tokens or tokenization schemes, hence fails the token-discrete criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech synthesis using self-supervised speech representations, but it does not describe the process of discretizing continuous audio into discrete token sequences or the use of neural audio codecs/vector quantization for discrete tokens, which is the core inclusion criterion. Thus, it lacks sufficient information on discrete audio tokens and their design, evaluation, or application as discrete token sequences for modeling or generation, making it not aligned with the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech synthesis using self-supervised speech representations, but it does not describe the process of discretizing continuous audio into discrete token sequences or the use of neural audio codecs/vector quantization for discrete tokens, which is the core inclusion criterion. Thus, it lacks sufficient information on discrete audio tokens and their design, evaluation, or application as discrete token sequences for modeling or generation, making it not aligned with the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform",
    "abstract": "This study presents a comprehensive evaluation of tools available on the HuggingFace platform for two pivotal applications in artificial intelligence: image segmentation and voice conversion. The primary objective was to identify the top three tools within each category and subsequently install and configure these tools on Linux systems. We leveraged the power of pre-trained segmentation models such as SAM and DETR Model with ResNet-50 backbone for image segmentation, and the so-vits-svc-fork model for voice conversion. This paper delves into the methodologies and challenges encountered during the implementation process, and showcases the successful combination of video segmentation and voice conversion in a unified project named AutoVisual Fusion Suite.",
    "metadata": {
      "arxiv_id": "2401.05379",
      "title": "AutoVisual Fusion Suite: A Comprehensive Evaluation of Image Segmentation and Voice Conversion Tools on HuggingFace Platform",
      "summary": "This study presents a comprehensive evaluation of tools available on the HuggingFace platform for two pivotal applications in artificial intelligence: image segmentation and voice conversion. The primary objective was to identify the top three tools within each category and subsequently install and configure these tools on Linux systems. We leveraged the power of pre-trained segmentation models such as SAM and DETR Model with ResNet-50 backbone for image segmentation, and the so-vits-svc-fork model for voice conversion. This paper delves into the methodologies and challenges encountered during the implementation process, and showcases the successful combination of video segmentation and voice conversion in a unified project named AutoVisual Fusion Suite.",
      "authors": [
        "Amirreza Hashemi"
      ],
      "published": "2023-12-17T16:38:41Z",
      "updated": "2024-01-12T10:06:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05379v2",
      "landing_url": "https://arxiv.org/abs/2401.05379v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.05379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on evaluating image segmentation and voice conversion tools on HuggingFace, with no discussion of discrete audio tokens, tokenization, or neural audio codecs as core methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on evaluating image segmentation and voice conversion tools on HuggingFace, with no discussion of discrete audio tokens, tokenization, or neural audio codecs as core methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating image segmentation and voice conversion tools on the HuggingFace platform, without mentioning the use or design of discrete audio tokens or neural audio codecs that produce token sequences as core representations. The abstract lacks details on discretization processes, tokenizer architecture, codebook settings, or evaluation relevant to discrete audio tokens, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and fails to provide sufficient information to assess their presence or impact.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating image segmentation and voice conversion tools on the HuggingFace platform, without mentioning the use or design of discrete audio tokens or neural audio codecs that produce token sequences as core representations. The abstract lacks details on discretization processes, tokenizer architecture, codebook settings, or evaluation relevant to discrete audio tokens, which are central to the inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and fails to provide sufficient information to assess their presence or impact.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Deduplication For Socia Media Data Selection",
    "abstract": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
    "metadata": {
      "arxiv_id": "2401.05883",
      "title": "Generative Deduplication For Socia Media Data Selection",
      "summary": "Social media data exhibits severe redundancy caused by its noisy nature. It leads to increased training time and model bias in its processing. To address this issue, we propose a novel Generative Deduplication framework for social media data selection by removing semantically duplicate data. While related work involves data selection in task-specific training, our model acts as an efficient pre-processing method to universally enhance social media NLP pipelines. Specifically, we train a generative model via self-supervised learning to predict a keyword to capture the semantics of noisy social media text for deduplication. Meanwhile, time-dimensional Gaussian noise is added to improve training complexity and avoid learning trivial features. Extensive experiments suggest that our model can better reduce training samples while improving performance than baselines. The results show our model's potential to broadly advance social media language understanding in effectiveness and efficiency.",
      "authors": [
        "Xianming Li",
        "Jing Li"
      ],
      "published": "2024-01-11T12:43:26Z",
      "updated": "2024-10-03T03:34:34Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.05883v3",
      "landing_url": "https://arxiv.org/abs/2401.05883v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.05883"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on generative deduplication for social media NLP data and does not involve discrete audio tokens, tokenizers, or audio token representations, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on generative deduplication for social media NLP data and does not involve discrete audio tokens, tokenizers, or audio token representations, thus it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on generative deduplication for social media text data selection, which is unrelated to the discrete audio tokens topic. There is no mention of discretization of continuous audio waveforms, neural audio codecs, vector quantization, or use of discrete token sequences for audio modeling or generation. The study concerns NLP on social media text and does not address audio token design, generation, or evaluation, thus failing the thematic inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on generative deduplication for social media text data selection, which is unrelated to the discrete audio tokens topic. There is no mention of discretization of continuous audio waveforms, neural audio codecs, vector quantization, or use of discrete token sequences for audio modeling or generation. The study concerns NLP on social media text and does not address audio token design, generation, or evaluation, thus failing the thematic inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses using discrete tokens (acoustic and phoneme tokens) in a neural LMs-based TTS framework with alignment-guided sequence ordering, which aligns with the topic of Discrete Audio Tokens, though it does not detail the token production process (tokenizer/codec setup) in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses using discrete tokens (acoustic and phoneme tokens) in a neural LMs-based TTS framework with alignment-guided sequence ordering, which aligns with the topic of Discrete Audio Tokens, though it does not detail the token production process (tokenizer/codec setup) in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a model (ELLA-V) focused on zero-shot text-to-speech generation using a language model approach that combines acoustic and phoneme tokens, but it does not specify the process of discretizing continuous audio into discrete tokens, nor does it detail the codec/tokenizer architecture, quantization, or codebook configurations that would confirm the use of neural discrete audio tokens as defined. Therefore, it is unclear if the study meets the inclusion criteria emphasizing discrete token generation and detailed token specifications necessary for reproducibility and analysis.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a model (ELLA-V) focused on zero-shot text-to-speech generation using a language model approach that combines acoustic and phoneme tokens, but it does not specify the process of discretizing continuous audio into discrete tokens, nor does it detail the codec/tokenizer architecture, quantization, or codebook configurations that would confirm the use of neural discrete audio tokens as defined. Therefore, it is unclear if the study meets the inclusion criteria emphasizing discrete token generation and detailed token specifications necessary for reproducibility and analysis.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper uses “acoustic and phoneme tokens” (which suggests it operates on discrete audio tokens for LM-based TTS) but disagreed on sufficiency: JuniorNano leaned toward inclusion noting alignment with discrete token topics, while JuniorMini flagged the abstract’s lack of explicit tokenizer/codec/quantization/codebook/bitrate details. The abstract describes interleaved acoustic and phoneme token sequences and improvements over VALL‑E, but does not provide the required methodological details to confirm that discrete tokens are produced/defined in a way that satisfies the inclusion criteria (tokenizer/codec architecture and settings). Because the abstract alone is insufficient to determine whether the study meets the strict requirement to document discrete-token generation, I cannot confidently include it based on the provided information.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper uses “acoustic and phoneme tokens” (which suggests it operates on discrete audio tokens for LM-based TTS) but disagreed on sufficiency: JuniorNano leaned toward inclusion noting alignment with discrete token topics, while JuniorMini flagged the abstract’s lack of explicit tokenizer/codec/quantization/codebook/bitrate details. The abstract describes interleaved acoustic and phoneme token sequences and improvements over VALL‑E, but does not provide the required methodological details to confirm that discrete tokens are produced/defined in a way that satisfies the inclusion criteria (tokenizer/codec architecture and settings). Because the abstract alone is insufficient to determine whether the study meets the strict requirement to document discrete-token generation, I cannot confidently include it based on the provided information.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
    "abstract": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
    "metadata": {
      "arxiv_id": "2401.08833",
      "title": "Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective",
      "summary": "Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without using any labels. Finally, we show that both supervised and unsupervised measures echo the performance of the models on layer-wise linear probing and speech recognition.",
      "authors": [
        "Alexander H. Liu",
        "Sung-Lin Yeh",
        "James Glass"
      ],
      "published": "2024-01-16T21:13:22Z",
      "updated": "2024-01-16T21:13:22Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08833v1",
      "landing_url": "https://arxiv.org/abs/2401.08833v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.08833"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on self-supervised speech representations and mutual information analysis of continuous features; it does not discuss discrete audio token generation, tokenization schemes, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on self-supervised speech representations and mutual information analysis of continuous features; it does not discuss discrete audio token generation, tokenization schemes, or token-based modeling, so it does not meet the discrete audio tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on self-supervised speech representation learning from an information-theoretic perspective, analyzing the quality of learned representations and their mutual information with target information. However, the abstract does not mention any discrete audio token discretization process, neural audio codec, vector quantization, or details about token vocabularies, nor does it present discrete tokens as core components for modeling, generation, or cross-modal alignment. The content is centered on learned continuous representations and their evaluation rather than on discrete sequences used as tokens for downstream tasks, so it does not meet the inclusion criteria related to discrete audio tokens and instead falls under exclusion due to continuous representation focus without discrete token usage or description.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on self-supervised speech representation learning from an information-theoretic perspective, analyzing the quality of learned representations and their mutual information with target information. However, the abstract does not mention any discrete audio token discretization process, neural audio codec, vector quantization, or details about token vocabularies, nor does it present discrete tokens as core components for modeling, generation, or cross-modal alignment. The content is centered on learned continuous representations and their evaluation rather than on discrete sequences used as tokens for downstream tasks, so it does not meet the inclusion criteria related to discrete audio tokens and instead falls under exclusion due to continuous representation focus without discrete token usage or description.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
    "abstract": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
    "metadata": {
      "arxiv_id": "2401.11857",
      "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
      "summary": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
      "authors": [
        "Shihao Chen",
        "Liping Chen",
        "Jie Zhang",
        "KongAik Lee",
        "Zhenhua Ling",
        "Lirong Dai"
      ],
      "published": "2024-01-22T11:26:59Z",
      "updated": "2024-01-22T11:26:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11857v1",
      "landing_url": "https://arxiv.org/abs/2401.11857v1",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447699"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on adversarial perturbations for voice privacy in speech-generation models and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on adversarial perturbations for voice privacy in speech-generation models and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adversarial attacks for speaker voice privacy protection, which involves perturbing continuous speech waveforms to prevent speech generation models from accurately replicating a speaker's voice; however, it does not mention the use or design of discrete audio tokens, tokenizers, vector quantization, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria. Hence, it does not meet the required criteria regarding discrete audio tokens and seems to focus on continuous audio signal perturbation, leading to exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adversarial attacks for speaker voice privacy protection, which involves perturbing continuous speech waveforms to prevent speech generation models from accurately replicating a speaker's voice; however, it does not mention the use or design of discrete audio tokens, tokenizers, vector quantization, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria. Hence, it does not meet the required criteria regarding discrete audio tokens and seems to focus on continuous audio signal perturbation, leading to exclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
    "abstract": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
    "metadata": {
      "arxiv_id": "2401.13527",
      "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
      "summary": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
      "authors": [
        "Dong Zhang",
        "Xin Zhang",
        "Jun Zhan",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2024-01-24T15:25:01Z",
      "updated": "2024-01-25T17:24:52Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13527v2",
      "landing_url": "https://arxiv.org/abs/2401.13527v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.13527"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses semantic/perceptual information modeling and flow-based perceptual modeling in SpeechGPT-Gen, but it does not describe discrete audio tokens, tokenizers, or any discretization of audio into a token vocabulary, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses semantic/perceptual information modeling and flow-based perceptual modeling in SpeechGPT-Gen, but it does not describe discrete audio tokens, tokenizers, or any discretization of audio into a token vocabulary, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents SpeechGPT-Gen, a large speech generation model using a Chain-of-Information Generation method that decouples semantic and perceptual information. However, the abstract does not explicitly describe the use or derivation of discrete audio tokens via neural audio codecs or self-supervised learning models, nor detail any tokenization, codebook, quantization, or discretization procedures. The focus appears to be on semantic and perceptual modeling rather than on the discrete token representation of audio necessary to satisfy the inclusion criteria. Furthermore, the description does not clarify the presence of discrete token sequences as the core representation for generative modeling. Therefore, based on the provided abstract and inclusion/exclusion criteria, it does not clearly meet the criteria for inclusion within the \"Discrete Audio Tokens\" theme and seems to lean towards continuous representation or alternative modeling approaches.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents SpeechGPT-Gen, a large speech generation model using a Chain-of-Information Generation method that decouples semantic and perceptual information. However, the abstract does not explicitly describe the use or derivation of discrete audio tokens via neural audio codecs or self-supervised learning models, nor detail any tokenization, codebook, quantization, or discretization procedures. The focus appears to be on semantic and perceptual modeling rather than on the discrete token representation of audio necessary to satisfy the inclusion criteria. Furthermore, the description does not clarify the presence of discrete token sequences as the core representation for generative modeling. Therefore, based on the provided abstract and inclusion/exclusion criteria, it does not clearly meet the criteria for inclusion within the \"Discrete Audio Tokens\" theme and seems to lean towards continuous representation or alternative modeling approaches.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Text to speech synthesis",
    "abstract": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
    "metadata": {
      "arxiv_id": "2401.13891",
      "title": "Text to speech synthesis",
      "summary": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
      "authors": [
        "Harini s",
        "Manoj G M"
      ],
      "published": "2024-01-25T02:13:45Z",
      "updated": "2024-01-25T02:13:45Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13891v1",
      "landing_url": "https://arxiv.org/abs/2401.13891v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13891"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract on text-to-speech synthesis discusses general TTS technologies and applications but does not mention discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract on text-to-speech synthesis discusses general TTS technologies and applications but does not mention discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract on Text-to-Speech (TTS) synthesis describes general technology converting text into speech without mentioning discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization methods used for audio modeling, generation, or alignment. It lacks details on tokenizer architecture, token vocabulary, quantization, or token-based evaluations. Thus, it does not meet the inclusion criteria focused on discrete audio token-based methods, nor does it meet the English evaluability requirements for discrete token generation. Therefore, this study should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract on Text-to-Speech (TTS) synthesis describes general technology converting text into speech without mentioning discrete audio tokens, neural audio codecs, vector quantization, or any discrete tokenization methods used for audio modeling, generation, or alignment. It lacks details on tokenizer architecture, token vocabulary, quantization, or token-based evaluations. Thus, it does not meet the inclusion criteria focused on discrete audio token-based methods, nor does it meet the English evaluability requirements for discrete token generation. Therefore, this study should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Massive Unsourced Random Access for Near-Field Communications",
    "abstract": "This paper investigates the unsourced random access (URA) problem with a massive multiple-input multiple-output receiver that serves wireless devices in the near-field of radiation. We employ an uncoupled transmission protocol without appending redundancies to the slot-wise encoded messages. To exploit the channel sparsity for block length reduction while facing the collapsed sparse structure in the angular domain of near-field channels, we propose a sparse channel sampling method that divides the angle-distance (polar) domain based on the maximum permissible coherence. Decoding starts with retrieving active codewords and channels from each slot. We address the issue by leveraging the structured channel sparsity in the spatial and polar domains and propose a novel turbo-based recovery algorithm. Furthermore, we investigate an off-grid compressed sensing method to refine discretely estimated channel parameters over the continuum that improves the detection performance. Afterward, without the assistance of redundancies, we recouple the separated messages according to the similarity of the users' channel information and propose a modified K-medoids method to handle the constraints and collisions involved in channel clustering. Simulations reveal that via exploiting the channel sparsity, the proposed URA scheme achieves high spectral efficiency and surpasses existing multi-slot-based schemes. Moreover, with more measurements provided by the overcomplete channel sampling, the near-field-suited scheme outperforms its counterpart of the far-field.",
    "metadata": {
      "arxiv_id": "2401.14008",
      "title": "Massive Unsourced Random Access for Near-Field Communications",
      "summary": "This paper investigates the unsourced random access (URA) problem with a massive multiple-input multiple-output receiver that serves wireless devices in the near-field of radiation. We employ an uncoupled transmission protocol without appending redundancies to the slot-wise encoded messages. To exploit the channel sparsity for block length reduction while facing the collapsed sparse structure in the angular domain of near-field channels, we propose a sparse channel sampling method that divides the angle-distance (polar) domain based on the maximum permissible coherence. Decoding starts with retrieving active codewords and channels from each slot. We address the issue by leveraging the structured channel sparsity in the spatial and polar domains and propose a novel turbo-based recovery algorithm. Furthermore, we investigate an off-grid compressed sensing method to refine discretely estimated channel parameters over the continuum that improves the detection performance. Afterward, without the assistance of redundancies, we recouple the separated messages according to the similarity of the users' channel information and propose a modified K-medoids method to handle the constraints and collisions involved in channel clustering. Simulations reveal that via exploiting the channel sparsity, the proposed URA scheme achieves high spectral efficiency and surpasses existing multi-slot-based schemes. Moreover, with more measurements provided by the overcomplete channel sampling, the near-field-suited scheme outperforms its counterpart of the far-field.",
      "authors": [
        "Xinyu Xie",
        "Yongpeng Wu",
        "Jianping An",
        "Derrick Wing Kwan Ng",
        "Chengwen Xing",
        "Wenjun Zhang"
      ],
      "published": "2024-01-25T08:17:51Z",
      "updated": "2024-01-25T08:17:51Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14008v1",
      "landing_url": "https://arxiv.org/abs/2401.14008v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.14008"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The article focuses on unsourced random access in near-field wireless communications with channel sparsity and recovery algorithms; there is no discussion of discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The article focuses on unsourced random access in near-field wireless communications with channel sparsity and recovery algorithms; there is no discussion of discrete audio tokens, tokenization, or token-based modeling, thus it does not meet the inclusion criteria for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on communication theory and signal processing for near-field communications using unsourced random access with massive MIMO, channel sparsity exploitation, and compressed sensing techniques; it does not address discrete audio tokens, tokenization of audio signals, neural audio codecs, or audio generation tasks, hence it does not meet any inclusion criteria related to discrete audio token research and instead falls into exclusion criteria of lacking discrete token representations or relevance to audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on communication theory and signal processing for near-field communications using unsourced random access with massive MIMO, channel sparsity exploitation, and compressed sensing techniques; it does not address discrete audio tokens, tokenization of audio signals, neural audio codecs, or audio generation tasks, hence it does not meet any inclusion criteria related to discrete audio token research and instead falls into exclusion criteria of lacking discrete token representations or relevance to audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MunTTS: A Text-to-Speech System for Mundari",
    "abstract": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
    "metadata": {
      "arxiv_id": "2401.15579",
      "title": "MunTTS: A Text-to-Speech System for Mundari",
      "summary": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
      "authors": [
        "Varun Gumma",
        "Rishav Hada",
        "Aditya Yadavalli",
        "Pamir Gogoi",
        "Ishani Mondal",
        "Vivek Seshadri",
        "Kalika Bali"
      ],
      "published": "2024-01-28T06:27:17Z",
      "updated": "2024-01-28T06:27:17Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15579v1",
      "landing_url": "https://arxiv.org/abs/2401.15579v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.15579"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes an end-to-end TTS system for Mundari with data collection and evaluation, but it does not mention discrete audio tokens, tokenization, or token-based modeling, so it fails the discrete token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes an end-to-end TTS system for Mundari with data collection and evaluation, but it does not mention discrete audio tokens, tokenization, or token-based modeling, so it fails the discrete token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a TTS system for Mundari that involves collecting and processing data to build speech synthesis models, but it does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process that maps continuous audio into discrete token sequences. There is no detailed description of token generation, vocabulary, quantization, or multi-codebook approaches, which are critical to meet the inclusion criteria focused on discrete audio tokens. Furthermore, the focus seems to be on end-to-end speech synthesis rather than on discrete representation for modeling or downstream tasks involving discrete tokens. Therefore, the paper does not satisfy the requirements that the core method involves discrete audio tokens and their detailed description or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a TTS system for Mundari that involves collecting and processing data to build speech synthesis models, but it does not mention the use of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process that maps continuous audio into discrete token sequences. There is no detailed description of token generation, vocabulary, quantization, or multi-codebook approaches, which are critical to meet the inclusion criteria focused on discrete audio tokens. Furthermore, the focus seems to be on end-to-end speech synthesis rather than on discrete representation for modeling or downstream tasks involving discrete tokens. Therefore, the paper does not satisfy the requirements that the core method involves discrete audio tokens and their detailed description or evaluation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
    "abstract": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
    "metadata": {
      "arxiv_id": "2401.16812",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "authors": [
        "Takaaki Saeki",
        "Soumi Maiti",
        "Shinnosuke Takamichi",
        "Shinji Watanabe",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-01-30T08:26:28Z",
      "updated": "2024-09-01T14:34:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.16812v3",
      "landing_url": "https://arxiv.org/abs/2401.16812v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.16812"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'speech discrete tokens' and token-based metrics, but the work centers on reference-aware evaluation metrics for speech generation rather than proposing or analyzing a discrete-token production/codec pipeline; it does not clearly describe a discrete audio token design or tokenization process as the core method, so it likely does not meet the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'speech discrete tokens' and token-based metrics, but the work centers on reference-aware evaluation metrics for speech generation rather than proposing or analyzing a discrete-token production/codec pipeline; it does not clearly describe a discrete audio token design or tokenization process as the core method, so it likely does not meet the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes automatic evaluation metrics for speech generation using dense speech features and speech discrete tokens, but it does not explicitly describe the discrete token generation process (e.g., tokenizer/codec details, vocabulary or codebook, or quantization method), nor does it focus on the design, impact, or systematic analysis of discrete audio tokens as core representations for generation or cross-modal modeling. Therefore, it does not fully meet the strict inclusion criteria centered around discrete audio tokens and their reproducible design and usage.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes automatic evaluation metrics for speech generation using dense speech features and speech discrete tokens, but it does not explicitly describe the discrete token generation process (e.g., tokenizer/codec details, vocabulary or codebook, or quantization method), nor does it focus on the design, impact, or systematic analysis of discrete audio tokens as core representations for generation or cross-modal modeling. Therefore, it does not fully meet the strict inclusion criteria centered around discrete audio tokens and their reproducible design and usage.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
    "abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
    "metadata": {
      "arxiv_id": "2402.01708",
      "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
      "summary": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
      "authors": [
        "Wiebke Hutiri",
        "Oresiti Papakyriakopoulos",
        "Alice Xiang"
      ],
      "published": "2024-01-25T11:47:06Z",
      "updated": "2024-05-15T15:26:42Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01708v2",
      "landing_url": "https://arxiv.org/abs/2402.01708v2",
      "doi": "https://doi.org/10.1145/3630106.3658911"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on ethical and safety harms of speech generators and do not discuss discrete audio tokens, tokenization, or token-based modeling, so it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on ethical and safety harms of speech generators and do not discuss discrete audio tokens, tokenization, or token-based modeling, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on ethical and safety harms of AI speech generators without discussing discrete audio tokens, their generation mechanisms, or their application in modeling or generation tasks; hence, it does not meet the core inclusion criteria centered on discrete token representations in audio AI systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on ethical and safety harms of AI speech generators without discussing discrete audio tokens, their generation mechanisms, or their application in modeling or generation tasks; hence, it does not meet the core inclusion criteria centered on discrete token representations in audio AI systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
    "abstract": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
    "metadata": {
      "arxiv_id": "2402.02302",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "authors": [
        "Nay San",
        "Georgios Paraskevopoulos",
        "Aryaman Arora",
        "Xiluo He",
        "Prabhjot Kaur",
        "Oliver Adams",
        "Dan Jurafsky"
      ],
      "published": "2024-02-03T23:54:03Z",
      "updated": "2024-02-03T23:54:03Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02302v1",
      "landing_url": "https://arxiv.org/abs/2402.02302v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.02302"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete acoustic tokens (‘acoustic pseudo-tokens’) and induced acoustic units (token-based representations) as core concepts, and evaluates them for downstream ASR performance, satisfying the discrete audio token criteria and related analyses; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete acoustic tokens (‘acoustic pseudo-tokens’) and induced acoustic units (token-based representations) as core concepts, and evaluates them for downstream ASR performance, satisfying the discrete audio token criteria and related analyses; thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving low-resource speech recognition by leveraging acoustic pseudo-tokens from low-resource and donor languages to enhance ASR; however, it does not explicitly describe the discrete token generation process, such as tokenizer architecture, quantization methods, or codebook settings, nor does it analyze design choices or provide a systematic evaluation relevant to discrete audio tokens as defined. Therefore, it does not fully meet the inclusion criteria about discrete audio tokens as core representations, nor does it provide sufficient methodological details on discrete token generation needed for reproducibility.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving low-resource speech recognition by leveraging acoustic pseudo-tokens from low-resource and donor languages to enhance ASR; however, it does not explicitly describe the discrete token generation process, such as tokenizer architecture, quantization methods, or codebook settings, nor does it analyze design choices or provide a systematic evaluation relevant to discrete audio tokens as defined. Therefore, it does not fully meet the inclusion criteria about discrete audio tokens as core representations, nor does it provide sufficient methodological details on discrete token generation needed for reproducibility.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one recommended inclusion, one recommended exclusion). The abstract explicitly refers to \"acoustic pseudo-tokens\" and \"induced acoustic units\" and builds a core analysis/metric (Acoustic Token Distribution Similarity, ATDS) on sequences of those units to predict ASR transfer — which satisfies the criterion that discrete/audio units are the primary representation source (e.g., self-supervised induced discrete units). However, the abstract does not specify tokenizer/quantization/codebook or other implementation details required for reproducibility, so inclusion depends on whether the full paper provides those method details; given the strong centrality of induced discrete units in the study, this is more likely to fit the Discrete Audio Tokens topic. Score explanation: better to include (4) because the work centers on induced discrete acoustic units as core representations, although full-text confirmation of tokenizer/codebook details is recommended.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one recommended inclusion, one recommended exclusion). The abstract explicitly refers to \"acoustic pseudo-tokens\" and \"induced acoustic units\" and builds a core analysis/metric (Acoustic Token Distribution Similarity, ATDS) on sequences of those units to predict ASR transfer — which satisfies the criterion that discrete/audio units are the primary representation source (e.g., self-supervised induced discrete units). However, the abstract does not specify tokenizer/quantization/codebook or other implementation details required for reproducibility, so inclusion depends on whether the full paper provides those method details; given the strong centrality of induced discrete units in the study, this is more likely to fit the Discrete Audio Tokens topic. Score explanation: better to include (4) because the work centers on induced discrete acoustic units as core representations, although full-text confirmation of tokenizer/codebook details is recommended.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
    "abstract": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
    "metadata": {
      "arxiv_id": "2402.03158",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "authors": [
        "Ran Ben-Basat",
        "Yaniv Ben-Itzhak",
        "Michael Mitzenmacher",
        "Shay Vargaftik"
      ],
      "published": "2024-02-05T16:27:59Z",
      "updated": "2025-07-31T13:53:50Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03158v2",
      "landing_url": "https://arxiv.org/abs/2402.03158v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03158"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract concerns adaptive vector quantization for general ML tasks (gradients, weights, activations) rather than discrete audio tokens, neural audio codecs, or token design/analysis for audio applications, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract concerns adaptive vector quantization for general ML tasks (gradients, weights, activations) rather than discrete audio tokens, neural audio codecs, or token design/analysis for audio applications, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on adaptive vector quantization algorithms aimed at compressing gradients, weights, activations, and datasets in machine learning, but it does not explicitly address discretized audio tokens, neural audio codecs, or the representation of continuous audio waveform as discrete token sequences used in audio generation, understanding, or cross-modal tasks. Hence, it does not meet the inclusion criteria for studies involving discrete audio tokens nor does it provide sufficient detail about tokenizer architecture or codebook configurations related to audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on adaptive vector quantization algorithms aimed at compressing gradients, weights, activations, and datasets in machine learning, but it does not explicitly address discretized audio tokens, neural audio codecs, or the representation of continuous audio waveform as discrete token sequences used in audio generation, understanding, or cross-modal tasks. Hence, it does not meet the inclusion criteria for studies involving discrete audio tokens nor does it provide sufficient detail about tokenizer architecture or codebook configurations related to audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
    "abstract": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
    "metadata": {
      "arxiv_id": "2402.03407",
      "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
      "summary": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
      "authors": [
        "Álvaro Martín-Cortinas",
        "Daniel Sáez-Trigueros",
        "Iván Vallés-Pérez",
        "Biel Tura-Vecino",
        "Piotr Biliński",
        "Mateusz Lajszczak",
        "Grzegorz Beringer",
        "Roberto Barra-Chicote",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2024-02-05T15:08:19Z",
      "updated": "2024-02-05T15:08:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03407v1",
      "landing_url": "https://arxiv.org/abs/2402.03407v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses self-supervised representations and speaker-disentangled voice conversion for LLM-based TTS but does not describe discrete audio tokens, tokenization, or codec-based token construction as required by the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses self-supervised representations and speaker-disentangled voice conversion for LLM-based TTS but does not describe discrete audio tokens, tokenization, or codec-based token construction as required by the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving LLM-based speech generation using self-supervised representations that disentangle speaker identity and content, but it does not explicitly describe the production or use of discrete audio tokens (e.g., via neural codecs or quantization) as finite token sequences for modeling; thus, it lacks the explicit definition and evaluation of discrete token mechanisms required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving LLM-based speech generation using self-supervised representations that disentangle speaker identity and content, but it does not explicitly describe the production or use of discrete audio tokens (e.g., via neural codecs or quantization) as finite token sequences for modeling; thus, it lacks the explicit definition and evaluation of discrete token mechanisms required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
    "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
    "metadata": {
      "arxiv_id": "2402.04563",
      "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
      "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "authors": [
        "Saebom Leem",
        "Hyunseok Seo"
      ],
      "published": "2024-02-07T03:43:56Z",
      "updated": "2024-02-07T03:43:56Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04563v1",
      "landing_url": "https://arxiv.org/abs/2402.04563v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses visual explanations for Vision Transformers, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses visual explanations for Vision Transformers, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a visualization method for Vision Transformer in computer vision tasks and does not involve discrete audio token generation, modeling, or analysis; it neither discusses audio waveform discretization nor neural audio codecs or related tokenization processes relevant to audio sequences.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a visualization method for Vision Transformer in computer vision tasks and does not involve discrete audio token generation, modeling, or analysis; it neither discusses audio waveform discretization nor neural audio codecs or related tokenization processes relevant to audio sequences.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation",
    "abstract": "Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naver-ai/usdm.",
    "metadata": {
      "arxiv_id": "2402.05706",
      "title": "Paralinguistics-Aware Speech-Empowered Large Language Models for Natural Conversation",
      "summary": "Recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech. However, an LLM-based strategy for modeling spoken dialogs remains elusive, calling for further investigation. This paper introduces an extensive speech-text LLM framework, the Unified Spoken Dialog Model (USDM), designed to generate coherent spoken responses with naturally occurring prosodic features relevant to the given input speech without relying on explicit automatic speech recognition (ASR) or text-to-speech (TTS) systems. We have verified the inclusion of prosody in speech tokens that predominantly contain semantic information and have used this foundation to construct a prosody-infused speech-text model. Additionally, we propose a generalized speech-text pretraining scheme that enhances the capture of cross-modal semantics. To construct USDM, we fine-tune our speech-text model on spoken dialog data using a multi-step spoken dialog template that stimulates the chain-of-reasoning capabilities exhibited by the underlying LLM. Automatic and human evaluations on the DailyTalk dataset demonstrate that our approach effectively generates natural-sounding spoken responses, surpassing previous and cascaded baselines. Our code and checkpoints are available at https://github.com/naver-ai/usdm.",
      "authors": [
        "Heeseung Kim",
        "Soonshin Seo",
        "Kyeongseok Jeong",
        "Ohsung Kwon",
        "Soyoon Kim",
        "Jungwhan Kim",
        "Jaehong Lee",
        "Eunwoo Song",
        "Myungwoo Oh",
        "Jung-Woo Ha",
        "Sungroh Yoon",
        "Kang Min Yoo"
      ],
      "published": "2024-02-08T14:35:09Z",
      "updated": "2024-11-28T01:10:49Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05706v3",
      "landing_url": "https://arxiv.org/abs/2402.05706v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.05706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes paralinguistics-aware LLMs and prosody in spoken dialogue but does not present discrete audio tokens, tokenizers, or neural audio codecs as its core representation or evaluation, thus not meeting the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes paralinguistics-aware LLMs and prosody in spoken dialogue but does not present discrete audio tokens, tokenizers, or neural audio codecs as its core representation or evaluation, thus not meeting the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a speech-text large language model that incorporates prosodic features without explicit ASR or TTS systems; however, the abstract lacks explicit mention of discrete audio tokens obtained via neural audio codecs or quantization, nor does it detail any discrete tokenization process or codebook design, which are essential for meeting the inclusion criteria centered on discrete audio tokens; thus, it fails to confirm that discrete tokens are the core representation used, leading to exclusion under the provided guidelines.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a speech-text large language model that incorporates prosodic features without explicit ASR or TTS systems; however, the abstract lacks explicit mention of discrete audio tokens obtained via neural audio codecs or quantization, nor does it detail any discrete tokenization process or codebook design, which are essential for meeting the inclusion criteria centered on discrete audio tokens; thus, it fails to confirm that discrete tokens are the core representation used, leading to exclusion under the provided guidelines.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A Survey on Transformer Compression",
    "abstract": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
    "metadata": {
      "arxiv_id": "2402.05964",
      "title": "A Survey on Transformer Compression",
      "summary": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
      "authors": [
        "Yehui Tang",
        "Yunhe Wang",
        "Jianyuan Guo",
        "Zhijun Tu",
        "Kai Han",
        "Hailin Hu",
        "Dacheng Tao"
      ],
      "published": "2024-02-05T12:16:28Z",
      "updated": "2024-04-07T13:03:58Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05964v2",
      "landing_url": "https://arxiv.org/abs/2402.05964v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.05964"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses Transformer model compression and lacks discussion of discrete audio tokens, tokenizers, or discrete token-based audio representations required by the inclusion criteria, so it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses Transformer model compression and lacks discussion of discrete audio tokens, tokenizers, or discrete token-based audio representations required by the inclusion criteria, so it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a comprehensive survey of Transformer compression methods for NLP and vision tasks, without mentioning discrete audio tokens or any audio-specific tokenization processes that meet the inclusion criteria regarding neural audio codecs or discrete token generation for audio waveforms. Thus, it does not satisfy the inclusion requirements related to discrete audio tokens, nor does it address audio token design or evaluation in audio generation or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a comprehensive survey of Transformer compression methods for NLP and vision tasks, without mentioning discrete audio tokens or any audio-specific tokenization processes that meet the inclusion criteria regarding neural audio codecs or discrete token generation for audio waveforms. Thus, it does not satisfy the inclusion requirements related to discrete audio tokens, nor does it address audio token design or evaluation in audio generation or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
    "abstract": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
    "metadata": {
      "arxiv_id": "2402.06492",
      "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
      "summary": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
      "authors": [
        "Yichen Jiang",
        "Xiang Zhou",
        "Mohit Bansal"
      ],
      "published": "2024-02-09T15:53:15Z",
      "updated": "2024-02-09T15:53:15Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06492v1",
      "landing_url": "https://arxiv.org/abs/2402.06492v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.06492"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on structural quantization of embeddings in Transformers for NLP tasks and does not discuss discrete audio tokens, neural audio codecs, or audio-specific tokenization; it therefore does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on structural quantization of embeddings in Transformers for NLP tasks and does not discuss discrete audio tokens, neural audio codecs, or audio-specific tokenization; it therefore does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing compositional generalization in Transformers via structured quantized embeddings and attention mechanisms but does not involve the discretization of continuous audio waveforms into token sequences or address neural audio codecs or audio-related token design, which are central to the inclusion criteria of discrete audio tokens in audio generation, understanding or cross-modal tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing compositional generalization in Transformers via structured quantized embeddings and attention mechanisms but does not involve the discretization of continuous audio waveforms into token sequences or address neural audio codecs or audio-related token design, which are central to the inclusion criteria of discrete audio tokens in audio generation, understanding or cross-modal tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
    "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
    "metadata": {
      "arxiv_id": "2402.08093",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "authors": [
        "Mateusz Łajszczak",
        "Guillermo Cámbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "Álvaro Martín-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "Alexis Moinet",
        "Sri Karlapati",
        "Ewa Muszyńska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad López Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
      ],
      "published": "2024-02-12T22:21:30Z",
      "updated": "2024-02-15T18:57:26Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08093v2",
      "landing_url": "https://arxiv.org/abs/2402.08093v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.08093"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This BASE TTS paper explicitly uses discrete speechcodes and a tokenization method, discusses token design choices and their impact on TTS generation, and treats discrete tokens as a core representation, aligning with the discrete-audio-token criteria without triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This BASE TTS paper explicitly uses discrete speechcodes and a tokenization method, discusses token design choices and their impact on TTS generation, and treats discrete tokens as a core representation, aligning with the discrete-audio-token criteria without triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a large-scale text-to-speech (TTS) model that uses a novel speech tokenization technique to convert text into discrete codes called \"speechcodes.\" These speechcodes are generated through a process that includes speaker disentanglement and compression with byte-pair encoding, indicating that the continuous audio waveform is discretized into token sequences as described in the inclusion criteria. The focus on discrete audio tokens for downstream generation (TTS) with detailed methodological descriptions aligns well with the inclusion criteria. There is no indication that this is a traditional audio compression study or a discriminative task; the study clearly emphasizes discrete token design and its use in audio generation, supporting its inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper describes a large-scale text-to-speech (TTS) model that uses a novel speech tokenization technique to convert text into discrete codes called \"speechcodes.\" These speechcodes are generated through a process that includes speaker disentanglement and compression with byte-pair encoding, indicating that the continuous audio waveform is discretized into token sequences as described in the inclusion criteria. The focus on discrete audio tokens for downstream generation (TTS) with detailed methodological descriptions aligns well with the inclusion criteria. There is no indication that this is a traditional audio compression study or a discriminative task; the study clearly emphasizes discrete token design and its use in audio generation, supporting its inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
    "abstract": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
    "metadata": {
      "arxiv_id": "2402.09378",
      "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
      "summary": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
      "authors": [
        "Shengpeng Ji",
        "Ziyue Jiang",
        "Hanting Wang",
        "Jialong Zuo",
        "Zhou Zhao"
      ],
      "published": "2024-02-14T18:24:41Z",
      "updated": "2024-06-02T16:11:18Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09378v2",
      "landing_url": "https://arxiv.org/abs/2402.09378v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.09378"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions a 'discrete codec' but provides no details on discrete token generation, tokenizer/codec architecture, vocabularies, or token-based modeling required by the discrete audio tokens criteria, so it does not meet the inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions a 'discrete codec' but provides no details on discrete token generation, tokenizer/codec architecture, vocabularies, or token-based modeling required by the discrete audio tokens criteria, so it does not meet the inclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes MobileSpeech, a zero-shot TTS system leveraging a discrete codec with a parallel speech mask decoder module. The method uses a neural audio codec to generate discrete tokens for speech representation, integral to the system's text-to-speech generation, fitting the definition of 'Discrete Audio Tokens' for sequence modeling and generation. It also provides novel design details and empirical evaluations, meeting the inclusion criteria regarding use of discrete tokens from neural codecs, detailed methodology, and downstream TTS application. The abstract is in English, with clear mentions of codec design and discrete token usage, supporting reproducibility assessment. Therefore, the study aligns with the topic focus and presents relevant, citable advances in discrete audio token-based TTS systems.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes MobileSpeech, a zero-shot TTS system leveraging a discrete codec with a parallel speech mask decoder module. The method uses a neural audio codec to generate discrete tokens for speech representation, integral to the system's text-to-speech generation, fitting the definition of 'Discrete Audio Tokens' for sequence modeling and generation. It also provides novel design details and empirical evaluations, meeting the inclusion criteria regarding use of discrete tokens from neural codecs, detailed methodology, and downstream TTS application. The abstract is in English, with clear mentions of codec design and discrete token usage, supporting reproducibility assessment. Therefore, the study aligns with the topic focus and presents relevant, citable advances in discrete audio token-based TTS systems.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers split: one excluded (saying abstract mentions a \"discrete codec\" but provides no tokenizer/codebook/quantization details), the other included (arguing the method clearly leverages a neural audio codec to produce discrete tokens for TTS). The abstract explicitly states the system leverages a \"discrete codec\" and that the model incorporates hierarchical information from codec layers during generation, which indicates the use of discrete audio tokens (neural codec outputs) as core representations for generation—this satisfies the primary token-source inclusion criterion (use of a neural audio codec to produce discrete tokens used in downstream TTS). However, the abstract does not report low-level tokenizer/codec configuration details (e.g., quantization method, codebook sizes, bitrate or time downsampling), so without the full paper there remains some uncertainty about whether the manuscript provides the reproducibility details required by the English-evaluable inclusion clause. Given the clear focus on codec-derived discrete representations for generation but the missing low-level details in the abstract, I recommend inclusion pending confirmation from the full paper; therefore I rate this as \"better to include\".",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers split: one excluded (saying abstract mentions a \"discrete codec\" but provides no tokenizer/codebook/quantization details), the other included (arguing the method clearly leverages a neural audio codec to produce discrete tokens for TTS). The abstract explicitly states the system leverages a \"discrete codec\" and that the model incorporates hierarchical information from codec layers during generation, which indicates the use of discrete audio tokens (neural codec outputs) as core representations for generation—this satisfies the primary token-source inclusion criterion (use of a neural audio codec to produce discrete tokens used in downstream TTS). However, the abstract does not report low-level tokenizer/codec configuration details (e.g., quantization method, codebook sizes, bitrate or time downsampling), so without the full paper there remains some uncertainty about whether the manuscript provides the reproducibility details required by the English-evaluable inclusion clause. Given the clear focus on codec-derived discrete representations for generation but the missing low-level details in the abstract, I recommend inclusion pending confirmation from the full paper; therefore I rate this as \"better to include\".",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
    "abstract": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
    "metadata": {
      "arxiv_id": "2402.11363",
      "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
      "summary": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
      "authors": [
        "Shiva Ebrahimi",
        "Xuan Guo"
      ],
      "published": "2024-02-17T19:04:23Z",
      "updated": "2024-06-26T07:45:33Z",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.11363v3",
      "landing_url": "https://arxiv.org/abs/2402.11363v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.11363"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is about transformer-based de novo peptide sequencing for DIA proteomics and contains no discussion of discrete audio tokens, tokenizers, or cross-modal/token-based representation; hence it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is about transformer-based de novo peptide sequencing for DIA proteomics and contains no discussion of discrete audio tokens, tokenizers, or cross-modal/token-based representation; hence it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on de novo peptide sequencing using transformer models on mass spectrometry data, which is a bioinformatics proteomics application, rather than discrete audio tokens or audio signal processing. It does not mention the production or use of discrete tokens from continuous audio waveform data, nor does it address neural audio codecs, vector quantization, or related discrete token representations for audio generation or cross-modal modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is out of scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on de novo peptide sequencing using transformer models on mass spectrometry data, which is a bioinformatics proteomics application, rather than discrete audio tokens or audio signal processing. It does not mention the production or use of discrete tokens from continuous audio waveform data, nor does it address neural audio codecs, vector quantization, or related discrete token representations for audio generation or cross-modal modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is out of scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
    "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
    "metadata": {
      "arxiv_id": "2402.12208",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "authors": [
        "Shengpeng Ji",
        "Minghui Fang",
        "Jialong Zuo",
        "Ziyue Jiang",
        "Dingdong Wang",
        "Hanting Wang",
        "Hai Huang",
        "Zhou Zhao"
      ],
      "published": "2024-02-19T15:12:12Z",
      "updated": "2025-06-04T05:50:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12208v4",
      "landing_url": "https://arxiv.org/abs/2402.12208v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.12208"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on discrete audio tokens produced by neural codecs (e.g., vector quantization, multi-codebooks) and their use with downstream speech-language models, including methods, token design details, and down-stream evaluations, which aligns with the inclusion criteria and does not fit the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on discrete audio tokens produced by neural codecs (e.g., vector quantization, multi-codebooks) and their use with downstream speech-language models, including methods, token design details, and down-stream evaluations, which aligns with the inclusion criteria and does not fit the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study involves discrete acoustic codecs that serve as intermediate representations for speech language models, addressing challenges related to discrete token generation and improving codec structures; it discusses vector quantization and codebooks in producing discrete acoustic tokens and evaluates the approach with extensive experiments and downstream tasks, aligning well with the inclusion criteria of discrete audio tokens produced via neural audio codecs and applied to downstream generative models with empirical validation. The abstract is in English and presents enough methodological detail to verify the use of discrete tokens. Therefore, the study fits the topic and inclusion criteria and does not meet any exclusion conditions.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study involves discrete acoustic codecs that serve as intermediate representations for speech language models, addressing challenges related to discrete token generation and improving codec structures; it discusses vector quantization and codebooks in producing discrete acoustic tokens and evaluates the approach with extensive experiments and downstream tasks, aligning well with the inclusion criteria of discrete audio tokens produced via neural audio codecs and applied to downstream generative models with empirical validation. The abstract is in English and presents enough methodological detail to verify the use of discrete tokens. Therefore, the study fits the topic and inclusion criteria and does not meet any exclusion conditions.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "abstract": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
    "metadata": {
      "arxiv_id": "2402.13572",
      "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
      "summary": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
      "authors": [
        "Yihang Gao",
        "Chuanyang Zheng",
        "Enze Xie",
        "Han Shi",
        "Tianyang Hu",
        "Yu Li",
        "Michael K. Ng",
        "Zhenguo Li",
        "Zhaoqiang Liu"
      ],
      "published": "2024-02-21T07:07:54Z",
      "updated": "2025-01-10T09:11:39Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13572v2",
      "landing_url": "https://arxiv.org/abs/2402.13572v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.13572"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on a transformer framework for algorithmic tasks and does not discuss discrete audio tokens, tokenizers, or audio-domain modeling, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on a transformer framework for algorithmic tasks and does not discuss discrete audio tokens, tokenizers, or audio-domain modeling, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on a novel transformer architecture (AlgoFormer) for algorithmic tasks and language processing, without any mention of discrete audio tokens, audio discretization, or audio representation techniques relevant to the given inclusion criteria. It does not discuss neural audio codecs, discrete audio token generation, or any audio-related experimental tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on a novel transformer architecture (AlgoFormer) for algorithmic tasks and language processing, without any mention of discrete audio tokens, audio discretization, or audio representation techniques relevant to the given inclusion criteria. It does not discuss neural audio codecs, discrete audio token generation, or any audio-related experimental tasks. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and their applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
    "abstract": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
    "metadata": {
      "arxiv_id": "2403.02002",
      "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
      "summary": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
      "authors": [
        "Sho Inoue",
        "Kun Zhou",
        "Shuai Wang",
        "Haizhou Li"
      ],
      "published": "2024-03-04T12:53:15Z",
      "updated": "2024-09-29T15:39:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.02002v2",
      "landing_url": "https://arxiv.org/abs/2403.02002v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.02002"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on fine-grained emotion editing in speech generation and does not involve discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on fine-grained emotion editing in speech generation and does not involve discrete audio tokens, tokenization, or token-based modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-grained quantitative emotion editing in speech generation but does not mention or describe the use or extraction of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process producing token sequences for speech representation, which is required to meet the inclusion criteria. The abstract lacks details about discrete token generation mechanisms or vocabulary/codebook settings. Hence, it does not fulfill the key aspects of the 'Discrete Audio Tokens' topic and fails to meet inclusion criteria while matching exclusion criteria regarding continuous audio representation without discrete token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-grained quantitative emotion editing in speech generation but does not mention or describe the use or extraction of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process producing token sequences for speech representation, which is required to meet the inclusion criteria. The abstract lacks details about discrete token generation mechanisms or vocabulary/codebook settings. Hence, it does not fulfill the key aspects of the 'Discrete Audio Tokens' topic and fails to meet inclusion criteria while matching exclusion criteria regarding continuous audio representation without discrete token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
    "abstract": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
    "metadata": {
      "arxiv_id": "2403.05010",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "published": "2024-03-08T03:16:47Z",
      "updated": "2024-10-07T02:08:05Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.05010v3",
      "landing_url": "https://arxiv.org/abs/2403.05010v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.05010"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions discrete tokens only as a potential input and centers on waveform reconstruction from spectrograms using Rectified Flow, lacking a detailed tokenization design or token-centric evaluation, thus not meeting the token-discrete criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions discrete tokens only as a potential input and centers on waveform reconstruction from spectrograms using Rectified Flow, lacking a detailed tokenization design or token-centric evaluation, thus not meeting the token-discrete criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces RFWave, a method for reconstructing audio waveforms from Mel-spectrograms or discrete acoustic tokens, but it does not specify the generation process of those discrete tokens, nor does it detail token design choices, vocabulary, codec architecture, or quantization approaches, which are essential for confirming inclusion under the given criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces RFWave, a method for reconstructing audio waveforms from Mel-spectrograms or discrete acoustic tokens, but it does not specify the generation process of those discrete tokens, nor does it detail token design choices, vocabulary, codec architecture, or quantization approaches, which are essential for confirming inclusion under the given criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
    "abstract": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
    "metadata": {
      "arxiv_id": "2403.06536",
      "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
      "summary": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
      "authors": [
        "Jinchen Zhu",
        "Mingjian Zhang",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2024-03-11T09:23:20Z",
      "updated": "2024-03-11T09:23:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06536v1",
      "landing_url": "https://arxiv.org/abs/2403.06536v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06536"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on image (not audio) arbitrary-scale super-resolution using implicit transformers, with no discrete audio token generation or tokenization discussion, so it fails inclusion and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on image (not audio) arbitrary-scale super-resolution using implicit transformers, with no discrete audio token generation or tokenization discussion, so it fails inclusion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a novel method for arbitrary-scale super-resolution using implicit neural representations, focusing on image features and transformer architectures; there is no mention of audio data, discrete audio tokens, or any audio-related discretization process. Hence, the study does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain (image super-resolution), thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a novel method for arbitrary-scale super-resolution using implicit neural representations, focusing on image features and transformer architectures; there is no mention of audio data, discrete audio tokens, or any audio-related discretization process. Hence, the study does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain (image super-resolution), thus it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
    "abstract": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
    "metadata": {
      "arxiv_id": "2403.07355",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "authors": [
        "Junyong Shin",
        "Yujin Kang",
        "Yo-Seb Jeon"
      ],
      "published": "2024-03-12T06:28:41Z",
      "updated": "2024-03-13T02:29:29Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07355v2",
      "landing_url": "https://arxiv.org/abs/2403.07355v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07355"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The input item focuses on DL-based CSI feedback and vector quantization for wireless MIMO, with no discretized audio token generation or token vocabulary/codebooks as the core design; it does not align with the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The input item focuses on DL-based CSI feedback and vector quantization for wireless MIMO, with no discretized audio token generation or token vocabulary/codebooks as the core design; it does not align with the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a vector-quantized variational autoencoder (VQ-VAE) framework applied to channel state information feedback in massive MIMO systems, which is related to wireless communications rather than audio processing. Although vector quantization is involved, the context is not audio token discretization for waveform sequences, nor does it aim at audio generation, understanding, or cross-modal modeling as defined in the inclusion criteria. There is no evidence of discrete audio tokens, neural audio codecs for audio signals, or focus on audio-related downstream tasks. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and mainly concerns continuous channel state representation in MIMO systems.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a vector-quantized variational autoencoder (VQ-VAE) framework applied to channel state information feedback in massive MIMO systems, which is related to wireless communications rather than audio processing. Although vector quantization is involved, the context is not audio token discretization for waveform sequences, nor does it aim at audio generation, understanding, or cross-modal modeling as defined in the inclusion criteria. There is no evidence of discrete audio tokens, neural audio codecs for audio signals, or focus on audio-related downstream tasks. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and mainly concerns continuous channel state representation in MIMO systems.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
    "abstract": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete semantic tokenization for CTR prediction in recommender systems, not on discrete audio tokens or audio-specific tokenization; it lacks any audio discretization details and therefore does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete semantic tokenization for CTR prediction in recommender systems, not on discrete audio tokens or audio-specific tokenization; it lacks any audio discretization details and therefore does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on discrete semantic tokenization for CTR prediction in recommendation systems, which is related to discrete token representations but not in the context of audio signals or audio tokenization as defined by the inclusion criteria; it lacks description of discretizing continuous audio waveforms nor does it address audio codec or neural audio encoding for audio modeling, generation, or cross-modal alignment.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on discrete semantic tokenization for CTR prediction in recommendation systems, which is related to discrete token representations but not in the context of audio signals or audio tokenization as defined by the inclusion criteria; it lacks description of discretizing continuous audio waveforms nor does it address audio codec or neural audio encoding for audio modeling, generation, or cross-modal alignment.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
    "abstract": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
    "metadata": {
      "arxiv_id": "2403.09673",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Jue Wang",
        "Yufei Huang",
        "Lirong Wu",
        "Stan Z. Li"
      ],
      "published": "2024-02-04T12:18:51Z",
      "updated": "2024-03-19T05:29:23Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09673v2",
      "landing_url": "https://arxiv.org/abs/2403.09673v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.09673"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work discretizes protein sequence-structure into FoldTokens for a protein language model, but the inclusion criteria target discrete audio tokens (neural codecs, audio tokenization, downstream audio tasks); thus it does not meet the topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work discretizes protein sequence-structure into FoldTokens for a protein language model, but the inclusion criteria target discrete audio tokens (neural codecs, audio tokenization, downstream audio tasks); thus it does not meet the topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a discrete representation of protein sequences and structures using vector quantization to create a protein language model; however, it does not involve audio waveforms, neural audio codecs, or discrete tokens derived from audio data, which are required by the inclusion criteria centered around discrete audio tokens and audio-related tasks. Therefore, it does not meet the theme of discretizing audio waveforms into tokens for audio generation, understanding, or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a discrete representation of protein sequences and structures using vector quantization to create a protein language model; however, it does not involve audio waveforms, neural audio codecs, or discrete tokens derived from audio data, which are required by the inclusion criteria centered around discrete audio tokens and audio-related tasks. Therefore, it does not meet the theme of discretizing audio waveforms into tokens for audio generation, understanding, or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
    "abstract": "Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.",
    "metadata": {
      "arxiv_id": "2403.14402",
      "title": "XLAVS-R: Cross-Lingual Audio-Visual Speech Representation Learning for Noise-Robust Speech Perception",
      "summary": "Speech recognition and translation systems perform poorly on noisy inputs, which are frequent in realistic environments. Augmenting these systems with visual signals has the potential to improve robustness to noise. However, audio-visual (AV) data is only available in limited amounts and for fewer languages than audio-only resources. To address this gap, we present XLAVS-R, a cross-lingual audio-visual speech representation model for noise-robust speech recognition and translation in over 100 languages. It is designed to maximize the benefits of limited multilingual AV pre-training data, by building on top of audio-only multilingual pre-training and simplifying existing pre-training schemes. Extensive evaluation on the MuAViC benchmark shows the strength of XLAVS-R on downstream audio-visual speech recognition and translation tasks, where it outperforms the previous state of the art by up to 18.5% WER and 4.7 BLEU given noisy AV inputs, and enables strong zero-shot audio-visual ability with audio-only fine-tuning.",
      "authors": [
        "HyoJung Han",
        "Mohamed Anwar",
        "Juan Pino",
        "Wei-Ning Hsu",
        "Marine Carpuat",
        "Bowen Shi",
        "Changhan Wang"
      ],
      "published": "2024-03-21T13:52:17Z",
      "updated": "2024-08-12T13:16:48Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14402v2",
      "landing_url": "https://arxiv.org/abs/2403.14402v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14402"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses continuous audio-visual speech representation learning without any mention of discrete audio tokens, tokenizers, or codebooks; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and falls under the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses continuous audio-visual speech representation learning without any mention of discrete audio tokens, tokenizers, or codebooks; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and falls under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on cross-lingual audio-visual speech representation learning for noise-robust speech recognition and translation but does not mention the use or design of discrete audio tokens through neural audio codecs or quantization methods. The abstract lacks details about discrete token generation, tokenizers, or vocabulary/codebook settings, which are essential for inclusion under the given criteria focusing on discrete audio tokens as core representations for modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on cross-lingual audio-visual speech representation learning for noise-robust speech recognition and translation but does not mention the use or design of discrete audio tokens through neural audio codecs or quantization methods. The abstract lacks details about discrete token generation, tokenizers, or vocabulary/codebook settings, which are essential for inclusion under the given criteria focusing on discrete audio tokens as core representations for modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Agentic AI: The Era of Semantic Decoding",
    "abstract": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses semantic decoding and semantic tokens in AI systems, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses semantic decoding and semantic tokens in AI systems, not discrete audio tokens or audio tokenization methods, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a conceptual framework of semantic decoding involving LLMs, humans, and tools interacting in semantic space, but it does not mention any discrete audio tokens or neural audio codec-based discretization of audio signals, nor does it describe tokenization or quantization processes related to audio. It lacks specific details on discrete audio token generation, codec architectures, or empirical evaluation of audio token usage, which are essential according to the inclusion criteria. Therefore, it does not meet the core inclusion requirements centered on discrete audio tokens and related analyses.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a conceptual framework of semantic decoding involving LLMs, humans, and tools interacting in semantic space, but it does not mention any discrete audio tokens or neural audio codec-based discretization of audio signals, nor does it describe tokenization or quantization processes related to audio. It lacks specific details on discrete audio token generation, codec architectures, or empirical evaluation of audio token usage, which are essential according to the inclusion criteria. Therefore, it does not meet the core inclusion requirements centered on discrete audio tokens and related analyses.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
    "abstract": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
    "metadata": {
      "arxiv_id": "2403.17378",
      "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
      "summary": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
      "authors": [
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-03-26T04:53:15Z",
      "updated": "2024-03-26T04:53:15Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17378v1",
      "landing_url": "https://arxiv.org/abs/2403.17378v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17378"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets continuous speech phase prediction without introducing discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the discrete-token inclusion criteria and aligns with the exclusion criterion for non-token-based representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets continuous speech phase prediction without introducing discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the discrete-token inclusion criteria and aligns with the exclusion criterion for non-token-based representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a neural model for predicting speech phase spectra directly from amplitude spectra but does not mention discretizing continuous audio waveforms into discrete tokens, nor the use or design of a token vocabulary or codec-related components. The focus is on phase prediction and synthesis efficiency, not on discrete audio tokens or their downstream usage, which is central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a neural model for predicting speech phase spectra directly from amplitude spectra but does not mention discretizing continuous audio waveforms into discrete tokens, nor the use or design of a token vocabulary or codec-related components. The focus is on phase prediction and synthesis efficiency, not on discrete audio tokens or their downstream usage, which is central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
    "abstract": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
    "metadata": {
      "arxiv_id": "2403.19461",
      "title": "Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization",
      "summary": "Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.",
      "authors": [
        "Simon Idoko",
        "Basant Sharma",
        "Arun Kumar Singh"
      ],
      "published": "2024-03-28T14:32:57Z",
      "updated": "2024-04-25T14:29:08Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19461v2",
      "landing_url": "https://arxiv.org/abs/2403.19461v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.19461"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the paper uses a VQ-VAE with a discrete latent space, the work focuses on learning sampling distributions and safety filtering for autonomous driving, not on discrete audio tokens or token-based audio modeling. It does not meet the 'Discrete Audio Tokens' criteria and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the paper uses a VQ-VAE with a discrete latent space, the work focuses on learning sampling distributions and safety filtering for autonomous driving, not on discrete audio tokens or token-based audio modeling. It does not meet the 'Discrete Audio Tokens' criteria and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses using vector quantized variational autoencoder (VQ-VAE) with vector quantization, a neural codec technique, to learn sampling distributions for autonomous driving trajectories. However, the domain focus is on autonomous driving trajectory planning rather than audio signals; no continuous audio waveforms are discretized into tokens for audio modeling, generation, or alignment, which is the core inclusion criterion. Despite mentioning vector quantization, it does not pertain to discrete audio tokens or related downstream audio/multimodal tasks but rather to trajectory sampling distributions for driving safety. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses using vector quantized variational autoencoder (VQ-VAE) with vector quantization, a neural codec technique, to learn sampling distributions for autonomous driving trajectories. However, the domain focus is on autonomous driving trajectory planning rather than audio signals; no continuous audio waveforms are discretized into tokens for audio modeling, generation, or alignment, which is the core inclusion criterion. Despite mentioning vector quantization, it does not pertain to discrete audio tokens or related downstream audio/multimodal tasks but rather to trajectory sampling distributions for driving safety. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and related audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Properties of Speech Language Models",
    "abstract": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
    "metadata": {
      "arxiv_id": "2404.00685",
      "title": "Scaling Properties of Speech Language Models",
      "summary": "Speech Language Models (SLMs) aim to learn language from raw audio, without textual resources. Despite significant advances, our current models exhibit weak syntax and semantic abilities. However, if the scaling properties of neural language models hold for the speech modality, these abilities will improve as the amount of compute used for training increases. In this paper, we use models of this scaling behavior to estimate the scale at which our current methods will yield a SLM with the English proficiency of text-based Large Language Models (LLMs). We establish a strong correlation between pre-training loss and downstream syntactic and semantic performance in SLMs and LLMs, which results in predictable scaling of linguistic performance. We show that the linguistic performance of SLMs scales up to three orders of magnitude more slowly than that of text-based LLMs. Additionally, we study the benefits of synthetic data designed to boost semantic understanding and the effects of coarser speech tokenization.",
      "authors": [
        "Santiago Cuervo",
        "Ricard Marxer"
      ],
      "published": "2024-03-31T13:30:12Z",
      "updated": "2024-04-16T06:46:18Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00685v2",
      "landing_url": "https://arxiv.org/abs/2404.00685v2",
      "doi": "https://doi.org/10.18653/v1/2024.emnlp-main.21"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract focuses on scaling laws of SLMs trained on raw audio and discusses tokenization only superficially; it does not present discrete audio tokens or tokenizers as core input representations, so it fails the discrete audio token criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on scaling laws of SLMs trained on raw audio and discusses tokenization only superficially; it does not present discrete audio tokens or tokenizers as core input representations, so it fails the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses Speech Language Models learning from raw audio without textual resources, emphasizing scaling properties and performance improvements, but does not describe or analyze the generation or use of discrete audio tokens via neural audio codecs or self-supervised learning models, nor does it provide detailed tokenizer/codec architecture, quantization, or vocabulary/codebook settings essential for evaluating discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete token sequences for audio modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses Speech Language Models learning from raw audio without textual resources, emphasizing scaling properties and performance improvements, but does not describe or analyze the generation or use of discrete audio tokens via neural audio codecs or self-supervised learning models, nor does it provide detailed tokenizer/codec architecture, quantization, or vocabulary/codebook settings essential for evaluating discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete token sequences for audio modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling",
    "abstract": "Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light's phonetic ABX task and SUPERB's speaker identification task.",
    "metadata": {
      "arxiv_id": "2404.00856",
      "title": "Removing Speaker Information from Speech Representation using Variable-Length Soft Pooling",
      "summary": "Recently, there have been efforts to encode the linguistic information of speech using a self-supervised framework for speech synthesis. However, predicting representations from surrounding representations can inadvertently entangle speaker information in the speech representation. This paper aims to remove speaker information by exploiting the structured nature of speech, composed of discrete units like phonemes with clear boundaries. A neural network predicts these boundaries, enabling variable-length pooling for event-based representation extraction instead of fixed-rate methods. The boundary predictor outputs a probability for the boundary between 0 and 1, making pooling soft. The model is trained to minimize the difference with the pooled representation of the data augmented by time-stretch and pitch-shift. To confirm that the learned representation includes contents information but is independent of speaker information, the model was evaluated with libri-light's phonetic ABX task and SUPERB's speaker identification task.",
      "authors": [
        "Injune Hwang",
        "Kyogu Lee"
      ],
      "published": "2024-04-01T01:49:09Z",
      "updated": "2024-04-01T01:49:09Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00856v1",
      "landing_url": "https://arxiv.org/abs/2404.00856v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.00856"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on removing speaker information via boundary-based pooling and does not explicitly introduce or analyze discrete audio tokens (tokenization/quantization) or a tokenizer/codec with a discrete vocabulary, so it does not meet the Discrete Audio Tokens criteria despite mentioning phoneme-like boundaries.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on removing speaker information via boundary-based pooling and does not explicitly introduce or analyze discrete audio tokens (tokenization/quantization) or a tokenizer/codec with a discrete vocabulary, so it does not meet the Discrete Audio Tokens criteria despite mentioning phoneme-like boundaries.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses removing speaker information from speech representations by using variable-length soft pooling based on predicted boundaries but does not describe any discretization process that converts continuous audio into discrete token sequences, nor does it mention any tokenizer, codebook, or vector quantization details. The focus is on speech representation learning rather than discrete audio token generation or usage, and no evidence shows the study involves discrete tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses removing speaker information from speech representations by using variable-length soft pooling based on predicted boundaries but does not describe any discretization process that converts continuous audio into discrete token sequences, nor does it mention any tokenizer, codebook, or vector quantization details. The focus is on speech representation learning rather than discrete audio token generation or usage, and no evidence shows the study involves discrete tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
    "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
    "metadata": {
      "arxiv_id": "2404.03204",
      "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis",
      "summary": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis. While previous work based on large language models (LLMs) shows impressive performance on zero-shot TTS, such methods often suffer from poor robustness, such as unstable prosody (weird pitch and rhythm/duration) and a high word error rate (WER), due to the autoregressive prediction style of language models. The core idea behind RALL-E is chain-of-thought (CoT) prompting, which decomposes the task into simpler steps to enhance the robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts prosody features (pitch and duration) of the input text and uses them as intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer to enforce the model to focus on the corresponding phonemes and prosody features when predicting speech tokens. Results of comprehensive objective and subjective evaluations demonstrate that, compared to a powerful baseline method VALL-E, RALL-E significantly improves the WER of zero-shot TTS from $5.6\\%$ (without reranking) and $1.7\\%$ (with reranking) to $2.5\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E correctly synthesizes sentences that are hard for VALL-E and reduces the error rate from $68\\%$ to $4\\%$.",
      "authors": [
        "Detai Xin",
        "Xu Tan",
        "Kai Shen",
        "Zeqian Ju",
        "Dongchao Yang",
        "Yuancheng Wang",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari",
        "Shujie Liu",
        "Jinyu Li",
        "Sheng Zhao"
      ],
      "published": "2024-04-04T05:15:07Z",
      "updated": "2024-05-19T21:34:28Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03204v3",
      "landing_url": "https://arxiv.org/abs/2404.03204v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.03204"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses chain-of-thought prompting for TTS and robustness but does not specify or analyze discrete audio tokens, tokenizers, or neural codecs as core representational units, so it does not meet the discrete-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses chain-of-thought prompting for TTS and robustness but does not specify or analyze discrete audio tokens, tokenizers, or neural codecs as core representational units, so it does not meet the discrete-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes RALL-E, a method for text-to-speech synthesis using large language models and chain-of-thought prompting to improve prosody prediction and reduce errors in TTS. However, the abstract does not describe the usage or design of discrete audio tokens derived via neural audio codecs or self-supervised models for discretizing continuous audio into finite vocabularies, nor does it detail any tokenizer, codebook, vector quantization, or related mechanisms central to discrete token generation. The focus is primarily on language model prompt strategies and prosody guidance for TTS rather than the discrete audio token representation core to the inclusion criteria. Therefore, it likely falls outside the defined topic of discrete audio tokens as the core modeling research, and more closely aligns with continuous or non-discrete representations or features in TTS.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes RALL-E, a method for text-to-speech synthesis using large language models and chain-of-thought prompting to improve prosody prediction and reduce errors in TTS. However, the abstract does not describe the usage or design of discrete audio tokens derived via neural audio codecs or self-supervised models for discretizing continuous audio into finite vocabularies, nor does it detail any tokenizer, codebook, vector quantization, or related mechanisms central to discrete token generation. The focus is primarily on language model prompt strategies and prosody guidance for TTS rather than the discrete audio token representation core to the inclusion criteria. Therefore, it likely falls outside the defined topic of discrete audio tokens as the core modeling research, and more closely aligns with continuous or non-discrete representations or features in TTS.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
    "abstract": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
    "metadata": {
      "arxiv_id": "2404.03663",
      "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
      "summary": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
      "authors": [
        "Man Yao",
        "Jiakui Hu",
        "Tianxiang Hu",
        "Yifan Xu",
        "Zhaokun Zhou",
        "Yonghong Tian",
        "Bo Xu",
        "Guoqi Li"
      ],
      "published": "2024-02-15T13:26:18Z",
      "updated": "2024-02-15T13:26:18Z",
      "categories": [
        "cs.NE",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03663v1",
      "landing_url": "https://arxiv.org/abs/2404.03663v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.03663"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦脈衝驅動的 Transformer 架構與神經形態晶片設計，未提出或分析任何離散音訊 token 的產生機制、碼本/字彙表設定，且主題不屬於離散音訊 token 的研究範疇，因此應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦脈衝驅動的 Transformer 架構與神經形態晶片設計，未提出或分析任何離散音訊 token 的產生機制、碼本/字彙表設定，且主題不屬於離散音訊 token 的研究範疇，因此應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on neuromorphic computing using Spiking Neural Networks and Transformer architectures for vision tasks, without addressing discrete audio tokens or their discretization from continuous audio waveforms as required by the inclusion criteria; it does not discuss audio tokenizers, codecs, or any related audio token representation or generation, so it is irrelevant to the designated theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on neuromorphic computing using Spiking Neural Networks and Transformer architectures for vision tasks, without addressing discrete audio tokens or their discretization from continuous audio waveforms as required by the inclusion criteria; it does not discuss audio tokenizers, codecs, or any related audio token representation or generation, so it is irrelevant to the designated theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Region Aware Autonomous Exploration for Multi-Type Map Construction in Unknown Indoor Environments",
    "abstract": "Mainstream autonomous exploration methods usually perform excessively-repeated explorations for the same region, leading to long exploration time and exploration trajectory in complex scenes. To handle this issue, we propose a novel semantic region aware autonomous exploration method, the core idea of which is considering the information of semantic regions to optimize the autonomous navigation strategy. Our method enables the mobile robot to fully explore the current semantic region before moving to the next region, contributing to avoid excessively-repeated explorations and accelerate the exploration speed. In addition, compared with existing au?tonomous exploration methods that usually construct the single-type map, our method allows to construct four types of maps including point cloud map, occupancy grid map, topological map, and semantic map. The experiment results demonstrate that our method achieves the highest 50.7% exploration time reduction and 48.1% exploration trajectory length reduction while maintaining >98% exploration rate when comparing with the classical RRT (Rapid-exploration Random Tree) based autonomous exploration method.",
    "metadata": {
      "arxiv_id": "2404.04879",
      "title": "Semantic Region Aware Autonomous Exploration for Multi-Type Map Construction in Unknown Indoor Environments",
      "summary": "Mainstream autonomous exploration methods usually perform excessively-repeated explorations for the same region, leading to long exploration time and exploration trajectory in complex scenes. To handle this issue, we propose a novel semantic region aware autonomous exploration method, the core idea of which is considering the information of semantic regions to optimize the autonomous navigation strategy. Our method enables the mobile robot to fully explore the current semantic region before moving to the next region, contributing to avoid excessively-repeated explorations and accelerate the exploration speed. In addition, compared with existing au?tonomous exploration methods that usually construct the single-type map, our method allows to construct four types of maps including point cloud map, occupancy grid map, topological map, and semantic map. The experiment results demonstrate that our method achieves the highest 50.7% exploration time reduction and 48.1% exploration trajectory length reduction while maintaining >98% exploration rate when comparing with the classical RRT (Rapid-exploration Random Tree) based autonomous exploration method.",
      "authors": [
        "Jianfang Mao"
      ],
      "published": "2024-04-07T08:49:09Z",
      "updated": "2024-10-10T11:56:00Z",
      "categories": [
        "cs.RO",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04879v2",
      "landing_url": "https://arxiv.org/abs/2404.04879v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04879"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about semantic-region aware autonomous exploration and multi-type map construction in unknown indoor environments, which is not about discrete audio tokens or tokenization; it fails to meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about semantic-region aware autonomous exploration and multi-type map construction in unknown indoor environments, which is not about discrete audio tokens or tokenization; it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on autonomous exploration methods for indoor environment mapping and does not address discrete audio tokens, neural audio codecs, or any audio discretization processes; therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is outside the scope of the review topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on autonomous exploration methods for indoor environment mapping and does not address discrete audio tokens, neural audio codecs, or any audio discretization processes; therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is outside the scope of the review topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MLP Can Be A Good Transformer Learner",
    "abstract": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
    "metadata": {
      "arxiv_id": "2404.05657",
      "title": "MLP Can Be A Good Transformer Learner",
      "summary": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
      "authors": [
        "Sihao Lin",
        "Pumeng Lyu",
        "Dongrui Liu",
        "Tao Tang",
        "Xiaodan Liang",
        "Andy Song",
        "Xiaojun Chang"
      ],
      "published": "2024-04-08T16:40:15Z",
      "updated": "2024-04-08T16:40:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05657v1",
      "landing_url": "https://arxiv.org/abs/2404.05657v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.05657"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns pruning attention layers in vision transformers for ImageNet, not about discrete audio tokens or tokenization/quantization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns pruning attention layers in vision transformers for ImageNet, not about discrete audio tokens or tokenization/quantization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on simplifying vision transformers by selectively removing attention layers in image processing tasks, without discussing discrete audio tokens, neural audio codecs, or discretization of continuous audio signals into token sequences, which are the core focus of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on simplifying vision transformers by selectively removing attention layers in image processing tasks, without discussing discrete audio tokens, neural audio codecs, or discretization of continuous audio signals into token sequences, which are the core focus of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
    "abstract": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe discrete speech tokens produced by neural/audio codec methods and used for downstream tasks (TTS, SVS, ASR) with English content and evaluable experiments, meeting all inclusion criteria and not triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe discrete speech tokens produced by neural/audio codec methods and used for downstream tasks (TTS, SVS, ASR) with English content and evaluable experiments, meeting all inclusion criteria and not triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes systems for TTS, SVS, and ASR based on discrete speech tokens derived from neural encoding and achieves strong performance, which aligns well with the inclusion criteria centered on discrete audio tokens for downstream generative and modeling tasks; however, the abstract does not explicitly detail the tokenizer architecture, quantization methods, or codebook settings to fully satisfy reproducibility requirements, leading to moderate uncertainty.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper describes systems for TTS, SVS, and ASR based on discrete speech tokens derived from neural encoding and achieves strong performance, which aligns well with the inclusion criteria centered on discrete audio tokens for downstream generative and modeling tasks; however, the abstract does not explicitly detail the tokenizer architecture, quantization methods, or codebook settings to fully satisfy reproducibility requirements, leading to moderate uncertainty.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
    "abstract": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
    "metadata": {
      "arxiv_id": "2404.06690",
      "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
      "summary": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
      "authors": [
        "Leying Zhang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Midia Yousefi",
        "Yanmin Qian",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "published": "2024-04-10T02:32:58Z",
      "updated": "2024-12-15T16:30:54Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06690v3",
      "landing_url": "https://arxiv.org/abs/2404.06690v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.06690"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions converting dialogue text into discrete token streams representing semantic info for talkers, but it does not describe discrete audio tokens or tokenization/codec settings for audio discretization, so it likely does not meet the discrete audio token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions converting dialogue text into discrete token streams representing semantic info for talkers, but it does not describe discrete audio tokens or tokenization/codec settings for audio discretization, so it likely does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a zero-shot speech generation model called CoVoMix that converts dialogue text into multiple streams of discrete tokens representing individual speaker semantics, then generates speech from these tokens. This indicates the use of discrete audio tokens as core representations for multi-speaker dialogue speech generation. However, the abstract lacks detailed information about the tokenizer/codec architecture, quantization methods, or vocabulary/codebook settings, which are necessary to confirm adherence to the inclusion criteria regarding the detailed description of discrete token generation to support reproducibility. Despite meeting the conceptual inclusion criteria, the missing technical detail on discrete token generation and processing prevents full assessment and reproducibility evaluation.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a zero-shot speech generation model called CoVoMix that converts dialogue text into multiple streams of discrete tokens representing individual speaker semantics, then generates speech from these tokens. This indicates the use of discrete audio tokens as core representations for multi-speaker dialogue speech generation. However, the abstract lacks detailed information about the tokenizer/codec architecture, quantization methods, or vocabulary/codebook settings, which are necessary to confirm adherence to the inclusion criteria regarding the detailed description of discrete token generation to support reproducibility. Despite meeting the conceptual inclusion criteria, the missing technical detail on discrete token generation and processing prevents full assessment and reproducibility evaluation.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers note missing evidence of discrete audio-token usage: JuniorNano correctly observed the abstract does not describe discrete audio tokenization, and JuniorMini noted conceptual token streams but highlighted absent tokenizer/codec/quantization/codebook details needed for inclusion; the paper describes converting dialogue text into discrete semantic token streams (text-side) feeding an acoustic model and HiFi-GAN, but it does not claim or document discretization of continuous audio waveforms via a neural codec or provide reproducible tokenizer/codec specifications required by the \"Discrete Audio Tokens\" topic—therefore it fails the inclusion criteria. Explanation (1 sentence): The work focuses on text-derived token streams for TTS and lacks any description of discrete audio-token generation (neural codec/quantization/codebook), so it should be excluded.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers note missing evidence of discrete audio-token usage: JuniorNano correctly observed the abstract does not describe discrete audio tokenization, and JuniorMini noted conceptual token streams but highlighted absent tokenizer/codec/quantization/codebook details needed for inclusion; the paper describes converting dialogue text into discrete semantic token streams (text-side) feeding an acoustic model and HiFi-GAN, but it does not claim or document discretization of continuous audio waveforms via a neural codec or provide reproducible tokenizer/codec specifications required by the \"Discrete Audio Tokens\" topic—therefore it fails the inclusion criteria. Explanation (1 sentence): The work focuses on text-derived token streams for TTS and lacks any description of discrete audio-token generation (neural codec/quantization/codebook), so it should be excluded.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "How Users Experience Closed Captions on Live Television: Quality Metrics Remain a Challenge",
    "abstract": "This paper presents a mixed methods study on how deaf, hard of hearing and hearing viewers perceive live TV caption quality with captioned video stimuli designed to mirror TV captioning experiences. To assess caption quality, we used four commonly-used quality metrics focusing on accuracy: word error rate, weighted word error rate, automated caption evaluation (ACE), and its successor ACE2. We calculated the correlation between the four quality metrics and viewer ratings for subjective quality and found that the correlation was weak, revealing that other factors besides accuracy affect user ratings. Additionally, even high-quality captions are perceived to have problems, despite controlling for confounding factors. Qualitative analysis of viewer comments revealed three major factors affecting their experience: Errors within captions, difficulty in following captions, and caption appearance. The findings raise questions as to how objective caption quality metrics can be reconciled with the user experience across a diverse spectrum of viewers.",
    "metadata": {
      "arxiv_id": "2404.10153",
      "title": "How Users Experience Closed Captions on Live Television: Quality Metrics Remain a Challenge",
      "summary": "This paper presents a mixed methods study on how deaf, hard of hearing and hearing viewers perceive live TV caption quality with captioned video stimuli designed to mirror TV captioning experiences. To assess caption quality, we used four commonly-used quality metrics focusing on accuracy: word error rate, weighted word error rate, automated caption evaluation (ACE), and its successor ACE2. We calculated the correlation between the four quality metrics and viewer ratings for subjective quality and found that the correlation was weak, revealing that other factors besides accuracy affect user ratings. Additionally, even high-quality captions are perceived to have problems, despite controlling for confounding factors. Qualitative analysis of viewer comments revealed three major factors affecting their experience: Errors within captions, difficulty in following captions, and caption appearance. The findings raise questions as to how objective caption quality metrics can be reconciled with the user experience across a diverse spectrum of viewers.",
      "authors": [
        "Mariana Arroyo Chavez",
        "Molly Feanny",
        "Matthew Seita",
        "Bernard Thompson",
        "Keith Delk",
        "Skyler Officer",
        "Abraham Glasser",
        "Raja Kushalnagar",
        "Christian Vogler"
      ],
      "published": "2024-04-15T21:55:29Z",
      "updated": "2024-04-15T21:55:29Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.10153v1",
      "landing_url": "https://arxiv.org/abs/2404.10153v1",
      "doi": "https://doi.org/10.1145/3613904.3641988"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on user-perceived quality of live television captions with traditional accuracy metrics, not on discrete audio tokens, tokenizers, or audio token representations; it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on user-perceived quality of live television captions with traditional accuracy metrics, not on discrete audio tokens, tokenizers, or audio token representations; it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating the quality of live TV closed captions from user experience and quality metrics perspectives, without any discussion or use of discrete audio tokens, neural audio codecs, or tokenization schemes mapping audio waveforms to discrete sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens or their applications in generation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating the quality of live TV closed captions from user experience and quality metrics perspectives, without any discussion or use of discrete audio tokens, neural audio codecs, or tokenization schemes mapping audio waveforms to discrete sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens or their applications in generation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MIDGET: Music Conditioned 3D Dance Generation",
    "abstract": "In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.",
    "metadata": {
      "arxiv_id": "2404.12062",
      "title": "MIDGET: Music Conditioned 3D Dance Generation",
      "summary": "In this paper, we introduce a MusIc conditioned 3D Dance GEneraTion model, named MIDGET based on Dance motion Vector Quantised Variational AutoEncoder (VQ-VAE) model and Motion Generative Pre-Training (GPT) model to generate vibrant and highquality dances that match the music rhythm. To tackle challenges in the field, we introduce three new components: 1) a pre-trained memory codebook based on the Motion VQ-VAE model to store different human pose codes, 2) employing Motion GPT model to generate pose codes with music and motion Encoders, 3) a simple framework for music feature extraction. We compare with existing state-of-the-art models and perform ablation experiments on AIST++, the largest publicly available music-dance dataset. Experiments demonstrate that our proposed framework achieves state-of-the-art performance on motion quality and its alignment with the music.",
      "authors": [
        "Jinwu Wang",
        "Wei Mao",
        "Miaomiao Liu"
      ],
      "published": "2024-04-18T10:20:37Z",
      "updated": "2024-04-18T10:20:37Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.GR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.12062v1",
      "landing_url": "https://arxiv.org/abs/2404.12062v1",
      "doi": "https://doi.org/10.1007/978-981-99-8388-9_23"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on music-conditioned 3D dance generation using discrete motion tokens via a Motion VQ-VAE and Motion GPT, but it does not define or evaluate discrete audio tokens (from audio sources) as the core input or representation; thus it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on music-conditioned 3D dance generation using discrete motion tokens via a Motion VQ-VAE and Motion GPT, but it does not define or evaluate discrete audio tokens (from audio sources) as the core input or representation; thus it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on music-conditioned 3D dance generation using a vector quantized variational autoencoder (VQ-VAE) and a GPT model to generate pose codes aligned with music rhythm. However, the discrete tokens discussed are related to dance motion poses rather than discrete audio tokens derived from audio waveform discretization, neural audio codecs, or self-supervised audio representation learning described in the inclusion criteria. The paper does not provide details on the generation of discrete audio tokens from continuous audio nor focuses on discrete audio token design, evaluation, or usage for audio generation or cross-modal alignment with text or images. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on music-conditioned 3D dance generation using a vector quantized variational autoencoder (VQ-VAE) and a GPT model to generate pose codes aligned with music rhythm. However, the discrete tokens discussed are related to dance motion poses rather than discrete audio tokens derived from audio waveform discretization, neural audio codecs, or self-supervised audio representation learning described in the inclusion criteria. The paper does not provide details on the generation of discrete audio tokens from continuous audio nor focuses on discrete audio token design, evaluation, or usage for audio generation or cross-modal alignment with text or images. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
    "abstract": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets semantic tokenization for generative recommendation and uses tokens for items, not discrete audio tokens, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets semantic tokenization for generative recommendation and uses tokens for items, not discrete audio tokens, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semantic tokenization for generative recommendation systems using contrastive quantization, but it does not address discretization of continuous audio waveforms into discrete audio tokens for audio modeling, generation, or cross-modal reasoning as defined in the inclusion criteria. It instead pertains to semantic tokens for item recommendation, unrelated to neural audio codecs or self-supervised audio representation discretization, and no audio-related tokenization details or evaluations are provided.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semantic tokenization for generative recommendation systems using contrastive quantization, but it does not address discretization of continuous audio waveforms into discrete audio tokens for audio modeling, generation, or cross-modal reasoning as defined in the inclusion criteria. It instead pertains to semantic tokens for item recommendation, unrelated to neural audio codecs or self-supervised audio representation discretization, and no audio-related tokenization details or evaluations are provided.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
    "abstract": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
    "metadata": {
      "arxiv_id": "2404.16123",
      "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
      "summary": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
      "authors": [
        "Eric Slyman",
        "Stefan Lee",
        "Scott Cohen",
        "Kushal Kafle"
      ],
      "published": "2024-04-24T18:28:17Z",
      "updated": "2024-04-24T18:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16123v1",
      "landing_url": "https://arxiv.org/abs/2404.16123v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.16123"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on vision-language fairness in semantic dataset deduplication and CLIP-style models, not on discrete audio tokens or tokenization schemes for audio, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on vision-language fairness in semantic dataset deduplication and CLIP-style models, not on discrete audio tokens or tokenization schemes for audio, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on fairness disparities in vision-language dataset deduplication and does not discuss any methods related to discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are required by the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on fairness disparities in vision-language dataset deduplication and does not discuss any methods related to discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are required by the inclusion criteria. Therefore, it does not meet the thematic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
    "abstract": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
    "metadata": {
      "arxiv_id": "2404.16743",
      "title": "Automatic Speech Recognition System-Independent Word Error Rate Estimation",
      "summary": "Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.",
      "authors": [
        "Chanho Park",
        "Mingjie Chen",
        "Thomas Hain"
      ],
      "published": "2024-04-25T16:57:05Z",
      "updated": "2024-04-26T11:11:02Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.16743v2",
      "landing_url": "https://arxiv.org/abs/2404.16743v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.16743"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns ASR-based word error rate estimation and system-independent evaluation not involving discrete audio tokens or tokenization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns ASR-based word error rate estimation and system-independent evaluation not involving discrete audio tokens or tokenization, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on estimating word error rate for ASR outputs but does not mention the use or design of discrete audio tokens or any discretization process mapping continuous audio into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens and their use in generation, understanding, or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on estimating word error rate for ASR outputs but does not mention the use or design of discrete audio tokens or any discretization process mapping continuous audio into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens and their use in generation, understanding, or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
    "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
    "metadata": {
      "arxiv_id": "2404.19441",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "authors": [
        "Yuzhe Gu",
        "Enmao Diao"
      ],
      "published": "2024-04-30T10:44:33Z",
      "updated": "2024-10-03T12:23:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19441v3",
      "landing_url": "https://arxiv.org/abs/2404.19441v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.19441"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The ESC paper introduces a neural speech codec built on cross-scale residual vector quantization, implying discrete token representations via vector quantization, which aligns with the discrete audio token topic and token-based modeling/downstream potential, and it provides English title/abstract with details on quantization and codebook-like schemes; thus it meets inclusion without hitting exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The ESC paper introduces a neural speech codec built on cross-scale residual vector quantization, implying discrete token representations via vector quantization, which aligns with the discrete audio token topic and token-based modeling/downstream potential, and it provides English title/abstract with details on quantization and codebook-like schemes; thus it meets inclusion without hitting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a neural speech codec (ESC) that uses cross-scale residual vector quantization and transformers, which suggests it outputs discrete tokens via vector quantization for speech compression; the abstract mentions hierarchical multi-scale representation and vector quantization techniques, aligning with the inclusion criteria about discrete audio tokens generated by neural codecs with multi-codebook and multi-granularity characteristics. The description focuses on codec design, efficiency, and reconstruction quality, consistent with research improving discrete audio token schemes, while explicitly detailing vector quantization and residual quantization components, thus fulfilling the requirement for clear token generation mechanisms. There is no indication that the tokens are continuous representations or only discriminative features, and the abstract is in English with sufficient technical detail. Therefore, the study fits inclusion criteria and has no evident exclusion reasons.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents a neural speech codec (ESC) that uses cross-scale residual vector quantization and transformers, which suggests it outputs discrete tokens via vector quantization for speech compression; the abstract mentions hierarchical multi-scale representation and vector quantization techniques, aligning with the inclusion criteria about discrete audio tokens generated by neural codecs with multi-codebook and multi-granularity characteristics. The description focuses on codec design, efficiency, and reconstruction quality, consistent with research improving discrete audio token schemes, while explicitly detailing vector quantization and residual quantization components, thus fulfilling the requirement for clear token generation mechanisms. There is no indication that the tokens are continuous representations or only discriminative features, and the abstract is in English with sufficient technical detail. Therefore, the study fits inclusion criteria and has no evident exclusion reasons.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
    "abstract": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
    "metadata": {
      "arxiv_id": "2405.00930",
      "title": "MAIN-VC: Lightweight Speech Representation Disentanglement for One-shot Voice Conversion",
      "summary": "One-shot voice conversion aims to change the timbre of any source speech to match that of the unseen target speaker with only one speech sample. Existing methods face difficulties in satisfactory speech representation disentanglement and suffer from sizable networks as some of them leverage numerous complex modules for disentanglement. In this paper, we propose a model named MAIN-VC to effectively disentangle via a concise neural network. The proposed model utilizes Siamese encoders to learn clean representations, further enhanced by the designed mutual information estimator. The Siamese structure and the newly designed convolution module contribute to the lightweight of our model while ensuring performance in diverse voice conversion tasks. The experimental results show that the proposed model achieves comparable subjective scores and exhibits improvements in objective metrics compared to existing methods in a one-shot voice conversion scenario.",
      "authors": [
        "Pengcheng Li",
        "Jianzong Wang",
        "Xulong Zhang",
        "Yong Zhang",
        "Jing Xiao",
        "Ning Cheng"
      ],
      "published": "2024-05-02T01:11:15Z",
      "updated": "2024-11-24T09:30:29Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00930v2",
      "landing_url": "https://arxiv.org/abs/2405.00930v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.00930"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on disentangled representations for one-shot voice conversion and lightweight networks, with no mention of discrete audio tokens, tokenization, or token-based downstream tasks required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on disentangled representations for one-shot voice conversion and lightweight networks, with no mention of discrete audio tokens, tokenization, or token-based downstream tasks required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on one-shot voice conversion using Siamese encoders and mutual information estimators to disentangle speech representations; however, there is no mention of discretizing continuous audio waveforms into discrete token sequences or the use of neural audio codecs/vector quantization that produce discrete tokens, nor is there an explanation of token vocabularies or codebooks, which are critical inclusion criteria. The method appears to operate on continuous or entangled speech representations rather than discrete audio tokens, and no discrete token design, evaluation, or downstream application involving discrete tokens is described, thus not meeting the core requirements for the \"Discrete Audio Tokens\" topic. Therefore, the study should be excluded according to the provided criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on one-shot voice conversion using Siamese encoders and mutual information estimators to disentangle speech representations; however, there is no mention of discretizing continuous audio waveforms into discrete token sequences or the use of neural audio codecs/vector quantization that produce discrete tokens, nor is there an explanation of token vocabularies or codebooks, which are critical inclusion criteria. The method appears to operate on continuous or entangled speech representations rather than discrete audio tokens, and no discrete token design, evaluation, or downstream application involving discrete tokens is described, thus not meeting the core requirements for the \"Discrete Audio Tokens\" topic. Therefore, the study should be excluded according to the provided criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
    "abstract": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
    "metadata": {
      "arxiv_id": "2405.01242",
      "title": "TRAMBA: A Hybrid Transformer and Mamba Architecture for Practical Audio and Bone Conduction Speech Super Resolution and Enhancement on Mobile and Wearable Platforms",
      "summary": "We propose TRAMBA, a hybrid transformer and Mamba architecture for acoustic and bone conduction speech enhancement, suitable for mobile and wearable platforms. Bone conduction speech enhancement has been impractical to adopt in mobile and wearable platforms for several reasons: (i) data collection is labor-intensive, resulting in scarcity; (ii) there exists a performance gap between state of-art models with memory footprints of hundreds of MBs and methods better suited for resource-constrained systems. To adapt TRAMBA to vibration-based sensing modalities, we pre-train TRAMBA with audio speech datasets that are widely available. Then, users fine-tune with a small amount of bone conduction data. TRAMBA outperforms state-of-art GANs by up to 7.3% in PESQ and 1.8% in STOI, with an order of magnitude smaller memory footprint and an inference speed up of up to 465 times. We integrate TRAMBA into real systems and show that TRAMBA (i) improves battery life of wearables by up to 160% by requiring less data sampling and transmission; (ii) generates higher quality voice in noisy environments than over-the-air speech; (iii) requires a memory footprint of less than 20.0 MB.",
      "authors": [
        "Yueyuan Sui",
        "Minghui Zhao",
        "Junxi Xia",
        "Xiaofan Jiang",
        "Stephen Xia"
      ],
      "published": "2024-05-02T12:45:48Z",
      "updated": "2024-05-29T15:46:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.01242v3",
      "landing_url": "https://arxiv.org/abs/2405.01242v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.01242"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not mention discrete audio tokens, tokenization, neural audio codecs, or token-based modeling; it focuses on a hybrid transformer architecture for enhancement on mobile wearables rather than tokenization-centric topics, so it fails all inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not mention discrete audio tokens, tokenization, neural audio codecs, or token-based modeling; it focuses on a hybrid transformer architecture for enhancement on mobile wearables rather than tokenization-centric topics, so it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a hybrid transformer architecture for speech enhancement on mobile and wearable platforms, but does not mention any discrete audio tokens, neural audio codecs, vector quantization, or discretization processes mapping continuous audio to token sequences — core to the inclusion criteria. The abstract emphasizes model performance, memory footprint, and real-world deployment rather than the design, generation, or evaluation of discrete audio tokens. It thus does not satisfy the requirement of using discrete token sequences as core representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a hybrid transformer architecture for speech enhancement on mobile and wearable platforms, but does not mention any discrete audio tokens, neural audio codecs, vector quantization, or discretization processes mapping continuous audio to token sequences — core to the inclusion criteria. The abstract emphasizes model performance, memory footprint, and real-world deployment rather than the design, generation, or evaluation of discrete audio tokens. It thus does not satisfy the requirement of using discrete token sequences as core representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
    "abstract": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Does not address discrete audio tokens or tokenization via neural audio codecs; focuses on general semantic token selection in transformers with image patches, not discrete audio token design or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio tokens or tokenization via neural audio codecs; focuses on general semantic token selection in transformers with image patches, not discrete audio token design or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses adaptive semantic token selection in transformer-based AI-native communication systems, focusing on image patch tokens under bandwidth and latency constraints. It does not mention audio signals or discrete audio tokens derived from neural audio codecs or self-supervised audio models for speech, music, or environmental sounds, nor does it address token design choices for audio generation, understanding, or cross-modal alignment, thus failing to meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses adaptive semantic token selection in transformer-based AI-native communication systems, focusing on image patch tokens under bandwidth and latency constraints. It does not mention audio signals or discrete audio tokens derived from neural audio codecs or self-supervised audio models for speech, music, or environmental sounds, nor does it address token design choices for audio generation, understanding, or cross-modal alignment, thus failing to meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
    "abstract": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
    "metadata": {
      "arxiv_id": "2405.02682",
      "title": "Deduplicator: When Computation Reuse Meets Load Balancing at the Network Edge",
      "summary": "Load balancing has been a fundamental building block of cloud and, more recently, edge computing environments. At the same time, in edge computing environments, prior research has highlighted that applications operate on similar (correlated) data. Based on this observation, prior research has advocated for the direction of \"computation reuse\", where the results of previously executed computational tasks are stored at the edge and are reused (if possible) to satisfy incoming tasks with similar input data, instead of executing incoming tasks from scratch. Both load balancing and computation reuse are critical to the deployment of scalable edge computing environments, yet they are contradictory in nature. In this paper, we propose the Deduplicator, a middlebox that aims to facilitate both load balancing and computation reuse at the edge. The Deduplicator features mechanisms to identify and deduplicate similar tasks offloaded by user devices, collect information about the usage of edge servers' resources, manage the addition of new edge servers and the failures of existing edge servers, and ultimately balance the load imposed on edge servers. Our evaluation results demonstrate that the Deduplicator achieves up to 20% higher percentages of computation reuse compared to several other load balancing approaches, while also effectively balancing the distribution of tasks among edge servers at line rate.",
      "authors": [
        "Md Washik Al Azad",
        "Spyridon Mastorakis"
      ],
      "published": "2024-05-04T14:48:19Z",
      "updated": "2024-05-04T14:48:19Z",
      "categories": [
        "cs.DC",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02682v1",
      "landing_url": "https://arxiv.org/abs/2405.02682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02682"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article is about a middlebox for deduplication and load balancing at the network edge and shows no discussion of discrete audio tokens, tokenization, or related token design; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article is about a middlebox for deduplication and load balancing at the network edge and shows no discussion of discrete audio tokens, tokenization, or related token design; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on computation reuse and load balancing in edge computing, without any indication of involving discrete audio tokens, neural audio codecs, or tokenization processes related to audio waveform discretization; thus, it does not meet the inclusion criteria related to discrete audio tokens and their applications.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on computation reuse and load balancing in edge computing, without any indication of involving discrete audio tokens, neural audio codecs, or tokenization processes related to audio waveform discretization; thus, it does not meet the inclusion criteria related to discrete audio tokens and their applications.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
    "abstract": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on vector quantization techniques in recommender systems, not on discrete audio tokens, tokenizers/codebooks, or audio token design/assessment as defined in the inclusion criteria; thus it does not meet the criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on vector quantization techniques in recommender systems, not on discrete audio tokens, tokenizers/codebooks, or audio token design/assessment as defined in the inclusion criteria; thus it does not meet the criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization techniques for recommender systems and does not address discretized audio tokens or neural audio codecs applied to audio waveform discretization, generation, or cross-modal reasoning. It lacks specific discussion on token design choices, evaluation metrics related to audio tokens, or applications in audio generation or understanding tasks, thus failing to meet the inclusion criteria centered on \"Discrete Audio Tokens.\"",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization techniques for recommender systems and does not address discretized audio tokens or neural audio codecs applied to audio waveform discretization, generation, or cross-modal reasoning. It lacks specific discussion on token design choices, evaluation metrics related to audio tokens, or applications in audio generation or understanding tasks, thus failing to meet the inclusion criteria centered on \"Discrete Audio Tokens.\"",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
    "abstract": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
    "metadata": {
      "arxiv_id": "2405.06266",
      "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
      "summary": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
      "authors": [
        "Jianli Xiao",
        "Baichao Long"
      ],
      "published": "2024-05-10T06:37:07Z",
      "updated": "2024-05-10T06:37:07Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06266v1",
      "landing_url": "https://arxiv.org/abs/2405.06266v1",
      "doi": "https://doi.org/10.1016/j.ins.2024.120648"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a multi-channel spatial-temporal transformer for traffic flow forecasting, with no discussion of discrete audio tokens, tokenization, or audio-centric modeling, thus it does not meet the inclusion criteria and falls under exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a multi-channel spatial-temporal transformer for traffic flow forecasting, with no discussion of discrete audio tokens, tokenization, or audio-centric modeling, thus it does not meet the inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on traffic flow forecasting using a transformer model, dealing with spatial-temporal features of traffic data, and does not discuss discrete audio tokens, audio waveform discretization, or related tokenization processes; thus, it does not meet the inclusion criteria related to discrete audio tokens and audio representation modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on traffic flow forecasting using a transformer model, dealing with spatial-temporal features of traffic data, and does not discuss discrete audio tokens, audio waveform discretization, or related tokenization processes; thus, it does not meet the inclusion criteria related to discrete audio tokens and audio representation modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable Image Generation With Composed Parallel Token Prediction",
    "abstract": "Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation). Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity. To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space. Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fréchet Inception Distance (FID) scores. Our method attains an average generation accuracy of $80.71\\%$ across the studied settings. Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting. We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation.",
    "metadata": {
      "arxiv_id": "2405.06535",
      "title": "Controllable Image Generation With Composed Parallel Token Prediction",
      "summary": "Compositional image generation requires models to generalise well in situations where two or more input concepts do not necessarily appear together in training (compositional generalisation). Despite recent progress in compositional image generation via composing continuous sampling processes such as diffusion and energy-based models, composing discrete generative processes has remained an open challenge, with the promise of providing improvements in efficiency, interpretability and simplicity. To this end, we propose a formulation for controllable conditional generation of images via composing the log-probability outputs of discrete generative models of the latent space. Our approach, when applied alongside VQ-VAE and VQ-GAN, achieves state-of-the-art generation accuracy in three distinct settings (FFHQ, Positional CLEVR and Relational CLEVR) while attaining competitive Fréchet Inception Distance (FID) scores. Our method attains an average generation accuracy of $80.71\\%$ across the studied settings. Our method also outperforms the next-best approach (ranked by accuracy) in terms of FID in seven out of nine experiments, with an average FID of $24.23$ (an average improvement of $-9.58$). Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable continuous compositional methods on our hardware. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image) in addition to offering an interpretable dimension of controllability via concept weighting. We further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model without any fine-tuning, allowing for fine-grained control of text-to-image generation.",
      "authors": [
        "Jamie Stirling",
        "Noura Al-Moubayed"
      ],
      "published": "2024-05-10T15:27:35Z",
      "updated": "2024-05-10T15:27:35Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06535v1",
      "landing_url": "https://arxiv.org/abs/2405.06535v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.06535"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria target discrete audio tokens and related tokenization/input modality specifics; this item discusses controllable image generation using discrete latent tokens (VQ-VAE/VQ-GAN) for images, not audio, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria target discrete audio tokens and related tokenization/input modality specifics; this item discusses controllable image generation using discrete latent tokens (VQ-VAE/VQ-GAN) for images, not audio, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on controllable image generation using discrete generative models for images, not on discretizing continuous audio waveforms into tokens for audio modeling or generation. It does not discuss audio codecs or discrete audio tokens as defined in the inclusion criteria, nor does it address audio tasks such as TTS, music generation, or audio-visual alignment, thus failing to meet the core inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on controllable image generation using discrete generative models for images, not on discretizing continuous audio waveforms into tokens for audio modeling or generation. It does not discuss audio codecs or discrete audio tokens as defined in the inclusion criteria, nor does it address audio tasks such as TTS, music generation, or audio-visual alignment, thus failing to meet the core inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
    "abstract": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
    "metadata": {
      "arxiv_id": "2405.06573",
      "title": "An Investigation of Incorporating Mamba for Speech Enhancement",
      "summary": "This work aims to investigate the use of a recently proposed, attention-free, scalable state-space model (SSM), Mamba, for the speech enhancement (SE) task. In particular, we employ Mamba to deploy different regression-based SE models (SEMamba) with different configurations, namely basic, advanced, causal, and non-causal. Furthermore, loss functions either based on signal-level distances or metric-oriented are considered. Experimental evidence shows that SEMamba attains a competitive PESQ of 3.55 on the VoiceBank-DEMAND dataset with the advanced, non-causal configuration. A new state-of-the-art PESQ of 3.69 is also reported when SEMamba is combined with Perceptual Contrast Stretching (PCS). Compared against Transformed-based equivalent SE solutions, a noticeable FLOPs reduction up to ~12% is observed with the advanced non-causal configurations. Finally, SEMamba can be used as a pre-processing step before automatic speech recognition (ASR), showing competitive performance against recent SE solutions.",
      "authors": [
        "Rong Chao",
        "Wen-Huang Cheng",
        "Moreno La Quatra",
        "Sabato Marco Siniscalchi",
        "Chao-Han Huck Yang",
        "Szu-Wei Fu",
        "Yu Tsao"
      ],
      "published": "2024-05-10T16:18:49Z",
      "updated": "2025-10-07T07:07:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06573v2",
      "landing_url": "https://arxiv.org/abs/2405.06573v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.06573"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on speech enhancement using Mamba and does not discuss discrete audio tokens or tokenization, so it does not meet inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on speech enhancement using Mamba and does not discuss discrete audio tokens or tokenization, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using an attention-free state-space model (Mamba) for speech enhancement, which operates on continuous speech signals to improve quality but does not mention any discrete audio tokenization, vector quantization, or discrete token sequences as core representations or mechanisms; thus, it does not meet the inclusion criteria of involving discrete audio tokens or related neural codec/tokenizer designs.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using an attention-free state-space model (Mamba) for speech enhancement, which operates on continuous speech signals to improve quality but does not mention any discrete audio tokenization, vector quantization, or discrete token sequences as core representations or mechanisms; thus, it does not meet the inclusion criteria of involving discrete audio tokens or related neural codec/tokenizer designs.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
    "abstract": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
    "metadata": {
      "arxiv_id": "2405.07682",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "authors": [
        "Jianyi Chen",
        "Wei Xue",
        "Xu Tan",
        "Zhen Ye",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "published": "2024-05-13T12:14:54Z",
      "updated": "2024-05-13T12:14:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07682v1",
      "landing_url": "https://arxiv.org/abs/2405.07682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.07682"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work targets non-autoregressive SAG using diffusion to generate Mel spectrograms (continuous audio representation) and does not discuss discrete audio tokens, tokenizers, or codebooks, thus failing the discrete-token inclusion criteria and aligning with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work targets non-autoregressive SAG using diffusion to generate Mel spectrograms (continuous audio representation) and does not discuss discrete audio tokens, tokenizers, or codebooks, thus failing the discrete-token inclusion criteria and aligning with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a diffusion-based non-autoregressive model for singing accompaniment generation focused on generating Mel spectrograms directly from vocal inputs, without mention or description of discretizing audio into token sequences, neural audio codecs, or vector quantization methods, which are central to the inclusion criteria. Furthermore, the abstract and title do not provide details about discrete token design, generation, or evaluation, which are required for assessing discrete audio token approaches. Therefore, the article does not meet the criteria requiring discrete audio tokens as core representations and does not provide sufficient methodological detail for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a diffusion-based non-autoregressive model for singing accompaniment generation focused on generating Mel spectrograms directly from vocal inputs, without mention or description of discretizing audio into token sequences, neural audio codecs, or vector quantization methods, which are central to the inclusion criteria. Furthermore, the abstract and title do not provide details about discrete token design, generation, or evaluation, which are required for assessing discrete audio token approaches. Therefore, the article does not meet the criteria requiring discrete audio tokens as core representations and does not provide sufficient methodological detail for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
    "abstract": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
    "metadata": {
      "arxiv_id": "2405.09508",
      "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
      "summary": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
      "authors": [
        "Demi Zhang",
        "Bushi Xiao",
        "Chao Gao",
        "Sangpil Youm",
        "Bonnie J Dorr"
      ],
      "published": "2024-05-15T17:01:02Z",
      "updated": "2024-10-15T20:24:00Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09508v2",
      "landing_url": "https://arxiv.org/abs/2405.09508v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.09508"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about bilingual sentence processing using RNN/Transformer and cross-language priming; it does not involve discrete audio tokens or tokenization/discretization of audio, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about bilingual sentence processing using RNN/Transformer and cross-language priming; it does not involve discrete audio tokens or tokenization/discretization of audio, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on modeling bilingual sentence processing using RNN and Transformer architectures to study structural priming across languages. However, it does not mention any approach related to discretizing continuous audio waveforms into discrete tokens, nor does it involve neural audio codecs or discrete audio token design, generation, or evaluation. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and is outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on modeling bilingual sentence processing using RNN and Transformer architectures to study structural priming across languages. However, it does not mention any approach related to discretizing continuous audio waveforms into discrete tokens, nor does it involve neural audio codecs or discrete audio token design, generation, or evaluation. Therefore, it does not meet the inclusion criteria regarding discrete audio tokens and is outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe TTS based on discrete audio tokens from a speech language model, discuss tokenization/codec design and evaluation, are in English, and address downstream generation—meeting all inclusion criteria and none of the exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe TTS based on discrete audio tokens from a speech language model, discuss tokenization/codec design and evaluation, are in English, and address downstream generation—meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper evaluates text-to-speech synthesis from a discrete token-based speech language model, which aligns with the inclusion criteria of using discrete audio tokens generated from neural audio codecs or similar mechanisms applied to speech generation tasks like TTS. However, the abstract does not explicitly describe the tokenization/codec architecture details, the token generation process, or give parameters like codebook settings or bitrate, which are required to confirm its eligibility and reproducibility. Therefore, while the topic fits, the lack of detailed token generation information places uncertainty on inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper evaluates text-to-speech synthesis from a discrete token-based speech language model, which aligns with the inclusion criteria of using discrete audio tokens generated from neural audio codecs or similar mechanisms applied to speech generation tasks like TTS. However, the abstract does not explicitly describe the tokenization/codec architecture details, the token generation process, or give parameters like codebook settings or bitrate, which are required to confirm its eligibility and reproducibility. Therefore, while the topic fits, the lack of detailed token generation information places uncertainty on inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano rated include (5) noting the paper clearly studies TTS from discrete audio tokens; JuniorMini rated uncertain (3) because the abstract lacks explicit tokenizer/codec/quantization/codebook/bitrate details required to confirm discrete-token design and reproducibility. The abstract does state a ‘‘discrete token-based SLM’’ and presents empirical TTS evaluations (automatic metrics and listening tests), which satisfies the downstream-use inclusion item, but it does not describe how the discrete tokens were produced or give codec/tokenizer settings as required by the English-assessability criterion—so based on the abstract alone I cannot confirm the paper meets all inclusion requirements nor rule out the exclusion for insufficient token-generation details. Therefore my judgement is uncertainty.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "JuniorNano rated include (5) noting the paper clearly studies TTS from discrete audio tokens; JuniorMini rated uncertain (3) because the abstract lacks explicit tokenizer/codec/quantization/codebook/bitrate details required to confirm discrete-token design and reproducibility. The abstract does state a ‘‘discrete token-based SLM’’ and presents empirical TTS evaluations (automatic metrics and listening tests), which satisfies the downstream-use inclusion item, but it does not describe how the discrete tokens were produced or give codec/tokenizer settings as required by the English-assessability criterion—so based on the abstract alone I cannot confirm the paper meets all inclusion requirements nor rule out the exclusion for insufficient token-generation details. Therefore my judgement is uncertainty.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
    "abstract": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
    "metadata": {
      "arxiv_id": "2405.09789",
      "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
      "summary": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
      "authors": [
        "Wentao Jiang",
        "Jing Zhang",
        "Di Wang",
        "Qiming Zhang",
        "Zengmao Wang",
        "Bo Du"
      ],
      "published": "2024-05-16T03:26:06Z",
      "updated": "2024-05-16T03:26:06Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09789v1",
      "landing_url": "https://arxiv.org/abs/2405.09789v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09789"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is a vision-transformer paper for remote sensing using learnable meta tokens, with no discrete audio token design or discretization of audio tokens, hence it does not meet the discrete audio tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is a vision-transformer paper for remote sensing using learnable meta tokens, with no discrete audio token design or discretization of audio tokens, hence it does not meet the discrete audio tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an efficient vision transformer model for remote sensing image interpretation, dealing with image tokenization and attention mechanisms in vision tasks; it does not involve audio waveform discretization into tokens nor discusses neural audio codecs, discrete audio token design, or audio-related modeling as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an efficient vision transformer model for remote sensing image interpretation, dealing with image tokenization and attention mechanisms in vision tasks; it does not involve audio waveform discretization into tokens nor discusses neural audio codecs, discrete audio token design, or audio-related modeling as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
    "abstract": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
    "metadata": {
      "arxiv_id": "2405.15310",
      "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
      "summary": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
      "authors": [
        "Duke Nguyen",
        "Du Yin",
        "Aditya Joshi",
        "Flora Salim"
      ],
      "published": "2024-05-24T07:52:53Z",
      "updated": "2025-09-23T03:21:45Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15310v5",
      "landing_url": "https://arxiv.org/abs/2405.15310v5",
      "doi": "https://doi.org/10.1145/3768161"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on Spectraformer, a random-feature framework for Transformer attention, with no discussion of discrete audio tokens, tokenizers, or audio-specific generation/understanding tasks, thus it fails to meet the inclusion criteria and is not aligned with the exclusion criteria focused on discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on Spectraformer, a random-feature framework for Transformer attention, with no discussion of discrete audio tokens, tokenizers, or audio-specific generation/understanding tasks, thus it fails to meet the inclusion criteria and is not aligned with the exclusion criteria focused on discrete audio token representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a Transformer attention approximation method and kernel learning in Transformers, without any mention of discrete audio tokens, neural audio codecs, or discretization of audio waveforms into tokens as required by the inclusion criteria. It does not address audio token design, representation, or applications in audio generation or understanding, nor does it provide details of token quantization or vocabularies. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a Transformer attention approximation method and kernel learning in Transformers, without any mention of discrete audio tokens, neural audio codecs, or discretization of audio waveforms into tokens as required by the inclusion criteria. It does not address audio token design, representation, or applications in audio generation or understanding, nor does it provide details of token quantization or vocabularies. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and is outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
    "abstract": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
    "metadata": {
      "arxiv_id": "2405.15405",
      "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
      "summary": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
      "authors": [
        "Barış Büyüktaş",
        "Kenneth Weitzel",
        "Sebastian Völkers",
        "Felix Zailskas",
        "Begüm Demir"
      ],
      "published": "2024-05-24T10:13:49Z",
      "updated": "2024-05-24T10:13:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15405v1",
      "landing_url": "https://arxiv.org/abs/2405.15405v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15405"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on transformer-based federated learning for multi-label remote sensing image classification and does not address discrete audio tokens or tokenization, so it fails the inclusion criteria for the discrete audio token topic and meets the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on transformer-based federated learning for multi-label remote sensing image classification and does not address discrete audio tokens or tokenization, so it fails the inclusion criteria for the discrete audio token topic and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on transformer-based federated learning for multi-label remote sensing image classification, which is unrelated to discrete audio tokens or audio modeling as defined by the inclusion criteria. There is no mention of audio tokenization, neural audio codecs, or audio-related tasks, thus it does not meet the key thematic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on transformer-based federated learning for multi-label remote sensing image classification, which is unrelated to discrete audio tokens or audio modeling as defined by the inclusion criteria. There is no mention of audio tokenization, neural audio codecs, or audio-related tasks, thus it does not meet the key thematic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
    "abstract": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
    "metadata": {
      "arxiv_id": "2405.15562",
      "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
      "summary": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
      "authors": [
        "Gao Tianci"
      ],
      "published": "2024-05-24T13:49:31Z",
      "updated": "2024-05-24T13:49:31Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15562v1",
      "landing_url": "https://arxiv.org/abs/2405.15562v1",
      "doi": "https://doi.org/10.1007/978-3-032-04758-8_3"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets Transformer-XL for long-sequence robotic learning with multi-modal sensors and does not involve discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets Transformer-XL for long-sequence robotic learning with multi-modal sensors and does not involve discrete audio tokens or tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on applying Transformer-XL for long sequence tasks in robotic learning from demonstration, integrating multi-modal sensor inputs such as RGB-D images, LiDAR, and tactile sensors, without discussing any discrete audio tokens or neural audio codec approaches related to audio waveform discretization as per the inclusion criteria. Therefore, it does not meet the defined topic of discrete audio tokens for audio generation, understanding, or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on applying Transformer-XL for long sequence tasks in robotic learning from demonstration, integrating multi-modal sensor inputs such as RGB-D images, LiDAR, and tactile sensors, without discussing any discrete audio tokens or neural audio codec approaches related to audio waveform discretization as per the inclusion criteria. Therefore, it does not meet the defined topic of discrete audio tokens for audio generation, understanding, or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
    "abstract": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes using discrete acoustic tokens generated via neural audio codecs/multi-codebooks and integrating them with an LLM for audio generation and cross-modal tasks, which directly aligns with the discrete audio token criteria and downstream generation/alignment focus.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes using discrete acoustic tokens generated via neural audio codecs/multi-codebooks and integrating them with an LLM for audio generation and cross-modal tasks, which directly aligns with the discrete audio token criteria and downstream generation/alignment focus.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multimodal framework using large language models to generate audio tokens hierarchically, explicitly mentioning the generation of audio semantic tokens from conditional inputs, and uses a discrete representation for audio generation akin to adding 'acoustic vocabulary' to LLM, indicating discrete audio token usage produced via pretrained audio codebooks; however, the abstract lacks detailed descriptions about the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or evaluation of design choices affecting fidelity or computational cost, thus only partially fulfilling the inclusion criteria and not fully ensuring reproducibility or detailed token design understanding.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multimodal framework using large language models to generate audio tokens hierarchically, explicitly mentioning the generation of audio semantic tokens from conditional inputs, and uses a discrete representation for audio generation akin to adding 'acoustic vocabulary' to LLM, indicating discrete audio token usage produced via pretrained audio codebooks; however, the abstract lacks detailed descriptions about the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or evaluation of design choices affecting fidelity or computational cost, thus only partially fulfilling the inclusion criteria and not fully ensuring reproducibility or detailed token design understanding.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly describes generating discrete audio semantic tokens using pre-trained audio codebooks and hierarchical/multi-level acoustic tokens integrated with an LLM for multimodal generation, which directly matches the core “Discrete Audio Tokens” theme (token source: neural audio codec / codebook and use for downstream generation/alignment). Junior reviewers split: one recommended inclusion noting discrete token usage, the other was uncertain because the abstract lacks detailed tokenizer/codec/quantization/codebook/bitrate specifications required for reproducibility. Given it clearly uses discrete audio tokens and applies them to cross-modal generation (meeting key inclusion groups) but the abstract omits low-level tokenizer/codebook details (which should be checked in the full paper), I recommend including the paper provisionally for further inspection of the methods section.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly describes generating discrete audio semantic tokens using pre-trained audio codebooks and hierarchical/multi-level acoustic tokens integrated with an LLM for multimodal generation, which directly matches the core “Discrete Audio Tokens” theme (token source: neural audio codec / codebook and use for downstream generation/alignment). Junior reviewers split: one recommended inclusion noting discrete token usage, the other was uncertain because the abstract lacks detailed tokenizer/codec/quantization/codebook/bitrate specifications required for reproducibility. Given it clearly uses discrete audio tokens and applies them to cross-modal generation (meeting key inclusion groups) but the abstract omits low-level tokenizer/codebook details (which should be checked in the full paper), I recommend including the paper provisionally for further inspection of the methods section.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
    "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
    "metadata": {
      "arxiv_id": "2405.20410",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "published": "2024-05-30T18:28:31Z",
      "updated": "2024-05-30T18:28:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20410v1",
      "landing_url": "https://arxiv.org/abs/2405.20410v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20410"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on an expressive S2ST LM with chain-of-thought prompting and ‘multi-stream acoustic units’ but provides no explicit details on discrete audio token generation, tokenizers, or neural codecs, so it does not satisfy the discrete audio token criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on an expressive S2ST LM with chain-of-thought prompting and ‘multi-stream acoustic units’ but provides no explicit details on discrete audio token generation, tokenizers, or neural codecs, so it does not satisfy the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on expressive speech-to-speech translation using a speech language model with chain-of-thought prompting. However, the abstract does not clearly describe the generation or use of discrete audio tokens nor detail the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or other discrete token-specific information necessary to fulfill the criteria. Therefore, it is not obvious that the core method involves discrete audio tokens as defined by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on expressive speech-to-speech translation using a speech language model with chain-of-thought prompting. However, the abstract does not clearly describe the generation or use of discrete audio tokens nor detail the tokenizer/codec architecture, quantization methods, vocabulary/codebook settings, or other discrete token-specific information necessary to fulfill the criteria. Therefore, it is not obvious that the core method involves discrete audio tokens as defined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
    "abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses two discrete speech representations quantized from audio and integrates them into a hierarchical transformer for one-stage generation, indicating discrete audio tokens as core representations for modeling, which aligns with the inclusion criteria for discrete token-based audio modeling; English abstract is provided and discusses tokenization/quantization and downstream generation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses two discrete speech representations quantized from audio and integrates them into a hierarchical transformer for one-stage generation, indicating discrete audio tokens as core representations for modeling, which aligns with the inclusion criteria for discrete token-based audio modeling; English abstract is provided and discusses tokenization/quantization and downstream generation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces GPST, a hierarchical transformer for speech language modeling that quantizes audio waveforms into two types of discrete speech representations and integrates them for generation, aligning with the inclusion criterion focusing on discrete audio tokens from neural audio codecs with multi-codebook or hierarchical nature. It provides details on token quantization, hierarchical transformer architecture, and evaluates downstream generation tasks (speech synthesis, multi-lingual generation) with empirical results, fulfilling key requirements of token design, evaluation, and application. The abstract is in English and provides sufficient methodological detail on tokenization and codec usage to be assessed as discrete audio token research, not continuous representation modeling. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper introduces GPST, a hierarchical transformer for speech language modeling that quantizes audio waveforms into two types of discrete speech representations and integrates them for generation, aligning with the inclusion criterion focusing on discrete audio tokens from neural audio codecs with multi-codebook or hierarchical nature. It provides details on token quantization, hierarchical transformer architecture, and evaluates downstream generation tasks (speech synthesis, multi-lingual generation) with empirical results, fulfilling key requirements of token design, evaluation, and application. The abstract is in English and provides sufficient methodological detail on tokenization and codec usage to be assessed as discrete audio token research, not continuous representation modeling. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
    "abstract": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete acoustic tokens from a pre-trained neural codec and trains a masked language model to reconstruct high-quality speech from corrupted input, satisfying the discrete-token criteria and token-based modeling focus.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete acoustic tokens from a pre-trained neural codec and trains a masked language model to reconstruct high-quality speech from corrupted input, satisfying the discrete-token criteria and token-based modeling focus.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes MaskSR using a masked language model based on discrete acoustic tokens extracted by a pre-trained neural codec, aiming at speech restoration. It clearly involves discrete audio tokens generated from a neural audio codec and applies sequence modeling, matching the inclusion criteria. The abstract provides sufficient English details about the use of discrete tokens and codec-based token extraction, enabling reproducibility. The task is speech restoration, which involves generation and understanding, and the method centers on discrete tokens, not continuous representation or purely discriminative tasks. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes MaskSR using a masked language model based on discrete acoustic tokens extracted by a pre-trained neural codec, aiming at speech restoration. It clearly involves discrete audio tokens generated from a neural audio codec and applies sequence modeling, matching the inclusion criteria. The abstract provides sufficient English details about the use of discrete tokens and codec-based token extraction, enabling reproducibility. The task is speech restoration, which involves generation and understanding, and the method centers on discrete tokens, not continuous representation or purely discriminative tasks. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
    "abstract": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
    "metadata": {
      "arxiv_id": "2406.02430",
      "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
      "summary": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
      "authors": [
        "Philip Anastassiou",
        "Jiawei Chen",
        "Jitong Chen",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Ziyi Chen",
        "Jian Cong",
        "Lelai Deng",
        "Chuang Ding",
        "Lu Gao",
        "Mingqing Gong",
        "Peisong Huang",
        "Qingqing Huang",
        "Zhiying Huang",
        "Yuanyuan Huo",
        "Dongya Jia",
        "Chumin Li",
        "Feiya Li",
        "Hui Li",
        "Jiaxin Li",
        "Xiaoyang Li",
        "Xingxing Li",
        "Lin Liu",
        "Shouda Liu",
        "Sichao Liu",
        "Xudong Liu",
        "Yuchen Liu",
        "Zhengxi Liu",
        "Lu Lu",
        "Junjie Pan",
        "Xin Wang",
        "Yuping Wang",
        "Yuxuan Wang",
        "Zhen Wei",
        "Jian Wu",
        "Chao Yao",
        "Yifeng Yang",
        "Yuanhao Yi",
        "Junteng Zhang",
        "Qidi Zhang",
        "Shuo Zhang",
        "Wenjie Zhang",
        "Yang Zhang",
        "Zilin Zhao",
        "Dejian Zhong",
        "Xiaobin Zhuang"
      ],
      "published": "2024-06-04T15:48:29Z",
      "updated": "2024-06-04T15:48:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02430v1",
      "landing_url": "https://arxiv.org/abs/2406.02430v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02430"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Seed-TTS TTS models and diffusion-based variants without discussing discrete audio tokens, tokenizers, or token-based representations, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Seed-TTS TTS models and diffusion-based variants without discussing discrete audio tokens, tokenizers, or token-based representations, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a family of text-to-speech models that generate high-quality speech but does not mention any process of discretizing continuous audio into discrete tokens or detailed descriptions of tokenizers, codec architectures, quantization methods, or vocabularies that serve as core components for modeling. The focus is on autoregressive and diffusion-based generation methods without reference to discrete token representations, which is central to the inclusion criteria. Therefore, it does not meet the key inclusion criterion of using discrete audio tokens as the foundational representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a family of text-to-speech models that generate high-quality speech but does not mention any process of discretizing continuous audio into discrete tokens or detailed descriptions of tokenizers, codec architectures, quantization methods, or vocabularies that serve as core components for modeling. The focus is on autoregressive and diffusion-based generation methods without reference to discrete token representations, which is central to the inclusion criteria. Therefore, it does not meet the key inclusion criterion of using discrete audio tokens as the foundational representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Local Search k-means++ with Foresight",
    "abstract": "Since its introduction in 1957, Lloyd's algorithm for $k$-means clustering has been extensively studied and has undergone several improvements. While in its original form it does not guarantee any approximation factor at all, Arthur and Vassilvitskii (SODA 2007) proposed $k$-means++ which enhances Lloyd's algorithm by a seeding method which guarantees a $\\mathcal{O}(\\log k)$-approximation in expectation. More recently, Lattanzi and Sohler (ICML 2019) proposed LS++ which further improves the solution quality of $k$-means++ by local search techniques to obtain a $\\mathcal{O}(1)$-approximation. On the practical side, the greedy variant of $k$-means++ is often used although its worst-case behaviour is provably worse than for the standard $k$-means++ variant. We investigate how to improve LS++ further in practice. We study two options for improving the practical performance: (a) Combining LS++ with greedy $k$-means++ instead of $k$-means++, and (b) Improving LS++ by better entangling it with Lloyd's algorithm. Option (a) worsens the theoretical guarantees of $k$-means++ but improves the practical quality also in combination with LS++ as we confirm in our experiments. Option (b) is our new algorithm, Foresight LS++. We experimentally show that FLS++ improves upon the solution quality of LS++. It retains its asymptotic runtime and its worst-case approximation bounds.",
    "metadata": {
      "arxiv_id": "2406.02739",
      "title": "Local Search k-means++ with Foresight",
      "summary": "Since its introduction in 1957, Lloyd's algorithm for $k$-means clustering has been extensively studied and has undergone several improvements. While in its original form it does not guarantee any approximation factor at all, Arthur and Vassilvitskii (SODA 2007) proposed $k$-means++ which enhances Lloyd's algorithm by a seeding method which guarantees a $\\mathcal{O}(\\log k)$-approximation in expectation. More recently, Lattanzi and Sohler (ICML 2019) proposed LS++ which further improves the solution quality of $k$-means++ by local search techniques to obtain a $\\mathcal{O}(1)$-approximation. On the practical side, the greedy variant of $k$-means++ is often used although its worst-case behaviour is provably worse than for the standard $k$-means++ variant.\n  We investigate how to improve LS++ further in practice. We study two options for improving the practical performance: (a) Combining LS++ with greedy $k$-means++ instead of $k$-means++, and (b) Improving LS++ by better entangling it with Lloyd's algorithm. Option (a) worsens the theoretical guarantees of $k$-means++ but improves the practical quality also in combination with LS++ as we confirm in our experiments. Option (b) is our new algorithm, Foresight LS++. We experimentally show that FLS++ improves upon the solution quality of LS++. It retains its asymptotic runtime and its worst-case approximation bounds.",
      "authors": [
        "Theo Conrads",
        "Lukas Drexler",
        "Joshua Könen",
        "Daniel R. Schmidt",
        "Melanie Schmidt"
      ],
      "published": "2024-06-04T19:42:15Z",
      "updated": "2024-06-04T19:42:15Z",
      "categories": [
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02739v1",
      "landing_url": "https://arxiv.org/abs/2406.02739v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02739"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on k-means clustering algorithms (LS++, Foresight LS++), with no discussion of discrete audio tokens, tokenization, or audio-related downstream tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on k-means clustering algorithms (LS++, Foresight LS++), with no discussion of discrete audio tokens, tokenization, or audio-related downstream tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving k-means++ clustering algorithms and do not discuss any discrete audio token generation, neural audio codecs, vector quantization, or audio-related tokenization. The work is purely on clustering algorithmic improvements without any mention of audio discretization, token sequences, or relevant audio modeling tasks, thus failing to meet any inclusion criteria and matching exclusion criteria for non-audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving k-means++ clustering algorithms and do not discuss any discrete audio token generation, neural audio codecs, vector quantization, or audio-related tokenization. The work is purely on clustering algorithmic improvements without any mention of audio discretization, token sequences, or relevant audio modeling tasks, thus failing to meet any inclusion criteria and matching exclusion criteria for non-audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
    "abstract": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
    "metadata": {
      "arxiv_id": "2406.02940",
      "title": "Addressing Index Collapse of Large-Codebook Speech Tokenizer with Dual-Decoding Product-Quantized Variational Auto-Encoder",
      "summary": "VQ-VAE, as a mainstream approach of speech tokenizer, has been troubled by ``index collapse'', where only a small number of codewords are activated in large codebooks. This work proposes product-quantized (PQ) VAE with more codebooks but fewer codewords to address this problem and build large-codebook speech tokenizers. It encodes speech features into multiple VQ subspaces and composes them into codewords in a larger codebook. Besides, to utilize each VQ subspace well, we also enhance PQ-VAE via a dual-decoding training strategy with the encoding and quantized sequences. The experimental results demonstrate that PQ-VAE addresses ``index collapse\" effectively, especially for larger codebooks. The model with the proposed training strategy further improves codebook perplexity and reconstruction quality, outperforming other multi-codebook VQ approaches. Finally, PQ-VAE demonstrates its effectiveness in language-model-based TTS, supporting higher-quality speech generation with larger codebooks.",
      "authors": [
        "Haohan Guo",
        "Fenglong Xie",
        "Dongchao Yang",
        "Hui Lu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-06-05T04:54:49Z",
      "updated": "2024-06-05T04:54:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02940v1",
      "landing_url": "https://arxiv.org/abs/2406.02940v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02940"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper proposes a discrete-token speech tokenizer (PQ-VAE with multiple codebooks) and analyzes its impact on codebook tokens and downstream TTS, aligning with the discrete audio token criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a discrete-token speech tokenizer (PQ-VAE with multiple codebooks) and analyzes its impact on codebook tokens and downstream TTS, aligning with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a speech tokenizer based on product-quantized variational auto-encoder (PQ-VAE) that addresses index collapse in large codebooks, clearly involving the generation of discrete tokens from continuous speech data via vector quantization and multi-codebook strategies. It discusses the tokenizer design, codebook configuration, and improvements in speech generation tasks like TTS, which aligns well with the inclusion criteria concerning discrete audio tokens utilized for modeling and generation, and provides sufficient English details for evaluation. Therefore, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a speech tokenizer based on product-quantized variational auto-encoder (PQ-VAE) that addresses index collapse in large codebooks, clearly involving the generation of discrete tokens from continuous speech data via vector quantization and multi-codebook strategies. It discusses the tokenizer design, codebook configuration, and improvements in speech generation tasks like TTS, which aligns well with the inclusion criteria concerning discrete audio tokens utilized for modeling and generation, and provides sufficient English details for evaluation. Therefore, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
    "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
    "metadata": {
      "arxiv_id": "2406.03044",
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "summary": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
      "authors": [
        "Geeling Chau",
        "Christopher Wang",
        "Sabera Talukder",
        "Vighnesh Subramaniam",
        "Saraswati Soedarmadji",
        "Yisong Yue",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "published": "2024-06-05T08:15:09Z",
      "updated": "2025-03-28T06:43:28Z",
      "categories": [
        "cs.LG",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03044v4",
      "landing_url": "https://arxiv.org/abs/2406.03044v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.03044"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about learning population-level representations of neural activity with a transformer, focusing on neural time-series decoding; it does not involve discretized audio tokens or tokenization of audio, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about learning population-level representations of neural activity with a transformer, focusing on neural time-series decoding; it does not involve discretized audio tokens or tokenization of audio, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised learning of population-level representations of neural activity from intracranial neural recordings, which is unrelated to discrete audio tokens derived from audio waveforms through discretization processes like vector quantization or clustering. It does not discuss or use discrete token sequences representing audio data nor addresses audio generation, understanding, or cross-modal alignment tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related modeling tasks, nor does it indicate use of neural audio codecs or discrete audio representations. ",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised learning of population-level representations of neural activity from intracranial neural recordings, which is unrelated to discrete audio tokens derived from audio waveforms through discretization processes like vector quantization or clustering. It does not discuss or use discrete token sequences representing audio data nor addresses audio generation, understanding, or cross-modal alignment tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related modeling tasks, nor does it indicate use of neural audio codecs or discrete audio representations. ",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
    "abstract": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
    "metadata": {
      "arxiv_id": "2406.03460",
      "title": "The PESQetarian: On the Relevance of Goodhart's Law for Speech Enhancement",
      "summary": "To obtain improved speech enhancement models, researchers often focus on increasing performance according to specific instrumental metrics. However, when the same metric is used in a loss function to optimize models, it may be detrimental to aspects that the given metric does not see. The goal of this paper is to illustrate the risk of overfitting a speech enhancement model to the metric used for evaluation. For this, we introduce enhancement models that exploit the widely used PESQ measure. Our \"PESQetarian\" model achieves 3.82 PESQ on VB-DMD while scoring very poorly in a listening experiment. While the obtained PESQ value of 3.82 would imply \"state-of-the-art\" PESQ-performance on the VB-DMD benchmark, our examples show that when optimizing w.r.t. a metric, an isolated evaluation on the same metric may be misleading. Instead, other metrics should be included in the evaluation and the resulting performance predictions should be confirmed by listening.",
      "authors": [
        "Danilo de Oliveira",
        "Simon Welker",
        "Julius Richter",
        "Timo Gerkmann"
      ],
      "published": "2024-06-05T17:07:39Z",
      "updated": "2024-06-05T17:07:39Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03460v1",
      "landing_url": "https://arxiv.org/abs/2406.03460v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03460"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on improving or evaluating speech enhancement via PESQ metrics and Goodhart’s Law, with no discussion of discrete audio token generation/representation, tokenizers, or token-based downstream tasks, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on improving or evaluating speech enhancement via PESQ metrics and Goodhart’s Law, with no discussion of discrete audio token generation/representation, tokenizers, or token-based downstream tasks, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on speech enhancement models optimized for a specific metric (PESQ) and discusses the pitfalls of overfitting to this metric without validating perceptual quality; however, it does not address discrete audio tokens, nor does it involve discretization of continuous audio into token sequences for modeling, generation, or multimodal reasoning, which are central to the inclusion criteria. There is no indication of neural audio codecs, vector quantization, or any tokenization process relevant to discrete audio tokens, so it does not meet the inclusion criteria and falls under exclusion for continuous representation focus without discrete token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on speech enhancement models optimized for a specific metric (PESQ) and discusses the pitfalls of overfitting to this metric without validating perceptual quality; however, it does not address discrete audio tokens, nor does it involve discretization of continuous audio into token sequences for modeling, generation, or multimodal reasoning, which are central to the inclusion criteria. There is no indication of neural audio codecs, vector quantization, or any tokenization process relevant to discrete audio tokens, so it does not meet the inclusion criteria and falls under exclusion for continuous representation focus without discrete token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
    "abstract": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on an audio codec-based TTS system that uses discrete acoustic/semantic tokens (acoustic tokens generated by a codec, and semantic tokens predicted by an LLM), aligning with the discrete audio token topic and evaluating token-based modeling in a downstream generation task.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The paper centers on an audio codec-based TTS system that uses discrete acoustic/semantic tokens (acoustic tokens generated by a codec, and semantic tokens predicted by an LLM), aligning with the discrete audio token topic and evaluating token-based modeling in a downstream generation task.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an audio codec-based zero-shot TTS model that utilizes discrete audio tokens (acoustic tokens generated via SoundStorm) within a multi-modal and LLM-enhanced framework. The abstract implies the use of discrete tokens derived from neural audio codecs, aligning with the inclusion criteria concerning discrete audio token generation and downstream TTS applications. However, the abstract lacks explicit description of tokenization details, token vocabulary/codebook specifics, or evaluation metrics related to token design, which are critical for confirming the discrete token aspect and reproducibility per inclusion criteria. Nevertheless, the study clearly applies discrete tokens in a TTS generation context, which is relevant and valuable. Overall, while some methodological details are missing from the abstract, the paper appears sufficiently aligned with the theme and focus areas to merit inclusion.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an audio codec-based zero-shot TTS model that utilizes discrete audio tokens (acoustic tokens generated via SoundStorm) within a multi-modal and LLM-enhanced framework. The abstract implies the use of discrete tokens derived from neural audio codecs, aligning with the inclusion criteria concerning discrete audio token generation and downstream TTS applications. However, the abstract lacks explicit description of tokenization details, token vocabulary/codebook specifics, or evaluation metrics related to token design, which are critical for confirming the discrete token aspect and reproducibility per inclusion criteria. Nevertheless, the study clearly applies discrete tokens in a TTS generation context, which is relevant and valuable. Overall, while some methodological details are missing from the abstract, the paper appears sufficiently aligned with the theme and focus areas to merit inclusion.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
    "abstract": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on diffusion-based upsampling of spectrograms/codec features in TTS and does not clearly specify producing or using discrete audio tokens (tokenizers/codecs) as a core method; although the abstract mentions discrete tokens in the broader TTS pipeline, the study’s main focus appears to be continuous representations and upsampling rather than discrete token design or analysis, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item centers on diffusion-based upsampling of spectrograms/codec features in TTS and does not clearly specify producing or using discrete audio tokens (tokenizers/codecs) as a core method; although the abstract mentions discrete tokens in the broader TTS pipeline, the study’s main focus appears to be continuous representations and upsampling rather than discrete token design or analysis, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving diffusion models for spectrogram up-sampling in TTS systems that generate speech tokens autoregressively, but it does not clearly describe the generation process or design of discrete audio tokens themselves, nor does it provide details about tokenization or vector quantization mechanisms central to discrete audio token research. Therefore, it lacks sufficient information on discrete token definition and design required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving diffusion models for spectrogram up-sampling in TTS systems that generate speech tokens autoregressively, but it does not clearly describe the generation process or design of discrete audio tokens themselves, nor does it provide details about tokenization or vector quantization mechanisms central to discrete audio token research. Therefore, it lacks sufficient information on discrete token definition and design required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
    "abstract": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
    "metadata": {
      "arxiv_id": "2406.04740",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "authors": [
        "Yang Ma",
        "Wenchi Cheng",
        "Jingqing Wang",
        "Wei Zhang"
      ],
      "published": "2024-06-07T08:40:53Z",
      "updated": "2024-06-07T08:40:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04740v1",
      "landing_url": "https://arxiv.org/abs/2406.04740v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04740"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本文主題聚焦於360度影像的語義通信與向量量化，未涉及將音訊離散化為 token 的機制、或以離散音訊 token 作為核心方法，因此不符合「Discrete Audio Tokens」主題的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本文主題聚焦於360度影像的語義通信與向量量化，未涉及將音訊離散化為 token 的機制、或以離散音訊 token 作為核心方法，因此不符合「Discrete Audio Tokens」主題的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on 360-degree image semantic communication using activation map-based vector quantization for image data compression and transmission, which involves vector quantization but does not pertain to discrete audio tokens derived from continuous audio waveforms for audio generation, understanding, or cross-modal modeling as defined by the inclusion criteria. There is no mention of audio tokens, neural audio codecs, or audio-related tokenization, so it does not meet the core topic requirements related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on 360-degree image semantic communication using activation map-based vector quantization for image data compression and transmission, which involves vector quantization but does not pertain to discrete audio tokens derived from continuous audio waveforms for audio generation, understanding, or cross-modal modeling as defined by the inclusion criteria. There is no mention of audio tokens, neural audio codecs, or audio-related tokenization, so it does not meet the core topic requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
    "abstract": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
    "metadata": {
      "arxiv_id": "2406.05551",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
      ],
      "published": "2024-06-08T18:57:13Z",
      "updated": "2024-06-08T18:57:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05551v1",
      "landing_url": "https://arxiv.org/abs/2406.05551v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05551"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work encodes audio as continuous vector sequences and uses a diffusion transformer for TTS, with no discrete tokenization/码本/詞彙表 definitions, hence it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work encodes audio as continuous vector sequences and uses a diffusion transformer for TTS, with no discrete tokenization/码本/詞彙表 definitions, hence it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on an autoregressive diffusion transformer for text-to-speech synthesis using continuous vector sequences in \\mathbb{R}^d instead of discrete audio tokens. The abstract emphasizes continuous representations for speech generation without describing the use or design of discrete tokens obtained via neural audio codecs or quantization methods. There is no specific mention of discrete token generation processes, tokenizers, vocabularies, quantization details, or any discrete token sequences, which fails to meet the inclusion criteria. Therefore, it fits the exclusion criteria related to continuous audio representation with no discrete token usage or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on an autoregressive diffusion transformer for text-to-speech synthesis using continuous vector sequences in \\mathbb{R}^d instead of discrete audio tokens. The abstract emphasizes continuous representations for speech generation without describing the use or design of discrete tokens obtained via neural audio codecs or quantization methods. There is no specific mention of discrete token generation processes, tokenizers, vocabularies, quantization details, or any discrete token sequences, which fails to meet the inclusion criteria. Therefore, it fits the exclusion criteria related to continuous audio representation with no discrete token usage or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
    "abstract": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
    "metadata": {
      "arxiv_id": "2406.06251",
      "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
      "summary": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
      "authors": [
        "Chung-Ming Chien",
        "Andros Tjandra",
        "Apoorv Vyas",
        "Matt Le",
        "Bowen Shi",
        "Wei-Ning Hsu"
      ],
      "published": "2024-06-10T13:31:18Z",
      "updated": "2024-06-10T13:31:18Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06251v1",
      "landing_url": "https://arxiv.org/abs/2406.06251v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.06251"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on fine-tuning and controllability for a speech generation model using adapters, with no discussion of discrete audio tokens, tokenization, or neural audio codecs, which are required for inclusion; hence it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on fine-tuning and controllability for a speech generation model using adapters, with no discussion of discrete audio tokens, tokenization, or neural audio codecs, which are required for inclusion; hence it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fine-grained controllability in speech generation by fine-tuning a pre-trained model, without mentioning the use or design of discrete audio tokens derived from neural codecs or self-supervised models for downstream generative or alignment tasks. There is no clear information about discretization, token vocabularies, quantization methods, or evaluation of discrete token design, which are essential for inclusion. Instead, the paper emphasizes efficient fine-tuning and controllability enhancement on continuous speech generation, and does not provide evidence that discrete token representations are core to the method or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fine-grained controllability in speech generation by fine-tuning a pre-trained model, without mentioning the use or design of discrete audio tokens derived from neural codecs or self-supervised models for downstream generative or alignment tasks. There is no clear information about discretization, token vocabularies, quantization methods, or evaluation of discrete token design, which are essential for inclusion. Instead, the paper emphasizes efficient fine-tuning and controllability enhancement on continuous speech generation, and does not provide evidence that discrete token representations are core to the method or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
    "abstract": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
    "metadata": {
      "arxiv_id": "2406.06582",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "authors": [
        "Viet Anh Trinh",
        "Rosy Southwell",
        "Yiwen Guan",
        "Xinlu He",
        "Zhiyong Wang",
        "Jacob Whitehill"
      ],
      "published": "2024-06-04T20:08:25Z",
      "updated": "2024-06-25T17:44:00Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06582v2",
      "landing_url": "https://arxiv.org/abs/2406.06582v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.06582"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a decoder-only Discrete Multimodal Language Model with a codebook derived from Whisper activations, indicating discrete audio tokenization as a core input and cross-modal tasks, thus aligning with token-design and downstream use criteria while offering reproducible design elements.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a decoder-only Discrete Multimodal Language Model with a codebook derived from Whisper activations, indicating discrete audio tokenization as a core input and cross-modal tasks, thus aligning with token-design and downstream use criteria while offering reproducible design elements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a discrete multimodal language model applied to speech processing tasks, involving the use of discrete speech tokens derived from neural audio encoders (e.g., codebooks from Whisper activations), aligning well with the inclusion criteria regarding discrete audio token representation and usage within downstream speech tasks; the abstract indicates the inclusion of detailed methodology on token/codebook design and the model is applied to multiple speech-related tasks, fulfilling both the thematic and technical criteria for inclusion, and it is presented in English with sufficient detail to understand the discrete token mechanism.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a discrete multimodal language model applied to speech processing tasks, involving the use of discrete speech tokens derived from neural audio encoders (e.g., codebooks from Whisper activations), aligning well with the inclusion criteria regarding discrete audio token representation and usage within downstream speech tasks; the abstract indicates the inclusion of detailed methodology on token/codebook design and the model is applied to multiple speech-related tasks, fulfilling both the thematic and technical criteria for inclusion, and it is presented in English with sufficient detail to understand the discrete token mechanism.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
    "abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
    "metadata": {
      "arxiv_id": "2406.07119",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "authors": [
        "Aoxiong Yin",
        "Haoyuan Li",
        "Kai Shen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2024-06-11T10:06:53Z",
      "updated": "2024-06-11T10:06:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
      "landing_url": "https://arxiv.org/abs/2406.07119v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07119"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper explicitly uses discrete token representations (dynamic vector quantization) to encode sign language sequences and trains an autoregressive model (GPT-like) to generate code sequences from text, with tokenization details and downstream sign-language generation tasks, aligning with the discrete audio tokens criteria and exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper explicitly uses discrete token representations (dynamic vector quantization) to encode sign language sequences and trains an autoregressive model (GPT-like) to generate code sequences from text, with tokenization details and downstream sign-language generation tasks, aligning with the discrete audio tokens criteria and exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sign language production by encoding sign language sequences into discrete codes using a novel dynamic vector quantization model, which relates to discrete token representations. However, the tokens represent sign language motion sequences rather than audio waveforms (speech, music, environment sounds) discretized into token sequences as required by the criteria. The tokens appear to represent visual sign language information, not audio tokens derived from neural audio codecs or self-supervised audio representations. Therefore, although it involves vector quantization for discrete code generation, it does not meet the inclusion criteria focusing specifically on discrete audio tokens from continuous audio waveforms for audio generation, understanding, or cross-modal tasks. Hence, the study does not qualify for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sign language production by encoding sign language sequences into discrete codes using a novel dynamic vector quantization model, which relates to discrete token representations. However, the tokens represent sign language motion sequences rather than audio waveforms (speech, music, environment sounds) discretized into token sequences as required by the criteria. The tokens appear to represent visual sign language information, not audio tokens derived from neural audio codecs or self-supervised audio representations. Therefore, although it involves vector quantization for discrete code generation, it does not meet the inclusion criteria focusing specifically on discrete audio tokens from continuous audio waveforms for audio generation, understanding, or cross-modal tasks. Hence, the study does not qualify for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The inclusion scope explicitly concerns discretizing continuous audio waveforms into token sequences (neural audio codecs or self-supervised audio units); this paper instead applies dynamic vector quantization to sign-language motion sequences (visual/pose tokens) and trains a GPT-like model to generate sign codes from text. I reviewed both junior reviewers: one noted the use of discrete token representations but did not distinguish audio vs. visual modality, while the other correctly pointed out the tokens represent visual sign motion rather than discretized audio, so the paper does not satisfy the core inclusion criterion. Brief explanation and score: Exclude (1) — the work discretizes sign-language motion, not continuous audio waveforms, so it falls outside the “Discrete Audio Tokens” topic.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "The inclusion scope explicitly concerns discretizing continuous audio waveforms into token sequences (neural audio codecs or self-supervised audio units); this paper instead applies dynamic vector quantization to sign-language motion sequences (visual/pose tokens) and trains a GPT-like model to generate sign codes from text. I reviewed both junior reviewers: one noted the use of discrete token representations but did not distinguish audio vs. visual modality, while the other correctly pointed out the tokens represent visual sign motion rather than discretized audio, so the paper does not satisfy the core inclusion criterion. Brief explanation and score: Exclude (1) — the work discretizes sign-language motion, not continuous audio waveforms, so it falls outside the “Discrete Audio Tokens” topic.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Sustainable self-supervised learning for speech representations",
    "abstract": "Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible. In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption. Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs. The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations. It pretrains using a single GPU in less than a day. On top of that, it improves the error rate performance of the baseline in downstream task evaluations. When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement.",
    "metadata": {
      "arxiv_id": "2406.07696",
      "title": "Sustainable self-supervised learning for speech representations",
      "summary": "Sustainable artificial intelligence focuses on data, hardware, and algorithms to make machine learning models more environmentally responsible. In particular, machine learning models for speech representations are computationally expensive, generating environmental concerns because of their high energy consumption. Thus, we propose a sustainable self-supervised model to learn speech representation, combining optimizations in neural layers and training to reduce computing costs. The proposed model improves over a resource-efficient baseline, reducing both memory usage and computing cost estimations. It pretrains using a single GPU in less than a day. On top of that, it improves the error rate performance of the baseline in downstream task evaluations. When comparing it to large speech representation approaches, there is an order of magnitude reduction in memory usage, while computing cost reductions represent almost three orders of magnitude improvement.",
      "authors": [
        "Luis Lugo",
        "Valentin Vielzeuf"
      ],
      "published": "2024-06-11T20:21:36Z",
      "updated": "2024-06-11T20:21:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07696v1",
      "landing_url": "https://arxiv.org/abs/2406.07696v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07696"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses sustainable self-supervised learning for speech representations but makes no mention of discrete audio tokens, tokenization, neural codecs, or token-based downstream tasks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses sustainable self-supervised learning for speech representations but makes no mention of discrete audio tokens, tokenization, neural codecs, or token-based downstream tasks, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on sustainable self-supervised learning for speech representations to reduce computational cost and energy use. However, the abstract does not mention any form of discretization mapping continuous audio into discrete token sequences or neural audio codecs producing vector-quantized tokens, which are required for inclusion. The paper emphasizes model efficiency and error rate improvements but does not report on discrete tokens as key representations or provide tokenization details like vocabulary, codebooks, or quantization. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens despite being relevant to speech representation learning.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on sustainable self-supervised learning for speech representations to reduce computational cost and energy use. However, the abstract does not mention any form of discretization mapping continuous audio into discrete token sequences or neural audio codecs producing vector-quantized tokens, which are required for inclusion. The paper emphasizes model efficiency and error rate improvements but does not report on discrete tokens as key representations or provide tokenization details like vocabulary, codebooks, or quantization. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens despite being relevant to speech representation learning.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
    "abstract": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
    "metadata": {
      "arxiv_id": "2406.07846",
      "title": "DualVC 3: Leveraging Language Model Generated Pseudo Context for End-to-end Low Latency Streaming Voice Conversion",
      "summary": "Streaming voice conversion has become increasingly popular for its potential in real-time applications. The recently proposed DualVC 2 has achieved robust and high-quality streaming voice conversion with a latency of about 180ms. Nonetheless, the recognition-synthesis framework hinders end-to-end optimization, and the instability of automatic speech recognition (ASR) model with short chunks makes it challenging to further reduce latency. To address these issues, we propose an end-to-end model, DualVC 3. With speaker-independent semantic tokens to guide the training of the content encoder, the dependency on ASR is removed and the model can operate under extremely small chunks, with cascading errors eliminated. A language model is trained on the content encoder output to produce pseudo context by iteratively predicting future frames, providing more contextual information for the decoder to improve conversion quality. Experimental results demonstrate that DualVC 3 achieves comparable performance to DualVC 2 in subjective and objective metrics, with a latency of only 50 ms.",
      "authors": [
        "Ziqian Ning",
        "Shuai Wang",
        "Pengcheng Zhu",
        "Zhichao Wang",
        "Jixun Yao",
        "Lei Xie",
        "Mengxiao Bi"
      ],
      "published": "2024-06-12T03:25:18Z",
      "updated": "2024-06-12T03:25:18Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07846v1",
      "landing_url": "https://arxiv.org/abs/2406.07846v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'semantic tokens' but does not clearly describe discrete audio token generation/codec, token vocabulary, or a token-centric analysis; the focus is on end-to-end voice conversion latency with an LLM-generated pseudo context, not on discrete token design or tokens as core representations, so it likely does not meet the token-centric inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'semantic tokens' but does not clearly describe discrete audio token generation/codec, token vocabulary, or a token-centric analysis; the focus is on end-to-end voice conversion latency with an LLM-generated pseudo context, not on discrete token design or tokens as core representations, so it likely does not meet the token-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes DualVC 3, an end-to-end streaming voice conversion model that removes dependency on ASR by using speaker-independent semantic tokens as intermediate representations; however, the abstract does not clearly describe the generation process or design details of these tokens, nor specify them as discrete audio tokens formed by neural audio codec or self-supervised learning clustering/quantization, failing the strict inclusion criteria on token definition and reproducibility; thus, it does not sufficiently demonstrate the use of discrete audio token sequences as required for inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes DualVC 3, an end-to-end streaming voice conversion model that removes dependency on ASR by using speaker-independent semantic tokens as intermediate representations; however, the abstract does not clearly describe the generation process or design details of these tokens, nor specify them as discrete audio tokens formed by neural audio codec or self-supervised learning clustering/quantization, failing the strict inclusion criteria on token definition and reproducibility; thus, it does not sufficiently demonstrate the use of discrete audio token sequences as required for inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
    "abstract": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
    "metadata": {
      "arxiv_id": "2406.07855",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "authors": [
        "Bing Han",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Lingwei Meng",
        "Yanming Qian",
        "Yanqing Liu",
        "Sheng Zhao",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2024-06-12T04:09:44Z",
      "updated": "2024-06-12T04:09:44Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07855v1",
      "landing_url": "https://arxiv.org/abs/2406.07855v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07855"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete neural audio codecs to produce acoustic tokens and downsample/align them for TTS, treating tokens as core representations for modeling and evaluation, which aligns with the discrete audio token inclusion criteria and not with the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete neural audio codecs to produce acoustic tokens and downsample/align them for TTS, treating tokens as core representations for modeling and evaluation, which aligns with the discrete audio token inclusion criteria and not with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes VALL-E R, a zero-shot TTS system utilizing discrete neural audio codecs to represent audio as discrete tokens, involving quantization and codec merging to efficiently downsample discrete codes, which aligns well with the inclusion criteria that require the use of neural audio codec-generated discrete tokens for audio generation tasks like TTS. The work also provides evaluation related to the quality, robustness, and computational cost of the discrete token-based system, and the abstract is in English with clearly described methods involving discrete tokens, meeting the criteria for reproducibility and systematic evaluation. Therefore, it fully meets the inclusion criteria and none of the exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes VALL-E R, a zero-shot TTS system utilizing discrete neural audio codecs to represent audio as discrete tokens, involving quantization and codec merging to efficiently downsample discrete codes, which aligns well with the inclusion criteria that require the use of neural audio codec-generated discrete tokens for audio generation tasks like TTS. The work also provides evaluation related to the quality, robustness, and computational cost of the discrete token-based system, and the abstract is in English with clearly described methods involving discrete tokens, meeting the criteria for reproducibility and systematic evaluation. Therefore, it fully meets the inclusion criteria and none of the exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
    "abstract": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
    "metadata": {
      "arxiv_id": "2406.08266",
      "title": "Refining Self-Supervised Learnt Speech Representation using Brain Activations",
      "summary": "It was shown in literature that speech representations extracted by self-supervised pre-trained models exhibit similarities with brain activations of human for speech perception and fine-tuning speech representation models on downstream tasks can further improve the similarity. However, it still remains unclear if this similarity can be used to optimize the pre-trained speech models. In this work, we therefore propose to use the brain activations recorded by fMRI to refine the often-used wav2vec2.0 model by aligning model representations toward human neural responses. Experimental results on SUPERB reveal that this operation is beneficial for several downstream tasks, e.g., speaker verification, automatic speech recognition, intent classification.One can then consider the proposed method as a new alternative to improve self-supervised speech models.",
      "authors": [
        "Hengyu Li",
        "Kangdi Mei",
        "Zhaoci Liu",
        "Yang Ai",
        "Liping Chen",
        "Jie Zhang",
        "Zhenhua Ling"
      ],
      "published": "2024-06-12T14:34:41Z",
      "updated": "2024-06-13T06:26:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08266v2",
      "landing_url": "https://arxiv.org/abs/2406.08266v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.08266"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本研究聚焦使用腦部激活（fMRI）來微調自監督語音表徵，並未描述離散化的音訊 token、碼本/詞彙表的產生設定或以離散 token 為核心的分析與下游任務，因此不符合「Discrete Audio Tokens」主題的納入條件，屬於排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本研究聚焦使用腦部激活（fMRI）來微調自監督語音表徵，並未描述離散化的音訊 token、碼本/詞彙表的產生設定或以離散 token 為核心的分析與下游任務，因此不符合「Discrete Audio Tokens」主題的納入條件，屬於排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on refining self-supervised speech representations by aligning them with brain activations, without explicitly mentioning the generation or use of discrete audio tokens through discretization, tokenizers, or neural audio codecs. There is no clear evidence that discrete token sequences or vector quantization methods are involved as core methods or analyses, which are essential inclusion criteria for the topic of \"Discrete Audio Tokens.\" The work primarily deals with continuous speech representations fine-tuned with human brain data and tests end-task performance improvements, lacking detailed information on discrete token design, vocabulary, codebooks, or tokenization strategies. Therefore, it does not meet the core inclusion criteria and falls under exclusion criteria relating to continuous audio modeling without discrete token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on refining self-supervised speech representations by aligning them with brain activations, without explicitly mentioning the generation or use of discrete audio tokens through discretization, tokenizers, or neural audio codecs. There is no clear evidence that discrete token sequences or vector quantization methods are involved as core methods or analyses, which are essential inclusion criteria for the topic of \"Discrete Audio Tokens.\" The work primarily deals with continuous speech representations fine-tuned with human brain data and tests end-task performance improvements, lacking detailed information on discrete token design, vocabulary, codebooks, or tokenization strategies. Therefore, it does not meet the core inclusion criteria and falls under exclusion criteria relating to continuous audio modeling without discrete token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
    "abstract": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
    "metadata": {
      "arxiv_id": "2406.08353",
      "title": "Speech Emotion Recognition with ASR Transcripts: A Comprehensive Study on Word Error Rate and Fusion Techniques",
      "summary": "Text data is commonly utilized as a primary input to enhance Speech Emotion Recognition (SER) performance and reliability. However, the reliance on human-transcribed text in most studies impedes the development of practical SER systems, creating a gap between in-lab research and real-world scenarios where Automatic Speech Recognition (ASR) serves as the text source. Hence, this study benchmarks SER performance using ASR transcripts with varying Word Error Rates (WERs) from eleven models on three well-known corpora: IEMOCAP, CMU-MOSI, and MSP-Podcast. Our evaluation includes both text-only and bimodal SER with six fusion techniques, aiming for a comprehensive analysis that uncovers novel findings and challenges faced by current SER research. Additionally, we propose a unified ASR error-robust framework integrating ASR error correction and modality-gated fusion, achieving lower WER and higher SER results compared to the best-performing ASR transcript. These findings provide insights into SER with ASR assistance, especially for real-world applications.",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "published": "2024-06-12T15:59:25Z",
      "updated": "2025-03-23T16:45:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08353v3",
      "landing_url": "https://arxiv.org/abs/2406.08353v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.08353"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item does not address discrete audio tokens or token-based representations; it focuses on speech emotion recognition using ASR transcripts and fusion methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item does not address discrete audio tokens or token-based representations; it focuses on speech emotion recognition using ASR transcripts and fusion methods, thus it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Speech Emotion Recognition using ASR transcripts without discussing or utilizing discrete audio tokens derived from discrete vector quantization, neural audio codecs, or clustering-based units, which are core to the inclusion criteria. The abstract does not mention discretizing continuous audio signals into finite token vocabularies, nor does it describe tokenizers, codebooks, or quantization methods related to discrete audio tokens. Therefore, it does not meet the specific inclusion criteria of using discrete audio tokens as the main input representation or modeling focus and seems more oriented to speech-text fusion and error-robust frameworks at the transcript level, rather than discrete audio token-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Speech Emotion Recognition using ASR transcripts without discussing or utilizing discrete audio tokens derived from discrete vector quantization, neural audio codecs, or clustering-based units, which are core to the inclusion criteria. The abstract does not mention discretizing continuous audio signals into finite token vocabularies, nor does it describe tokenizers, codebooks, or quantization methods related to discrete audio tokens. Therefore, it does not meet the specific inclusion criteria of using discrete audio tokens as the main input representation or modeling focus and seems more oriented to speech-text fusion and error-robust frameworks at the transcript level, rather than discrete audio token-based modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Precipitation Nowcasting Using Physics Informed Discriminator Generative Models",
    "abstract": "Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.",
    "metadata": {
      "arxiv_id": "2406.10108",
      "title": "Precipitation Nowcasting Using Physics Informed Discriminator Generative Models",
      "summary": "Nowcasting leverages real-time atmospheric conditions to forecast weather over short periods. State-of-the-art models, including PySTEPS, encounter difficulties in accurately forecasting extreme weather events because of their unpredictable distribution patterns. In this study, we design a physics-informed neural network to perform precipitation nowcasting using the precipitation and meteorological data from the Royal Netherlands Meteorological Institute (KNMI). This model draws inspiration from the novel Physics-Informed Discriminator GAN (PID-GAN) formulation, directly integrating physics-based supervision within the adversarial learning framework. The proposed model adopts a GAN structure, featuring a Vector Quantization Generative Adversarial Network (VQ-GAN) and a Transformer as the generator, with a temporal discriminator serving as the discriminator. Our findings demonstrate that the PID-GAN model outperforms numerical and SOTA deep generative models in terms of precipitation nowcasting downstream metrics.",
      "authors": [
        "Junzhe Yin",
        "Cristian Meo",
        "Ankush Roy",
        "Zeineh Bou Cher",
        "Yanbo Wang",
        "Ruben Imhoff",
        "Remko Uijlenhoet",
        "Justin Dauwels"
      ],
      "published": "2024-06-14T15:12:53Z",
      "updated": "2024-06-14T15:12:53Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10108v1",
      "landing_url": "https://arxiv.org/abs/2406.10108v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10108"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns precipitation nowcasting with physics-informed GANs and does not involve discrete audio tokens, tokenization, or any token-based modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns precipitation nowcasting with physics-informed GANs and does not involve discrete audio tokens, tokenization, or any token-based modeling, thus it fails the inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on precipitation nowcasting using physics-informed GAN models and does not mention any discrete audio tokens, neural audio codec, or discretization of audio waveforms into token sequences, which are essential inclusion criteria. Hence, it does not meet the inclusion criteria relating to discrete audio token research and is unrelated to audio, focusing instead on meteorological data and weather forecasting.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on precipitation nowcasting using physics-informed GAN models and does not mention any discrete audio tokens, neural audio codec, or discretization of audio waveforms into token sequences, which are essential inclusion criteria. Hence, it does not meet the inclusion criteria relating to discrete audio token research and is unrelated to audio, focusing instead on meteorological data and weather forecasting.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
    "abstract": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
    "metadata": {
      "arxiv_id": "2406.10223",
      "title": "Diffusion Synthesizer for Efficient Multilingual Speech to Speech Translation",
      "summary": "We introduce DiffuseST, a low-latency, direct speech-to-speech translation system capable of preserving the input speaker's voice zero-shot while translating from multiple source languages into English. We experiment with the synthesizer component of the architecture, comparing a Tacotron-based synthesizer to a novel diffusion-based synthesizer. We find the diffusion-based synthesizer to improve MOS and PESQ audio quality metrics by 23\\% each and speaker similarity by 5\\% while maintaining comparable BLEU scores. Despite having more than double the parameter count, the diffusion synthesizer has lower latency, allowing the entire model to run more than 5$\\times$ faster than real-time.",
      "authors": [
        "Nameer Hirschkind",
        "Xiao Yu",
        "Mahesh Kumar Nandwana",
        "Joseph Liu",
        "Eloi DuBois",
        "Dao Le",
        "Nicolas Thiebaut",
        "Colin Sinclair",
        "Kyle Spence",
        "Charles Shang",
        "Zoe Abrams",
        "Morgan McGuire"
      ],
      "published": "2024-06-14T17:55:55Z",
      "updated": "2024-06-14T17:55:55Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10223v1",
      "landing_url": "https://arxiv.org/abs/2406.10223v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10223"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a diffusion-based speech synthesizer for multilingual speech-to-speech translation and compares synthesizers; it does not mention discrete audio tokens, tokenizers, or token-based representations, thus it does not satisfy the token-centric inclusion criteria and aligns with exclusion criteria focused on continuous audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a diffusion-based speech synthesizer for multilingual speech-to-speech translation and compares synthesizers; it does not mention discrete audio tokens, tokenizers, or token-based representations, thus it does not satisfy the token-centric inclusion criteria and aligns with exclusion criteria focused on continuous audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a speech-to-speech translation system focusing on diffusion-based synthesizers for speech generation and quality improvement but does not mention any discretization process that produces discrete audio tokens or a tokenization mechanism relevant to the inclusion criteria. The core of the study revolves around continuous audio waveform synthesis rather than representing audio as discrete tokens for subsequent modeling or cross-modal reasoning. Therefore, it does not meet the requirements of including discrete audio tokens as the main representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a speech-to-speech translation system focusing on diffusion-based synthesizers for speech generation and quality improvement but does not mention any discretization process that produces discrete audio tokens or a tokenization mechanism relevant to the inclusion criteria. The core of the study revolves around continuous audio waveform synthesis rather than representing audio as discrete tokens for subsequent modeling or cross-modal reasoning. Therefore, it does not meet the requirements of including discrete audio tokens as the main representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
    "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract explicitly address discrete audio tokens produced by neural codecs or self-supervised models, discuss tokenization and design, and evaluate across downstream audio generation/understanding tasks, meeting all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract explicitly address discrete audio tokens produced by neural codecs or self-supervised models, discuss tokenization and design, and evaluate across downstream audio generation/understanding tasks, meeting all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses extracting discrete audio tokens from self-supervised models, focusing on semantic tokenization methods, quantization setups, and evaluation on discriminative and generative tasks, aligning well with the inclusion criteria of research on discrete audio tokens produced by self-supervised learning models and their impacts on downstream tasks; the abstract indicates attention to token design and task-specific performance, supporting replicability and system-level considerations, with clear English title and abstract.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper discusses extracting discrete audio tokens from self-supervised models, focusing on semantic tokenization methods, quantization setups, and evaluation on discriminative and generative tasks, aligning well with the inclusion criteria of research on discrete audio tokens produced by self-supervised learning models and their impacts on downstream tasks; the abstract indicates attention to token design and task-specific performance, supporting replicability and system-level considerations, with clear English title and abstract.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
    "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on speech tokenization for speech language models and discusses a discrete token-based representation with robustness to noise, suggesting a tokenization/quantization approach used for downstream modeling, which aligns with the discrete audio token criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work focuses on speech tokenization for speech language models and discusses a discrete token-based representation with robustness to noise, suggesting a tokenization/quantization approach used for downstream modeling, which aligns with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes NAST, a noise-aware speech tokenization method that produces discrete token sequences representing speech signals for use in speech language models, including clear design details and empirical evaluations demonstrating robustness and applicability. It aligns well with the inclusion criteria of discrete audio token generation and usage in downstream modeling, providing English abstract and method description with code availability, and does not meet any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes NAST, a noise-aware speech tokenization method that produces discrete token sequences representing speech signals for use in speech language models, including clear design details and empirical evaluations demonstrating robustness and applicability. It aligns well with the inclusion criteria of discrete audio token generation and usage in downstream modeling, providing English abstract and method description with code availability, and does not meet any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Wake dynamics of wind turbines in unsteady streamwise flow conditions",
    "abstract": "The unsteady flow physics of wind-turbine wakes under dynamic forcing conditions are critical to the modeling and control of wind farms for optimal power density. Unsteady forcing in the streamwise direction may be generated by unsteady inflow conditions in the atmospheric boundary layer, dynamic induction control of the turbine, or streamwise surge motions of a floating offshore wind turbine due to floating-platform oscillations. This study seeks to identify the dominant flow mechanisms in unsteady wakes forced by a periodic upstream inflow condition. A theoretical framework for the problem is derived, which describes traveling-wave undulations in the wake radius and streamwise velocity. These dynamics encourage the aggregation of tip vortices into large structures that are advected along in the wake. Flow measurements in the wake of a periodically surging turbine were obtained in an optically accessible towing-tank facility, with an average diameter-based Reynolds number of 300,000 and with surge-velocity amplitudes of up to 40% of the mean inflow velocity. Qualitative agreement between trends in the measurements and model predictions is observed, supporting the validity of the theoretical analyses. The experiments also demonstrate large enhancements in the recovery of the wake relative to the steady-flow case, with wake-length reductions of up to 46.5% and improvements in the available power at 10 diameters downstream of up to 15.7%. These results provide fundamental insights into the dynamics of unsteady wakes and serve as additional evidence that unsteady fluid mechanics can be leveraged to increase the power density of wind farms.",
    "metadata": {
      "arxiv_id": "2406.11693",
      "title": "Wake dynamics of wind turbines in unsteady streamwise flow conditions",
      "summary": "The unsteady flow physics of wind-turbine wakes under dynamic forcing conditions are critical to the modeling and control of wind farms for optimal power density. Unsteady forcing in the streamwise direction may be generated by unsteady inflow conditions in the atmospheric boundary layer, dynamic induction control of the turbine, or streamwise surge motions of a floating offshore wind turbine due to floating-platform oscillations. This study seeks to identify the dominant flow mechanisms in unsteady wakes forced by a periodic upstream inflow condition. A theoretical framework for the problem is derived, which describes traveling-wave undulations in the wake radius and streamwise velocity. These dynamics encourage the aggregation of tip vortices into large structures that are advected along in the wake. Flow measurements in the wake of a periodically surging turbine were obtained in an optically accessible towing-tank facility, with an average diameter-based Reynolds number of 300,000 and with surge-velocity amplitudes of up to 40% of the mean inflow velocity. Qualitative agreement between trends in the measurements and model predictions is observed, supporting the validity of the theoretical analyses. The experiments also demonstrate large enhancements in the recovery of the wake relative to the steady-flow case, with wake-length reductions of up to 46.5% and improvements in the available power at 10 diameters downstream of up to 15.7%. These results provide fundamental insights into the dynamics of unsteady wakes and serve as additional evidence that unsteady fluid mechanics can be leveraged to increase the power density of wind farms.",
      "authors": [
        "Nathaniel J. Wei",
        "Adnan El Makdah",
        "JiaCheng Hu",
        "Frieder Kaiser",
        "David E. Rival",
        "John O. Dabiri"
      ],
      "published": "2024-06-17T16:10:50Z",
      "updated": "2024-06-17T16:10:50Z",
      "categories": [
        "physics.flu-dyn"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11693v1",
      "landing_url": "https://arxiv.org/abs/2406.11693v1",
      "doi": "https://doi.org/10.1017/jfm.2024.999"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on wind-turbine wake dynamics under unsteady flow and does not address discrete audio tokens, tokenization methods, or audio modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on wind-turbine wake dynamics under unsteady flow and does not address discrete audio tokens, tokenization methods, or audio modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the fluid dynamics of wind turbine wakes in unsteady flow conditions, with no mention of discrete audio tokens, neural audio codecs, or relevant tokenization methods for audio waveforms; it does not meet any inclusion criteria related to discrete audio tokens and thus fails the scope of the review.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the fluid dynamics of wind turbine wakes in unsteady flow conditions, with no mention of discrete audio tokens, neural audio codecs, or relevant tokenization methods for audio waveforms; it does not meet any inclusion criteria related to discrete audio tokens and thus fails the scope of the review.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Image Generation without Vector Quantization",
    "abstract": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
    "metadata": {
      "arxiv_id": "2406.11838",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "authors": [
        "Tianhong Li",
        "Yonglong Tian",
        "He Li",
        "Mingyang Deng",
        "Kaiming He"
      ],
      "published": "2024-06-17T17:59:58Z",
      "updated": "2024-11-01T14:45:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11838v3",
      "landing_url": "https://arxiv.org/abs/2406.11838v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.11838"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses autoregressive image generation in continuous space without discrete audio tokens, and does not involve discrete audio tokenization or token-based modeling, thus it fails the inclusion criteria and meets the exclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses autoregressive image generation in continuous space without discrete audio tokens, and does not involve discrete audio tokenization or token-based modeling, thus it fails the inclusion criteria and meets the exclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autoregressive image generation techniques without using discrete tokens (vector quantization), specifically addressing continuous-valued token spaces for images rather than audio. The inclusion criteria require studies to address discrete audio tokens derived from audio waveforms for tasks like TTS, speech generation, or audio-visual alignment, which this paper does not cover. There is no mention of any audio domain, neural audio encoders, or discrete audio tokenization processes relevant to the topic. Hence, it falls outside the scope defined by the inclusion and exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autoregressive image generation techniques without using discrete tokens (vector quantization), specifically addressing continuous-valued token spaces for images rather than audio. The inclusion criteria require studies to address discrete audio tokens derived from audio waveforms for tasks like TTS, speech generation, or audio-visual alignment, which this paper does not cover. There is no mention of any audio domain, neural audio encoders, or discrete audio tokenization processes relevant to the topic. Hence, it falls outside the scope defined by the inclusion and exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Binaural Selective Attention Model for Target Speaker Extraction",
    "abstract": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
    "metadata": {
      "arxiv_id": "2406.12236",
      "title": "Binaural Selective Attention Model for Target Speaker Extraction",
      "summary": "The remarkable ability of humans to selectively focus on a target speaker in cocktail party scenarios is facilitated by binaural audio processing. In this paper, we present a binaural time-domain Target Speaker Extraction model based on the Filter-and-Sum Network (FaSNet). Inspired by human selective hearing, our proposed model introduces target speaker embedding into separators using a multi-head attention-based selective attention block. We also compared two binaural interaction approaches -- the cosine similarity of time-domain signals and inter-channel correlation in learned spectral representations. Our experimental results show that our proposed model outperforms monaural configurations and state-of-the-art multi-channel target speaker extraction models, achieving best-in-class performance with 18.52 dB SI-SDR, 19.12 dB SDR, and 3.05 PESQ scores under anechoic two-speaker test configurations.",
      "authors": [
        "Hanyu Meng",
        "Qiquan Zhang",
        "Xiangyu Zhang",
        "Vidhyasaharan Sethu",
        "Eliathamby Ambikairajah"
      ],
      "published": "2024-06-18T03:24:52Z",
      "updated": "2024-06-18T03:24:52Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12236v1",
      "landing_url": "https://arxiv.org/abs/2406.12236v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12236"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a binaural time-domain target speaker extraction model and does not discuss discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the token/discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a binaural time-domain target speaker extraction model and does not discuss discrete audio tokens, tokenizers, or token-based representations, thus it does not meet the token/discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on binaural selective attention and target speaker extraction using time-domain models but does not mention or describe any discretization of audio into discrete tokens, neural audio codecs, vector quantization, or any discrete token design or usage relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on binaural selective attention and target speaker extraction using time-domain models but does not mention or describe any discretization of audio into discrete tokens, neural audio codecs, vector quantization, or any discrete token design or usage relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
    "abstract": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
    "metadata": {
      "arxiv_id": "2406.12428",
      "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
      "summary": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
      "authors": [
        "Kentaro Mitsui",
        "Koh Mitsuda",
        "Toshiaki Wakatsuki",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2024-06-18T09:23:54Z",
      "updated": "2024-10-03T05:17:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12428v2",
      "landing_url": "https://arxiv.org/abs/2406.12428v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.12428"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The inclusion criteria require explicit discussion of discrete audio tokens and tokenization methods; the article focuses on parallel generation of text and speech for low-latency dialogue and does not address discrete audio tokens or token design, so it does not meet the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The inclusion criteria require explicit discussion of discrete audio tokens and tokenization methods; the article focuses on parallel generation of text and speech for low-latency dialogue and does not address discrete audio tokens or token design, so it does not meet the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses parallel generation of text and speech using large language models (LLMs) for spoken dialogue systems, aimed at reducing latency. However, it does not explicitly mention the use of discrete audio tokens derived via neural audio codecs or quantization methods, nor does it describe tokenization or codebook design crucial for the inclusion criteria. The focus is on sequence generation and latency rather than on discrete token representations or their design, evaluation, and application as core components. Therefore, it does not sufficiently meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses parallel generation of text and speech using large language models (LLMs) for spoken dialogue systems, aimed at reducing latency. However, it does not explicitly mention the use of discrete audio tokens derived via neural audio codecs or quantization methods, nor does it describe tokenization or codebook design crucial for the inclusion criteria. The focus is on sequence generation and latency rather than on discrete token representations or their design, evaluation, and application as core components. Therefore, it does not sufficiently meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
    "abstract": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'acoustic tokens' and optimized audio encoding for AAC but does not explicitly describe discrete token generation, a tokenizer/codec with a codebook, or discrete token-based input for modeling, making its alignment with the 'Discrete Audio Tokens' criteria unclear; inclusion is not certain based on the provided information.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'acoustic tokens' and optimized audio encoding for AAC but does not explicitly describe discrete token generation, a tokenizer/codec with a codebook, or discrete token-based input for modeling, making its alignment with the 'Discrete Audio Tokens' criteria unclear; inclusion is not certain based on the provided information.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses enhancing automated audio captioning by integrating large language models with optimized audio encoding but does not clearly describe the generation or use of discrete audio tokens through discretization processes such as vector quantization or codec-based tokenization; it focuses on using a pre-trained audio encoder and LLM decoders without explicit mention of discrete token sequences being a core representation, thus it likely does not meet the core inclusion criteria regarding discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses enhancing automated audio captioning by integrating large language models with optimized audio encoding but does not clearly describe the generation or use of discrete audio tokens through discretization processes such as vector quantization or codec-based tokenization; it focuses on using a pre-trained audio encoder and LLM decoders without explicit mention of discrete token sequences being a core representation, thus it likely does not meet the core inclusion criteria regarding discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged that the abstract does not clearly describe discrete audio-token generation; although it mentions \"acoustic tokens\" and compressing them via a Q-Former, there is no explicit description of discretization mechanisms (e.g., neural codec/vector quantization, tokenizer/codebook settings, bitrate or token sequences) required by the Discrete Audio Tokens inclusion criteria. The study appears to optimize continuous audio encoders and LLM decoders for automated audio captioning rather than propose or evaluate discrete audio-token representations, so it meets the exclusion criterion for continuous-representation-focused work. Brief explanation: Lacks explicit discretization/tokenizer/codebook details needed to qualify as a discrete-audio-token paper.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged that the abstract does not clearly describe discrete audio-token generation; although it mentions \"acoustic tokens\" and compressing them via a Q-Former, there is no explicit description of discretization mechanisms (e.g., neural codec/vector quantization, tokenizer/codebook settings, bitrate or token sequences) required by the Discrete Audio Tokens inclusion criteria. The study appears to optimize continuous audio encoders and LLM decoders for automated audio captioning rather than propose or evaluate discrete audio-token representations, so it meets the exclusion criterion for continuous-representation-focused work. Brief explanation: Lacks explicit discretization/tokenizer/codebook details needed to qualify as a discrete-audio-token paper.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Children's Speech Recognition through Discrete Token Enhancement",
    "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on using discrete audio tokens generated via neural audio codecs/self-supervised representations as input for children's ASR, discusses token creation/encoding strategies, and provides empirical evaluation of performance and parameter efficiency, aligning with all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on using discrete audio tokens generated via neural audio codecs/self-supervised representations as input for children's ASR, discusses token creation/encoding strategies, and provides empirical evaluation of performance and parameter efficiency, aligning with all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates discrete speech tokens generated presumably via neural methods as core representations for children's speech recognition, aligning with the discrete audio token definition. However, the abstract lacks specific details on the token generation process (e.g., tokenizer/codec architecture, quantization method, codebook settings) needed to confirm eligibility and reproducibility, which is critical for inclusion. As it focuses on using discrete tokens in ASR systems rather than continuous representations, it mostly fits the theme but without enough English methodological detail for reliable assessment.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study investigates discrete speech tokens generated presumably via neural methods as core representations for children's speech recognition, aligning with the discrete audio token definition. However, the abstract lacks specific details on the token generation process (e.g., tokenizer/codec architecture, quantization method, codebook settings) needed to confirm eligibility and reproducibility, which is critical for inclusion. As it focuses on using discrete tokens in ASR systems rather than continuous representations, it mostly fits the theme but without enough English methodological detail for reliable assessment.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The title/abstract clearly targets using discrete speech tokens as the core input for children's ASR and reports experiments (including single-view and multi-view token-creation strategies) and strong parameter-efficiency gains, which aligns with the Discrete Audio Tokens topic; however, the abstract omits crucial methodological details (e.g., tokenizer/codec architecture, quantization/codebook settings, bitrate or exact discretization method) required by the inclusion criteria to verify that the work uses neural/self-supervised discrete tokens and to support reproducibility. JuniorNano recommended inclusion based on topical alignment, while JuniorMini flagged the lack of token-generation details; given these mixed reviews and the missing technical specifics demanded by the criteria, the correct judgment is uncertainty. Score explanation: Ambiguous—the study claims discrete-token-based ASR but lacks sufficient English methodological detail to confirm it meets the discrete-token definition and reproducibility requirements.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The title/abstract clearly targets using discrete speech tokens as the core input for children's ASR and reports experiments (including single-view and multi-view token-creation strategies) and strong parameter-efficiency gains, which aligns with the Discrete Audio Tokens topic; however, the abstract omits crucial methodological details (e.g., tokenizer/codec architecture, quantization/codebook settings, bitrate or exact discretization method) required by the inclusion criteria to verify that the work uses neural/self-supervised discrete tokens and to support reproducibility. JuniorNano recommended inclusion based on topical alignment, while JuniorMini flagged the lack of token-generation details; given these mixed reviews and the missing technical specifics demanded by the criteria, the correct judgment is uncertainty. Score explanation: Ambiguous—the study claims discrete-token-based ASR but lacks sufficient English methodological detail to confirm it meets the discrete-token definition and reproducibility requirements.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
    "abstract": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
    "metadata": {
      "arxiv_id": "2406.13695",
      "title": "Multilingual De-Duplication Strategies: Applying scalable similarity search with monolingual & multilingual embedding models",
      "summary": "This paper addresses the deduplication of multilingual textual data using advanced NLP tools. We compare a two-step method involving translation to English followed by embedding with mpnet, and a multilingual embedding model (distiluse). The two-step approach achieved a higher F1 score (82% vs. 60%), particularly with less widely used languages, which can be increased up to 89% by leveraging expert rules based on domain knowledge. We also highlight limitations related to token length constraints and computational efficiency. Our methodology suggests improvements for future multilingual deduplication tasks.",
      "authors": [
        "Stefan Pasch",
        "Dimitirios Petridis",
        "Jannic Cutura"
      ],
      "published": "2024-06-19T16:48:14Z",
      "updated": "2024-06-19T16:48:14Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13695v1",
      "landing_url": "https://arxiv.org/abs/2406.13695v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.13695"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article deals with multilingual text deduplication and embedding methods for text data, with no discrete audio tokenization or audio token design discussed, thus it does not satisfy the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article deals with multilingual text deduplication and embedding methods for text data, with no discrete audio tokenization or audio token design discussed, thus it does not satisfy the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multilingual text deduplication using embedding models and does not address discrete audio tokens derived from discretization of continuous audio waveforms, nor does it discuss any neural audio codec or related tokenization mechanisms for audio representation, which is the core inclusion criterion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multilingual text deduplication using embedding models and does not address discrete audio tokens derived from discretization of continuous audio waveforms, nor does it discuss any neural audio codec or related tokenization mechanisms for audio representation, which is the core inclusion criterion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
    "abstract": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
    "metadata": {
      "arxiv_id": "2406.14017",
      "title": "EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration",
      "summary": "Generative retrieval has recently emerged as a promising approach to sequential recommendation, framing candidate item retrieval as an autoregressive sequence generation problem. However, existing generative methods typically focus solely on either behavioral or semantic aspects of item information, neglecting their complementary nature and thus resulting in limited effectiveness. To address this limitation, we introduce EAGER, a novel generative recommendation framework that seamlessly integrates both behavioral and semantic information. Specifically, we identify three key challenges in combining these two types of information: a unified generative architecture capable of handling two feature types, ensuring sufficient and independent learning for each type, and fostering subtle interactions that enhance collaborative information utilization. To achieve these goals, we propose (1) a two-stream generation architecture leveraging a shared encoder and two separate decoders to decode behavior tokens and semantic tokens with a confidence-based ranking strategy; (2) a global contrastive task with summary tokens to achieve discriminative decoding for each type of information; and (3) a semantic-guided transfer task designed to implicitly promote cross-interactions through reconstruction and estimation objectives. We validate the effectiveness of EAGER on four public benchmarks, demonstrating its superior performance compared to existing methods.",
      "authors": [
        "Ye Wang",
        "Jiahao Xun",
        "Minjie Hong",
        "Jieming Zhu",
        "Tao Jin",
        "Wang Lin",
        "Haoyuan Li",
        "Linjun Li",
        "Yan Xia",
        "Zhou Zhao",
        "Zhenhua Dong"
      ],
      "published": "2024-06-20T06:21:56Z",
      "updated": "2024-07-03T10:00:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14017v2",
      "landing_url": "https://arxiv.org/abs/2406.14017v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a two-stream generative recommender system using behavior and semantic tokens, not on discrete audio tokenization or discretized audio representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a two-stream generative recommender system using behavior and semantic tokens, not on discrete audio tokenization or discretized audio representations, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a generative recommender system integrating behavioral and semantic information for recommendation tasks, with no mention of discrete audio tokens, discretization of continuous audio waveforms, or any audio-related tokenization process. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and audio-based modeling and falls under the exclusion criteria of lacking discrete audio token definitions or usage as a core method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a generative recommender system integrating behavioral and semantic information for recommendation tasks, with no mention of discrete audio tokens, discretization of continuous audio waveforms, or any audio-related tokenization process. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and audio-based modeling and falls under the exclusion criteria of lacking discrete audio token definitions or usage as a core method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DASB - Discrete Audio and Speech Benchmark",
    "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item directly addresses discrete audio tokens and tokenizer design, provides a systematic benchmark and cross-task evaluation comparing token types (semantic vs compression), and includes English abstract; thus it meets inclusion criteria and should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item directly addresses discrete audio tokens and tokenizer design, provides a systematic benchmark and cross-task evaluation comparing token types (semantic vs compression), and includes English abstract; thus it meets inclusion criteria and should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on discrete audio tokens that map continuous audio waveforms into token sequences for modeling and generation tasks, aligning well with the inclusion criteria definition. The abstract discusses the evaluation of various discrete audio tokens produced by neural audio encoders and self-supervised models, covering design choices and task performance across both discriminative and generative tasks, fitting the 'More Than a Survey' orientation. The presence of a comprehensive benchmark addressing design, evaluation, and reproducibility further supports inclusion. No exclusion criteria appear met, and sufficient English details are provided in the abstract to understand the token generation and evaluation framework.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on discrete audio tokens that map continuous audio waveforms into token sequences for modeling and generation tasks, aligning well with the inclusion criteria definition. The abstract discusses the evaluation of various discrete audio tokens produced by neural audio encoders and self-supervised models, covering design choices and task performance across both discriminative and generative tasks, fitting the 'More Than a Survey' orientation. The presence of a comprehensive benchmark addressing design, evaluation, and reproducibility further supports inclusion. No exclusion criteria appear met, and sufficient English details are provided in the abstract to understand the token generation and evaluation framework.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
    "abstract": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe TacoLM, a neural codec language model that uses discrete audio tokens produced by a neural codec and introduces architecture improvements for efficient zero-shot TTS, aligning with discrete-token based audio modeling and downstream TTS generation; thus it should be included under the stated criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe TacoLM, a neural codec language model that uses discrete audio tokens produced by a neural codec and introduces architecture improvements for efficient zero-shot TTS, aligning with discrete-token based audio modeling and downstream TTS generation; thus it should be included under the stated criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a neural codec language model (TacoLM) for zero-shot text-to-speech synthesis, mentioning neural audio codec usage and discrete tokens implicitly through the codec mechanism and language modeling over tokens; it provides evaluation metrics and compares to prior models, indicating relevance. However, the abstract does not detail the discrete token generation process, such as tokenizer architecture, codebook settings, or quantization specifics, which are critical for inclusion. Despite this, the topic centers on codec-based discrete token sequences for TTS, aligning with the inclusion criteria's thematic scope. Thus, the paper is likely relevant but lacks clear explicit detail on discrete token generation and design choices in the abstract, making inclusion not fully certain based on provided information alone.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a neural codec language model (TacoLM) for zero-shot text-to-speech synthesis, mentioning neural audio codec usage and discrete tokens implicitly through the codec mechanism and language modeling over tokens; it provides evaluation metrics and compares to prior models, indicating relevance. However, the abstract does not detail the discrete token generation process, such as tokenizer architecture, codebook settings, or quantization specifics, which are critical for inclusion. Despite this, the topic centers on codec-based discrete token sequences for TTS, aligning with the inclusion criteria's thematic scope. Thus, the paper is likely relevant but lacks clear explicit detail on discrete token generation and design choices in the abstract, making inclusion not fully certain based on provided information alone.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended inclusion (4) noting TacoLM is a neural codec language model for zero-shot TTS and aligns with discrete-token based audio modeling; the other was uncertain (3) because the abstract does not explicitly describe the discrete token generation (tokenizer/codec architecture, quantization, codebook/bitrate) required by the inclusion criteria. My assessment: the work likely uses a neural audio codec and discrete tokens (following VALL‑E style codec-LM), so thematically it fits the Discrete Audio Tokens topic, but the provided abstract fails to give the explicit, reproducibility-relevant details about how discrete tokens are produced and configured (tokenizer/codebook/bitrate/time downsampling) that the inclusion criteria mandate. Therefore, based on the abstract alone I cannot confidently confirm it meets ALL inclusion criteria and NONE of the exclusions.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended inclusion (4) noting TacoLM is a neural codec language model for zero-shot TTS and aligns with discrete-token based audio modeling; the other was uncertain (3) because the abstract does not explicitly describe the discrete token generation (tokenizer/codec architecture, quantization, codebook/bitrate) required by the inclusion criteria. My assessment: the work likely uses a neural audio codec and discrete tokens (following VALL‑E style codec-LM), so thematically it fits the Discrete Audio Tokens topic, but the provided abstract fails to give the explicit, reproducibility-relevant details about how discrete tokens are produced and configured (tokenizer/codebook/bitrate/time downsampling) that the inclusion criteria mandate. Therefore, based on the abstract alone I cannot confidently confirm it meets ALL inclusion criteria and NONE of the exclusions.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss",
    "abstract": "Contextualized end-to-end automatic speech recognition has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network.",
    "metadata": {
      "arxiv_id": "2406.16120",
      "title": "Contextualized End-to-end Automatic Speech Recognition with Intermediate Biasing Loss",
      "summary": "Contextualized end-to-end automatic speech recognition has been an active research area, with recent efforts focusing on the implicit learning of contextual phrases based on the final loss objective. However, these approaches ignore the useful contextual knowledge encoded in the intermediate layers. We hypothesize that employing explicit biasing loss as an auxiliary task in the encoder intermediate layers may better align text tokens or audio frames with the desired objectives. Our proposed intermediate biasing loss brings more regularization and contextualization to the network. Our method outperforms a conventional contextual biasing baseline on the LibriSpeech corpus, achieving a relative improvement of 22.5% in biased word error rate (B-WER) and up to 44% compared to the non-contextual baseline with a biasing list size of 100. Moreover, employing RNN-transducer-driven joint decoding further reduces the unbiased word error rate (U-WER), resulting in a more robust network.",
      "authors": [
        "Muhammad Shakeel",
        "Yui Sudo",
        "Yifan Peng",
        "Shinji Watanabe"
      ],
      "published": "2024-06-23T14:22:59Z",
      "updated": "2024-06-23T14:22:59Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16120v1",
      "landing_url": "https://arxiv.org/abs/2406.16120v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-1257"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on end-to-end ASR with an intermediate biasing loss and contextualization, with no discussion of discrete audio tokens, tokenizers, or token-level representations, hence it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on end-to-end ASR with an intermediate biasing loss and contextualization, with no discussion of discrete audio tokens, tokenizers, or token-level representations, hence it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on end-to-end automatic speech recognition with intermediate biasing loss, emphasizing improvements in word error rates; however, it does not discuss or define any discrete audio token representation or neural audio codec that discretizes continuous audio into token sequences, which is the primary inclusion criteria. The study seems to work on continuous speech modeling rather than tokenized discrete representations and lacks detail about any discrete tokenization process or codebook generation, thus not fulfilling the criteria required.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on end-to-end automatic speech recognition with intermediate biasing loss, emphasizing improvements in word error rates; however, it does not discuss or define any discrete audio token representation or neural audio codec that discretizes continuous audio into token sequences, which is the primary inclusion criteria. The study seems to work on continuous speech modeling rather than tokenized discrete representations and lacks detail about any discrete tokenization process or codebook generation, thus not fulfilling the criteria required.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
    "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on discrete audio tokens (semantic/acoustic) as the core representation for TTS, with descriptions of token generation and use in downstream synthesis, English abstract, and explicit analysis of token design, meeting inclusion criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on discrete audio tokens (semantic/acoustic) as the core representation for TTS, with descriptions of token generation and use in downstream synthesis, English abstract, and explicit analysis of token design, meeting inclusion criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a two-stage TTS system using semantic and acoustic discrete tokens, where semantic tokens focus on linguistic content and acoustic tokens enrich speech reconstruction, aligning well with the inclusion criteria of using discrete audio tokens derived from a neural codec-like process; however, the abstract lacks explicit detail on the token quantization process, tokenizer design, vocabulary specifics, or codebook settings, which affects reproducibility judgment, but the framework clearly uses discrete tokens for TTS generation and evaluation in zero-shot settings, fitting the theme and providing empirical results.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study presents a two-stage TTS system using semantic and acoustic discrete tokens, where semantic tokens focus on linguistic content and acoustic tokens enrich speech reconstruction, aligning well with the inclusion criteria of using discrete audio tokens derived from a neural codec-like process; however, the abstract lacks explicit detail on the token quantization process, tokenizer design, vocabulary specifics, or codebook settings, which affects reproducibility judgment, but the framework clearly uses discrete tokens for TTS generation and evaluation in zero-shot settings, fitting the theme and providing empirical results.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
    "abstract": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
    "metadata": {
      "arxiv_id": "2406.17722",
      "title": "Spatial Voice Conversion: Voice Conversion Preserving Spatial Information and Non-target Signals",
      "summary": "This paper proposes a new task called spatial voice conversion, which aims to convert a target voice while preserving spatial information and non-target signals. Traditional voice conversion methods focus on single-channel waveforms, ignoring the stereo listening experience inherent in human hearing. Our baseline approach addresses this gap by integrating blind source separation (BSS), voice conversion (VC), and spatial mixing to handle multi-channel waveforms. Through experimental evaluations, we organize and identify the key challenges inherent in this task, such as maintaining audio quality and accurately preserving spatial information. Our results highlight the fundamental difficulties in balancing these aspects, providing a benchmark for future research in spatial voice conversion. The proposed method's code is publicly available to encourage further exploration in this domain.",
      "authors": [
        "Kentaro Seki",
        "Shinnosuke Takamichi",
        "Norihiro Takamune",
        "Yuki Saito",
        "Kanami Imamura",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-06-25T17:10:39Z",
      "updated": "2024-06-25T17:10:39Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17722v1",
      "landing_url": "https://arxiv.org/abs/2406.17722v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17722"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses spatial voice conversion for multi-channel audio but does not introduce discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses spatial voice conversion for multi-channel audio but does not introduce discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses spatial voice conversion focusing on preserving spatial information and non-target signals. However, it does not mention discretization of audio into token sequences, nor the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any token-based representation or modeling as required by the inclusion criteria. Therefore, it does not meet the core criteria for inclusion as defined by the discrete audio tokens theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses spatial voice conversion focusing on preserving spatial information and non-target signals. However, it does not mention discretization of audio into token sequences, nor the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any token-based representation or modeling as required by the inclusion criteria. Therefore, it does not meet the core criteria for inclusion as defined by the discrete audio tokens theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
    "abstract": "Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens. Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.",
    "metadata": {
      "arxiv_id": "2406.17957",
      "title": "Improving Robustness of LLM-based Speech Synthesis by Learning Monotonic Alignment",
      "summary": "Large Language Model (LLM) based text-to-speech (TTS) systems have demonstrated remarkable capabilities in handling large speech datasets and generating natural speech for new speakers. However, LLM-based TTS models are not robust as the generated output can contain repeating words, missing words and mis-aligned speech (referred to as hallucinations or attention errors), especially when the text contains multiple occurrences of the same token. We examine these challenges in an encoder-decoder transformer model and find that certain cross-attention heads in such models implicitly learn the text and speech alignment when trained for predicting speech tokens for a given text. To make the alignment more robust, we propose techniques utilizing CTC loss and attention priors that encourage monotonic cross-attention over the text tokens. Our guided attention training technique does not introduce any new learnable parameters and significantly improves robustness of LLM-based TTS models.",
      "authors": [
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Subhankar Ghosh",
        "Jason Li",
        "Rafael Valle",
        "Rohan Badlani",
        "Boris Ginsburg"
      ],
      "published": "2024-06-25T22:18:52Z",
      "updated": "2024-06-25T22:18:52Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17957v1",
      "landing_url": "https://arxiv.org/abs/2406.17957v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17957"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses robustness of LLM-based TTS and monotonic cross-attention but does not specify any discrete audio token (tokenizer/codec) generation or evaluation as core; hence it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses robustness of LLM-based TTS and monotonic cross-attention but does not specify any discrete audio token (tokenizer/codec) generation or evaluation as core; hence it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving robustness in LLM-based text-to-speech systems using monotonic alignment techniques and attention mechanisms but does not explicitly describe the use or generation of discrete audio tokens as defined by the inclusion criteria, such as neural audio codecs or vector quantization processes producing discrete token sequences. The abstract also lacks details on token vocabulary, quantization, or discrete token representations crucial for inclusion, making it uncertain whether the core method revolves around discrete audio tokens rather than continuous speech representations or text alignment techniques.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving robustness in LLM-based text-to-speech systems using monotonic alignment techniques and attention mechanisms but does not explicitly describe the use or generation of discrete audio tokens as defined by the inclusion criteria, such as neural audio codecs or vector quantization processes producing discrete token sequences. The abstract also lacks details on token vocabulary, quantization, or discrete token representations crucial for inclusion, making it uncertain whether the core method revolves around discrete audio tokens rather than continuous speech representations or text alignment techniques.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
    "abstract": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
    "metadata": {
      "arxiv_id": "2407.00261",
      "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
      "summary": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
      "authors": [
        "Yubo Huang",
        "Jia Wang",
        "Peipei Li",
        "Liuyu Xiang",
        "Peigang Li",
        "Zhaofeng He"
      ],
      "published": "2024-06-28T23:20:57Z",
      "updated": "2024-09-20T23:54:19Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00261v2",
      "landing_url": "https://arxiv.org/abs/2407.00261v2",
      "doi": "https://doi.org/10.1109/ICME55011.2023.00094"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on iris restoration using a Transformer architecture and iris priors, not on discrete audio tokens or tokenization, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on iris restoration using a Transformer architecture and iris priors, not on discrete audio tokens or tokenization, so it does not meet the inclusion criteria for 'Discrete Audio Tokens' and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on iris image restoration using a Transformer and generative adversarial networks, without discussing audio or discrete audio tokens derived from waveforms. There is no mention of discretization, neural audio codecs, tokenization, or audio generation, which are essential for inclusion based on the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on iris image restoration using a Transformer and generative adversarial networks, without discussing audio or discrete audio tokens derived from waveforms. There is no mention of discretization, neural audio codecs, tokenization, or audio generation, which are essential for inclusion based on the criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
    "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
    "metadata": {
      "arxiv_id": "2407.01290",
      "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
      "summary": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
      "authors": [
        "Menglin Yang",
        "Harshit Verma",
        "Delvin Ce Zhang",
        "Jiahong Liu",
        "Irwin King",
        "Rex Ying"
      ],
      "published": "2024-07-01T13:44:38Z",
      "updated": "2025-08-24T12:17:27Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01290v2",
      "landing_url": "https://arxiv.org/abs/2407.01290v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.01290"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a hyperbolic Transformer (Hypformer) and does not involve discrete audio token generation, tokenization, or analysis of token design, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a hyperbolic Transformer (Hypformer) and does not involve discrete audio token generation, tokenization, or analysis of token design, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Hyperbolic Transformers for processing complex structured data and large-scale graphs, with no mention of discretizing continuous audio waveforms into token sequences or neural audio codecs, vector quantization, or related audio token generation processes; thus, it does not address the core topic of discrete audio tokens as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Hyperbolic Transformers for processing complex structured data and large-scale graphs, with no mention of discretizing continuous audio waveforms into token sequences or neural audio codecs, vector quantization, or related audio token generation processes; thus, it does not address the core topic of discrete audio tokens as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly studies discrete acoustic tokens produced by acoustic BPE, uses self-supervised semantic token representations, evaluates their effect on decoder-only TTS, and provides an English abstract, meeting all inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly studies discrete acoustic tokens produced by acoustic BPE, uses self-supervised semantic token representations, evaluates their effect on decoder-only TTS, and provides an English abstract, meeting all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates acoustic byte-pair encoding (BPE) to discretize speech tokens for decoder-only text-to-speech (TTS) models, applying self-supervised semantic speech token representations and evaluating their impact on intelligibility and diversity, which aligns with the discrete audio tokens topic definition; it focuses on token design's effect on downstream TTS tasks and provides experimental evaluation, indicating sufficient methodological details in English for assessment, meeting inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study investigates acoustic byte-pair encoding (BPE) to discretize speech tokens for decoder-only text-to-speech (TTS) models, applying self-supervised semantic speech token representations and evaluating their impact on intelligibility and diversity, which aligns with the discrete audio tokens topic definition; it focuses on token design's effect on downstream TTS tasks and provides experimental evaluation, indicating sufficient methodological details in English for assessment, meeting inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
    "abstract": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
    "metadata": {
      "arxiv_id": "2407.04291",
      "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
      "summary": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
      "authors": [
        "Ismail Rasim Ulgen",
        "John H. L. Hansen",
        "Carlos Busso",
        "Berrak Sisman"
      ],
      "published": "2024-07-05T06:54:24Z",
      "updated": "2025-09-18T20:22:33Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04291v3",
      "landing_url": "https://arxiv.org/abs/2407.04291v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.04291"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on speaker embeddings and intra-speaker diversity for speech generation, with no discussion of discrete audio tokens or tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on speaker embeddings and intra-speaker diversity for speech generation, with no discussion of discrete audio tokens or tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker embedding optimization for speech generation by capturing prosodic variations, but it does not mention discretizing continuous audio waveforms into discrete audio tokens nor specify using neural audio codecs or vector quantization methods for token generation, which are central to the inclusion criteria. Additionally, there is no indication that the work involves design or evaluation of discrete token schemes or uses token-based modeling, suggesting it focuses on continuous speaker embeddings rather than discrete audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker embedding optimization for speech generation by capturing prosodic variations, but it does not mention discretizing continuous audio waveforms into discrete audio tokens nor specify using neural audio codecs or vector quantization methods for token generation, which are central to the inclusion criteria. Additionally, there is no indication that the work involves design or evaluation of discrete token schemes or uses token-based modeling, suggesting it focuses on continuous speaker embeddings rather than discrete audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Balance of Number of Embedding and their Dimensions in Vector Quantization",
    "abstract": "The dimensionality of the embedding and the number of available embeddings ( also called codebook size) are critical factors influencing the performance of Vector Quantization(VQ), a discretization process used in many models such as the Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study examines the balance between the codebook sizes and dimensions of embeddings in VQ, while maintaining their product constant. Traditionally, these hyper parameters are static during training; however, our findings indicate that augmenting the codebook size while simultaneously reducing the embedding dimension can significantly boost the effectiveness of the VQ-VAE. As a result, the strategic selection of codebook size and embedding dimensions, while preserving the capacity of the discrete codebook space, is critically important. To address this, we propose a novel adaptive dynamic quantization approach, underpinned by the Gumbel-Softmax mechanism, which allows the model to autonomously determine the optimal codebook configuration for each data instance. This dynamic discretizer gives the VQ-VAE remarkable flexibility. Thorough empirical evaluations across multiple benchmark datasets validate the notable performance enhancements achieved by our approach, highlighting the significant potential of adaptive dynamic quantization to improve model performance.",
    "metadata": {
      "arxiv_id": "2407.04939",
      "title": "Balance of Number of Embedding and their Dimensions in Vector Quantization",
      "summary": "The dimensionality of the embedding and the number of available embeddings ( also called codebook size) are critical factors influencing the performance of Vector Quantization(VQ), a discretization process used in many models such as the Vector Quantized Variational Autoencoder (VQ-VAE) architecture. This study examines the balance between the codebook sizes and dimensions of embeddings in VQ, while maintaining their product constant. Traditionally, these hyper parameters are static during training; however, our findings indicate that augmenting the codebook size while simultaneously reducing the embedding dimension can significantly boost the effectiveness of the VQ-VAE. As a result, the strategic selection of codebook size and embedding dimensions, while preserving the capacity of the discrete codebook space, is critically important. To address this, we propose a novel adaptive dynamic quantization approach, underpinned by the Gumbel-Softmax mechanism, which allows the model to autonomously determine the optimal codebook configuration for each data instance. This dynamic discretizer gives the VQ-VAE remarkable flexibility. Thorough empirical evaluations across multiple benchmark datasets validate the notable performance enhancements achieved by our approach, highlighting the significant potential of adaptive dynamic quantization to improve model performance.",
      "authors": [
        "Hang Chen",
        "Sankepally Sainath Reddy",
        "Ziwei Chen",
        "Dianbo Liu"
      ],
      "published": "2024-07-06T03:07:31Z",
      "updated": "2024-07-06T03:07:31Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04939v1",
      "landing_url": "https://arxiv.org/abs/2407.04939v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.04939"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on neural vector quantization with discrete codebooks/tokens (VQ-VAE), introduces adaptive discretization via Gumbel-Softmax, and evaluates token-related design choices affecting audio modeling, meeting the discrete audio token criteria and not falling under exclusions.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on neural vector quantization with discrete codebooks/tokens (VQ-VAE), introduces adaptive discretization via Gumbel-Softmax, and evaluates token-related design choices affecting audio modeling, meeting the discrete audio token criteria and not falling under exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses vector quantization and embedding dimension/codebook size balance in VQ-VAE models, proposing a novel adaptive dynamic quantization method validated on benchmark datasets. However, the focus is on improving vector quantization techniques generally rather than explicitly on generating discrete audio tokens for audio waveforms like speech, music, or environmental sounds for sequence modeling as defined by the inclusion criteria. The abstract does not explicitly clarify the tokenization process for audio or its use in downstream audio generation or cross-modal tasks, nor does it provide detailed information about discrete token vocabulary or codebook settings tied to audio data. Therefore, it does not fully meet the criteria related to discrete audio token generation and usage as core to the study’s contribution.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study discusses vector quantization and embedding dimension/codebook size balance in VQ-VAE models, proposing a novel adaptive dynamic quantization method validated on benchmark datasets. However, the focus is on improving vector quantization techniques generally rather than explicitly on generating discrete audio tokens for audio waveforms like speech, music, or environmental sounds for sequence modeling as defined by the inclusion criteria. The abstract does not explicitly clarify the tokenization process for audio or its use in downstream audio generation or cross-modal tasks, nor does it provide detailed information about discrete token vocabulary or codebook settings tied to audio data. Therefore, it does not fully meet the criteria related to discrete audio token generation and usage as core to the study’s contribution.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Integrated assessment: JuniorNano favored inclusion (5) assuming VQ-VAE work maps to discrete audio tokens, but JuniorMini correctly flagged that the abstract discusses vector quantization methods generally without any explicit mention of audio/waveforms, tokenizer/codec details, codebook/time-downsampling settings, or using discrete tokens for downstream audio generation/sequence modeling—requirements in the inclusion criteria. Because the paper abstract lacks explicit application to audio tokenization and necessary methodological details to verify it produces discrete audio tokens, it fails the topic-specific inclusion rules and should be excluded. One-sentence explanation: Score 2 (better to exclude) because the study focuses on general VQ design and adaptive quantization but does not explicitly describe discrete audio-token generation, codec/tokenizer settings, or audio downstream use as required.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Integrated assessment: JuniorNano favored inclusion (5) assuming VQ-VAE work maps to discrete audio tokens, but JuniorMini correctly flagged that the abstract discusses vector quantization methods generally without any explicit mention of audio/waveforms, tokenizer/codec details, codebook/time-downsampling settings, or using discrete tokens for downstream audio generation/sequence modeling—requirements in the inclusion criteria. Because the paper abstract lacks explicit application to audio tokenization and necessary methodological details to verify it produces discrete audio tokens, it fails the topic-specific inclusion rules and should be excluded. One-sentence explanation: Score 2 (better to exclude) because the study focuses on general VQ design and adaptive quantization but does not explicitly describe discrete audio-token generation, codec/tokenizer settings, or audio downstream use as required.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
    "abstract": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
    "metadata": {
      "arxiv_id": "2407.05361",
      "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
      "summary": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-07-07T13:24:54Z",
      "updated": "2024-09-07T15:08:24Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05361v3",
      "landing_url": "https://arxiv.org/abs/2407.05361v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.05361"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a multilingual speech dataset and preprocessing pipeline, with no discussion of discrete audio tokens, tokenization, or token-level analysis; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a multilingual speech dataset and preprocessing pipeline, with no discussion of discrete audio tokens, tokenization, or token-level analysis; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces a large-scale multilingual speech dataset aimed at speech generation but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, tokenizers, or discrete unit mappings, which are central to the inclusion criteria; thus, it lacks necessary details on discrete token generation or their use in modeling or analysis, making it unlikely to meet inclusion requirements related to discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study introduces a large-scale multilingual speech dataset aimed at speech generation but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, tokenizers, or discrete unit mappings, which are central to the inclusion criteria; thus, it lacks necessary details on discrete token generation or their use in modeling or analysis, making it unlikely to meet inclusion requirements related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
    "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper introduces discrete speech tokens produced by vector quantization (supervised semantic tokens) and uses them as the core representation in a TTS system, with an explicit token production process and token-to-speech generation, matching the discrete-audio-token inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper introduces discrete speech tokens produced by vector quantization (supervised semantic tokens) and uses them as the core representation in a TTS system, with an explicit token production process and token-to-speech generation, matching the discrete-audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper clearly aligns with the inclusion criteria as it proposes a new discrete audio token representation derived from supervised semantic tokens, obtained via vector quantization embedded in a multilingual speech recognition model encoder; it uses these discrete tokens as core inputs in a scalable zero-shot TTS system, thoroughly detailing token derivation and demonstrating empirical improvements in downstream tasks. There is sufficient English detail describing token generation and utilization, fulfilling the criteria for systematic study of discrete audio tokens in TTS, making it a strong fit for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper clearly aligns with the inclusion criteria as it proposes a new discrete audio token representation derived from supervised semantic tokens, obtained via vector quantization embedded in a multilingual speech recognition model encoder; it uses these discrete tokens as core inputs in a scalable zero-shot TTS system, thoroughly detailing token derivation and demonstrating empirical improvements in downstream tasks. There is sufficient English detail describing token generation and utilization, fulfilling the criteria for systematic study of discrete audio tokens in TTS, making it a strong fit for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
    "abstract": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
    "metadata": {
      "arxiv_id": "2407.08152",
      "title": "Privacy-Preserving Data Deduplication for Enhancing Federated Learning of Language Models (Extended Version)",
      "summary": "Deduplication is a vital preprocessing step that enhances machine learning model performance and saves training time and energy. However, enhancing federated learning through deduplication poses challenges, especially regarding scalability and potential privacy violations if deduplication involves sharing all clients' data. In this paper, we address the problem of deduplication in a federated setup by introducing a pioneering protocol, Efficient Privacy-Preserving Multi-Party Deduplication (EP-MPD). It efficiently removes duplicates from multiple clients' datasets without compromising data privacy. EP-MPD is constructed in a modular fashion, utilizing two novel variants of the Private Set Intersection protocol. Our extensive experiments demonstrate the significant benefits of deduplication in federated learning of large language models. For instance, we observe up to 19.62\\% improvement in perplexity and up to 27.95\\% reduction in running time while varying the duplication level between 10\\% and 30\\%. EP-MPD effectively balances privacy and performance in federated learning, making it a valuable solution for large-scale applications.",
      "authors": [
        "Aydin Abadi",
        "Vishnu Asutosh Dasu",
        "Sumanta Sarkar"
      ],
      "published": "2024-07-11T03:10:27Z",
      "updated": "2024-12-04T17:56:57Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08152v2",
      "landing_url": "https://arxiv.org/abs/2407.08152v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08152"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on privacy-preserving data deduplication in federated learning for language models and does not address discrete audio tokens, tokenization, or audio token design; thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on privacy-preserving data deduplication in federated learning for language models and does not address discrete audio tokens, tokenization, or audio token design; thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on privacy-preserving data deduplication in federated learning for language models and does not address any aspect of discrete audio tokens, their production, design, or use in audio generation or understanding. There is no mention of discrete tokenization of continuous audio signals or related neural codecs, which are essential to meet the inclusion criteria. Therefore, it does not meet the required topic definitions and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on privacy-preserving data deduplication in federated learning for language models and does not address any aspect of discrete audio tokens, their production, design, or use in audio generation or understanding. There is no mention of discrete tokenization of continuous audio signals or related neural codecs, which are essential to meet the inclusion criteria. Therefore, it does not meet the required topic definitions and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Speech Synthesis without Vector Quantization",
    "abstract": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
    "metadata": {
      "arxiv_id": "2407.08551",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "authors": [
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Bing Han",
        "Shujie Hu",
        "Yanqing Liu",
        "Jinyu Li",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng",
        "Furu Wei"
      ],
      "published": "2024-07-11T14:36:53Z",
      "updated": "2025-05-27T05:07:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551v2",
      "landing_url": "https://arxiv.org/abs/2407.08551v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08551"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on continuous mel-spectrogram frames and explicitly avoids vector quantization, i.e., it does not involve discrete audio tokens, so it does not meet the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on continuous mel-spectrogram frames and explicitly avoids vector quantization, i.e., it does not involve discrete audio tokens, so it does not meet the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes MELLE, a continuous-valued token-based TTS approach that explicitly bypasses vector quantization and relies on continuous mel-spectrogram frames rather than discrete token sequences derived from neural audio codecs or self-supervised models; hence it does not meet the core inclusion criterion of using discrete audio tokens and does not describe any discrete token design or vocabulary.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes MELLE, a continuous-valued token-based TTS approach that explicitly bypasses vector quantization and relies on continuous mel-spectrogram frames rather than discrete token sequences derived from neural audio codecs or self-supervised models; hence it does not meet the core inclusion criterion of using discrete audio tokens and does not describe any discrete token design or vocabulary.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Representing Rule-based Chatbots with Transformers",
    "abstract": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
    "metadata": {
      "arxiv_id": "2407.10949",
      "title": "Representing Rule-based Chatbots with Transformers",
      "summary": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
      "authors": [
        "Dan Friedman",
        "Abhishek Panigrahi",
        "Danqi Chen"
      ],
      "published": "2024-07-15T17:45:53Z",
      "updated": "2025-02-12T15:18:32Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10949v2",
      "landing_url": "https://arxiv.org/abs/2407.10949v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.10949"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns transforming rule-based chatbots and mechanistic analysis of Transformers for text dialogue, with no discussion of discrete audio tokens, tokenization, or audio discretization relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns transforming rule-based chatbots and mechanistic analysis of Transformers for text dialogue, with no discussion of discrete audio tokens, tokenization, or audio discretization relevant to the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on representing rule-based chatbots with Transformers and analyzes conversational mechanisms, but it does not involve discrete audio tokens nor the discretization of continuous audio waveforms into token sequences; there is no mention of audio tokenizers, neural audio codecs, or audio-related tasks such as speech or music generation or cross-modal modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on representing rule-based chatbots with Transformers and analyzes conversational mechanisms, but it does not involve discrete audio tokens nor the discretization of continuous audio waveforms into token sequences; there is no mention of audio tokenizers, neural audio codecs, or audio-related tasks such as speech or music generation or cross-modal modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Computing $k$-means in mixed precision",
    "abstract": "The $k$-means algorithm is one of the most popular and critical techniques in data mining and machine learning, and it has achieved significant success in numerous science and engineering domains. Computing $k$-means to a global optimum is NP-hard in Euclidean space, yet there are a variety of efficient heuristic algorithms, such as Lloyd's algorithm, that converge to a local optimum with superpolynomial complexity in the worst case. Motivated by the emergence and prominence of mixed precision capabilities in hardware, a current trend is to develop low and mixed precision variants of algorithms in order to improve the runtime and energy consumption. In this paper we study the numerical stability of Lloyd's $k$-means algorithm, and, in particular, we confirm the stability of the widely used distance computation formula. We propose a mixed-precision framework for $k$-means computation and investigate the effects of low-precision distance computation within the framework. Through extensive simulations on various data clustering and image segmentation tasks, we verify the applicability and robustness of the mixed precision $k$-means method. We find that, in $k$-means computation, normalized data is more tolerant to the reduction of precision in the distance computation, while for nonnormalized data more care is needed in the use of reduced precision, mainly to avoid overflow. Our study demonstrates the potential for the use of mixed precision to accelerate the $k$-means computation and offers some insights into other distance-based machine learning methods.",
    "metadata": {
      "arxiv_id": "2407.12208",
      "title": "Computing $k$-means in mixed precision",
      "summary": "The $k$-means algorithm is one of the most popular and critical techniques in data mining and machine learning, and it has achieved significant success in numerous science and engineering domains. Computing $k$-means to a global optimum is NP-hard in Euclidean space, yet there are a variety of efficient heuristic algorithms, such as Lloyd's algorithm, that converge to a local optimum with superpolynomial complexity in the worst case. Motivated by the emergence and prominence of mixed precision capabilities in hardware, a current trend is to develop low and mixed precision variants of algorithms in order to improve the runtime and energy consumption. In this paper we study the numerical stability of Lloyd's $k$-means algorithm, and, in particular, we confirm the stability of the widely used distance computation formula. We propose a mixed-precision framework for $k$-means computation and investigate the effects of low-precision distance computation within the framework. Through extensive simulations on various data clustering and image segmentation tasks, we verify the applicability and robustness of the mixed precision $k$-means method. We find that, in $k$-means computation, normalized data is more tolerant to the reduction of precision in the distance computation, while for nonnormalized data more care is needed in the use of reduced precision, mainly to avoid overflow. Our study demonstrates the potential for the use of mixed precision to accelerate the $k$-means computation and offers some insights into other distance-based machine learning methods.",
      "authors": [
        "Erin Carson",
        "Xinye Chen",
        "Xiaobo Liu"
      ],
      "published": "2024-07-16T22:48:35Z",
      "updated": "2024-07-16T22:48:35Z",
      "categories": [
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12208v1",
      "landing_url": "https://arxiv.org/abs/2407.12208v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.12208"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on k-means clustering and mixed-precision computation with no discussion of discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on k-means clustering and mixed-precision computation with no discussion of discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on improving the $k$-means algorithm through mixed precision computation to enhance runtime and energy efficiency, and does not relate to discrete audio tokens, neural audio codecs, or any aspect of audio tokenization and modeling as defined in the inclusion criteria. There is no mention of discretizing continuous audio waveforms into token sequences, nor any focus on audio generation, understanding, or cross-modal modeling. Hence, the study does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on improving the $k$-means algorithm through mixed precision computation to enhance runtime and energy efficiency, and does not relate to discrete audio tokens, neural audio codecs, or any aspect of audio tokenization and modeling as defined in the inclusion criteria. There is no mention of discretizing continuous audio waveforms into token sequences, nor any focus on audio generation, understanding, or cross-modal modeling. Hence, the study does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TTSDS -- Text-to-Speech Distribution Score",
    "abstract": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
    "metadata": {
      "arxiv_id": "2407.12707",
      "title": "TTSDS -- Text-to-Speech Distribution Score",
      "summary": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
      "authors": [
        "Christoph Minixhofer",
        "Ondřej Klejch",
        "Peter Bell"
      ],
      "published": "2024-07-17T16:30:27Z",
      "updated": "2024-12-02T03:45:42Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12707v3",
      "landing_url": "https://arxiv.org/abs/2407.12707v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.12707"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "No mention of discrete audio tokens, tokenization, neural codecs producing discrete tokens, or token-based downstream tasks; the work focuses on evaluating TTS quality rather than tokenization or token-level representations, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "No mention of discrete audio tokens, tokenization, neural codecs producing discrete tokens, or token-based downstream tasks; the work focuses on evaluating TTS quality rather than tokenization or token-level representations, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating synthetic speech quality through multiple factors but does not mention the generation, use, or analysis of discrete audio tokens derived from neural audio codecs or self-supervised models as required by the inclusion criteria. No tokenization, quantization, or discrete token vocabulary details are provided, thus it does not meet the essential conditions for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating synthetic speech quality through multiple factors but does not mention the generation, use, or analysis of discrete audio tokens derived from neural audio codecs or self-supervised models as required by the inclusion criteria. No tokenization, quantization, or discrete token vocabulary details are provided, thus it does not meet the essential conditions for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
    "abstract": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
    "metadata": {
      "arxiv_id": "2407.13333",
      "title": "Using Speech Foundational Models in Loss Functions for Hearing Aid Speech Enhancement",
      "summary": "Machine learning techniques are an active area of research for speech enhancement for hearing aids, with one particular focus on improving the intelligibility of a noisy speech signal. Recent work has shown that feature encodings from self-supervised speech representation models can effectively capture speech intelligibility. In this work, it is shown that the distance between self-supervised speech representations of clean and noisy speech correlates more strongly with human intelligibility ratings than other signal-based metrics. Experiments show that training a speech enhancement model using this distance as part of a loss function improves the performance over using an SNR-based loss function, demonstrated by an increase in HASPI, STOI, PESQ and SI-SNR scores. This method takes inference of a high parameter count model only at training time, meaning the speech enhancement model can remain smaller, as is required for hearing aids.",
      "authors": [
        "Robert Sutherland",
        "George Close",
        "Thomas Hain",
        "Stefan Goetze",
        "Jon Barker"
      ],
      "published": "2024-07-18T09:32:57Z",
      "updated": "2024-07-18T09:32:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.13333v1",
      "landing_url": "https://arxiv.org/abs/2407.13333v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.13333"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract focus on loss functions using distances between self-supervised speech representations for hearing aid enhancement, with no mention of discretized audio tokens or tokenization design, hence it does not meet the discrete audio tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract focus on loss functions using distances between self-supervised speech representations for hearing aid enhancement, with no mention of discretized audio tokens or tokenization design, hence it does not meet the discrete audio tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using self-supervised speech representation models for speech enhancement, but there is no evidence that it involves discretizing continuous audio waveforms into discrete token sequences or discusses the design, generation, or use of discrete audio tokens; moreover, the methods revolve around continuous speech representations rather than discrete token generation or usage, thus it does not meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using self-supervised speech representation models for speech enhancement, but there is no evidence that it involves discretizing continuous audio waveforms into discrete token sequences or discusses the design, generation, or use of discrete audio tokens; moreover, the methods revolve around continuous speech representations rather than discrete token generation or usage, thus it does not meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "dMel: Speech Tokenization made Simple",
    "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
    "metadata": {
      "arxiv_id": "2407.15835",
      "title": "dMel: Speech Tokenization made Simple",
      "summary": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data. Inspired by this success, researchers have investigated various compression-based speech tokenization methods to discretize continuous speech signals, enabling the application of language modeling techniques to discrete tokens. However, audio compressor introduces additional complexity and computational cost, and often fail on out-of-domain audio signals. In this work, we introduce a novel speech representation (dmel) that discretizes mel-filterbank channels into intensity bins, creating a simpler yet more effective representation compared to existing speech tokenization methods. Our approach demonstrates superior performance in preserving audio content, robustness to out-of-domain data, and offers a training-free, natural, and streamable representation. To address the high-dimensional nature of log-mel spectrograms, we propose an efficient parallel encoding and decoding method for high-dimensional tokens using an LM-style transformer architecture. This innovation enables us to develop RichTTS and RichASR, two models sharing the same architecture while achieving comparable or better results than specialized existing methods. Our results demonstrate the effectiveness of dmel in achieving high performance on both speech synthesis and recognition tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text.",
      "authors": [
        "Richard He Bai",
        "Tatiana Likhomanenko",
        "Ruixiang Zhang",
        "Zijin Gu",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-07-22T17:51:53Z",
      "updated": "2025-05-21T16:55:34Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15835v3",
      "landing_url": "https://arxiv.org/abs/2407.15835v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15835"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract proposes dMel, a discrete speech tokenization method that discretizes mel-filterbank channels into tokens and uses an LM-style architecture for TTS/ASR, addressing token design, downstream tasks, and cross-domain robustness, thus fitting the discrete audio token criteria and evaluation requirements.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract proposes dMel, a discrete speech tokenization method that discretizes mel-filterbank channels into tokens and uses an LM-style architecture for TTS/ASR, addressing token design, downstream tasks, and cross-domain robustness, thus fitting the discrete audio token criteria and evaluation requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces dmel, a novel speech representation that discretizes mel-filterbank channels into intensity bins, effectively creating discrete tokens for speech. It focuses on speech tokenization through discretization suitable for sequence modeling, presents a training-free method with a clear discretization process, and evaluates the approach on speech synthesis and recognition tasks, aligning well with the inclusion criteria for discrete audio tokens research. The abstract provides sufficient methodological and experimental details in English, making the study’s discrete token design and evaluation clear and reproducible, and the application scope includes TTS and ASR, which are relevant downstream tasks. Therefore, the study meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces dmel, a novel speech representation that discretizes mel-filterbank channels into intensity bins, effectively creating discrete tokens for speech. It focuses on speech tokenization through discretization suitable for sequence modeling, presents a training-free method with a clear discretization process, and evaluates the approach on speech synthesis and recognition tasks, aligning well with the inclusion criteria for discrete audio tokens research. The abstract provides sufficient methodological and experimental details in English, making the study’s discrete token design and evaluation clear and reproducible, and the application scope includes TTS and ASR, which are relevant downstream tasks. Therefore, the study meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "High Efficiency Image Compression for Large Visual-Language Models",
    "abstract": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
    "metadata": {
      "arxiv_id": "2407.17060",
      "title": "High Efficiency Image Compression for Large Visual-Language Models",
      "summary": "In recent years, large visual language models (LVLMs) have shown impressive performance and promising generalization capability in multi-modal tasks, thus replacing humans as receivers of visual information in various application scenarios. In this paper, we pioneer to propose a variable bitrate image compression framework consisting of a pre-editing module and an end-to-end codec to achieve promising rate-accuracy performance for different LVLMs. In particular, instead of optimizing an adaptive pre-editing network towards a particular task or several representative tasks, we propose a new optimization strategy tailored for LVLMs, which is designed based on the representation and discrimination capability with token-level distortion and rank. The pre-editing module and the variable bitrate end-to-end image codec are jointly trained by the losses based on semantic tokens of the large model, which introduce enhanced generalization capability for various data and tasks. {Experimental results demonstrate that the proposed framework could efficiently achieve much better rate-accuracy performance compared to the state-of-the-art coding standard, Versatile Video Coding.} Meanwhile, experiments with multi-modal tasks have revealed the robustness and generalization capability of the proposed framework.",
      "authors": [
        "Binzhe Li",
        "Shurun Wang",
        "Shiqi Wang",
        "Yan Ye"
      ],
      "published": "2024-07-24T07:37:12Z",
      "updated": "2024-07-24T07:37:12Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.17060v1",
      "landing_url": "https://arxiv.org/abs/2407.17060v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.17060"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on image compression and semantic tokens for LVLMs, not on discrete audio tokens or their tokenization/codec design, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on image compression and semantic tokens for LVLMs, not on discrete audio tokens or their tokenization/codec design, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on image compression techniques for large visual-language models, not on audio or discrete audio tokens derived from continuous audio waveforms. The study does not mention the generation, usage, or analysis of discrete audio tokens, neural audio codecs, or related audio tokenization processes specified in the inclusion criteria, and does not pertain to audio generation, understanding, or cross-modal alignment involving audio tokens. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria related to topic mismatch, making it unsuitable for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on image compression techniques for large visual-language models, not on audio or discrete audio tokens derived from continuous audio waveforms. The study does not mention the generation, usage, or analysis of discrete audio tokens, neural audio codecs, or related audio tokenization processes specified in the inclusion criteria, and does not pertain to audio generation, understanding, or cross-modal alignment involving audio tokens. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria related to topic mismatch, making it unsuitable for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
    "abstract": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
    "metadata": {
      "arxiv_id": "2407.19051",
      "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
      "summary": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
      "authors": [
        "Bruna Bazaluk",
        "Mosab Hamdan",
        "Mustafa Ghaleb",
        "Mohammed S. M. Gismalla",
        "Flavio S. Correa da Silva",
        "Daniel Macêdo Batista"
      ],
      "published": "2024-07-26T19:13:11Z",
      "updated": "2024-07-26T19:13:11Z",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19051v1",
      "landing_url": "https://arxiv.org/abs/2407.19051v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.19051"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe IoT traffic classification using Transformer-based models on network data; there is no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe IoT traffic classification using Transformer-based models on network data; there is no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on IoT traffic classification using a transformer-based model and does not address discrete audio tokens or discretization of continuous audio waveforms into token sequences; therefore, it does not meet the inclusion criteria related to audio tokenization or representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on IoT traffic classification using a transformer-based model and does not address discrete audio tokens or discretization of continuous audio waveforms into token sequences; therefore, it does not meet the inclusion criteria related to audio tokenization or representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving noisy student training for low-resource languages in End-to-End ASR using CycleGAN and inter-domain losses",
    "abstract": "Training a semi-supervised end-to-end speech recognition system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired speech-text and unlabeled speech, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic speech recognition where there are limited paired speech-text, unlabeled speech (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning \"CycleGAN and inter-domain losses\" solely with external text. Secondly, we enhance \"CycleGAN and inter-domain losses\" by incorporating automatic hyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\" Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.",
    "metadata": {
      "arxiv_id": "2407.21061",
      "title": "Improving noisy student training for low-resource languages in End-to-End ASR using CycleGAN and inter-domain losses",
      "summary": "Training a semi-supervised end-to-end speech recognition system using noisy student training has significantly improved performance. However, this approach requires a substantial amount of paired speech-text and unlabeled speech, which is costly for low-resource languages. Therefore, this paper considers a more extreme case of semi-supervised end-to-end automatic speech recognition where there are limited paired speech-text, unlabeled speech (less than five hours), and abundant external text. Firstly, we observe improved performance by training the model using our previous work on semi-supervised learning \"CycleGAN and inter-domain losses\" solely with external text. Secondly, we enhance \"CycleGAN and inter-domain losses\" by incorporating automatic hyperparameter tuning, calling it \"enhanced CycleGAN inter-domain losses.\" Thirdly, we integrate it into the noisy student training approach pipeline for low-resource scenarios. Our experimental results, conducted on six non-English languages from Voxforge and Common Voice, show a 20% word error rate reduction compared to the baseline teacher model and a 10% word error rate reduction compared to the baseline best student model, highlighting the significant improvements achieved through our proposed method.",
      "authors": [
        "Chia-Yu Li",
        "Ngoc Thang Vu"
      ],
      "published": "2024-07-26T10:57:06Z",
      "updated": "2024-07-26T10:57:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.21061v1",
      "landing_url": "https://arxiv.org/abs/2407.21061v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.21061"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on semi-supervised ASR training (CycleGAN, noisy student) for low-resource languages; it does not discuss discrete audio tokens, tokenizers, or token-level representations, so it does not meet the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on semi-supervised ASR training (CycleGAN, noisy student) for low-resource languages; it does not discuss discrete audio tokens, tokenizers, or token-level representations, so it does not meet the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses improvements in semi-supervised end-to-end speech recognition for low-resource languages using CycleGAN and inter-domain losses, focusing on ASR performance; however, it does not mention the use or generation of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process central to the theme of discrete audio tokens, nor does it provide details on discrete token vocabularies or quantization setups.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses improvements in semi-supervised end-to-end speech recognition for low-resource languages using CycleGAN and inter-domain losses, focusing on ASR performance; however, it does not mention the use or generation of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process central to the theme of discrete audio tokens, nor does it provide details on discrete token vocabularies or quantization setups.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
    "abstract": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
    "metadata": {
      "arxiv_id": "2408.00284",
      "title": "Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation",
      "summary": "Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \\url{https://c9412600.github.io/bltts_tech_report/index.html}.",
      "authors": [
        "Xinhan Di",
        "Zihao Chen",
        "Yunming Liang",
        "Junjie Zheng",
        "Yihua Wang",
        "Chaofan Ding"
      ],
      "published": "2024-08-01T04:57:31Z",
      "updated": "2024-08-01T04:57:31Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00284v1",
      "landing_url": "https://arxiv.org/abs/2408.00284v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.00284"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on TTS for Chinese dialectal speech with alignment between text and speech tokens but does not describe discrete audio tokens, tokenizers/codecs, or analyze discrete token designs as core representations; it thus does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item focuses on TTS for Chinese dialectal speech with alignment between text and speech tokens but does not describe discrete audio tokens, tokenizers/codecs, or analyze discrete token designs as core representations; it thus does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a TTS model for Chinese dialectal speech synthesis but does not mention the use of discrete audio token representations or neural audio codecs for discretization, nor does it specify any token design or quantization details; thus, it lacks clear evidence of meeting the core inclusion criteria centered on discrete audio tokens for generation or modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a TTS model for Chinese dialectal speech synthesis but does not mention the use of discrete audio token representations or neural audio codecs for discretization, nor does it specify any token design or quantization details; thus, it lacks clear evidence of meeting the core inclusion criteria centered on discrete audio tokens for generation or modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
    "abstract": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
    "metadata": {
      "arxiv_id": "2408.01391",
      "title": "FT K-means: A High-Performance K-means on GPU with Fault Tolerance",
      "summary": "K-means is a widely used algorithm in clustering, however, its efficiency is primarily constrained by the computational cost of distance computing. Existing implementations suffer from suboptimal utilization of computational units and lack resilience against soft errors. To address these challenges, we introduce FT K-means, a high-performance GPU-accelerated implementation of K-means with online fault tolerance. We first present a stepwise optimization strategy that achieves competitive performance compared to NVIDIA's cuML library. We further improve FT K-means with a template-based code generation framework that supports different data types and adapts to different input shapes. A novel warp-level tensor-core error correction scheme is proposed to address the failure of existing fault tolerance methods due to memory asynchronization during copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100 GPU demonstrate that FT K-means without fault tolerance outperforms cuML's K-means implementation, showing a performance increase of 10\\%-300\\% in scenarios involving irregular data shapes. Moreover, the fault tolerance feature of FT K-means introduces only an overhead of 11\\%, maintaining robust performance even with tens of errors injected per second.",
      "authors": [
        "Shixun Wu",
        "Yitong Ding",
        "Yujia Zhai",
        "Jinyang Liu",
        "Jiajun Huang",
        "Zizhe Jian",
        "Huangliang Dai",
        "Sheng Di",
        "Bryan M. Wong",
        "Zizhong Chen",
        "Franck Cappello"
      ],
      "published": "2024-08-02T17:01:36Z",
      "updated": "2024-08-07T21:55:08Z",
      "categories": [
        "cs.DC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01391v2",
      "landing_url": "https://arxiv.org/abs/2408.01391v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.01391"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses high-performance GPU K-means with fault tolerance, unrelated to discrete audio tokens or tokenization/discretization of audio data, so it fails the inclusion criteria and meets exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses high-performance GPU K-means with fault tolerance, unrelated to discrete audio tokens or tokenization/discretization of audio data, so it fails the inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a GPU-accelerated K-means clustering implementation with fault tolerance but does not mention any discrete audio token generation, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are essential for inclusion. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different research topic unrelated to audio tokenization or audio generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a GPU-accelerated K-means clustering implementation with fault tolerance but does not mention any discrete audio token generation, neural audio codecs, or discretization of continuous audio waveforms into token sequences, which are essential for inclusion. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different research topic unrelated to audio tokenization or audio generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
    "abstract": "In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We take style images from the WikiArt dataset and content images from the COCO dataset. Experimental results demonstrate that D$^2$Styler produces high-quality style-transferred images and outperforms twelve existing methods on nearly all the metrics. The qualitative results and ablation studies provide further insights into the efficacy of our technique. The code is available at https://github.com/Onkarsus13/D2Styler.",
    "metadata": {
      "arxiv_id": "2408.03558",
      "title": "D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion Methods",
      "summary": "In image processing, one of the most challenging tasks is to render an image's semantic meaning using a variety of artistic approaches. Existing techniques for arbitrary style transfer (AST) frequently experience mode-collapse, over-stylization, or under-stylization due to a disparity between the style and content images. We propose a novel framework called D$^2$Styler (Discrete Diffusion Styler) that leverages the discrete representational capability of VQ-GANs and the advantages of discrete diffusion, including stable training and avoidance of mode collapse. Our method uses Adaptive Instance Normalization (AdaIN) features as a context guide for the reverse diffusion process. This makes it easy to move features from the style image to the content image without bias. The proposed method substantially enhances the visual quality of style-transferred images, allowing the combination of content and style in a visually appealing manner. We take style images from the WikiArt dataset and content images from the COCO dataset. Experimental results demonstrate that D$^2$Styler produces high-quality style-transferred images and outperforms twelve existing methods on nearly all the metrics. The qualitative results and ablation studies provide further insights into the efficacy of our technique. The code is available at https://github.com/Onkarsus13/D2Styler.",
      "authors": [
        "Onkar Susladkar",
        "Gayatri Deshmukh",
        "Sparsh Mittal",
        "Parth Shastri"
      ],
      "published": "2024-08-07T05:47:06Z",
      "updated": "2024-08-07T05:47:06Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03558v1",
      "landing_url": "https://arxiv.org/abs/2408.03558v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.03558"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses image style transfer using discrete diffusion with VQ-GAN latent tokens, not discrete audio tokens or audio generation/understanding tasks, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses image style transfer using discrete diffusion with VQ-GAN latent tokens, not discrete audio tokens or audio generation/understanding tasks, so it does not meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on image style transfer using discrete diffusion and VQ-GANs for images, not on discretizing continuous audio waveforms into token sequences for audio modeling, generation, or cross-modal tasks as per the inclusion criteria. It does not discuss discrete audio tokens, neural audio codecs, or any audio-related tokenization, thus failing the core thematic requirement for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on image style transfer using discrete diffusion and VQ-GANs for images, not on discretizing continuous audio waveforms into token sequences for audio modeling, generation, or cross-modal tasks as per the inclusion criteria. It does not discuss discrete audio tokens, neural audio codecs, or any audio-related tokenization, thus failing the core thematic requirement for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
    "abstract": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
    "metadata": {
      "arxiv_id": "2408.03887",
      "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
      "summary": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
      "authors": [
        "Hawraz A. Ahmad",
        "Tarik A. Rashid"
      ],
      "published": "2024-08-06T07:04:59Z",
      "updated": "2024-08-06T07:04:59Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03887v1",
      "landing_url": "https://arxiv.org/abs/2408.03887v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.03887"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on an end-to-end Kurdish TTS system using VAE/adversarial training, with latent variables, but there is no discussion of discrete audio tokens, tokenizers/codec architectures, or token-based representations, so it does not meet the core inclusion criterion for discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on an end-to-end Kurdish TTS system using VAE/adversarial training, with latent variables, but there is no discussion of discrete audio tokens, tokenizers/codec architectures, or token-based representations, so it does not meet the core inclusion criterion for discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on an end-to-end TTS model for generating Kurdish speech using variational autoencoder and adversarial training, but it does not explicitly mention the usage or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process producing token sequences. Consequently, it lacks explicit details on discrete token generation or vocabulary/codebook settings, which are essential for inclusion. Therefore, it mainly deals with continuous audio waveform modeling rather than discrete audio token-based modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on an end-to-end TTS model for generating Kurdish speech using variational autoencoder and adversarial training, but it does not explicitly mention the usage or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process producing token sequences. Consequently, it lacks explicit details on discrete token generation or vocabulary/codebook settings, which are essential for inclusion. Therefore, it mainly deals with continuous audio waveform modeling rather than discrete audio token-based modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
    "abstract": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
    "metadata": {
      "arxiv_id": "2408.04205",
      "title": "High-Efficiency Urban 3D Radio Map Estimation Based on Sparse Measurements",
      "summary": "Recent widespread applications for unmanned aerial vehicles (UAVs) -- from infrastructure inspection to urban logistics -- have prompted an urgent need for high-accuracy three-dimensional (3D) radio maps. However, existing methods designed for two-dimensional radio maps face challenges of high measurement costs and limited data availability when extended to 3D scenarios. To tackle these challenges, we first build a real-world large-scale 3D radio map dataset, covering over 4.2 million m^3 and over 4 thousand data points in complex urban environments. We propose a Gaussian Process Regression-based scheme for 3D radio map estimation, allowing us to realize more accurate map recovery with a lower RMSE than state-of-the-art schemes by over 2.5 dB. To further enhance data efficiency, we propose two methods for training point selection, including an offline clustering-based method and an online maximum a posterior (MAP)-based method. Extensive experiments demonstrate that the proposed scheme not only achieves full-map recovery with only 2% of UAV measurements, but also sheds light on future studies on 3D radio maps.",
      "authors": [
        "Xinwei Chen",
        "Xiaofeng Zhong",
        "Zijian Zhang",
        "Linglong Dai",
        "Shidong Zhou"
      ],
      "published": "2024-08-08T04:05:18Z",
      "updated": "2024-08-08T04:05:18Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04205v1",
      "landing_url": "https://arxiv.org/abs/2408.04205v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04205"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article concerns 3D radio map estimation for UAVs and data-efficient methods, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article concerns 3D radio map estimation for UAVs and data-efficient methods, with no discussion of discrete audio tokens or tokenization, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on 3D radio map estimation for UAV applications, involving Gaussian Process Regression and measurement strategies; it does not discuss discretization of audio waveforms into discrete audio tokens or use discrete audio tokens in modeling, generation, alignment, or cross-modal reasoning, nor does it elaborate on tokenizer architectures, quantization, or lexicon settings, which are core to the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on 3D radio map estimation for UAV applications, involving Gaussian Process Regression and measurement strategies; it does not discuss discretization of audio waveforms into discrete audio tokens or use discrete audio tokens in modeling, generation, alignment, or cross-modal reasoning, nor does it elaborate on tokenizer architectures, quantization, or lexicon settings, which are core to the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
    "abstract": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
    "metadata": {
      "arxiv_id": "2408.04505",
      "title": "Feedback Design with VQ-VAE for Robust Precoding in Multi-User FDD Systems",
      "summary": "In this letter, we propose a vector quantized-variational autoencoder (VQ-VAE)-based feedback scheme for robust precoder design in multi-user frequency division duplex (FDD) systems. We demonstrate how the VQ-VAE can be tailored to specific propagation environments, focusing on systems with low pilot overhead, which is crucial in massive multiple-input multiple-output (MIMO). Extensive simulations with real-world measurement data show that our proposed feedback scheme outperforms state-of-the-art autoencoder (AE)-based compression schemes and conventional Discrete Fourier transform (DFT) codebook-based schemes. These improvements enable the deployment of systems with fewer feedback bits or pilots.",
      "authors": [
        "Nurettin Turan",
        "Michael Baur",
        "Jianqing Li",
        "Wolfgang Utschick"
      ],
      "published": "2024-08-08T15:03:45Z",
      "updated": "2024-08-08T15:03:45Z",
      "categories": [
        "cs.IT",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04505v1",
      "landing_url": "https://arxiv.org/abs/2408.04505v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04505"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on VQ-VAE for robust precoding in multi-user FDD MIMO systems and does not discuss discrete audio tokens or audio tokenization, thus fails the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on VQ-VAE for robust precoding in multi-user FDD MIMO systems and does not discuss discrete audio tokens or audio tokenization, thus fails the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a VQ-VAE-based feedback scheme utilizing vector quantization, which is a form of discrete token representation, but its application is in robust precoding for multi-user FDD communication systems rather than audio waveform discretization for audio generation, understanding, or cross-modal modeling. It lacks any focus on audio tokens, audio waveform discretization, or related tasks like TTS, music generation, or audio-text/image alignment as outlined in the inclusion criteria. Therefore, it does not meet the core subject requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a VQ-VAE-based feedback scheme utilizing vector quantization, which is a form of discrete token representation, but its application is in robust precoding for multi-user FDD communication systems rather than audio waveform discretization for audio generation, understanding, or cross-modal modeling. It lacks any focus on audio tokens, audio waveform discretization, or related tasks like TTS, music generation, or audio-text/image alignment as outlined in the inclusion criteria. Therefore, it does not meet the core subject requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency",
    "abstract": "Voice conversion aims to modify the source speaker's voice to resemble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take inspiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).",
    "metadata": {
      "arxiv_id": "2408.04708",
      "title": "MulliVC: Multi-lingual Voice Conversion With Cycle Consistency",
      "summary": "Voice conversion aims to modify the source speaker's voice to resemble the target speaker while preserving the original speech content. Despite notable advancements in voice conversion these days, multi-lingual voice conversion (including both monolingual and cross-lingual scenarios) has yet to be extensively studied. It faces two main challenges: 1) the considerable variability in prosody and articulation habits across languages; and 2) the rarity of paired multi-lingual datasets from the same speaker. In this paper, we propose MulliVC, a novel voice conversion system that only converts timbre and keeps original content and source language prosody without multi-lingual paired data. Specifically, each training step of MulliVC contains three substeps: In step one the model is trained with monolingual speech data; then, steps two and three take inspiration from back translation, construct a cyclical process to disentangle the timbre and other information (content, prosody, and other language-related information) in the absence of multi-lingual data from the same speaker. Both objective and subjective results indicate that MulliVC significantly surpasses other methods in both monolingual and cross-lingual contexts, demonstrating the system's efficacy and the viability of the three-step approach with cycle consistency. Audio samples can be found on our demo page (mullivc.github.io).",
      "authors": [
        "Jiawei Huang",
        "Chen Zhang",
        "Yi Ren",
        "Ziyue Jiang",
        "Zhenhui Ye",
        "Jinglin Liu",
        "Jinzheng He",
        "Xiang Yin",
        "Zhou Zhao"
      ],
      "published": "2024-08-08T18:12:51Z",
      "updated": "2024-08-08T18:12:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.04708v1",
      "landing_url": "https://arxiv.org/abs/2408.04708v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.04708"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on multilingual voice conversion with cycle consistency and timbre-content disentanglement, and does not discuss discrete audio tokens, tokenization methods, vocabularies, or tokenizer-based downstream tasks; thus it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on multilingual voice conversion with cycle consistency and timbre-content disentanglement, and does not discuss discrete audio tokens, tokenization methods, vocabularies, or tokenizer-based downstream tasks; thus it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multi-lingual voice conversion by converting timbre while preserving content and prosody, but it does not mention any discretization of audio into discrete tokens, vector quantization, or tokenizers, nor does it discuss neural audio codec architectures or the generation of discrete token sequences essential for modeling audio as text-like sequences. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and mainly deals with continuous voice conversion techniques.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multi-lingual voice conversion by converting timbre while preserving content and prosody, but it does not mention any discretization of audio into discrete tokens, vector quantization, or tokenizers, nor does it discuss neural audio codec architectures or the generation of discrete token sequences essential for modeling audio as text-like sequences. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and mainly deals with continuous voice conversion techniques.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
    "abstract": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
    "metadata": {
      "arxiv_id": "2408.09483",
      "title": "CMD: A Cache-assisted GPU Memory Deduplication Architecture",
      "summary": "Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.",
      "authors": [
        "Wei Zhao",
        "Dan Feng",
        "Wei Tong",
        "Xueliang Wei",
        "Bing Wu"
      ],
      "published": "2024-08-18T13:54:46Z",
      "updated": "2024-08-18T13:54:46Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09483v1",
      "landing_url": "https://arxiv.org/abs/2408.09483v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09483"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses CMD, a cache-assisted GPU memory deduplication architecture, which is unrelated to discrete audio tokens, tokenization, or audio-centric modeling as required by the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses CMD, a cache-assisted GPU memory deduplication architecture, which is unrelated to discrete audio tokens, tokenization, or audio-centric modeling as required by the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on GPU memory deduplication techniques to improve performance and energy efficiency, with no mention of discrete audio tokens, neural audio codecs, tokenization, or any audio-related modeling or generation tasks. It does not satisfy any inclusion criteria related to discrete audio tokens and clearly falls into irrelevant research territory focused on GPU hardware architecture rather than audio signal processing or tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on GPU memory deduplication techniques to improve performance and energy efficiency, with no mention of discrete audio tokens, neural audio codecs, tokenization, or any audio-related modeling or generation tasks. It does not satisfy any inclusion criteria related to discrete audio tokens and clearly falls into irrelevant research territory focused on GPU hardware architecture rather than audio signal processing or tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hear Your Face: Face-based voice conversion with F0 estimation",
    "abstract": "This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency.",
    "metadata": {
      "arxiv_id": "2408.09802",
      "title": "Hear Your Face: Face-based voice conversion with F0 estimation",
      "summary": "This paper delves into the emerging field of face-based voice conversion, leveraging the unique relationship between an individual's facial features and their vocal characteristics. We present a novel face-based voice conversion framework that particularly utilizes the average fundamental frequency of the target speaker, derived solely from their facial images. Through extensive analysis, our framework demonstrates superior speech generation quality and the ability to align facial features with voice characteristics, including tracking of the target speaker's fundamental frequency.",
      "authors": [
        "Jaejun Lee",
        "Yoori Oh",
        "Injune Hwang",
        "Kyogu Lee"
      ],
      "published": "2024-08-19T08:47:03Z",
      "updated": "2024-08-19T08:47:03Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09802v1",
      "landing_url": "https://arxiv.org/abs/2408.09802v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09802"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study does not address discrete audio tokens or token-based discretization; it focuses on face-based voice conversion using F0 estimation from facial images, which falls outside the inclusion scope for token-based audio representation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study does not address discrete audio tokens or token-based discretization; it focuses on face-based voice conversion using F0 estimation from facial images, which falls outside the inclusion scope for token-based audio representation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on face-based voice conversion using average fundamental frequency estimation from facial images but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization method to represent audio as token sequences, which are the core aspects of the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on face-based voice conversion using average fundamental frequency estimation from facial images but does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization method to represent audio as token sequences, which are the core aspects of the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
    "abstract": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
    "metadata": {
      "arxiv_id": "2408.11842",
      "title": "Non-Causal to Causal SSL-Supported Transfer Learning: Towards a High-Performance Low-Latency Speech Vocoder",
      "summary": "Recently, BigVGAN has emerged as high-performance speech vocoder. Its sequence-to-sequence-based synthesis, however, prohibits usage in low-latency conversational applications. Our work addresses this shortcoming in three steps. First, we introduce low latency into BigVGAN via implementing causal convolutions, yielding decreased performance. Second, to regain performance, we propose a teacher-student transfer learning scheme to distill the high-delay non-causal BigVGAN into our low-latency causal vocoder. Third, taking advantage of a self-supervised learning (SSL) model, in our case wav2vec 2.0, we align its encoder speech representations extracted from our low-latency causal vocoder to the ground truth ones. In speaker-independent settings, both proposed training schemes notably elevate the performance of our low-latency vocoder, closing up to the original high-delay BigVGAN. At only 21% higher complexity, our best small causal vocoder achieves 3.96 PESQ and 1.25 MCD, excelling even the original small non-causal BigVGAN (3.64 PESQ) by 0.32 PESQ and 0.1 MCD points, respectively.",
      "authors": [
        "Renzheng Shi",
        "Andreas Bär",
        "Marvin Sach",
        "Wouter Tirry",
        "Tim Fingscheidt"
      ],
      "published": "2024-08-07T12:49:40Z",
      "updated": "2024-08-26T12:01:07Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11842v2",
      "landing_url": "https://arxiv.org/abs/2408.11842v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.11842"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on low-latency causal vocoding and SSL alignment without emitting or analyzing discrete audio tokens/quantization, thus it does not meet the 'Discrete Audio Tokens' scope.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on low-latency causal vocoding and SSL alignment without emitting or analyzing discrete audio tokens/quantization, thus it does not meet the 'Discrete Audio Tokens' scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on improving a speech vocoder's architecture for low-latency applications using transfer learning and self-supervised learning, but it does not describe or use discrete audio tokens derived via neural audio codecs or quantization-based tokenization schemes needed to meet the inclusion criteria. There is no mention of discretization processes, token vocabularies, or token-based sequence modeling, which are core to the defined scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on improving a speech vocoder's architecture for low-latency applications using transfer learning and self-supervised learning, but it does not describe or use discrete audio tokens derived via neural audio codecs or quantization-based tokenization schemes needed to meet the inclusion criteria. There is no mention of discretization processes, token vocabularies, or token-based sequence modeling, which are core to the defined scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Convexity-based Pruning of Speech Representation Models",
    "abstract": "Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.",
    "metadata": {
      "arxiv_id": "2408.11858",
      "title": "Convexity-based Pruning of Speech Representation Models",
      "summary": "Speech representation models based on the transformer architecture and trained by self-supervised learning have shown great promise for solving tasks such as speech and speaker recognition, keyword spotting, emotion detection, and more. Typically, it is found that larger models lead to better performance. However, the significant computational effort involved in such large transformer systems is a challenge for embedded and real-world applications. Recent work has shown that there is significant redundancy in the transformer models for NLP and massive layer pruning is feasible (Sajjad et al., 2023). Here, we investigate layer pruning in audio models. We base the pruning decision on a convexity criterion. Convexity of classification regions has recently been proposed as an indicator of subsequent fine-tuning performance in a range of application domains, including NLP and audio. In empirical investigations, we find a massive reduction in the computational effort with no loss of performance or even improvements in certain cases.",
      "authors": [
        "Teresa Dorszewski",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-08-16T09:04:54Z",
      "updated": "2024-08-16T09:04:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.11858v1",
      "landing_url": "https://arxiv.org/abs/2408.11858v1",
      "doi": "https://doi.org/10.1109/MLSP58920.2024.10734716"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article discusses convexity-based pruning of speech representation models and layer pruning for computational efficiency, with no discussion of discrete audio token generation, tokenization architectures, or token-based downstream tasks, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article discusses convexity-based pruning of speech representation models and layer pruning for computational efficiency, with no discussion of discrete audio token generation, tokenization architectures, or token-based downstream tasks, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on pruning transformer-based speech representation models to reduce computational effort, but it does not discuss discrete audio tokens derived from neural audio codecs or self-supervised tokenization (e.g., vector quantization), nor does it address discrete token design, generation, or usage for audio generation, understanding, or cross-modal modeling, which is central to the inclusion criteria. Hence, it lacks the core focus on discrete token sequences and their systematic evaluation relevant to the given topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on pruning transformer-based speech representation models to reduce computational effort, but it does not discuss discrete audio tokens derived from neural audio codecs or self-supervised tokenization (e.g., vector quantization), nor does it address discrete token design, generation, or usage for audio generation, understanding, or cross-modal modeling, which is central to the inclusion criteria. Hence, it lacks the core focus on discrete token sequences and their systematic evaluation relevant to the given topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
    "abstract": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
    "metadata": {
      "arxiv_id": "2408.13893",
      "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models",
      "summary": "Scaling Text-to-speech (TTS) to large-scale datasets has been demonstrated as an effective method for improving the diversity and naturalness of synthesized speech. At the high level, previous large-scale TTS models can be categorized into either Auto-regressive (AR) based (\\textit{e.g.}, VALL-E) or Non-auto-regressive (NAR) based models (\\textit{e.g.}, NaturalSpeech 2/3). Although these works demonstrate good performance, they still have potential weaknesses. For instance, AR-based models are plagued by unstable generation quality and slow generation speed; meanwhile, some NAR-based models need phoneme-level duration alignment information, thereby increasing the complexity of data pre-processing, model design, and loss design. In this work, we build upon our previous publication by implementing a simple and efficient non-autoregressive (NAR) TTS framework, termed SimpleSpeech 2. SimpleSpeech 2 effectively combines the strengths of both autoregressive (AR) and non-autoregressive (NAR) methods, offering the following key advantages: (1) simplified data preparation; (2) straightforward model and loss design; and (3) stable, high-quality generation performance with fast inference speed. Compared to our previous publication, we present ({\\romannumeral1}) a detailed analysis of the influence of speech tokenizer and noisy label for TTS performance; ({\\romannumeral2}) four distinct types of sentence duration predictors; ({\\romannumeral3}) a novel flow-based scalar latent transformer diffusion model. With these improvement, we show a significant improvement in generation performance and generation speed compared to our previous work and other state-of-the-art (SOTA) large-scale TTS models. Furthermore, we show that SimpleSpeech 2 can be seamlessly extended to multilingual TTS by training it on multilingual speech datasets. Demos are available on: {https://dongchaoyang.top/SimpleSpeech2\\_demo/}.",
      "authors": [
        "Dongchao Yang",
        "Rongjie Huang",
        "Yuanyuan Wang",
        "Haohan Guo",
        "Dading Chong",
        "Songxiang Liu",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2024-08-25T17:07:39Z",
      "updated": "2024-08-28T07:16:37Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.13893v2",
      "landing_url": "https://arxiv.org/abs/2408.13893v2",
      "doi": "https://doi.org/10.48550/arXiv.2408.13893"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on a non-autoregressive TTS framework with diffusion models and discusses speech tokenizer analysis, but it does not present or evaluate a discrete audio token generation/representation (tokenizer/codec/quantization) as a core method, so it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper centers on a non-autoregressive TTS framework with diffusion models and discusses speech tokenizer analysis, but it does not present or evaluate a discrete audio token generation/representation (tokenizer/codec/quantization) as a core method, so it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a TTS model (SimpleSpeech 2) focusing on efficient text-to-speech synthesis, describing innovations in model design and performance, but it does not explicitly mention the use or design of discrete audio tokens generated via neural audio codecs or self-supervised discrete units as core components. There is no clear detail on tokenization, codebook configurations, or discrete token-based modeling integral to the methods or analysis, which is required for inclusion. Hence, it does not meet the core inclusion criteria focused on discrete audio token techniques.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study presents a TTS model (SimpleSpeech 2) focusing on efficient text-to-speech synthesis, describing innovations in model design and performance, but it does not explicitly mention the use or design of discrete audio tokens generated via neural audio codecs or self-supervised discrete units as core components. There is no clear detail on tokenization, codebook configurations, or discrete token-based modeling integral to the methods or analysis, which is required for inclusion. Hence, it does not meet the core inclusion criteria focused on discrete audio token techniques.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
    "abstract": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
    "metadata": {
      "arxiv_id": "2408.15616",
      "title": "Beyond Levenshtein: Leveraging Multiple Algorithms for Robust Word Error Rate Computations And Granular Error Classifications",
      "summary": "The Word Error Rate (WER) is the common measure of accuracy for Automatic Speech Recognition (ASR). Transcripts are usually pre-processed by substituting specific characters to account for non-semantic differences. As a result of this normalisation, information on the accuracy of punctuation or capitalisation is lost. We present a non-destructive, token-based approach using an extended Levenshtein distance algorithm to compute a robust WER and additional orthographic metrics. Transcription errors are also classified more granularly by existing string similarity and phonetic algorithms. An evaluation on several datasets demonstrates the practical equivalence of our approach compared to common WER computations. We also provide an exemplary analysis of derived use cases, such as a punctuation error rate, and a web application for interactive use and visualisation of our implementation. The code is available open-source.",
      "authors": [
        "Korbinian Kuhn",
        "Verena Kersken",
        "Gottfried Zimmermann"
      ],
      "published": "2024-08-28T08:14:51Z",
      "updated": "2024-08-28T08:14:51Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15616v1",
      "landing_url": "https://arxiv.org/abs/2408.15616v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-32"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe text-based tokenization and WER computation rather than discrete audio tokens, tokenizers, or multi-codebook representations of audio required by the inclusion criteria, so it does not meet the topic and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe text-based tokenization and WER computation rather than discrete audio tokens, tokenizers, or multi-codebook representations of audio required by the inclusion criteria, so it does not meet the topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on evaluating word error rate computations for ASR transcripts using extended Levenshtein distance and various similarity algorithms, but it does not discuss or involve the generation or use of discrete audio tokens produced by neural audio codecs or self-supervised models, nor does it cover token design, quantization, or token-based audio generation and understanding as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on evaluating word error rate computations for ASR transcripts using extended Levenshtein distance and various similarity algorithms, but it does not discuss or involve the generation or use of discrete audio tokens produced by neural audio codecs or self-supervised models, nor does it cover token design, quantization, or token-based audio generation and understanding as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
    "abstract": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
    "metadata": {
      "arxiv_id": "2408.15676",
      "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
      "summary": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
      "authors": [
        "Yixuan Zhou",
        "Xiaoyu Qin",
        "Zeyu Jin",
        "Shuoyi Zhou",
        "Shun Lei",
        "Songtao Zhou",
        "Zhiyong Wu",
        "Jia Jia"
      ],
      "published": "2024-08-28T09:57:17Z",
      "updated": "2024-08-28T09:57:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15676v1",
      "landing_url": "https://arxiv.org/abs/2408.15676v1",
      "doi": "https://doi.org/10.1145/3664647.3681680"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions 'speech semantic tokens' as an intermediate representation and a codec-based framework, but it does not clearly specify discrete token generation, tokenizer/codec architecture, codebooks, or an explicit evaluation of discrete audio tokens, leaving it unclear whether it fulfills the discrete-audio-token criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions 'speech semantic tokens' as an intermediate representation and a codec-based framework, but it does not clearly specify discrete token generation, tokenizer/codec architecture, codebooks, or an explicit evaluation of discrete audio tokens, leaving it unclear whether it fulfills the discrete-audio-token criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes VoxInstruct, a human instruction-to-speech generation model using a unified multilingual codec language modeling framework; it introduces speech semantic tokens as intermediate representations derived from neural audio codecs to guide speech synthesis, aligning with the inclusion criteria focusing on discrete audio tokens produced from neural audio codecs for generation and multimodal modeling. The abstract explicitly mentions the use of speech semantic tokens as intermediate representations and codec language modeling, indicating discrete tokenization of audio content supporting AIGC applications, satisfying the inclusion criteria, and no exclusion criteria are met as the work is not on continuous audio modeling alone nor is it a discriminative task or lacking English details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes VoxInstruct, a human instruction-to-speech generation model using a unified multilingual codec language modeling framework; it introduces speech semantic tokens as intermediate representations derived from neural audio codecs to guide speech synthesis, aligning with the inclusion criteria focusing on discrete audio tokens produced from neural audio codecs for generation and multimodal modeling. The abstract explicitly mentions the use of speech semantic tokens as intermediate representations and codec language modeling, indicating discrete tokenization of audio content supporting AIGC applications, satisfying the inclusion criteria, and no exclusion criteria are met as the work is not on continuous audio modeling alone nor is it a discriminative task or lacking English details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' comments were considered: one flagged uncertainty due to absence of tokenization/codebook details in the abstract, while the other noted the explicit mention of “speech semantic tokens” and a “codec language modeling” framework (and release of code) which strongly indicates use of discrete audio tokens from a neural codec. The paper therefore likely fits the “Discrete Audio Tokens” topic (uses codec-derived semantic tokens for instruction-to-speech generation and codec-language modeling), but the abstract lacks explicit method-level details (tokenizer/quantization/codebook/rate) required for full reproducibility, so inclusion is warranted but not certain without the full paper. 1-sentence justification: The work explicitly claims speech semantic tokens and a codec language model (meeting the core inclusion signal), but because the abstract omits tokenization/codebook specifics I rate it as better to include rather than absolutely include.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' comments were considered: one flagged uncertainty due to absence of tokenization/codebook details in the abstract, while the other noted the explicit mention of “speech semantic tokens” and a “codec language modeling” framework (and release of code) which strongly indicates use of discrete audio tokens from a neural codec. The paper therefore likely fits the “Discrete Audio Tokens” topic (uses codec-derived semantic tokens for instruction-to-speech generation and codec-language modeling), but the abstract lacks explicit method-level details (tokenizer/quantization/codebook/rate) required for full reproducibility, so inclusion is warranted but not certain without the full paper. 1-sentence justification: The work explicitly claims speech semantic tokens and a codec language model (meeting the core inclusion signal), but because the abstract omits tokenization/codebook specifics I rate it as better to include rather than absolutely include.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
    "abstract": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
    "metadata": {
      "arxiv_id": "2408.16373",
      "title": "Enabling Beam Search for Language Model-Based Text-to-Speech Synthesis",
      "summary": "Tokenising continuous speech into sequences of discrete tokens and modelling them with language models (LMs) has led to significant success in text-to-speech (TTS) synthesis. Although these models can generate speech with high quality and naturalness, their synthesised samples can still suffer from artefacts, mispronunciation, word repeating, etc. In this paper, we argue these undesirable properties could partly be caused by the randomness of sampling-based strategies during the autoregressive decoding of LMs. Therefore, we look at maximisation-based decoding approaches and propose Temporal Repetition Aware Diverse Beam Search (TRAD-BS) to find the most probable sequences of the generated speech tokens. Experiments with two state-of-the-art LM-based TTS models demonstrate that our proposed maximisation-based decoding strategy generates speech with fewer mispronunciations and improved speaker consistency.",
      "authors": [
        "Zehai Tu",
        "Guangyan Zhang",
        "Yiting Lu",
        "Adaeze Adigwe",
        "Simon King",
        "Yiwen Guo"
      ],
      "published": "2024-08-29T09:31:06Z",
      "updated": "2024-08-29T09:31:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16373v1",
      "landing_url": "https://arxiv.org/abs/2408.16373v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16373"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses tokenising speech into discrete tokens and modeling with language models for TTS, plus a decoding strategy to improve token-based generation, aligning with the discrete audio token criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses tokenising speech into discrete tokens and modeling with language models for TTS, plus a decoding strategy to improve token-based generation, aligning with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using language model-based text-to-speech synthesis, involving tokenizing continuous speech into discrete tokens, and employs these tokens for generation and beam search decoding; however, it does not specify the detailed process of discrete token generation (e.g., tokenizer architecture, quantization details, vocabulary/codebook settings) that would allow verification of its alignment with the discrete audio token criteria, nor does it discuss token design impacts or evaluations explicitly, making it unclear if the tokens fit the defined discrete audio tokens topic as per the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using language model-based text-to-speech synthesis, involving tokenizing continuous speech into discrete tokens, and employs these tokens for generation and beam search decoding; however, it does not specify the detailed process of discrete token generation (e.g., tokenizer architecture, quantization details, vocabulary/codebook settings) that would allow verification of its alignment with the discrete audio token criteria, nor does it discuss token design impacts or evaluations explicitly, making it unclear if the tokens fit the defined discrete audio tokens topic as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers' notes were considered: JuniorNano recommends inclusion because the abstract explicitly describes tokenising continuous speech into discrete tokens and modelling them with language models for TTS, while JuniorMini flagged uncertainty due to lack of tokenizer/quantization/codebook details in the abstract. The paper clearly centers on discrete token sequences for LM-based audio generation and evaluates downstream TTS behavior (fits the inclusion group that uses discrete audio tokens for generation), but the abstract does not specify how the tokens are produced or provide reproducibility details required by the stricter inclusion subcriteria—so it should likely be included for full-text review but not marked as an unambiguous include from the abstract alone.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers' notes were considered: JuniorNano recommends inclusion because the abstract explicitly describes tokenising continuous speech into discrete tokens and modelling them with language models for TTS, while JuniorMini flagged uncertainty due to lack of tokenizer/quantization/codebook details in the abstract. The paper clearly centers on discrete token sequences for LM-based audio generation and evaluates downstream TTS behavior (fits the inclusion group that uses discrete audio tokens for generation), but the abstract does not specify how the tokens are produced or provide reproducibility details required by the stricter inclusion subcriteria—so it should likely be included for full-text review but not marked as an unambiguous include from the abstract alone.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates",
    "abstract": "Voice conversion has gained increasing popularity within the field of audio manipulation and speech synthesis. Often, the main objective is to transfer the input identity to that of a target speaker without changing its linguistic content. While current work provides high-fidelity solutions they rarely focus on model simplicity, high-sampling rate environments or stream-ability. By incorporating speech representation learning into a generative timbre transfer model, traditionally created for musical purposes, we investigate the realm of voice conversion generated directly in the time domain at high sampling rates. More specifically, we guide the latent space of a baseline model towards linguistically relevant representations and condition it on external speaker information. Through objective and subjective assessments, we demonstrate that the proposed solution can attain levels of naturalness, quality, and intelligibility comparable to those of a state-of-the-art solution for seen speakers, while significantly decreasing inference time. However, despite the presence of target speaker characteristics in the converted output, the actual similarity to unseen speakers remains a challenge.",
    "metadata": {
      "arxiv_id": "2408.16546",
      "title": "RAVE for Speech: Efficient Voice Conversion at High Sampling Rates",
      "summary": "Voice conversion has gained increasing popularity within the field of audio manipulation and speech synthesis. Often, the main objective is to transfer the input identity to that of a target speaker without changing its linguistic content. While current work provides high-fidelity solutions they rarely focus on model simplicity, high-sampling rate environments or stream-ability. By incorporating speech representation learning into a generative timbre transfer model, traditionally created for musical purposes, we investigate the realm of voice conversion generated directly in the time domain at high sampling rates. More specifically, we guide the latent space of a baseline model towards linguistically relevant representations and condition it on external speaker information. Through objective and subjective assessments, we demonstrate that the proposed solution can attain levels of naturalness, quality, and intelligibility comparable to those of a state-of-the-art solution for seen speakers, while significantly decreasing inference time. However, despite the presence of target speaker characteristics in the converted output, the actual similarity to unseen speakers remains a challenge.",
      "authors": [
        "Anders R. Bargum",
        "Simon Lajboschitz",
        "Cumhur Erkut"
      ],
      "published": "2024-08-29T14:09:37Z",
      "updated": "2024-08-29T14:09:37Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.16546v1",
      "landing_url": "https://arxiv.org/abs/2408.16546v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.16546"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes time-domain, high-sampling-rate voice conversion using latent representations and speaker conditioning, but there is no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes time-domain, high-sampling-rate voice conversion using latent representations and speaker conditioning, but there is no mention of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using a generative timbre transfer model but does not mention the use of discrete audio tokens through discretization into token sequences or neural audio codec/vector quantization techniques, which are core to the inclusion criteria. It emphasizes continuous time-domain generation rather than discrete token modeling, and no details on tokenizers, codebooks, or discrete token design are provided, thus failing to meet the criteria for inclusion related to discrete audio token representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using a generative timbre transfer model but does not mention the use of discrete audio tokens through discretization into token sequences or neural audio codec/vector quantization techniques, which are core to the inclusion criteria. It emphasizes continuous time-domain generation rather than discrete token modeling, and no details on tokenizers, codebooks, or discrete token design are provided, thus failing to meet the criteria for inclusion related to discrete audio token representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
    "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
    "metadata": {
      "arxiv_id": "2408.17131",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Hong Gu",
        "Kedong Xu",
        "Kejie Huang"
      ],
      "published": "2024-08-30T09:15:54Z",
      "updated": "2024-08-30T09:15:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17131v1",
      "landing_url": "https://arxiv.org/abs/2408.17131v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is about post-training vector quantization of diffusion transformer models for image generation, focusing on compressing model weights, which concerns general DL model quantization rather than discrete audio tokens or token-based audio modeling; thus it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item is about post-training vector quantization of diffusion transformer models for image generation, focusing on compressing model weights, which concerns general DL model quantization rather than discrete audio tokens or token-based audio modeling; thus it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on vector quantization for Diffusion Transformer models applied to image generation, with no mention of discrete audio tokens derived from neural audio codecs or self-supervised audio models, nor does it address audio waveforms or discrete token sequences for audio tasks such as TTS, speech generation, or audio alignment; hence it does not meet the inclusion criteria focused on discrete audio tokens, and is more related to visual model compression.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on vector quantization for Diffusion Transformer models applied to image generation, with no mention of discrete audio tokens derived from neural audio codecs or self-supervised audio models, nor does it address audio waveforms or discrete token sequences for audio tasks such as TTS, speech generation, or audio alignment; hence it does not meet the inclusion criteria focused on discrete audio tokens, and is more related to visual model compression.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
    "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a neural audio codec with discrete tokens (RVQ) and a semantic encoder, proposes a tokenization-based approach (X-Codec), and evaluates on TTS and other downstream tasks, aligning with the discrete audio token criteria without falling into exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural audio codec with discrete tokens (RVQ) and a semantic encoder, proposes a tokenization-based approach (X-Codec), and evaluates on TTS and other downstream tasks, aligning with the discrete audio token criteria without falling into exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing neural audio codecs used for discretizing continuous audio waveforms into token sequences, specifically addressing semantic shortcomings in vector-quantized codecs for audio LLMs. It proposes a novel codec, X-Codec, integrating semantic features before residual vector quantization (RVQ) and evaluates the impact on speech and non-speech generation tasks, aligning well with the inclusion criteria around discrete audio tokens, codec design improvements, semantic and acoustic granularity, and application in downstream audio generation tasks with reproducible methodology and empirical results.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing neural audio codecs used for discretizing continuous audio waveforms into token sequences, specifically addressing semantic shortcomings in vector-quantized codecs for audio LLMs. It proposes a novel codec, X-Codec, integrating semantic features before residual vector quantization (RVQ) and evaluates the impact on speech and non-speech generation tasks, aligning well with the inclusion criteria around discrete audio tokens, codec design improvements, semantic and acoustic granularity, and application in downstream audio generation tasks with reproducible methodology and empirical results.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
    "abstract": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper MaskGCT for zero-shot TTS explicitly uses discrete tokens (semantic tokens and acoustic tokens) in a two-stage, mask-and-predict framework, aligning with the Discrete Audio Tokens topic; it provides token-level modeling details and is in English, supporting reproducibility, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper MaskGCT for zero-shot TTS explicitly uses discrete tokens (semantic tokens and acoustic tokens) in a two-stage, mask-and-predict framework, aligning with the Discrete Audio Tokens topic; it provides token-level modeling details and is in English, supporting reproducibility, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes MaskGCT, a text-to-speech system that predicts semantic and acoustic tokens derived from a speech self-supervised learning model, indicating a discrete token representation of audio. It clearly details a two-stage transformer-based approach handling masked semantic and acoustic tokens and provides evaluations with large-scale speech data, suggesting it fits the \"Discrete Audio Tokens\" topic by using discrete token sequences produced by a self-supervised model for TTS generation, with relevant technical details and empirical results shared. Thus, it fulfills all inclusion criteria and none of the exclusion criteria regarding discrete token generation, usage in downstream TTS tasks, and English-described methodology and evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes MaskGCT, a text-to-speech system that predicts semantic and acoustic tokens derived from a speech self-supervised learning model, indicating a discrete token representation of audio. It clearly details a two-stage transformer-based approach handling masked semantic and acoustic tokens and provides evaluations with large-scale speech data, suggesting it fits the \"Discrete Audio Tokens\" topic by using discrete token sequences produced by a self-supervised model for TTS generation, with relevant technical details and empirical results shared. Thus, it fulfills all inclusion criteria and none of the exclusion criteria regarding discrete token generation, usage in downstream TTS tasks, and English-described methodology and evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
    "abstract": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
    "metadata": {
      "arxiv_id": "2409.00942",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "authors": [
        "Yixuan Zhou",
        "Xing Xu",
        "Zhe Sun",
        "Jingkuan Song",
        "Andrzej Cichocki",
        "Heng Tao Shen"
      ],
      "published": "2024-09-02T05:01:41Z",
      "updated": "2024-09-02T05:01:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00942v1",
      "landing_url": "https://arxiv.org/abs/2409.00942v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.00942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item targets vector quantization within normalizing flows for image anomaly detection, not discrete audio tokens or audio tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item targets vector quantization within normalizing flows for image anomaly detection, not discrete audio tokens or audio tokenization, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on anomaly detection using normalizing flows combined with hierarchical vector quantization, but it does not involve discrete audio tokens or the discretization of continuous audio waveforms into token sequences; it is unrelated to audio token design, encoding, or generation, and thus does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on anomaly detection using normalizing flows combined with hierarchical vector quantization, but it does not involve discrete audio tokens or the discretization of continuous audio waveforms into token sequences; it is unrelated to audio token design, encoding, or generation, and thus does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A multilingual training strategy for low resource Text to Speech",
    "abstract": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
    "metadata": {
      "arxiv_id": "2409.01217",
      "title": "A multilingual training strategy for low resource Text to Speech",
      "summary": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
      "authors": [
        "Asma Amalas",
        "Mounir Ghogho",
        "Mohamed Chetouani",
        "Rachid Oulad Haj Thami"
      ],
      "published": "2024-09-02T12:53:01Z",
      "updated": "2024-09-02T12:53:01Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01217v1",
      "landing_url": "https://arxiv.org/abs/2409.01217v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01217"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on multilingual TTS training strategies and data sources rather than discrete audio token design/tokenization, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on multilingual TTS training strategies and data sources rather than discrete audio token design/tokenization, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual TTS with cross-lingual transfer learning and data selection, but there is no mention of using discrete audio tokens produced by neural audio codecs or self-supervised models for sequence modeling; details about tokenization, codebooks, or quantization are absent, so it does not meet the inclusion criteria for discrete audio token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual TTS with cross-lingual transfer learning and data selection, but there is no mention of using discrete audio tokens produced by neural audio codecs or self-supervised models for sequence modeling; details about tokenization, codebooks, or quantization are absent, so it does not meet the inclusion criteria for discrete audio token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "STAB: Speech Tokenizer Assessment Benchmark",
    "abstract": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
    "metadata": {
      "arxiv_id": "2409.02384",
      "title": "STAB: Speech Tokenizer Assessment Benchmark",
      "summary": "Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.",
      "authors": [
        "Shikhar Vashishth",
        "Harman Singh",
        "Shikhar Bharadwaj",
        "Sriram Ganapathy",
        "Chulayuth Asawaroengchai",
        "Kartik Audhkhasi",
        "Andrew Rosenberg",
        "Ankur Bapna",
        "Bhuvana Ramabhadran"
      ],
      "published": "2024-09-04T02:20:59Z",
      "updated": "2024-09-04T02:20:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.02384v1",
      "landing_url": "https://arxiv.org/abs/2409.02384v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.02384"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "It discusses discrete speech tokens/tokenizers and provides a systematic benchmark to evaluate tokenizer properties and downstream-task impact, aligning with inclusion criteria for discrete audio tokens and comparative analysis across token designs.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "It discusses discrete speech tokens/tokenizers and provides a systematic benchmark to evaluate tokenizer properties and downstream-task impact, aligning with inclusion criteria for discrete audio tokens and comparative analysis across token designs.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a systematic evaluation framework (STAB) for assessing speech tokenizers by representing speech as discrete tokens, aligning well with the topic of discrete audio tokens and focusing on analysis and benchmarking of tokenizer characteristics; it discusses tokenizers produced via neural audio codecs or self-supervised models and their evaluation metrics correlated to downstream tasks, fulfilling inclusion criteria related to token generation and evaluation, and provides sufficient English detail in title and abstract clarity, with no evidence meeting exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a systematic evaluation framework (STAB) for assessing speech tokenizers by representing speech as discrete tokens, aligning well with the topic of discrete audio tokens and focusing on analysis and benchmarking of tokenizer characteristics; it discusses tokenizers produced via neural audio codecs or self-supervised models and their evaluation metrics correlated to downstream tasks, fulfilling inclusion criteria related to token generation and evaluation, and provides sufficient English detail in title and abstract clarity, with no evidence meeting exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
    "abstract": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
    "metadata": {
      "arxiv_id": "2409.03283",
      "title": "FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications",
      "summary": "This work proposes FireRedTTS, a foundation text-to-speech framework, to meet the growing demands for personalized and diverse generative speech applications. The framework comprises three parts: data processing, foundation system, and downstream applications. First, we comprehensively present our data processing pipeline, which transforms massive raw audio into a large-scale high-quality TTS dataset with rich annotations and a wide coverage of content, speaking style, and timbre. Then, we propose a language-model-based foundation TTS system. The speech signal is compressed into discrete semantic tokens via a semantic-aware speech tokenizer, and can be generated by a language model from the prompt text and audio. Then, a two-stage waveform generator is proposed to decode them to the high-fidelity waveform. We present two applications of this system: voice cloning for dubbing and human-like speech generation for chatbots. The experimental results demonstrate the solid in-context learning capability of FireRedTTS, which can stably synthesize high-quality speech consistent with the prompt text and audio. For dubbing, FireRedTTS can clone target voices in a zero-shot way for the UGC scenario and adapt to studio-level expressive voice characters in the PUGC scenario via few-shot fine-tuning with 1-hour recording. Moreover, FireRedTTS achieves controllable human-like speech generation in a casual style with paralinguistic behaviors and emotions via instruction tuning, to better serve spoken chatbots.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Kun Liu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie",
        "Kai-Tuo Xu"
      ],
      "published": "2024-09-05T06:48:02Z",
      "updated": "2025-04-11T07:36:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03283v2",
      "landing_url": "https://arxiv.org/abs/2409.03283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03283"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract explicitly mentions compressing speech into discrete semantic tokens via a semantic-aware tokenizer and using them in a foundation TTS system, which aligns with the discrete-audio-token topic; although detailed token design/quantization is not provided, the core use of discrete tokens meets the inclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Abstract explicitly mentions compressing speech into discrete semantic tokens via a semantic-aware tokenizer and using them in a foundation TTS system, which aligns with the discrete-audio-token topic; although detailed token design/quantization is not provided, the core use of discrete tokens meets the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes FireRedTTS, a TTS system that includes a semantic-aware speech tokenizer compressing speech into discrete semantic tokens, which are then generated by a language model. This aligns with the inclusion criteria focusing on discrete audio tokens produced via neural audio codec or self-supervised learning models, used in downstream generation tasks like TTS. The paper also discusses system design and applications, indicating a practical token design and evaluation. It provides an English title and abstract with sufficient details to understand the token generation method, fulfilling the English assessability criteria. Therefore, it meets all inclusion and no exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes FireRedTTS, a TTS system that includes a semantic-aware speech tokenizer compressing speech into discrete semantic tokens, which are then generated by a language model. This aligns with the inclusion criteria focusing on discrete audio tokens produced via neural audio codec or self-supervised learning models, used in downstream generation tasks like TTS. The paper also discusses system design and applications, indicating a practical token design and evaluation. It provides an English title and abstract with sufficient details to understand the token generation method, fulfilling the English assessability criteria. Therefore, it meets all inclusion and no exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
    "abstract": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
    "metadata": {
      "arxiv_id": "2409.03377",
      "title": "aTENNuate: Optimized Real-time Speech Enhancement with Deep SSMs on Raw Audio",
      "summary": "We present aTENNuate, a simple deep state-space autoencoder configured for efficient online raw speech enhancement in an end-to-end fashion. The network's performance is primarily evaluated on raw speech denoising, with additional assessments on tasks such as super-resolution and de-quantization. We benchmark aTENNuate on the VoiceBank + DEMAND and the Microsoft DNS1 synthetic test sets. The network outperforms previous real-time denoising models in terms of PESQ score, parameter count, MACs, and latency. Even as a raw waveform processing model, the model maintains high fidelity to the clean signal with minimal audible artifacts. In addition, the model remains performant even when the noisy input is compressed down to 4000Hz and 4 bits, suggesting general speech enhancement capabilities in low-resource environments. Try it out by pip install attenuate",
      "authors": [
        "Yan Ru Pei",
        "Ritik Shrivastava",
        "FNU Sidharth"
      ],
      "published": "2024-09-05T09:28:56Z",
      "updated": "2025-05-20T02:23:19Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03377v4",
      "landing_url": "https://arxiv.org/abs/2409.03377v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.03377"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on real-time speech enhancement on raw audio without describing discrete token generation, token vocabularies, or token-based modeling; it does not meet the Discrete Audio Tokens criteria and offers no token-centric methodology or evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on real-time speech enhancement on raw audio without describing discrete token generation, token vocabularies, or token-based modeling; it does not meet the Discrete Audio Tokens criteria and offers no token-centric methodology or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on real-time speech enhancement using a deep state-space model applied directly on raw audio for denoising and related tasks, without mentioning any discretization process that maps continuous audio waveforms into discrete token sequences; it lacks details about tokenizers, codecs, quantization schemes, vocabularies, or representations that fit the discrete audio tokens topic defined in the inclusion criteria, hence it does not meet the core requirements for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on real-time speech enhancement using a deep state-space model applied directly on raw audio for denoising and related tasks, without mentioning any discretization process that maps continuous audio waveforms into discrete token sequences; it lacks details about tokenizers, codecs, quantization schemes, vocabularies, or representations that fit the discrete audio tokens topic defined in the inclusion criteria, hence it does not meet the core requirements for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
    "abstract": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
    "metadata": {
      "arxiv_id": "2409.03393",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "authors": [
        "Yongyi Miao",
        "Zhongdang Li",
        "Yang Wang",
        "Die Hu",
        "Jun Yan",
        "Youfang Wang"
      ],
      "published": "2024-09-05T09:53:53Z",
      "updated": "2024-09-05T09:53:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03393v1",
      "landing_url": "https://arxiv.org/abs/2409.03393v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.03393"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets video semantic communication with dual-stage vector quantization for video frames, not discrete audio tokens; it does not satisfy the inclusion criteria for 'Discrete Audio Tokens' research and thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets video semantic communication with dual-stage vector quantization for video frames, not discrete audio tokens; it does not satisfy the inclusion criteria for 'Discrete Audio Tokens' research and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a dual-stage vector quantization framework for video semantic communication, specifically targeting video transmission over wireless channels. The research is centered on video data and does not discuss audio waveform discretization, neural audio codecs, or discrete audio tokens applicable to speech, music, or environmental sound. There is no mention of token sequences derived from audio signals, neural audio codec architectures, or how discrete tokens influence audio generation, understanding, or cross-modal modeling. Thus, it does not meet the inclusion criteria focused on discrete audio tokens and instead falls outside the scope, focusing instead on video communication and compression.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a dual-stage vector quantization framework for video semantic communication, specifically targeting video transmission over wireless channels. The research is centered on video data and does not discuss audio waveform discretization, neural audio codecs, or discrete audio tokens applicable to speech, music, or environmental sound. There is no mention of token sequences derived from audio signals, neural audio codec architectures, or how discrete tokens influence audio generation, understanding, or cross-modal modeling. Thus, it does not meet the inclusion criteria focused on discrete audio tokens and instead falls outside the scope, focusing instead on video communication and compression.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LAST: Language Model Aware Speech Tokenization",
    "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work directly proposes a discrete speech tokenization approach by converting speech into a tokenized representation suitable for language-model processing and evaluates its impact on LM-style tasks, aligning with the Discrete Audio Tokens criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This work directly proposes a discrete speech tokenization approach by converting speech into a tokenized representation suitable for language-model processing and evaluates its impact on LM-style tasks, aligning with the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on training a speech tokenizer that integrates objectives from pre-trained textual language models to transform speech features into discrete tokens, aiming to improve spoken language modeling and speech-to-text tasks. It describes the tokenization process, investigation of design choices such as vocabulary size, and usage of discrete tokens for downstream tasks. This aligns with the inclusion criteria of producing discrete audio tokens from continuous speech for modeling and generation, with attention to tokenizer design and evaluation. There is no indication of purely continuous representations, lack of discrete token definition, or focus on non-generative tasks. The abstract is in English and provides sufficient detail to judge the discrete token process and downstream evaluation. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on training a speech tokenizer that integrates objectives from pre-trained textual language models to transform speech features into discrete tokens, aiming to improve spoken language modeling and speech-to-text tasks. It describes the tokenization process, investigation of design choices such as vocabulary size, and usage of discrete tokens for downstream tasks. This aligns with the inclusion criteria of producing discrete audio tokens from continuous speech for modeling and generation, with attention to tokenizer design and evaluation. There is no indication of purely continuous representations, lack of discrete token definition, or focus on non-generative tasks. The abstract is in English and provides sufficient detail to judge the discrete token process and downstream evaluation. Therefore, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
    "abstract": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper investigates discrete audio tokens produced by neural codecs and their impact on downstream speech-language-model–based speech generation, including token design, quantization, and cross-model evaluation, which aligns with the discrete-token, tokenization, and downstream-task analysis emphasis of the inclusion criteria; it provides an English abstract and methodological detail suitable for reproducibility.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper investigates discrete audio tokens produced by neural codecs and their impact on downstream speech-language-model–based speech generation, including token design, quantization, and cross-model evaluation, which aligns with the discrete-token, tokenization, and downstream-task analysis emphasis of the inclusion criteria; it provides an English abstract and methodological detail suitable for reproducibility.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates neural audio codec tokens used as discrete tokens for speech generation via speech language models, providing systematic comparison and insights on codec design impacting intelligibility and naturalness, aligning well with the inclusion criteria about discrete audio tokens generated by neural audio codecs and applied to downstream generation tasks; the abstract is in English and details codec retraining and integration into speech generation models, supporting reproducibility and evaluation of discrete token effects. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study investigates neural audio codec tokens used as discrete tokens for speech generation via speech language models, providing systematic comparison and insights on codec design impacting intelligibility and naturalness, aligning well with the inclusion criteria about discrete audio tokens generated by neural audio codecs and applied to downstream generation tasks; the abstract is in English and details codec retraining and integration into speech generation models, supporting reproducibility and evaluation of discrete token effects. Hence, it meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
    "abstract": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work discusses discrete audio tokens (semantic tokens) and a tokenization approach (k-means) as part of a pre-trained model for voice conversion, aligning with the discrete audio token criteria, though the main aim is VC with ICL rather than token design exclusively.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The work discusses discrete audio tokens (semantic tokens) and a tokenization approach (k-means) as part of a pre-trained model for voice conversion, aligning with the discrete audio token criteria, though the main aim is VC with ICL rather than token design exclusively.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses voice conversion using semantic tokens derived from self-supervised models and tokenization methods like k-means, indicating the use of discrete audio tokens as core representation; it also deals with token design and application in downstream generation tasks such as TTS, satisfying inclusion criteria related to discrete token usage and meaningful evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study discusses voice conversion using semantic tokens derived from self-supervised models and tokenization methods like k-means, indicating the use of discrete audio tokens as core representation; it also deals with token design and application in downstream generation tasks such as TTS, satisfying inclusion criteria related to discrete token usage and meaningful evaluation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
    "abstract": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
    "metadata": {
      "arxiv_id": "2409.06066",
      "title": "A Thorough Investigation of Content-Defined Chunking Algorithms for Data Deduplication",
      "summary": "Data deduplication emerged as a powerful solution for reducing storage and bandwidth costs in cloud settings by eliminating redundancies at the level of chunks. This has spurred the development of numerous Content-Defined Chunking (CDC) algorithms over the past two decades. Despite advancements, the current state-of-the-art remains obscure, as a thorough and impartial analysis and comparison is lacking. We conduct a rigorous theoretical analysis and impartial experimental comparison of several leading CDC algorithms. Using four realistic datasets, we evaluate these algorithms against four key metrics: throughput, deduplication ratio, average chunk size, and chunk-size variance. Our analyses, in many instances, extend the findings of their original publications by reporting new results and putting existing ones into context. Moreover, we highlight limitations that have previously gone unnoticed. Our findings provide valuable insights that inform the selection and optimization of CDC algorithms for practical applications in data deduplication.",
      "authors": [
        "Marcel Gregoriadis",
        "Leonhard Balduf",
        "Björn Scheuermann",
        "Johan Pouwelse"
      ],
      "published": "2024-09-09T20:58:40Z",
      "updated": "2024-09-28T19:05:45Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06066v3",
      "landing_url": "https://arxiv.org/abs/2409.06066v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.06066"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses data deduplication algorithms and content-defined chunking, not discrete audio tokens or their tokenization design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses data deduplication algorithms and content-defined chunking, not discrete audio tokens or their tokenization design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on data deduplication using content-defined chunking algorithms and does not discuss discrete audio tokenization methods, neural audio codecs, or any form of discretization of continuous audio into token sequences as specified in the inclusion criteria. Additionally, it lacks any mention of audio generation, understanding, or cross-modal modeling involving discrete tokens, and thus does not align with the thematic focus required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on data deduplication using content-defined chunking algorithms and does not discuss discrete audio tokenization methods, neural audio codecs, or any form of discretization of continuous audio into token sequences as specified in the inclusion criteria. Additionally, it lacks any mention of audio generation, understanding, or cross-modal modeling involving discrete tokens, and thus does not align with the thematic focus required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
    "abstract": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
    "metadata": {
      "arxiv_id": "2409.07151",
      "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
      "summary": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
      "authors": [
        "Tien-Hong Lo",
        "Meng-Ting Tsai",
        "Yao-Ting Sung",
        "Berlin Chen"
      ],
      "published": "2024-09-11T09:55:57Z",
      "updated": "2025-07-26T06:14:20Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07151v2",
      "landing_url": "https://arxiv.org/abs/2409.07151v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07151"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper does not address discrete audio tokens or tokenization; it focuses on zero-shot TTS and golden speech for pronunciation assessment without a discrete-token framework.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper does not address discrete audio tokens or tokenization; it focuses on zero-shot TTS and golden speech for pronunciation assessment without a discrete-token framework.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech and automatic pronunciation assessment, but it does not discuss or present any discrete audio token generation through neural audio codecs, vector quantization, or self-supervised discrete unit extraction, nor does it describe token vocabularies or quantization details; thus, it lacks the core elements related to discrete audio tokens required by the inclusion criteria and centers on continuous speech synthesis and assessment methods instead.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech and automatic pronunciation assessment, but it does not discuss or present any discrete audio token generation through neural audio codecs, vector quantization, or self-supervised discrete unit extraction, nor does it describe token vocabularies or quantization details; thus, it lacks the core elements related to discrete audio tokens required by the inclusion criteria and centers on continuous speech synthesis and assessment methods instead.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
    "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses semantic tokenization for items in recommender systems, not discrete audio tokens or audio token vocabularies, so it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses semantic tokenization for items in recommender systems, not discrete audio tokens or audio token vocabularies, so it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on semantic tokenization of recommendation items for generative models, which pertains to discrete token representations but specifically for recommendation systems and item semantics rather than discrete audio tokens derived from audio waveforms. It does not address audio signal discretization, neural audio codecs, or audio tokenization for tasks like TTS, audio generation, or cross-modal audio-language modeling. Therefore, it does not meet the inclusion criteria which require the study to involve discrete audio tokens and related audio tokenization techniques.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on semantic tokenization of recommendation items for generative models, which pertains to discrete token representations but specifically for recommendation systems and item semantics rather than discrete audio tokens derived from audio waveforms. It does not address audio signal discretization, neural audio codecs, or audio tokenization for tasks like TTS, audio generation, or cross-modal audio-language modeling. Therefore, it does not meet the inclusion criteria which require the study to involve discrete audio tokens and related audio tokenization techniques.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
    "abstract": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The SSR-Speech abstract describes a neural codec-based speech editing/synthesis model but does not explicitly describe discrete audio token generation, tokenization, or discrete codebooks, so it does not meet the discrete audio tokens criterion.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The SSR-Speech abstract describes a neural codec-based speech editing/synthesis model but does not explicitly describe discrete audio token generation, tokenization, or discrete codebooks, so it does not meet the discrete audio tokens criterion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents SSR-Speech, a neural codec autoregressive model for zero-shot text-based speech editing and synthesis, incorporating a Transformer decoder and a watermark Encodec that embeds frame-level watermarks; however, the abstract does not explicitly detail the discrete token generation process, tokenizer design, or codec specifics such as vector quantization or codebook configurations, making it difficult to confirm if discrete audio tokens, as strictly defined in the inclusion criteria, are central to the method rather than continuous representations or standard neural codecs; thus, despite relevance to audio generation tasks, the lack of clear discrete token explanations leads to uncertainty regarding inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper presents SSR-Speech, a neural codec autoregressive model for zero-shot text-based speech editing and synthesis, incorporating a Transformer decoder and a watermark Encodec that embeds frame-level watermarks; however, the abstract does not explicitly detail the discrete token generation process, tokenizer design, or codec specifics such as vector quantization or codebook configurations, making it difficult to confirm if discrete audio tokens, as strictly defined in the inclusion criteria, are central to the method rather than continuous representations or standard neural codecs; thus, despite relevance to audio generation tasks, the lack of clear discrete token explanations leads to uncertainty regarding inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: one recommended exclusion (2) and the other was uncertain (3). The abstract describes a neural-codec autoregressive model (SSR-Speech) and mentions Encodec and waveform reconstruction, but it does not explicitly describe discrete tokenization (e.g., tokenizer/codec architecture, quantization, codebook/vocabulary, time downsampling or token sequences) required by the “Discrete Audio Tokens” inclusion criteria. Therefore it is better to exclude: the submission lacks sufficient evidence that discrete audio tokens are the core representation or that token-generation details are provided for reproducibility.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: one recommended exclusion (2) and the other was uncertain (3). The abstract describes a neural-codec autoregressive model (SSR-Speech) and mentions Encodec and waveform reconstruction, but it does not explicitly describe discrete tokenization (e.g., tokenizer/codec architecture, quantization, codebook/vocabulary, time downsampling or token sequences) required by the “Discrete Audio Tokens” inclusion criteria. Therefore it is better to exclude: the submission lacks sufficient evidence that discrete audio tokens are the core representation or that token-generation details are provided for reproducibility.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
    "abstract": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",
    "metadata": {
      "arxiv_id": "2409.07966",
      "title": "ProbTalk3D: Non-Deterministic Emotion Controllable Speech-Driven 3D Facial Animation Synthesis Using VQ-VAE",
      "summary": "Audio-driven 3D facial animation synthesis has been an active field of research with attention from both academia and industry. While there are promising results in this area, recent approaches largely focus on lip-sync and identity control, neglecting the role of emotions and emotion control in the generative process. That is mainly due to the lack of emotionally rich facial animation data and algorithms that can synthesize speech animations with emotional expressions at the same time. In addition, majority of the models are deterministic, meaning given the same audio input, they produce the same output motion. We argue that emotions and non-determinism are crucial to generate diverse and emotionally-rich facial animations. In this paper, we propose ProbTalk3D a non-deterministic neural network approach for emotion controllable speech-driven 3D facial animation synthesis using a two-stage VQ-VAE model and an emotionally rich facial animation dataset 3DMEAD. We provide an extensive comparative analysis of our model against the recent 3D facial animation synthesis approaches, by evaluating the results objectively, qualitatively, and with a perceptual user study. We highlight several objective metrics that are more suitable for evaluating stochastic outputs and use both in-the-wild and ground truth data for subjective evaluation. To our knowledge, that is the first non-deterministic 3D facial animation synthesis method incorporating a rich emotion dataset and emotion control with emotion labels and intensity levels. Our evaluation demonstrates that the proposed model achieves superior performance compared to state-of-the-art emotion-controlled, deterministic and non-deterministic models. We recommend watching the supplementary video for quality judgement. The entire codebase is publicly available (https://github.com/uuembodiedsocialai/ProbTalk3D/).",
      "authors": [
        "Sichun Wu",
        "Kazi Injamamul Haque",
        "Zerrin Yumak"
      ],
      "published": "2024-09-12T11:53:05Z",
      "updated": "2025-02-16T14:23:08Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07966v4",
      "landing_url": "https://arxiv.org/abs/2409.07966v4",
      "doi": "https://doi.org/10.1145/3677388.3696320"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly employs VQ-VAE to produce discrete audio tokens and uses them to drive emotion-controllable 3D facial animation, satisfying the discrete audio token criteria and downstream generation use—thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly employs VQ-VAE to produce discrete audio tokens and uses them to drive emotion-controllable 3D facial animation, satisfying the discrete audio token criteria and downstream generation use—thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech-driven 3D facial animation synthesis using VQ-VAE, a vector quantized variational autoencoder, which involves vector quantization - a form of discretization - to process audio inputs. However, the abstract centers on facial animation and emotion control rather than the design, use, or evaluation of discrete audio tokens for audio generation, modeling, alignment, or cross-modal reasoning. It does not explicitly describe producing discrete audio token sequences as core representations for downstream tasks, nor does it discuss token design choices, evaluation metrics, or reproducibility related to audio tokenization. Therefore, although it uses a discrete latent representation model (VQ-VAE), the study is not primarily about discrete audio tokens as defined by the inclusion criteria for this review task.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech-driven 3D facial animation synthesis using VQ-VAE, a vector quantized variational autoencoder, which involves vector quantization - a form of discretization - to process audio inputs. However, the abstract centers on facial animation and emotion control rather than the design, use, or evaluation of discrete audio tokens for audio generation, modeling, alignment, or cross-modal reasoning. It does not explicitly describe producing discrete audio token sequences as core representations for downstream tasks, nor does it discuss token design choices, evaluation metrics, or reproducibility related to audio tokenization. Therefore, although it uses a discrete latent representation model (VQ-VAE), the study is not primarily about discrete audio tokens as defined by the inclusion criteria for this review task.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper explicitly states it uses a two-stage VQ-VAE (vector-quantized) model together with an emotionally rich audio-driven dataset to produce non-deterministic, emotion-controllable speech-driven 3D facial animation—which implies discrete audio latent codes/tokens are used as the core representation for downstream cross-modal generation, matching the inclusion group that uses neural audio codecs/vector quantization for downstream modeling; I note the two junior reviewers disagreed (one argued inclusion based on VQ-VAE usage, the other cautioned that the abstract does not detail tokenizer/codebook settings), so while the abstract lacks low-level tokenization details required for full reproducibility assessment, the described use of VQ-VAE as the central mechanism for driving downstream animation and the provided empirical evaluations support inclusion.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The paper explicitly states it uses a two-stage VQ-VAE (vector-quantized) model together with an emotionally rich audio-driven dataset to produce non-deterministic, emotion-controllable speech-driven 3D facial animation—which implies discrete audio latent codes/tokens are used as the core representation for downstream cross-modal generation, matching the inclusion group that uses neural audio codecs/vector quantization for downstream modeling; I note the two junior reviewers disagreed (one argued inclusion based on VQ-VAE usage, the other cautioned that the abstract does not detail tokenizer/codebook settings), so while the abstract lacks low-level tokenization details required for full reproducibility assessment, the described use of VQ-VAE as the central mechanism for driving downstream animation and the provided empirical evaluations support inclusion.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Zero-Shot Sing Voice Conversion: built upon clustering-based phoneme representations",
    "abstract": "This study presents an innovative Zero-Shot any-to-any Singing Voice Conversion (SVC) method, leveraging a novel clustering-based phoneme representation to effectively separate content, timbre, and singing style. This approach enables precise voice characteristic manipulation. We discovered that datasets with fewer recordings per artist are more susceptible to timbre leakage. Extensive testing on over 10,000 hours of singing and user feedback revealed our model significantly improves sound quality and timbre accuracy, aligning with our objectives and advancing voice conversion technology. Furthermore, this research advances zero-shot SVC and sets the stage for future work on discrete speech representation, emphasizing the preservation of rhyme.",
    "metadata": {
      "arxiv_id": "2409.08039",
      "title": "Zero-Shot Sing Voice Conversion: built upon clustering-based phoneme representations",
      "summary": "This study presents an innovative Zero-Shot any-to-any Singing Voice Conversion (SVC) method, leveraging a novel clustering-based phoneme representation to effectively separate content, timbre, and singing style. This approach enables precise voice characteristic manipulation. We discovered that datasets with fewer recordings per artist are more susceptible to timbre leakage. Extensive testing on over 10,000 hours of singing and user feedback revealed our model significantly improves sound quality and timbre accuracy, aligning with our objectives and advancing voice conversion technology. Furthermore, this research advances zero-shot SVC and sets the stage for future work on discrete speech representation, emphasizing the preservation of rhyme.",
      "authors": [
        "Wangjin Zhou",
        "Fengrun Zhang",
        "Yiming Liu",
        "Wenhao Guan",
        "Yi Zhao",
        "Tatsuya Kawahara"
      ],
      "published": "2024-09-12T13:42:04Z",
      "updated": "2024-10-14T08:05:40Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08039v2",
      "landing_url": "https://arxiv.org/abs/2409.08039v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08039"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract describe a clustering-based phoneme representation that functions as discrete units for content/timbre/style separation in zero-shot singing voice conversion, satisfying the discrete audio token criterion, with English presentation and intent to enable discrete representations and downstream modeling; it aligns with inclusion and does not obviously fall under exclusion.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe a clustering-based phoneme representation that functions as discrete units for content/timbre/style separation in zero-shot singing voice conversion, satisfying the discrete audio token criterion, with English presentation and intent to enable discrete representations and downstream modeling; it aligns with inclusion and does not obviously fall under exclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel singing voice conversion method using clustering-based phoneme representations, which suggests some discrete units used for modeling timbre and style. However, the abstract does not explicitly clarify the process of discretization to tokens, nor detailed tokenizer architecture, codebooks, or vector quantization settings, making it unclear if the work meets the inclusion criteria related to discrete audio token generation and evaluation. Also, as the method is about voice conversion, it partially overlaps with discrete unit representations but lacks explicit core focus on discrete token modeling and their design evaluation, resulting in insufficient evidence for inclusion.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel singing voice conversion method using clustering-based phoneme representations, which suggests some discrete units used for modeling timbre and style. However, the abstract does not explicitly clarify the process of discretization to tokens, nor detailed tokenizer architecture, codebooks, or vector quantization settings, making it unclear if the work meets the inclusion criteria related to discrete audio token generation and evaluation. Also, as the method is about voice conversion, it partially overlaps with discrete unit representations but lacks explicit core focus on discrete token modeling and their design evaluation, resulting in insufficient evidence for inclusion.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Integration of junior reviews: Junior reviewer A judged this likely meets the discrete-token criterion because it uses a clustering-based phoneme representation as discrete units; Junior reviewer B flagged insufficient methodological detail in the abstract about how tokens are produced (tokenizer/codebook/quantization/settings) and about evaluation. My assessment: the title/abstract explicitly mentions a “clustering-based phoneme representation,” which plausibly corresponds to self-supervised clustering-derived discrete units (one of the acceptable token sources). However, the abstract does not provide the required English methodological details to confirm it meets the inclusion requirements (no clear description of the discretization/tokenizer pipeline, vocab/codebook settings, time downsampling/rate, or reproducible evaluation of token design). Because inclusion requires that the work explicitly define and evaluate discrete audio token generation (or be clearly within the clustering-derived-discrete-units group with sufficient detail), and the abstract alone is ambiguous, I cannot confidently judge that ALL inclusion criteria are satisfied and that NONE of the exclusion criteria apply. Therefore the appropriate screening decision is uncertain pending the full paper/methods.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Integration of junior reviews: Junior reviewer A judged this likely meets the discrete-token criterion because it uses a clustering-based phoneme representation as discrete units; Junior reviewer B flagged insufficient methodological detail in the abstract about how tokens are produced (tokenizer/codebook/quantization/settings) and about evaluation. My assessment: the title/abstract explicitly mentions a “clustering-based phoneme representation,” which plausibly corresponds to self-supervised clustering-derived discrete units (one of the acceptable token sources). However, the abstract does not provide the required English methodological details to confirm it meets the inclusion requirements (no clear description of the discretization/tokenizer pipeline, vocab/codebook settings, time downsampling/rate, or reproducible evaluation of token design). Because inclusion requires that the work explicitly define and evaluate discrete audio token generation (or be clearly within the clustering-derived-discrete-units group with sufficient detail), and the abstract alone is ambiguous, I cannot confidently judge that ALL inclusion criteria are satisfied and that NONE of the exclusion criteria apply. Therefore the appropriate screening decision is uncertain pending the full paper/methods.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Text-To-Speech Synthesis In The Wild",
    "abstract": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
    "metadata": {
      "arxiv_id": "2409.08711",
      "title": "Text-To-Speech Synthesis In The Wild",
      "summary": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
      "authors": [
        "Jee-weon Jung",
        "Wangyou Zhang",
        "Soumi Maiti",
        "Yihan Wu",
        "Xin Wang",
        "Ji-Hoon Kim",
        "Yuta Matsunaga",
        "Seyun Um",
        "Jinchuan Tian",
        "Hye-jin Shim",
        "Nicholas Evans",
        "Joon Son Chung",
        "Shinnosuke Takamichi",
        "Shinji Watanabe"
      ],
      "published": "2024-09-13T10:58:55Z",
      "updated": "2025-06-01T09:29:36Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08711v2",
      "landing_url": "https://arxiv.org/abs/2409.08711v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08711"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on a noisy-TTS in-the-wild dataset (TITW) and its evaluation, with no discussion of discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a noisy-TTS in-the-wild dataset (TITW) and its evaluation, with no discussion of discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Text-To-Speech (TTS) synthesis using in-the-wild data and introduces a new dataset for training TTS models. However, the abstract does not mention any discrete audio token generation, neural audio codec, vector quantization, or any form of discretization of audio waveforms into token sequences, nor does it describe token vocabularies, codebooks, or any discrete token design choices. Its focus is on dataset creation and performance evaluation of TTS models without addressing discrete audio token representations as defined by the inclusion criteria. Therefore, it does not meet the core inclusion criteria and mostly aligns with continuous audio waveform modeling and synthesis rather than discrete token-based methods.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Text-To-Speech (TTS) synthesis using in-the-wild data and introduces a new dataset for training TTS models. However, the abstract does not mention any discrete audio token generation, neural audio codec, vector quantization, or any form of discretization of audio waveforms into token sequences, nor does it describe token vocabularies, codebooks, or any discrete token design choices. Its focus is on dataset creation and performance evaluation of TTS models without addressing discrete audio token representations as defined by the inclusion criteria. Therefore, it does not meet the core inclusion criteria and mostly aligns with continuous audio waveform modeling and synthesis rather than discrete token-based methods.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
    "abstract": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets LLM-based recommender systems and semantic token generation, not discrete audio tokens or tokenization methods for audio; it does not meet the inclusion criteria for discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets LLM-based recommender systems and semantic token generation, not discrete audio tokens or tokenization methods for audio; it does not meet the inclusion criteria for discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses leveraging large language models for recommendation systems and does not mention any discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms; thus, it does not meet the inclusion criteria related to discrete audio token representation and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article discusses leveraging large language models for recommendation systems and does not mention any discrete audio tokens, neural audio codecs, or discretization of continuous audio waveforms; thus, it does not meet the inclusion criteria related to discrete audio token representation and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
    "abstract": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
    "metadata": {
      "arxiv_id": "2409.09357",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "published": "2024-09-14T08:09:55Z",
      "updated": "2024-09-14T08:09:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09357v1",
      "landing_url": "https://arxiv.org/abs/2409.09357v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09357"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete acoustic tokens (predicting acoustic tokens) and token-based modeling for speech restoration, with an English abstract and downstream tasks, satisfying the inclusion criteria and not triggering exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete acoustic tokens (predicting acoustic tokens) and token-based modeling for speech restoration, with an English abstract and downstream tasks, satisfying the inclusion criteria and not triggering exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech restoration using semantic knowledge distillation and masked acoustic modeling, but the abstract does not clearly describe the usage or generation of discrete audio tokens through discretization mechanisms like neural audio codecs or self-supervised discrete units; thus, it does not sufficiently address core criteria related to discrete token design, production, or application as required.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech restoration using semantic knowledge distillation and masked acoustic modeling, but the abstract does not clearly describe the usage or generation of discrete audio tokens through discretization mechanisms like neural audio codecs or self-supervised discrete units; thus, it does not sufficiently address core criteria related to discrete token design, production, or application as required.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the paper’s relevance to token-based modeling but disagreed: JuniorNano accepted it because the abstract explicitly references predicting \"acoustic tokens\" with a masked language model, implying discrete token usage; JuniorMini rejected it due to lack of explicit description in the abstract of how those tokens are produced (e.g., neural codec/quantization, codebook settings) so the paper may not meet the discrete-token generation/description requirements. The abstract does state acoustic tokens and a masked language model conditioned on semantic features, which suggests tokens are central to the method, but it omits critical details about tokenizer/quantization mechanisms and token vocabulary/codebook settings required by the inclusion criteria—information that likely appears in the full paper but cannot be confirmed from title+abstract alone. Therefore I cannot confidently determine that ALL inclusion criteria (especially explicit English-method description of discrete token generation) are satisfied nor that NONE of the exclusion criteria apply. 1-sentence explanation: Ambiguous—the abstract mentions \"acoustic tokens\" but lacks the necessary tokenizer/quantization details to confirm it meets the discrete audio-token inclusion criteria, so inclusion is uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the paper’s relevance to token-based modeling but disagreed: JuniorNano accepted it because the abstract explicitly references predicting \"acoustic tokens\" with a masked language model, implying discrete token usage; JuniorMini rejected it due to lack of explicit description in the abstract of how those tokens are produced (e.g., neural codec/quantization, codebook settings) so the paper may not meet the discrete-token generation/description requirements. The abstract does state acoustic tokens and a masked language model conditioned on semantic features, which suggests tokens are central to the method, but it omits critical details about tokenizer/quantization mechanisms and token vocabulary/codebook settings required by the inclusion criteria—information that likely appears in the full paper but cannot be confirmed from title+abstract alone. Therefore I cannot confidently determine that ALL inclusion criteria (especially explicit English-method description of discrete token generation) are satisfied nor that NONE of the exclusion criteria apply. 1-sentence explanation: Ambiguous—the abstract mentions \"acoustic tokens\" but lacks the necessary tokenizer/quantization details to confirm it meets the discrete audio-token inclusion criteria, so inclusion is uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms",
    "abstract": "Multimodal schizophrenia assessment systems have gained traction over the last few years. This work introduces a schizophrenia assessment system to discern between prominent symptom classes of schizophrenia and predict an overall schizophrenia severity score. We develop a Vector Quantized Variational Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to produce task-agnostic speech representations from vocal Tract Variables (TVs) and Facial Action Units (FAUs). These representations are then used in a Multi-Task Learning (MTL) based downstream prediction model to obtain class labels and an overall severity score. The proposed framework outperforms the previous works on the multi-class classification task across all evaluation metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy). Additionally, it estimates the schizophrenia severity score, a task not addressed by earlier approaches.",
    "metadata": {
      "arxiv_id": "2409.09733",
      "title": "Self-supervised Multimodal Speech Representations for the Assessment of Schizophrenia Symptoms",
      "summary": "Multimodal schizophrenia assessment systems have gained traction over the last few years. This work introduces a schizophrenia assessment system to discern between prominent symptom classes of schizophrenia and predict an overall schizophrenia severity score. We develop a Vector Quantized Variational Auto-Encoder (VQ-VAE) based Multimodal Representation Learning (MRL) model to produce task-agnostic speech representations from vocal Tract Variables (TVs) and Facial Action Units (FAUs). These representations are then used in a Multi-Task Learning (MTL) based downstream prediction model to obtain class labels and an overall severity score. The proposed framework outperforms the previous works on the multi-class classification task across all evaluation metrics (Weighted F1 score, AUC-ROC score, and Weighted Accuracy). Additionally, it estimates the schizophrenia severity score, a task not addressed by earlier approaches.",
      "authors": [
        "Gowtham Premananth",
        "Carol Espy-Wilson"
      ],
      "published": "2024-09-15T13:45:04Z",
      "updated": "2024-11-17T16:48:38Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09733v5",
      "landing_url": "https://arxiv.org/abs/2409.09733v5",
      "doi": "https://doi.org/10.48550/arXiv.2409.09733"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses a Vector Quantized VAE to produce discrete audio tokens for multimodal representation learning (speech signals plus facial action units) and applies these representations to downstream prediction tasks (schizophrenia symptom severity), fitting the discrete audio token and cross-modal modeling criteria without appearing to be purely conventional continuous-signal work.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses a Vector Quantized VAE to produce discrete audio tokens for multimodal representation learning (speech signals plus facial action units) and applies these representations to downstream prediction tasks (schizophrenia symptom severity), fitting the discrete audio token and cross-modal modeling criteria without appearing to be purely conventional continuous-signal work.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study uses a Vector Quantized Variational Auto-Encoder (VQ-VAE) for producing task-agnostic speech representations, which implies discretization via vector quantization to obtain discrete audio tokens; however, the abstract does not provide explicit details on the discrete token design choices, codebooks, vocabulary, or token generation workflow necessary for reproducibility and clear identification as discrete audio tokens in core representation. Furthermore, the primary application is schizophrenia symptom assessment, a downstream classification/regression task, with no mention of audio generation, alignment, or cross-modal reasoning using these discrete tokens. Due to partial alignment with inclusion criteria but lacking explicit token design and systematical token analysis details, the inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study uses a Vector Quantized Variational Auto-Encoder (VQ-VAE) for producing task-agnostic speech representations, which implies discretization via vector quantization to obtain discrete audio tokens; however, the abstract does not provide explicit details on the discrete token design choices, codebooks, vocabulary, or token generation workflow necessary for reproducibility and clear identification as discrete audio tokens in core representation. Furthermore, the primary application is schizophrenia symptom assessment, a downstream classification/regression task, with no mention of audio generation, alignment, or cross-modal reasoning using these discrete tokens. Due to partial alignment with inclusion criteria but lacking explicit token design and systematical token analysis details, the inclusion is uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' opinions: JuniorNano judged inclusion (5) based on the use of a VQ-VAE suggesting discrete tokens, while JuniorMini flagged uncertainty (3) because the abstract lacks explicit tokenization/codebook/vocabulary details and does not show tokens being used for generation, alignment, or systematic token analysis. My assessment: although the abstract mentions a VQ-VAE (which can produce vector-quantized representations), it does not provide the required English-method details about tokenizer/codec architecture, codebook/vocabulary settings, token sequence usage, or evaluation of token design—information mandated by the inclusion criteria; therefore there is insufficient evidence that this work treats discrete audio tokens as the core representational object or reports reproducible token design choices. One-sentence explanation: Because the abstract lacks explicit, reproducible details about discrete-token generation and use (codebooks/vocabulary, tokenization workflow, or token-focused evaluations), it does not meet the inclusion criteria and is better excluded.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' opinions: JuniorNano judged inclusion (5) based on the use of a VQ-VAE suggesting discrete tokens, while JuniorMini flagged uncertainty (3) because the abstract lacks explicit tokenization/codebook/vocabulary details and does not show tokens being used for generation, alignment, or systematic token analysis. My assessment: although the abstract mentions a VQ-VAE (which can produce vector-quantized representations), it does not provide the required English-method details about tokenizer/codec architecture, codebook/vocabulary settings, token sequence usage, or evaluation of token design—information mandated by the inclusion criteria; therefore there is insufficient evidence that this work treats discrete audio tokens as the core representational object or reports reproducible token design choices. One-sentence explanation: Because the abstract lacks explicit, reproducible details about discrete-token generation and use (codebooks/vocabulary, tokenization workflow, or token-focused evaluations), it does not meet the inclusion criteria and is better excluded.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
    "abstract": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract mentions 'acoustic tokens' datasets, the work centers on adaptive depth via layerwise attention in transformers, not on designing or evaluating discrete audio tokens, tokenizers, or codex-like tokenization methods; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although the abstract mentions 'acoustic tokens' datasets, the work centers on adaptive depth via layerwise attention in transformers, not on designing or evaluating discrete audio tokens, tokenizers, or codex-like tokenization methods; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes an adaptive Transformer architecture with layerwise attention shortcuts and demonstrates this on multiple datasets including acoustic tokens, natural language, and symbolic music; however, the abstract does not mention discrete audio tokens generated via neural audio codecs or self-supervised discretization, nor does it detail the tokenization process, codebook settings, or quantization that are central to the inclusion criteria. Since the work focuses more on model architecture improvements rather than discrete audio token design, generation, or analysis, it does not meet all inclusion criteria and falls under exclusion criteria related to lack of discrete token methodology and insufficient details for reproducibility on discrete token generation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes an adaptive Transformer architecture with layerwise attention shortcuts and demonstrates this on multiple datasets including acoustic tokens, natural language, and symbolic music; however, the abstract does not mention discrete audio tokens generated via neural audio codecs or self-supervised discretization, nor does it detail the tokenization process, codebook settings, or quantization that are central to the inclusion criteria. Since the work focuses more on model architecture improvements rather than discrete audio token design, generation, or analysis, it does not meet all inclusion criteria and falls under exclusion criteria related to lack of discrete token methodology and insufficient details for reproducibility on discrete token generation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
    "abstract": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss token modeling and semantic tokens, but do not explicitly describe a discrete audio tokenization/codec setup (tokenizers, codebooks, vocab, or discrete token generation) as the core method; without clear evidence of discrete token representations driving the method, it is uncertain whether it meets the inclusion criteria for discrete audio tokens. Therefore, risk of misclassification is non-negligible.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss token modeling and semantic tokens, but do not explicitly describe a discrete audio tokenization/codec setup (tokenizers, codebooks, vocab, or discrete token generation) as the core method; without clear evidence of discrete token representations driving the method, it is uncertain whether it meets the inclusion criteria for discrete audio tokens. Therefore, risk of misclassification is non-negligible.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses a single-stage text-to-speech (TTS) model using semantic knowledge distillation and audio token modeling, implying the use of discrete audio tokens as core representations for speech synthesis. However, the abstract does not detail the discrete token generation process, tokenizer or codec architecture, or provide enough technical specifics to confirm the nature or design of the tokens used. Without explicit information on token discretization or vocabulary/codebook settings, it is unclear whether the study fully aligns with the inclusion criteria focusing on discrete audio tokens and reproducibility aspects.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses a single-stage text-to-speech (TTS) model using semantic knowledge distillation and audio token modeling, implying the use of discrete audio tokens as core representations for speech synthesis. However, the abstract does not detail the discrete token generation process, tokenizer or codec architecture, or provide enough technical specifics to confirm the nature or design of the tokens used. Without explicit information on token discretization or vocabulary/codebook settings, it is unclear whether the study fully aligns with the inclusion criteria focusing on discrete audio tokens and reproducibility aspects.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged the same concern: the title/abstract uses terms like “audio token modeling” and “semantic tokens” and claims a single-stage TTS, but it does not describe any discrete-token generation details (tokenizer/codec architecture, quantization, codebook/vocabulary, bitrate or sampling) required by the inclusion criteria. Because the inclusion rules require explicit, English-described discrete-token production or clear use of neural/audio-codec-derived discrete tokens, and the abstract lacks those specifics, I cannot confidently judge this as meeting the Discrete Audio Tokens topic—further inspection of the full paper is needed to confirm. Therefore the item is uncertain for inclusion.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged the same concern: the title/abstract uses terms like “audio token modeling” and “semantic tokens” and claims a single-stage TTS, but it does not describe any discrete-token generation details (tokenizer/codec architecture, quantization, codebook/vocabulary, bitrate or sampling) required by the inclusion criteria. Because the inclusion rules require explicit, English-described discrete-token production or clear use of neural/audio-codec-derived discrete tokens, and the abstract lacks those specifics, I cannot confidently judge this as meeting the Discrete Audio Tokens topic—further inspection of the full paper is needed to confirm. Therefore the item is uncertain for inclusion.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling",
    "abstract": "Learning compact and meaningful latent space representations has been shown to be very useful in generative modeling tasks for visual data. One particular example is applying Vector Quantization (VQ) in variational autoencoders (VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance in many modern generative modeling applications. Quantizing the latent space has been justified by the assumption that the data themselves are inherently discrete in the latent space (like pixel values). In this paper, we propose an alternative representation of the latent space by relaxing the structural assumption than the VQ formulation. Specifically, we assume that the latent space can be approximated by a union of subspaces model corresponding to a dictionary-based representation under a sparsity constraint. The dictionary is learned/updated during the training process. We apply this approach to look at two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs with Generative Adversarial Networks (DL-GANs). We show empirically that our more latent space is more expressive and has leads to better representations than the VQ approach in terms of reconstruction quality at the expense of a small computational overhead for the latent space computation. Our results thus suggest that the true benefit of the VQ approach might not be from discretization of the latent space, but rather the lossy compression of the latent space. We confirm this hypothesis by showing that our sparse representations also address the codebook collapse issue as found common in VQ-family models.",
    "metadata": {
      "arxiv_id": "2409.11184",
      "title": "LASERS: LAtent Space Encoding for Representations with Sparsity for Generative Modeling",
      "summary": "Learning compact and meaningful latent space representations has been shown to be very useful in generative modeling tasks for visual data. One particular example is applying Vector Quantization (VQ) in variational autoencoders (VQ-VAEs, VQ-GANs, etc.), which has demonstrated state-of-the-art performance in many modern generative modeling applications. Quantizing the latent space has been justified by the assumption that the data themselves are inherently discrete in the latent space (like pixel values). In this paper, we propose an alternative representation of the latent space by relaxing the structural assumption than the VQ formulation. Specifically, we assume that the latent space can be approximated by a union of subspaces model corresponding to a dictionary-based representation under a sparsity constraint. The dictionary is learned/updated during the training process. We apply this approach to look at two models: Dictionary Learning Variational Autoencoders (DL-VAEs) and DL-VAEs with Generative Adversarial Networks (DL-GANs). We show empirically that our more latent space is more expressive and has leads to better representations than the VQ approach in terms of reconstruction quality at the expense of a small computational overhead for the latent space computation. Our results thus suggest that the true benefit of the VQ approach might not be from discretization of the latent space, but rather the lossy compression of the latent space. We confirm this hypothesis by showing that our sparse representations also address the codebook collapse issue as found common in VQ-family models.",
      "authors": [
        "Xin Li",
        "Anand Sarwate"
      ],
      "published": "2024-09-16T08:20:58Z",
      "updated": "2024-09-16T08:20:58Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11184v1",
      "landing_url": "https://arxiv.org/abs/2409.11184v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11184"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on latent-space sparsity and dictionary-based representations for generative models (primarily for visual data) and does not discuss discrete audio tokens, tokenization methods, or audio-specific token design; it thus does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on latent-space sparsity and dictionary-based representations for generative models (primarily for visual data) and does not discuss discrete audio tokens, tokenization methods, or audio-specific token design; it thus does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on latent space encoding methods for visual data generative modeling, discussing Vector Quantization and sparse representations in variational autoencoders; however, it does not concern discrete audio tokens, audio waveform discretization, or audio-related tokenization for generative or cross-modal modeling as per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on latent space encoding methods for visual data generative modeling, discussing Vector Quantization and sparse representations in variational autoencoders; however, it does not concern discrete audio tokens, audio waveform discretization, or audio-related tokenization for generative or cross-modal modeling as per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Simulating Native Speaker Shadowing for Nonnative Speech Assessment with Latent Speech Representations",
    "abstract": "Evaluating speech intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic speech recognition (ASR) as intelligibility scores. However, this approach has significant limitations due to notable differences between human speech recognition (HSR) and ASR. A promising alternative is to involve a native (L1) speaker in shadowing what nonnative (L2) speakers say. Breakdowns or mispronunciations in the L1 speaker's shadowing utterance can serve as indicators for assessing L2 speech intelligibility. In this study, we propose a speech generation system that simulates the L1 shadowing process using voice conversion (VC) techniques and latent speech representations. Our experimental results demonstrate that this method effectively replicates the L1 shadowing process, offering an innovative tool to evaluate L2 speech intelligibility. Notably, systems that utilize self-supervised speech representations (S3R) show a higher degree of similarity to real L1 shadowing utterances in both linguistic accuracy and naturalness.",
    "metadata": {
      "arxiv_id": "2409.11742",
      "title": "Simulating Native Speaker Shadowing for Nonnative Speech Assessment with Latent Speech Representations",
      "summary": "Evaluating speech intelligibility is a critical task in computer-aided language learning systems. Traditional methods often rely on word error rates (WER) provided by automatic speech recognition (ASR) as intelligibility scores. However, this approach has significant limitations due to notable differences between human speech recognition (HSR) and ASR. A promising alternative is to involve a native (L1) speaker in shadowing what nonnative (L2) speakers say. Breakdowns or mispronunciations in the L1 speaker's shadowing utterance can serve as indicators for assessing L2 speech intelligibility. In this study, we propose a speech generation system that simulates the L1 shadowing process using voice conversion (VC) techniques and latent speech representations. Our experimental results demonstrate that this method effectively replicates the L1 shadowing process, offering an innovative tool to evaluate L2 speech intelligibility. Notably, systems that utilize self-supervised speech representations (S3R) show a higher degree of similarity to real L1 shadowing utterances in both linguistic accuracy and naturalness.",
      "authors": [
        "Haopeng Geng",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2024-09-18T06:54:46Z",
      "updated": "2024-09-19T01:19:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11742v2",
      "landing_url": "https://arxiv.org/abs/2409.11742v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11742"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on simulating native shadowing using voice conversion and latent representations for L2 intelligibility, and it does not discuss discrete audio tokens, tokenization, or token design, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on simulating native shadowing using voice conversion and latent representations for L2 intelligibility, and it does not discuss discrete audio tokens, tokenization, or token design, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on simulating native speaker shadowing for assessing nonnative speech intelligibility using latent speech representations derived from self-supervised speech representations (S3R). However, the abstract does not explicitly mention the use of discrete audio tokens obtained via discretization methods such as neural audio codecs or vector quantization that produce finite token vocabularies. It centers on assessment and speech generation via voice conversion rather than on representing audio as discrete token sequences for generation or alignment tasks. Therefore, it does not meet the core inclusion criteria related to discrete audio token generation, design, or downstream usage and lacks detailed information on tokenization architecture or vocabularies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on simulating native speaker shadowing for assessing nonnative speech intelligibility using latent speech representations derived from self-supervised speech representations (S3R). However, the abstract does not explicitly mention the use of discrete audio tokens obtained via discretization methods such as neural audio codecs or vector quantization that produce finite token vocabularies. It centers on assessment and speech generation via voice conversion rather than on representing audio as discrete token sequences for generation or alignment tasks. Therefore, it does not meet the core inclusion criteria related to discrete audio token generation, design, or downstream usage and lacks detailed information on tokenization architecture or vocabularies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Upgrading edges in the maximal covering location problem",
    "abstract": "We study the upgrading version of the maximal covering location problem with edge length modifications on networks. This problem aims at locating p facilities on the vertices (of the network) so as to maximise coverage, considering that the length of the edges can be reduced at a cost, subject to a given budget. Hence, we have to decide on: the optimal location of p facilities and the optimal edge length reductions. This problem is NP-hard on general graphs. To solve it, we propose three different mixed-integer formulations and a preprocessing phase for fixing variables and removing some of the constraints. Moreover, we strengthen the proposed formulations including valid inequalities. Finally, we compare the three formulations and their corresponding improvements by testing their performance over different datasets.",
    "metadata": {
      "arxiv_id": "2409.11883",
      "title": "Upgrading edges in the maximal covering location problem",
      "summary": "We study the upgrading version of the maximal covering location problem with edge length modifications on networks. This problem aims at locating p facilities on the vertices (of the network) so as to maximise coverage, considering that the length of the edges can be reduced at a cost, subject to a given budget. Hence, we have to decide on: the optimal location of p facilities and the optimal edge length reductions.\n  This problem is NP-hard on general graphs. To solve it, we propose three different mixed-integer formulations and a preprocessing phase for fixing variables and removing some of the constraints. Moreover, we strengthen the proposed formulations including valid inequalities. Finally, we compare the three formulations and their corresponding improvements by testing their performance over different datasets.",
      "authors": [
        "Marta Baldomero-Naranjo",
        "Jörg Kalcsics",
        "Alfredo Marín",
        "Antonio M. Rodríguez-Chía"
      ],
      "published": "2024-09-18T11:30:00Z",
      "updated": "2024-09-18T11:30:00Z",
      "categories": [
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11883v1",
      "landing_url": "https://arxiv.org/abs/2409.11883v1",
      "doi": "https://doi.org/10.1016/j.ejor.2022.02.001"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on upgrading edges in a maximal covering location problem with MILP formulations, not on discrete audio token design or evaluation, thus failing the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on upgrading edges in a maximal covering location problem with MILP formulations, not on discrete audio token design or evaluation, thus failing the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on the maximal covering location problem with edge length modifications in networks, which is a combinatorial optimization problem unrelated to discrete audio tokens, audio waveform discretization, or neural audio codecs; hence, it does not meet any inclusion criteria related to audio token design or audio representation and pertains to a different research domain entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on the maximal covering location problem with edge length modifications in networks, which is a combinatorial optimization problem unrelated to discrete audio tokens, audio waveform discretization, or neural audio codecs; hence, it does not meet any inclusion criteria related to audio token design or audio representation and pertains to a different research domain entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
    "abstract": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
    "metadata": {
      "arxiv_id": "2409.12176",
      "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
      "summary": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
      "authors": [
        "Podakanti Satyajith Chary"
      ],
      "published": "2024-09-10T11:50:35Z",
      "updated": "2024-09-10T11:50:35Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12176v1",
      "landing_url": "https://arxiv.org/abs/2409.12176v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12176"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets prosodic control in TTS and continuous feature manipulation, with no discrete audio tokens, tokenizers, or token-based representations described, hence it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets prosodic control in TTS and continuous feature manipulation, with no discrete audio tokens, tokenizers, or token-based representations described, hence it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on manipulating prosodic parameters in TTS to produce more natural speech without mentioning the use or generation of discrete audio tokens, neural audio codecs, vector quantization, or any discrete token-based modeling or analysis, which are core inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor does it discuss tokenization or discrete representations relevant to the review focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on manipulating prosodic parameters in TTS to produce more natural speech without mentioning the use or generation of discrete audio tokens, neural audio codecs, vector quantization, or any discrete token-based modeling or analysis, which are core inclusion criteria. Therefore, it does not meet the inclusion criteria related to discrete audio tokens, nor does it discuss tokenization or discrete representations relevant to the review focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
    "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
    "metadata": {
      "arxiv_id": "2409.12717",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "authors": [
        "Zhikang Niu",
        "Sanyuan Chen",
        "Long Zhou",
        "Ziyang Ma",
        "Xie Chen",
        "Shujie Liu"
      ],
      "published": "2024-09-19T12:41:30Z",
      "updated": "2024-09-19T12:41:30Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12717v1",
      "landing_url": "https://arxiv.org/abs/2409.12717v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12717"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "NDVQ employs vector quantization in a neural audio codec and uses a discrete codebook (each entry representing a distribution) to produce discrete tokens for reconstruction and downstream tasks, fitting the discrete audio token paradigm and providing methodological details in English.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "NDVQ employs vector quantization in a neural audio codec and uses a discrete codebook (each entry representing a distribution) to produce discrete tokens for reconstruction and downstream tasks, fitting the discrete audio token paradigm and providing methodological details in English.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a novel neural audio codec utilizing vector quantization to produce discrete tokens from continuous audio waveforms, explicitly describing the quantization method, codebook design with normal distributions, and demonstrates application in low bandwidth audio compression and zero-shot TTS tasks. This aligns well with the inclusion criteria of discrete audio tokens generated via neural audio codec with vector quantization, used for downstream generation tasks, and provides sufficient methodological details in English to assess token design and reproducibility. There is no indication of exclusion criteria being met, such as reliance on continuous features without discrete token generation or lacking details on tokenization. Hence, the paper fits well within the defined topic scope and criteria for inclusion.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a novel neural audio codec utilizing vector quantization to produce discrete tokens from continuous audio waveforms, explicitly describing the quantization method, codebook design with normal distributions, and demonstrates application in low bandwidth audio compression and zero-shot TTS tasks. This aligns well with the inclusion criteria of discrete audio tokens generated via neural audio codec with vector quantization, used for downstream generation tasks, and provides sufficient methodological details in English to assess token design and reproducibility. There is no indication of exclusion criteria being met, such as reliance on continuous features without discrete token generation or lacking details on tokenization. Hence, the paper fits well within the defined topic scope and criteria for inclusion.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice Conversion-based Privacy through Adversarial Information Hiding",
    "abstract": "Privacy-preserving voice conversion aims to remove only the attributes of speech audio that convey identity information, keeping other speech characteristics intact. This paper presents a mechanism for privacy-preserving voice conversion that allows controlling the leakage of identity-bearing information using adversarial information hiding. This enables a deliberate trade-off between maintaining source-speech characteristics and modification of speaker identity. As such, the approach improves on voice-conversion techniques like CycleGAN and StarGAN, which were not designed for privacy, meaning that converted speech may leak personal information in unpredictable ways. Our approach is also more flexible than ASR-TTS voice conversion pipelines, which by design discard all prosodic information linked to textual content. Evaluations show that the proposed system successfully modifies perceived speaker identity whilst well maintaining source lexical content.",
    "metadata": {
      "arxiv_id": "2409.14919",
      "title": "Voice Conversion-based Privacy through Adversarial Information Hiding",
      "summary": "Privacy-preserving voice conversion aims to remove only the attributes of speech audio that convey identity information, keeping other speech characteristics intact. This paper presents a mechanism for privacy-preserving voice conversion that allows controlling the leakage of identity-bearing information using adversarial information hiding. This enables a deliberate trade-off between maintaining source-speech characteristics and modification of speaker identity. As such, the approach improves on voice-conversion techniques like CycleGAN and StarGAN, which were not designed for privacy, meaning that converted speech may leak personal information in unpredictable ways. Our approach is also more flexible than ASR-TTS voice conversion pipelines, which by design discard all prosodic information linked to textual content. Evaluations show that the proposed system successfully modifies perceived speaker identity whilst well maintaining source lexical content.",
      "authors": [
        "Jacob J Webber",
        "Oliver Watts",
        "Gustav Eje Henter",
        "Jennifer Williams",
        "Simon King"
      ],
      "published": "2024-09-23T11:16:49Z",
      "updated": "2024-09-23T11:16:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14919v1",
      "landing_url": "https://arxiv.org/abs/2409.14919v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14919"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on privacy-preserving voice conversion and adversarial hiding of speaker identity, not on discrete audio tokens or tokenization schemes; it does not provide token-level architectures or discrete vocabularies, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on privacy-preserving voice conversion and adversarial hiding of speaker identity, not on discrete audio tokens or tokenization schemes; it does not provide token-level architectures or discrete vocabularies, thus it fails the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on privacy-preserving voice conversion by modifying speaker identity in continuous speech audio, but there is no indication that it involves discretizing the audio waveform into discrete tokens nor uses neural audio codecs or discrete token sequences as core representations; it mainly deals with continuous voice conversion methods without discussing tokenization or discrete representations, which are essential inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on privacy-preserving voice conversion by modifying speaker identity in continuous speech audio, but there is no indication that it involves discretizing the audio waveform into discrete tokens nor uses neural audio codecs or discrete token sequences as core representations; it mainly deals with continuous voice conversion methods without discussing tokenization or discrete representations, which are essential inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
    "abstract": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
    "metadata": {
      "arxiv_id": "2409.15741",
      "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
      "summary": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
      "authors": [
        "Zhiyong Chen",
        "Xinnuo Li",
        "Zhiqi Ai",
        "Shugong Xu"
      ],
      "published": "2024-09-24T04:55:17Z",
      "updated": "2024-09-24T04:55:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15741v1",
      "landing_url": "https://arxiv.org/abs/2409.15741v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15741"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on multimodal style-control in zero-shot TTS without discussing discrete audio tokens, tokenization, or neural audio codecs, so it does not meet the core inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on multimodal style-control in zero-shot TTS without discussing discrete audio tokens, tokenization, or neural audio codecs, so it does not meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on zero-shot text-to-speech synthesis with style and speaker controllability using multimodal inputs, but there is no mention of discretizing continuous audio signals into discrete tokens, nor details on tokenizer/codec design, quantization methods, or vocab/codebook configurations. Therefore, it does not meet the inclusion criteria related to discrete audio tokens as core representations for generation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on zero-shot text-to-speech synthesis with style and speaker controllability using multimodal inputs, but there is no mention of discretizing continuous audio signals into discrete tokens, nor details on tokenizer/codec design, quantization methods, or vocab/codebook configurations. Therefore, it does not meet the inclusion criteria related to discrete audio tokens as core representations for generation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploring VQ-VAE with Prosody Parameters for Speaker Anonymization",
    "abstract": "Human speech conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these speech components. This approach is designed to disentangle these components to specifically target and modify the speaker identity while preserving the linguistic and emotionalcontent. To do so, three separate branches compute embeddings for content, prosody, and speaker identity respectively. During synthesis, taking these embeddings, the decoder of the proposed architecture is conditioned on both speaker and prosody information, allowing for capturing more nuanced emotional states and precise adjustments to speaker identification. Findings indicate that this method outperforms most baseline techniques in preserving emotional information. However, it exhibits more limited performance on other voice privacy tasks, emphasizing the need for further improvements.",
    "metadata": {
      "arxiv_id": "2409.15882",
      "title": "Exploring VQ-VAE with Prosody Parameters for Speaker Anonymization",
      "summary": "Human speech conveys prosody, linguistic content, and speaker identity. This article investigates a novel speaker anonymization approach using an end-to-end network based on a Vector-Quantized Variational Auto-Encoder (VQ-VAE) to deal with these speech components. This approach is designed to disentangle these components to specifically target and modify the speaker identity while preserving the linguistic and emotionalcontent. To do so, three separate branches compute embeddings for content, prosody, and speaker identity respectively. During synthesis, taking these embeddings, the decoder of the proposed architecture is conditioned on both speaker and prosody information, allowing for capturing more nuanced emotional states and precise adjustments to speaker identification. Findings indicate that this method outperforms most baseline techniques in preserving emotional information. However, it exhibits more limited performance on other voice privacy tasks, emphasizing the need for further improvements.",
      "authors": [
        "Sotheara Leang",
        "Anderson Augusma",
        "Eric Castelli",
        "Frédérique Letué",
        "Sethserey Sam",
        "Dominique Vaufreydaz"
      ],
      "published": "2024-09-24T08:55:10Z",
      "updated": "2024-09-24T08:55:10Z",
      "categories": [
        "cs.CV",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15882v1",
      "landing_url": "https://arxiv.org/abs/2409.15882v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15882"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study uses a Vector-Quantized VAE, i.e., discrete latent tokens, to disentangle content, prosody, and speaker identity for speaker anonymization, addressing discrete audio token representations and providing token-based modeling context; it has an English abstract and discusses neural discrete tokens, meeting the inclusion criteria while not falling into exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study uses a Vector-Quantized VAE, i.e., discrete latent tokens, to disentangle content, prosody, and speaker identity for speaker anonymization, addressing discrete audio token representations and providing token-based modeling context; it has an English abstract and discusses neural discrete tokens, meeting the inclusion criteria while not falling into exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speaker anonymization using a VQ-VAE, which involves vector quantization (a form of discretization) to generate embeddings, but the abstract does not clearly describe the discrete token generation process, vocabulary/codebook design, or provide sufficient evidence that discrete audio tokens are the core representation used for downstream modeling or generation as required by the inclusion criteria. Thus, while related, it lacks detail on the discrete token aspect to confidently include it.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speaker anonymization using a VQ-VAE, which involves vector quantization (a form of discretization) to generate embeddings, but the abstract does not clearly describe the discrete token generation process, vocabulary/codebook design, or provide sufficient evidence that discrete audio tokens are the core representation used for downstream modeling or generation as required by the inclusion criteria. Thus, while related, it lacks detail on the discrete token aspect to confidently include it.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano argued inclusion because the use of VQ-VAE implies discrete latent tokens, while JuniorMini cautioned that the abstract lacks explicit details about discrete token generation (tokenizer/codec, codebook settings, downsampling) and token-centric downstream use. Although the paper uses vector quantization conceptually, the abstract does not describe discrete token sequences, tokenizer/codec architecture, codebook/vocabulary or codec parameters, nor does it frame the work as using discrete audio tokens as the core representation for generation/LM-style modeling or cross-modal tasks—therefore it fails the requirement for clear, reproducible discrete-token design and token-centric contribution. Score explanation: Better to exclude because the abstract lacks the explicit, reproducible discrete-token/tokenizer details and token-centric framing required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "JuniorNano argued inclusion because the use of VQ-VAE implies discrete latent tokens, while JuniorMini cautioned that the abstract lacks explicit details about discrete token generation (tokenizer/codec, codebook settings, downsampling) and token-centric downstream use. Although the paper uses vector quantization conceptually, the abstract does not describe discrete token sequences, tokenizer/codec architecture, codebook/vocabulary or codec parameters, nor does it frame the work as using discrete audio tokens as the core representation for generation/LM-style modeling or cross-modal tasks—therefore it fails the requirement for clear, reproducible discrete-token design and token-centric contribution. Score explanation: Better to exclude because the abstract lacks the explicit, reproducible discrete-token/tokenizer details and token-centric framing required by the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
    "abstract": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
    "metadata": {
      "arxiv_id": "2409.16302",
      "title": "How Redundant Is the Transformer Stack in Speech Representation Models?",
      "summary": "Self-supervised speech representation models, particularly those leveraging transformer architectures, have demonstrated remarkable performance across various tasks such as speech recognition, speaker identification, and emotion detection. Recent studies on transformer models revealed a high redundancy between layers and the potential for significant pruning, which we will investigate here for transformer-based speech representation models. We perform a detailed analysis of layer similarity in speech representation models using three similarity metrics: cosine similarity, centered kernel alignment, and mutual nearest-neighbor alignment. Our findings reveal a block-like structure of high similarity, suggesting two main processing steps and significant redundancy of layers. We demonstrate the effectiveness of pruning transformer-based speech representation models without the need for post-training, achieving up to 40% reduction in transformer layers while maintaining over 95% of the model's predictive capacity. Furthermore, we employ a knowledge distillation method to substitute the entire transformer stack with mimicking layers, reducing the network size 95-98% and the inference time by up to 94%. This substantial decrease in computational load occurs without considerable performance loss, suggesting that the transformer stack is almost completely redundant for downstream applications of speech representation models.",
      "authors": [
        "Teresa Dorszewski",
        "Albert Kjøller Jacobsen",
        "Lenka Tětková",
        "Lars Kai Hansen"
      ],
      "published": "2024-09-10T11:00:24Z",
      "updated": "2025-01-17T12:27:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.16302v2",
      "landing_url": "https://arxiv.org/abs/2409.16302v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.16302"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on pruning/redistributing transformer layers in speech representation models and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the token-centered inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on pruning/redistributing transformer layers in speech representation models and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the token-centered inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on self-supervised speech representation models using transformer architectures and analyzes layer redundancy and pruning effectiveness, but it does not address the discretization of continuous audio signals into token sequences or the use of discrete audio tokens as core inputs or outputs. There is no mention of neural audio codecs, tokenizers, quantization methods, or vocabulary/codebook setups, nor does it discuss downstream tasks involving discrete audio tokens or their design impacts. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead aligns more with continuous audio representation modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on self-supervised speech representation models using transformer architectures and analyzes layer redundancy and pruning effectiveness, but it does not address the discretization of continuous audio signals into token sequences or the use of discrete audio tokens as core inputs or outputs. There is no mention of neural audio codecs, tokenizers, quantization methods, or vocabulary/codebook setups, nor does it discuss downstream tasks involving discrete audio tokens or their design impacts. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead aligns more with continuous audio representation modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
    "abstract": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
    "metadata": {
      "arxiv_id": "2409.18042",
      "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
      "summary": "GPT-4o, an omni-modal model that enables vocal conversations with diverse emotions and tones, marks a milestone for omni-modal foundation models. However, empowering Large Language Models to perceive and generate images, texts, and speeches end-to-end with publicly available data remains challenging for the open-source community. Existing vision-language models rely on external tools for speech processing, while speech-language models still suffer from limited or totally without vision-understanding capabilities. To address this gap, we propose the EMOVA (EMotionally Omni-present Voice Assistant), to enable Large Language Models with end-to-end speech abilities while maintaining the leading vision-language performance. With a semantic-acoustic disentangled speech tokenizer, we surprisingly notice that omni-modal alignment can further enhance vision-language and speech abilities compared with the bi-modal aligned counterparts. Moreover, a lightweight style module is introduced for the flexible speech style controls including emotions and pitches. For the first time, EMOVA achieves state-of-the-art performance on both the vision-language and speech benchmarks, and meanwhile, supporting omni-modal spoken dialogue with vivid emotions.",
      "authors": [
        "Kai Chen",
        "Yunhao Gou",
        "Runhui Huang",
        "Zhili Liu",
        "Daxin Tan",
        "Jing Xu",
        "Chunwei Wang",
        "Yi Zhu",
        "Yihan Zeng",
        "Kuo Yang",
        "Dingdong Wang",
        "Kun Xiang",
        "Haoyuan Li",
        "Haoli Bai",
        "Jianhua Han",
        "Xiaohui Li",
        "Weike Jin",
        "Nian Xie",
        "Yu Zhang",
        "James T. Kwok",
        "Hengshuang Zhao",
        "Xiaodan Liang",
        "Dit-Yan Yeung",
        "Xiao Chen",
        "Zhenguo Li",
        "Wei Zhang",
        "Qun Liu",
        "Jun Yao",
        "Lanqing Hong",
        "Lu Hou",
        "Hang Xu"
      ],
      "published": "2024-09-26T16:44:02Z",
      "updated": "2025-03-20T08:47:39Z",
      "categories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.18042v4",
      "landing_url": "https://arxiv.org/abs/2409.18042v4",
      "doi": "https://doi.org/10.48550/arXiv.2409.18042"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions a semantic-acoustic disentangled speech tokenizer and treats discrete tokens as part of enabling omni-modal speech and vision-language alignment, with downstream benchmarks and end-to-end capabilities, which aligns with the discrete audio token criteria and excludes continuous-only representations.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions a semantic-acoustic disentangled speech tokenizer and treats discrete tokens as part of enabling omni-modal speech and vision-language alignment, with downstream benchmarks and end-to-end capabilities, which aligns with the discrete audio token criteria and excludes continuous-only representations.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract mentions a 'semantic-acoustic disentangled speech tokenizer,' indicating a form of discrete tokenization of audio, aligning well with the inclusion criteria of studies focusing on discrete audio tokens generated by neural audio codecs. The study emphasizes omni-modal alignment with vision-language and speech, and proposes a novel method (EMOVA) with speech style control and demonstrated state-of-the-art performance, suggesting a significant contribution. However, the details on tokenizer architecture, quantization, vocabulary/codebook settings, or bitrate are not fully specified in the abstract, limiting full assessment on reproducibility and exact discrete token design. Still, the core idea tightly matches the inclusion criteria related to discrete audio tokens used in downstream modalities including multi-modal LLMs, supporting its inclusion with some minor uncertainty due to missing detailed tokenization specifics in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The abstract mentions a 'semantic-acoustic disentangled speech tokenizer,' indicating a form of discrete tokenization of audio, aligning well with the inclusion criteria of studies focusing on discrete audio tokens generated by neural audio codecs. The study emphasizes omni-modal alignment with vision-language and speech, and proposes a novel method (EMOVA) with speech style control and demonstrated state-of-the-art performance, suggesting a significant contribution. However, the details on tokenizer architecture, quantization, vocabulary/codebook settings, or bitrate are not fully specified in the abstract, limiting full assessment on reproducibility and exact discrete token design. Still, the core idea tightly matches the inclusion criteria related to discrete audio tokens used in downstream modalities including multi-modal LLMs, supporting its inclusion with some minor uncertainty due to missing detailed tokenization specifics in the abstract.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Moshi: a speech-text foundation model for real-time dialogue",
    "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly states Moshi generates speech as tokens from the residual quantizer of a neural audio codec and uses time-aligned text tokens as a prefix to audio tokens, i.e., discrete audio tokens central to the model; this satisfies the inclusion criteria (token sources via neural audio codec/discrete units) and does not trigger any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly states Moshi generates speech as tokens from the residual quantizer of a neural audio codec and uses time-aligned text tokens as a prefix to audio tokens, i.e., discrete audio tokens central to the model; this satisfies the inclusion criteria (token sources via neural audio codec/discrete units) and does not trigger any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Moshi, a speech-text foundation model that utilizes a neural audio codec with residual quantization to generate speech tokens, modeling speech as discrete sequences, which aligns with the inclusion criteria of using neural audio codecs for discrete audio tokens. The paper details a method of generating discrete speech tokens and applies them in real-time full-duplex dialogue modeling, involving audio token generation, streaming speech recognition, and TTS, matching the research contribution and token usage requirements. Furthermore, the abstract is fully in English and provides enough details about the tokenizer architecture (residual quantizer of a neural audio codec) and its hierarchical generation, allowing assessability and reproducibility potential. Hence, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents Moshi, a speech-text foundation model that utilizes a neural audio codec with residual quantization to generate speech tokens, modeling speech as discrete sequences, which aligns with the inclusion criteria of using neural audio codecs for discrete audio tokens. The paper details a method of generating discrete speech tokens and applies them in real-time full-duplex dialogue modeling, involving audio token generation, streaming speech recognition, and TTS, matching the research contribution and token usage requirements. Furthermore, the abstract is fully in English and provides enough details about the tokenizer architecture (residual quantizer of a neural audio codec) and its hierarchical generation, allowing assessability and reproducibility potential. Hence, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
    "abstract": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
    "metadata": {
      "arxiv_id": "2410.01141",
      "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs",
      "summary": "This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.",
      "authors": [
        "Doohee You",
        "S Fraiberger"
      ],
      "published": "2024-10-02T00:43:10Z",
      "updated": "2025-06-30T18:26:08Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01141v3",
      "landing_url": "https://arxiv.org/abs/2410.01141v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.01141"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about deduplication techniques for NLP paper titles and semantic similarity; it discusses textual data and does not involve discrete audio tokens, tokenizers, or any token-based audio representation, hence it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about deduplication techniques for NLP paper titles and semantic similarity; it discusses textual data and does not involve discrete audio tokens, tokenizers, or any token-based audio representation, hence it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on deduplication techniques for economic research paper titles using NLP and LLMs and does not involve discrete audio tokens, neural audio codecs, or audio-related tokenization processes as required by the inclusion criteria; therefore, it does not meet the topic requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on deduplication techniques for economic research paper titles using NLP and LLMs and does not involve discrete audio tokens, neural audio codecs, or audio-related tokenization processes as required by the inclusion criteria; therefore, it does not meet the topic requirements and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
    "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
    "metadata": {
      "arxiv_id": "2410.01912",
      "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
      "summary": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
      "authors": [
        "Liang Chen",
        "Sinan Tan",
        "Zefan Cai",
        "Weichu Xie",
        "Haozhe Zhao",
        "Yichi Zhang",
        "Junyang Lin",
        "Jinze Bai",
        "Tianyu Liu",
        "Baobao Chang"
      ],
      "published": "2024-10-02T18:10:05Z",
      "updated": "2024-10-02T18:10:05Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01912v1",
      "landing_url": "https://arxiv.org/abs/2410.01912v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.01912"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a 2D autoregressive Transformer for image generation and vision–language capabilities, with no discussion of discrete audio tokens, tokenization, or audio-domain discretization, so it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a 2D autoregressive Transformer for image generation and vision–language capabilities, with no discussion of discrete audio tokens, tokenization, or audio-domain discretization, so it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a transformer model for image generation involving vision-language modalities, without any mention of audio, discrete audio tokens, audio codec, or discretization of audio waveforms, which are the core of the inclusion criteria. Therefore, it does not meet the topic requirements focused on discrete audio tokens or related audio modeling methodologies.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a transformer model for image generation involving vision-language modalities, without any mention of audio, discrete audio tokens, audio codec, or discretization of audio waveforms, which are the core of the inclusion criteria. Therefore, it does not meet the topic requirements focused on discrete audio tokens or related audio modeling methodologies.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
    "abstract": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly produces discrete speech tokens (semantic speech tokens) for streaming speech-to-speech translation and uses an acoustic codec to reconstruct waveforms, with English title/abstract, aligning with the discrete audio token criteria and excluding none of the criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly produces discrete speech tokens (semantic speech tokens) for streaming speech-to-speech translation and uses an acoustic codec to reconstruct waveforms, with English title/abstract, aligning with the discrete audio token criteria and excluding none of the criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a model that outputs discrete speech tokens from continuous speech input, which qualifies as discretization of audio waveforms into a finite vocabulary of tokens, aligning with the inclusion criteria. It uses acoustic language models and audio codec models to generate and decode these tokens, implying the use of neural audio codec or vector quantization approaches. The abstract provides sufficient English details on the tokenization process and downstream application to speech-to-speech translation with reported experimental results, supporting reproducibility and evaluation. Therefore, the study meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a model that outputs discrete speech tokens from continuous speech input, which qualifies as discretization of audio waveforms into a finite vocabulary of tokens, aligning with the inclusion criteria. It uses acoustic language models and audio codec models to generate and decode these tokens, implying the use of neural audio codec or vector quantization approaches. The abstract provides sufficient English details on the tokenization process and downstream application to speech-to-speech translation with reported experimental results, supporting reproducibility and evaluation. Therefore, the study meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SegINR: Segment-wise Implicit Neural Representation for Sequence Alignment in Neural Text-to-Speech",
    "abstract": "We present SegINR, a novel approach to neural Text-to-Speech (TTS) that addresses sequence alignment without relying on an auxiliary duration predictor and complex autoregressive (AR) or non-autoregressive (NAR) frame-level sequence modeling. SegINR simplifies the process by converting text sequences directly into frame-level features. It leverages an optimal text encoder to extract embeddings, transforming each into a segment of frame-level features using a conditional implicit neural representation (INR). This method, named segment-wise INR (SegINR), models temporal dynamics within each segment and autonomously defines segment boundaries, reducing computational costs. We integrate SegINR into a two-stage TTS framework, using it for semantic token prediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate that SegINR outperforms conventional methods in speech quality with computational efficiency.",
    "metadata": {
      "arxiv_id": "2410.04690",
      "title": "SegINR: Segment-wise Implicit Neural Representation for Sequence Alignment in Neural Text-to-Speech",
      "summary": "We present SegINR, a novel approach to neural Text-to-Speech (TTS) that addresses sequence alignment without relying on an auxiliary duration predictor and complex autoregressive (AR) or non-autoregressive (NAR) frame-level sequence modeling. SegINR simplifies the process by converting text sequences directly into frame-level features. It leverages an optimal text encoder to extract embeddings, transforming each into a segment of frame-level features using a conditional implicit neural representation (INR). This method, named segment-wise INR (SegINR), models temporal dynamics within each segment and autonomously defines segment boundaries, reducing computational costs. We integrate SegINR into a two-stage TTS framework, using it for semantic token prediction. Our experiments in zero-shot adaptive TTS scenarios demonstrate that SegINR outperforms conventional methods in speech quality with computational efficiency.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-10-07T02:04:58Z",
      "updated": "2024-10-07T02:04:58Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04690v1",
      "landing_url": "https://arxiv.org/abs/2410.04690v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04690"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on segment-wise implicit neural representations for TTS and frame-level features, with no explicit use or design of discrete audio tokens, tokenizers, or neural codecs, so it does not meet the inclusion criteria focused on discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on segment-wise implicit neural representations for TTS and frame-level features, with no explicit use or design of discrete audio tokens, tokenizers, or neural codecs, so it does not meet the inclusion criteria focused on discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes SegINR, a new neural TTS alignment method focusing on sequence-to-frame feature conversion, but it does not describe the use or design of discrete audio tokens (e.g., tokenization of continuous audio into discrete token sequences via neural audio codecs or quantization), nor does it provide the details on token vocabularies or quantization needed to determine if discrete tokens are central to the approach; hence, it fails the inclusion criteria centered on discrete audio tokens and focuses on continuous representation instead.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes SegINR, a new neural TTS alignment method focusing on sequence-to-frame feature conversion, but it does not describe the use or design of discrete audio tokens (e.g., tokenization of continuous audio into discrete token sequences via neural audio codecs or quantization), nor does it provide the details on token vocabularies or quantization needed to determine if discrete tokens are central to the approach; hence, it fails the inclusion criteria centered on discrete audio tokens and focuses on continuous representation instead.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mini-Batch Kernel $k$-means",
    "abstract": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetildeΩ(\\max \\{γ^{4}, γ^{2}\\} \\cdot ε^{-2})$, the algorithm terminates in $O(γ^2/ε)$ iterations with high probability, where $γ$ bounds the norm of points in feature space and $ε$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $γ=1$. Taking $ε= O(1)$ and $b=Θ(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time.",
    "metadata": {
      "arxiv_id": "2410.05902",
      "title": "Mini-Batch Kernel $k$-means",
      "summary": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetildeΩ(\\max \\{γ^{4}, γ^{2}\\} \\cdot ε^{-2})$, the algorithm terminates in $O(γ^2/ε)$ iterations with high probability, where $γ$ bounds the norm of points in feature space and $ε$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $γ=1$. Taking $ε= O(1)$ and $b=Θ(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time.",
      "authors": [
        "Ben Jourdan",
        "Gregory Schwartzman"
      ],
      "published": "2024-10-08T10:59:14Z",
      "updated": "2024-10-08T10:59:14Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.05902v1",
      "landing_url": "https://arxiv.org/abs/2410.05902v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.05902"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a mini-batch kernel k-means clustering algorithm and its runtime/accuracy analysis, with no discussion of discrete audio tokenization, token design, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a mini-batch kernel k-means clustering algorithm and its runtime/accuracy analysis, with no discussion of discrete audio tokenization, token design, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a mini-batch kernel k-means algorithm, which is a clustering method unrelated to discrete audio tokens or their generation, representation, or usage in audio-related tasks. There is no mention of discretization of audio waveforms, neural audio codecs, vector quantization, or any tokenization process relevant to the defined inclusion criteria. Therefore, this work does not meet the inclusion criteria and falls under the exclusion criteria pertaining to non-discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a mini-batch kernel k-means algorithm, which is a clustering method unrelated to discrete audio tokens or their generation, representation, or usage in audio-related tasks. There is no mention of discretization of audio waveforms, neural audio codecs, vector quantization, or any tokenization process relevant to the defined inclusion criteria. Therefore, this work does not meet the inclusion criteria and falls under the exclusion criteria pertaining to non-discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
    "abstract": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
    "metadata": {
      "arxiv_id": "2410.06016",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "authors": [
        "Yunkee Chae",
        "Woosung Choi",
        "Yuhta Takida",
        "Junghyun Koo",
        "Yukara Ikemiya",
        "Zhi Zhong",
        "Kin Wai Cheuk",
        "Marco A. Martínez-Ramírez",
        "Kyogu Lee",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2024-10-08T13:18:24Z",
      "updated": "2025-04-27T15:10:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06016v3",
      "landing_url": "https://arxiv.org/abs/2410.06016v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.06016"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on neural audio codecs using residual vector quantization with per-frame codebooks, i.e., discrete tokens generated via quantization, which aligns with the discrete audio token criteria and discusses token/codec design and training; it is English and contains actionable tokenization details, thus satisfying inclusion and not triggering exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper centers on neural audio codecs using residual vector quantization with per-frame codebooks, i.e., discrete tokens generated via quantization, which aligns with the discrete audio token criteria and discusses token/codec design and training; it is English and contains actionable tokenization details, thus satisfying inclusion and not triggering exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a variable bitrate residual vector quantization (RVQ) method for neural audio codecs which inherently involve discrete token sequences via vector quantization codebooks for audio representation. The abstract discusses codebook adaptation and training improvements for audio compression but does not explicitly describe token design choices, vocabulary or codebook settings, or downstream tasks like generation or multi-modal alignment. However, it focuses on discrete codebook quantization methods that produce discrete audio tokens, relevant to the defined topic \"Discrete Audio Tokens.\" Given the absence of explicit details about token sequences used in downstream tasks and evaluation, but clear relevance on the codec/token generation side, it moderately meets the inclusion criteria with some ambiguity.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study proposes a variable bitrate residual vector quantization (RVQ) method for neural audio codecs which inherently involve discrete token sequences via vector quantization codebooks for audio representation. The abstract discusses codebook adaptation and training improvements for audio compression but does not explicitly describe token design choices, vocabulary or codebook settings, or downstream tasks like generation or multi-modal alignment. However, it focuses on discrete codebook quantization methods that produce discrete audio tokens, relevant to the defined topic \"Discrete Audio Tokens.\" Given the absence of explicit details about token sequences used in downstream tasks and evaluation, but clear relevance on the codec/token generation side, it moderately meets the inclusion criteria with some ambiguity.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Restructuring Vector Quantization with the Rotation Trick",
    "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on enabling gradient flow through a vector quantization layer in VQ-VAEs, i.e., training mechanics of discrete latent representations, but does not present explicit discrete audio token production, token vocabulary design, or downstream audio-generation/understanding analyses specific to discrete audio tokens as defined in the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on enabling gradient flow through a vector quantization layer in VQ-VAEs, i.e., training mechanics of discrete latent representations, but does not present explicit discrete audio token production, token vocabulary design, or downstream audio-generation/understanding analyses specific to discrete audio tokens as defined in the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The work focuses on improving the vector quantization process in VQ-VAEs, which compress continuous inputs into discrete latent representations, akin to discrete token sequences; however, the paper does not explicitly discuss audio signals or audio tokenization, nor does it provide explicit design details or evaluations related to discrete audio tokens for downstream audio generation, understanding, or cross-modal tasks, limiting its direct relevance to the \"Discrete Audio Tokens\" theme.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The work focuses on improving the vector quantization process in VQ-VAEs, which compress continuous inputs into discrete latent representations, akin to discrete token sequences; however, the paper does not explicitly discuss audio signals or audio tokenization, nor does it provide explicit design details or evaluations related to discrete audio tokens for downstream audio generation, understanding, or cross-modal tasks, limiting its direct relevance to the \"Discrete Audio Tokens\" theme.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract proposes Sylber, a self-supervised method that derives discrete syllabic tokens from raw audio (tokenization, codebook-like segmentation, and token-to-speech generation), addressing efficiency and downstream modeling, which aligns with the Discrete Audio Tokens inclusion criteria and excludes only continuous-centric approaches.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract proposes Sylber, a self-supervised method that derives discrete syllabic tokens from raw audio (tokenization, codebook-like segmentation, and token-to-speech generation), addressing efficiency and downstream modeling, which aligns with the Discrete Audio Tokens inclusion criteria and excludes only continuous-centric approaches.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised learning framework producing structured syllabic embeddings from raw speech, which can be tokenized into discrete syllabic units enabling speech tokenization and spoken language modeling; it also discusses tokenization efficiency, bitrate reduction, and reconstruction from tokens, which aligns with the inclusion criteria focusing on discrete audio tokens derived from self-supervised models and used for downstream generative and modeling tasks with detailed tokenization methodology.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised learning framework producing structured syllabic embeddings from raw speech, which can be tokenized into discrete syllabic units enabling speech tokenization and spoken language modeling; it also discusses tokenization efficiency, bitrate reduction, and reconstruction from tokens, which aligns with the inclusion criteria focusing on discrete audio tokens derived from self-supervised models and used for downstream generative and modeling tasks with detailed tokenization methodology.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
    "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly discusses a discrete speech tokenizer (RVQGAN-based), provides English abstract and details on token rate/codec design, and focuses on discrete tokens for speech modeling, meeting inclusion criteria without triggering any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly discusses a discrete speech tokenizer (RVQGAN-based), provides English abstract and details on token rate/codec design, and focuses on discrete tokens for speech modeling, meeting inclusion criteria without triggering any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study specifically focuses on a neural audio codec (RVQGAN) generating discrete audio tokens at low bitrates with detailed descriptions of the tokenization process, bitrate, and evaluation, fulfilling the inclusion criteria of discrete audio token generation, comprehensive evaluation, and public release of data and model; it does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study specifically focuses on a neural audio codec (RVQGAN) generating discrete audio tokens at low bitrates with detailed descriptions of the tokenization process, bitrate, and evaluation, fulfilling the inclusion criteria of discrete audio token generation, comprehensive evaluation, and public release of data and model; it does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study investigates semantic token reweighting of text embeddings in CLIP, not any discrete audio tokenization or audio token design; thus it fails to meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study investigates semantic token reweighting of text embeddings in CLIP, not any discrete audio tokenization or audio token design; thus it fails to meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Semantic Token Reweighting for text embeddings in CLIP, a vision-language model, and does not involve discrete audio tokens or audio waveform discretization; therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage and is irrelevant to the topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Semantic Token Reweighting for text embeddings in CLIP, a vision-language model, and does not involve discrete audio tokens or audio waveform discretization; therefore, it does not meet the inclusion criteria related to discrete audio token generation or usage and is irrelevant to the topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
    "abstract": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
    "metadata": {
      "arxiv_id": "2410.10180",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "authors": [
        "Mingyuan Yan",
        "Jiawei Wu",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-10-14T05:58:11Z",
      "updated": "2024-10-14T05:58:11Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10180v1",
      "landing_url": "https://arxiv.org/abs/2410.10180v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10180"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on a general discrete latent token framework (GM-VQ/VQ-VAE) without explicit audio tokenization, audio codecs, or audio-specific experiments, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on a general discrete latent token framework (GM-VQ/VQ-VAE) without explicit audio tokenization, audio codecs, or audio-specific experiments, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a probabilistic vector quantization framework with Gaussian mixtures for latent discrete embeddings, framed within machine learning generative models. However, the abstract does not mention audio-specific applications, tokenization of audio waveforms, or explicit generation/use of discrete audio tokens for tasks like speech or music modeling, nor does it detail token vocabulary, codebook settings, or time subsampling related to audio. The work focuses more on general vector quantization and codebook utilization improvements in variational autoencoders without clear connection to discrete audio token design, discrete audio representation, or downstream audio generation/understanding tasks required by the inclusion criteria. Therefore, it fails to meet critical inclusion criteria related to discrete audio tokens and their practical design or use for audio sequence modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a probabilistic vector quantization framework with Gaussian mixtures for latent discrete embeddings, framed within machine learning generative models. However, the abstract does not mention audio-specific applications, tokenization of audio waveforms, or explicit generation/use of discrete audio tokens for tasks like speech or music modeling, nor does it detail token vocabulary, codebook settings, or time subsampling related to audio. The work focuses more on general vector quantization and codebook utilization improvements in variational autoencoders without clear connection to discrete audio token design, discrete audio representation, or downstream audio generation/understanding tasks required by the inclusion criteria. Therefore, it fails to meet critical inclusion criteria related to discrete audio tokens and their practical design or use for audio sequence modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
    "abstract": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
    "metadata": {
      "arxiv_id": "2410.11062",
      "title": "CleanUMamba: A Compact Mamba Network for Speech Denoising using Channel Pruning",
      "summary": "This paper presents CleanUMamba, a time-domain neural network architecture designed for real-time causal audio denoising directly applied to raw waveforms. CleanUMamba leverages a U-Net encoder-decoder structure, incorporating the Mamba state-space model in the bottleneck layer. By replacing conventional self-attention and LSTM mechanisms with Mamba, our architecture offers superior denoising performance while maintaining a constant memory footprint, enabling streaming operation. To enhance efficiency, we applied structured channel pruning, achieving an 8X reduction in model size without compromising audio quality. Our model demonstrates strong results in the Interspeech 2020 Deep Noise Suppression challenge. Specifically, CleanUMamba achieves a PESQ score of 2.42 and STOI of 95.1% with only 442K parameters and 468M MACs, matching or outperforming larger models in real-time performance. Code will be available at: https://github.com/lab-emi/CleanUMamba",
      "authors": [
        "Sjoerd Groot",
        "Qinyu Chen",
        "Jan C. van Gemert",
        "Chang Gao"
      ],
      "published": "2024-10-14T20:18:03Z",
      "updated": "2025-02-10T18:07:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11062v2",
      "landing_url": "https://arxiv.org/abs/2410.11062v2",
      "doi": "https://doi.org/10.1109/ISCAS56072.2025.11043389"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work describes a time-domain neural network for speech denoising with channel pruning, operating on raw waveforms (continuous signals) and does not introduce or analyze discrete audio tokens, tokenizers, or codebooks, thus it does not meet the token-centric inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work describes a time-domain neural network for speech denoising with channel pruning, operating on raw waveforms (continuous signals) and does not introduce or analyze discrete audio tokens, tokenizers, or codebooks, thus it does not meet the token-centric inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a neural network architecture for real-time audio denoising operating directly on raw waveforms and emphasizes model size reduction via pruning. However, it does not mention discretizing continuous audio waveforms into discrete token sequences, no neural audio codec or vector quantization methods or tokenization processes are described, and no references to discrete audio tokens or downstream generative tasks involving discrete tokens are made. Therefore, it does not satisfy the inclusion criteria about discrete audio tokens and mainly concerns continuous audio representations for denoising, which aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a neural network architecture for real-time audio denoising operating directly on raw waveforms and emphasizes model size reduction via pruning. However, it does not mention discretizing continuous audio waveforms into discrete token sequences, no neural audio codec or vector quantization methods or tokenization processes are described, and no references to discrete audio tokens or downstream generative tasks involving discrete tokens are made. Therefore, it does not satisfy the inclusion criteria about discrete audio tokens and mainly concerns continuous audio representations for denoising, which aligns with exclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
    "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
    "metadata": {
      "arxiv_id": "2410.12359",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "authors": [
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-16T08:21:37Z",
      "updated": "2025-06-11T08:43:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12359v2",
      "landing_url": "https://arxiv.org/abs/2410.12359v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.12359"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on discrete token representations via residual vector quantization and multi-codebook optimization in neural audio codecs, directly addressing token generation/quantization for downstream modeling, thus matching the 'Discrete Audio Tokens' topic.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on discrete token representations via residual vector quantization and multi-codebook optimization in neural audio codecs, directly addressing token generation/quantization for downstream modeling, thus matching the 'Discrete Audio Tokens' topic.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing residual vector quantization (RVQ) in neural audio codecs to mitigate codebook collapse and improve codec performance, which directly relates to generating discrete tokens from continuous audio through vector quantization, fitting the topic definition of \"Discrete Audio Tokens.\" The paper describes token generation via neural audio codec mechanisms, addresses multi-codebook optimization, evaluates effects on downstream tasks including large language models and zero-shot TTS, and provides experimental results that support reproducibility—all meeting inclusion criteria without exclusion issues.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing residual vector quantization (RVQ) in neural audio codecs to mitigate codebook collapse and improve codec performance, which directly relates to generating discrete tokens from continuous audio through vector quantization, fitting the topic definition of \"Discrete Audio Tokens.\" The paper describes token generation via neural audio codec mechanisms, addresses multi-codebook optimization, evaluates effects on downstream tasks including large language models and zero-shot TTS, and provides experimental results that support reproducibility—all meeting inclusion criteria without exclusion issues.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "abstract": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly targets discrete speech tokens by distilling multimodal representations (acoustic, semantic, contextual) into a tokenizer using a Residual Vector Quantizer, includes LM/SM-guided distillation, provides English title/abstract and detailed methodology, and reports quantitative improvements on LibriSpeech, aligning with the 'Discrete Audio Tokens' criteria and downstream task relevance; it is not limited to continuous representations and presents token design and evaluation, meeting inclusion while avoiding exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly targets discrete speech tokens by distilling multimodal representations (acoustic, semantic, contextual) into a tokenizer using a Residual Vector Quantizer, includes LM/SM-guided distillation, provides English title/abstract and detailed methodology, and reports quantitative improvements on LibriSpeech, aligning with the 'Discrete Audio Tokens' criteria and downstream task relevance; it is not limited to continuous representations and presents token design and evaluation, meeting inclusion while avoiding exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel method (DM-Codec) for distilling multimodal representations into a discrete speech tokenizer capable of converting continuous speech waveforms into discrete tokens incorporating acoustic, semantic, and contextual information, using vector quantization within a neural codec framework; it reports detailed experimental results on tokenization performance, speech quality, and intelligibility, and includes code and checkpoints for reproducibility, aligning well with the inclusion criteria and not meeting any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel method (DM-Codec) for distilling multimodal representations into a discrete speech tokenizer capable of converting continuous speech waveforms into discrete tokens incorporating acoustic, semantic, and contextual information, using vector quantization within a neural codec framework; it reports detailed experimental results on tokenization performance, speech quality, and intelligibility, and includes code and checkpoints for reproducibility, aligning well with the inclusion criteria and not meeting any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Residual vector quantization for KV cache compression in large language model",
    "abstract": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
    "metadata": {
      "arxiv_id": "2410.15704",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "authors": [
        "Ankur Kumar"
      ],
      "published": "2024-10-21T07:20:41Z",
      "updated": "2024-10-21T07:20:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15704v1",
      "landing_url": "https://arxiv.org/abs/2410.15704v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15704"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此文內容聚焦對大型語言模型的 KV 快取進行 residual vector quantization 壓縮，屬於模型壓縮/量化領域，與離散音訊 token 的產生、編碼與下游跨模態任務之核心需求不符，因此不符合本主題，應排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此文內容聚焦對大型語言模型的 KV 快取進行 residual vector quantization 壓縮，屬於模型壓縮/量化領域，與離散音訊 token 的產生、編碼與下游跨模態任務之核心需求不符，因此不符合本主題，應排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on residual vector quantization for KV cache compression in large language models, which is related to model compression and efficiency, but it does not involve discretization of continuous audio waveforms into token sequences nor does it address discrete audio tokens, neural audio codecs, or audio generation/understanding tasks as required by the inclusion criteria. Hence, it does not fit the thematic scope centered on discrete audio tokens for audio modeling or cross-modal tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on residual vector quantization for KV cache compression in large language models, which is related to model compression and efficiency, but it does not involve discretization of continuous audio waveforms into token sequences nor does it address discrete audio tokens, neural audio codecs, or audio generation/understanding tasks as required by the inclusion criteria. Hence, it does not fit the thematic scope centered on discrete audio tokens for audio modeling or cross-modal tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
    "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "LSCodec explicitly proposes a discrete speech tokenization pipeline (continuous bottleneck → vector quantization → discrete token space) with a low bitrate and speaker decoupling, plus downstream evaluation, satisfying the discrete-audio-token criteria and not triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "LSCodec explicitly proposes a discrete speech tokenization pipeline (continuous bottleneck → vector quantization → discrete token space) with a low bitrate and speaker decoupling, plus downstream evaluation, satisfying the discrete-audio-token criteria and not triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly describes a neural audio codec (LSCodec) that produces discrete, speaker-decoupled speech tokens using vector quantization and a multi-stage training framework, targeting low bitrate and speaker disentanglement, which aligns with the inclusion criteria on discrete audio tokens for generation and understanding; it provides English title and abstract with technical details on tokenization and codec design, fulfilling reproducibility needs. There is no indication that the work focuses solely on continuous representations, traditional codec standards, purely discriminative tasks, or lacks English details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly describes a neural audio codec (LSCodec) that produces discrete, speaker-decoupled speech tokens using vector quantization and a multi-stage training framework, targeting low bitrate and speaker disentanglement, which aligns with the inclusion criteria on discrete audio tokens for generation and understanding; it provides English title and abstract with technical details on tokenization and codec design, fulfilling reproducibility needs. There is no indication that the work focuses solely on continuous representations, traditional codec standards, purely discriminative tasks, or lacks English details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Pyramid Vector Quantization for LLMs",
    "abstract": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
    "metadata": {
      "arxiv_id": "2410.16926",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "authors": [
        "Tycho F. A. van der Ouderaa",
        "Maximilian L. Croci",
        "Agrin Hilmkil",
        "James Hensman"
      ],
      "published": "2024-10-22T11:57:32Z",
      "updated": "2024-12-04T10:52:04Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16926v2",
      "landing_url": "https://arxiv.org/abs/2410.16926v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16926"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Not about discrete audio tokens; the paper targets PVQ-based weight quantization for LLMs, with no discrete audio token generation, tokenizer design, or audio-domain evaluation.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Not about discrete audio tokens; the paper targets PVQ-based weight quantization for LLMs, with no discrete audio token generation, tokenizer design, or audio-domain evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on quantization techniques for large language models (LLMs) to optimize weight representation and model performance, without discussing discrete audio tokens or audio waveform discretization. It lacks any relevance to discrete audio token generation, neural audio codecs, or audio-related tasks as specified in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on quantization techniques for large language models (LLMs) to optimize weight representation and model performance, without discussing discrete audio tokens or audio waveform discretization. It lacks any relevance to discrete audio token generation, neural audio codecs, or audio-related tasks as specified in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokenizer in Text To Speech",
    "abstract": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on a continuous speech tokenizer and does not provide a discrete-token generation method (vocabulary/codebooks) or downstream evaluations based on discrete tokens, thus failing the inclusion criteria and aligning with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on a continuous speech tokenizer and does not provide a discrete-token generation method (vocabulary/codebooks) or downstream evaluations based on discrete tokens, thus failing the inclusion criteria and aligning with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a continuous speech tokenizer (Cont-SPT) aimed at improving information preservation over discrete speech tokenizers in text-to-speech; however, it focuses on continuous tokens rather than discrete audio tokens generated via neural audio codecs or self-supervised models with clear quantization/vocabulary details. It lacks explicit mention of discretization methods, token vocabularies, or quantization mechanisms, which are essential for inclusion, and the main contribution centers on continuous tokenization, thus not meeting the discrete token inclusion criteria and aligns with exclusion due to continuous representation focus without discrete token details.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a continuous speech tokenizer (Cont-SPT) aimed at improving information preservation over discrete speech tokenizers in text-to-speech; however, it focuses on continuous tokens rather than discrete audio tokens generated via neural audio codecs or self-supervised models with clear quantization/vocabulary details. It lacks explicit mention of discretization methods, token vocabularies, or quantization mechanisms, which are essential for inclusion, and the main contribution centers on continuous tokenization, thus not meeting the discrete token inclusion criteria and aligns with exclusion due to continuous representation focus without discrete token details.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Inference with K-means",
    "abstract": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
    "metadata": {
      "arxiv_id": "2410.17256",
      "title": "Inference with K-means",
      "summary": "This thesis aims to invent new approaches for making inferences with the k-means algorithm. k-means is an iterative clustering algorithm that randomly assigns k centroids, then assigns data points to the nearest centroid, and updates centroids based on the mean of assigned points. This process continues until convergence, forming k clusters where each point belongs to the closest centroid. This research investigates the prediction of the last component of data points obtained from a distribution of clustered data using the online balanced k-means approach. Through extensive experimentation and analysis, key findings have emerged. It is observed that a larger number of clusters or partitions tends to yield lower errors while increasing the number of assigned data points does not significantly improve inference errors. Reducing losses in the learning process does not significantly impact overall inference errors. Indicating that as learning is going on inference errors remain unchanged. Recommendations include the need for specialized inference techniques to estimate better data points derived from multi-clustered data and exploring methods that yield improved results with larger assigned datasets. By addressing these recommendations, this research advances the accuracy and reliability of inferences made with the k-means algorithm, bridging the gap between clustering and non-parametric density estimation and inference.",
      "authors": [
        "Alfred K. Adzika",
        "Prudence Djagba"
      ],
      "published": "2024-10-04T06:51:58Z",
      "updated": "2024-10-04T06:51:58Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17256v1",
      "landing_url": "https://arxiv.org/abs/2410.17256v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.17256"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discusses inference with K-means clustering and online balanced k-means; there is no discussion of discrete audio tokens, tokenizers, or audio generation/understanding tasks, so it does not meet the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discusses inference with K-means clustering and online balanced k-means; there is no discussion of discrete audio tokens, tokenizers, or audio generation/understanding tasks, so it does not meet the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on k-means clustering algorithms to improve inference, with no mention of discrete audio tokens, neural audio codecs, vector quantization, or any audio-related tokenization process; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their usage in audio or multimodal modeling, and it meets exclusion criteria regarding lack of discrete token generation or usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on k-means clustering algorithms to improve inference, with no mention of discrete audio tokens, neural audio codecs, vector quantization, or any audio-related tokenization process; thus, it does not meet the inclusion criteria centered on discrete audio tokens and their usage in audio or multimodal modeling, and it meets exclusion criteria regarding lack of discrete token generation or usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals",
    "abstract": "Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.",
    "metadata": {
      "arxiv_id": "2410.18283",
      "title": "Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals",
      "summary": "Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.",
      "authors": [
        "Srihari Kamesh Kompella",
        "Kemal Davaslioglu",
        "Yalin E. Sagduyu",
        "Sastry Kompella"
      ],
      "published": "2024-10-23T21:17:45Z",
      "updated": "2024-10-23T21:17:45Z",
      "categories": [
        "cs.LG",
        "cs.NI",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.18283v1",
      "landing_url": "https://arxiv.org/abs/2410.18283v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.18283"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on data augmentation for RF signal classification using Vector-Quantized VAE but does not center on discrete audio tokens or tokenization design/usage as required by the inclusion criteria, so it should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on data augmentation for RF signal classification using Vector-Quantized VAE but does not center on discrete audio tokens or tokenization design/usage as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using a Vector-Quantized Variational Autoencoder (VQ-VAE) to generate synthetic RF signals for classification enhancement; however, the subject is on radio frequency (RF) wireless signals rather than audio signals and does not involve discrete audio tokens derived from audio waveform discretization or neural audio codecs for audio generation, understanding, or multimodal modeling. Thus, it does not meet the inclusion criteria related to discrete audio tokens and is more aligned with a discriminative classification task outside of audio token representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using a Vector-Quantized Variational Autoencoder (VQ-VAE) to generate synthetic RF signals for classification enhancement; however, the subject is on radio frequency (RF) wireless signals rather than audio signals and does not involve discrete audio tokens derived from audio waveform discretization or neural audio codecs for audio generation, understanding, or multimodal modeling. Thus, it does not meet the inclusion criteria related to discrete audio tokens and is more aligned with a discriminative classification task outside of audio token representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
    "abstract": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
    "metadata": {
      "arxiv_id": "2410.19199",
      "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
      "summary": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
      "authors": [
        "Suparna De",
        "Ionut Bostan",
        "Nishanth Sastry"
      ],
      "published": "2024-10-24T23:18:02Z",
      "updated": "2024-10-24T23:18:02Z",
      "categories": [
        "cs.SI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19199v1",
      "landing_url": "https://arxiv.org/abs/2410.19199v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.19199"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on emotion-aware TTS for accessibility but does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on emotion-aware TTS for accessibility but does not discuss discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes an end-to-end Text-to-Speech synthesis system that focuses on emotional speech generation and integrates NLP and speech synthesis techniques for accessibility applications. However, the abstract does not mention the use of discrete audio tokens, neural audio codecs, or any discretization methods transforming continuous audio waveforms into token sequences, nor does it detail tokenizer/codec architectures, vector quantization, or codebook settings. Therefore, it does not meet the core inclusion criteria focusing on research centered on discrete audio token representations and their use or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study describes an end-to-end Text-to-Speech synthesis system that focuses on emotional speech generation and integrates NLP and speech synthesis techniques for accessibility applications. However, the abstract does not mention the use of discrete audio tokens, neural audio codecs, or any discretization methods transforming continuous audio waveforms into token sequences, nor does it detail tokenizer/codec architectures, vector quantization, or codebook settings. Therefore, it does not meet the core inclusion criteria focusing on research centered on discrete audio token representations and their use or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
    "metadata": {
      "arxiv_id": "2410.20336",
      "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
      "summary": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
      "authors": [
        "Maohao Shen",
        "Shun Zhang",
        "Jilong Wu",
        "Zhiping Xiu",
        "Ehab AlBadawy",
        "Yiting Lu",
        "Mike Seltzer",
        "Qing He"
      ],
      "published": "2024-10-27T04:28:57Z",
      "updated": "2024-10-27T04:28:57Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20336v1",
      "landing_url": "https://arxiv.org/abs/2410.20336v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.20336"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on late-fusion LLM-based speech generation and multimodal dialogue; there is no discussion of discrete audio tokens, tokenizers/codecs, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on late-fusion LLM-based speech generation and multimodal dialogue; there is no discussion of discrete audio tokens, tokenizers/codecs, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech generation using large language models but does not explicitly describe the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it provide sufficient detail on tokenization or quantization processes necessary to meet the inclusion criteria centered on discrete token methodology; thus, it fails to satisfy the core requirements of the topic.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech generation using large language models but does not explicitly describe the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it provide sufficient detail on tokenization or quantization processes necessary to meet the inclusion criteria centered on discrete token methodology; thus, it fails to satisfy the core requirements of the topic.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
    "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
    "metadata": {
      "arxiv_id": "2410.20573",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "authors": [
        "Mohammad Hassan Vali",
        "Tom Bäckström"
      ],
      "published": "2024-10-27T19:56:02Z",
      "updated": "2025-07-02T10:27:13Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20573v2",
      "landing_url": "https://arxiv.org/abs/2410.20573v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.20573"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on interpreting GAN latent spaces in images using space-filling vector quantization, not on discrete audio tokens or audio-specific tokenization/codec design, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on interpreting GAN latent spaces in images using space-filling vector quantization, not on discrete audio tokens or audio-specific tokenization/codec design, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised interpretation of latent spaces in GANs using space-filling vector quantization to model image generation latent spaces, which is unrelated to discrete audio tokens or audio tokenization methods. It neither addresses discretization of continuous audio waveforms nor the use of discrete audio tokens for audio generation, understanding, or cross-modal modeling, and thus does not meet any inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised interpretation of latent spaces in GANs using space-filling vector quantization to model image generation latent spaces, which is unrelated to discrete audio tokens or audio tokenization methods. It neither addresses discretization of continuous audio waveforms nor the use of discrete audio tokens for audio generation, understanding, or cross-modal modeling, and thus does not meet any inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
    "abstract": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
    "metadata": {
      "arxiv_id": "2410.21951",
      "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative Decoding",
      "summary": "The auto-regressive architecture, like GPTs, is widely used in modern Text-to-Speech (TTS) systems. However, it incurs substantial inference time, particularly due to the challenges in the next-token prediction posed by lengthy sequences of speech tokens. In this work, we introduce VADUSA, one of the first approaches to accelerate auto-regressive TTS through speculative decoding. Our results show that VADUSA not only significantly improves inference speed but also enhances performance by incorporating draft heads to predict future speech content auto-regressively. Furthermore, the inclusion of a tolerance mechanism during sampling accelerates inference without compromising quality. Our approach demonstrates strong generalization across large datasets and various types of speech tokens.",
      "authors": [
        "Bohan Li",
        "Hankun Wang",
        "Situo Zhang",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2024-10-29T11:12:01Z",
      "updated": "2025-02-10T04:22:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21951v2",
      "landing_url": "https://arxiv.org/abs/2410.21951v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.21951"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses accelerating autoregressive TTS using speculative decoding and mentions 'speech tokens', but it does not provide explicit details on discrete tokenization/codec design (tokenizers, codebooks, or discrete token generation as the core representation). Therefore it remains unclear whether it fulfills the discrete audio token criteria as defined, and it may not satisfy the token-centric inclusion requirement.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses accelerating autoregressive TTS using speculative decoding and mentions 'speech tokens', but it does not provide explicit details on discrete tokenization/codec design (tokenizers, codebooks, or discrete token generation as the core representation). Therefore it remains unclear whether it fulfills the discrete audio token criteria as defined, and it may not satisfy the token-centric inclusion requirement.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on accelerating auto-regressive TTS with speculative decoding, but the abstract does not mention any discrete audio tokenization details such as neural audio codecs, quantization, vocabulary settings, or explicit token generation processes; thus, it is unclear whether the method uses discrete audio tokens as defined by the inclusion criteria. Without detailed information on discrete token generation or usage, the paper does not fully meet the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on accelerating auto-regressive TTS with speculative decoding, but the abstract does not mention any discrete audio tokenization details such as neural audio codecs, quantization, vocabulary settings, or explicit token generation processes; thus, it is unclear whether the method uses discrete audio tokens as defined by the inclusion criteria. Without detailed information on discrete token generation or usage, the paper does not fully meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed the two junior reviewers' comments and the abstract: both note the paper targets accelerating auto-regressive TTS (VADUSA) and refers to \"speech tokens,\" but the abstract gives no explicit description of discrete audio tokenization (e.g., neural codec/tokenizer architecture, quantization/codebook/vocabulary, bitrate or time downsampling). The inclusion criteria require clear, reproducible details about discrete audio token generation or use; that requirement is not satisfied here, so we cannot confirm this paper centers on discrete audio tokens. Therefore it is better to exclude this item.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "I reviewed the two junior reviewers' comments and the abstract: both note the paper targets accelerating auto-regressive TTS (VADUSA) and refers to \"speech tokens,\" but the abstract gives no explicit description of discrete audio tokenization (e.g., neural codec/tokenizer architecture, quantization/codebook/vocabulary, bitrate or time downsampling). The inclusion criteria require clear, reproducible details about discrete audio token generation or use; that requirement is not satisfied here, so we cannot confirm this paper centers on discrete audio tokens. Therefore it is better to exclude this item.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly focuses on discrete speech tokens produced by neural audio codec/self-supervised tokenization, analyzes token design and its impact on downstream SLM tasks, and thus aligns with the 'Discrete Audio Tokens' topic and related evaluation criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly focuses on discrete speech tokens produced by neural audio codec/self-supervised tokenization, analyzes token design and its impact on downstream SLM tasks, and thus aligns with the 'Discrete Audio Tokens' topic and related evaluation criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes DC-Spin, a speaker-invariant speech tokenizer that generates discrete speech tokens with phonetic information for spoken language models. It focuses on tokenization bridging audio signals and SLM tokens, comparing tokenization methods including self-supervised and neural audio codecs, indicating the presence of discrete token generation. The tokenizer details, evaluation on downstream tasks, and mentions of multi-codebook and phoneme alignment align with the inclusion criteria emphasizing discrete audio tokens and their design/impacts on speech modeling and generation. The abstract is in English and provides clear methodological insights, supporting reproducibility judgment. Therefore, it meets the inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper proposes DC-Spin, a speaker-invariant speech tokenizer that generates discrete speech tokens with phonetic information for spoken language models. It focuses on tokenization bridging audio signals and SLM tokens, comparing tokenization methods including self-supervised and neural audio codecs, indicating the presence of discrete token generation. The tokenizer details, evaluation on downstream tasks, and mentions of multi-codebook and phoneme alignment align with the inclusion criteria emphasizing discrete audio tokens and their design/impacts on speech modeling and generation. The abstract is in English and provides clear methodological insights, supporting reproducibility judgment. Therefore, it meets the inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Collaborative Navigation through Perception Fusion for Multi-Robots in Unknown Environments",
    "abstract": "For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.",
    "metadata": {
      "arxiv_id": "2411.01274",
      "title": "Efficient Collaborative Navigation through Perception Fusion for Multi-Robots in Unknown Environments",
      "summary": "For tasks conducted in unknown environments with efficiency requirements, real-time navigation of multi-robot systems remains challenging due to unfamiliarity with surroundings.In this paper, we propose a novel multi-robot collaborative planning method that leverages the perception of different robots to intelligently select search directions and improve planning efficiency. Specifically, a foundational planner is employed to ensure reliable exploration towards targets in unknown environments and we introduce Graph Attention Architecture with Information Gain Weight(GIWT) to synthesizes the information from the target robot and its teammates to facilitate effective navigation around obstacles.In GIWT, after regionally encoding the relative positions of the robots along with their perceptual features, we compute the shared attention scores and incorporate the information gain obtained from neighboring robots as a supplementary weight. We design a corresponding expert data generation scheme to simulate real-world decision-making conditions for network training. Simulation experiments and real robot tests demonstrates that the proposed method significantly improves efficiency and enables collaborative planning for multiple robots. Our method achieves approximately 82% accuracy on the expert dataset and reduces the average path length by about 8% and 6% across two types of tasks compared to the fundamental planner in ROS tests, and a path length reduction of over 6% in real-world experiments.",
      "authors": [
        "Qingquan Lin",
        "Weining Lu",
        "Litong Meng",
        "Chenxi Li",
        "Bin Liang"
      ],
      "published": "2024-11-02T14:53:26Z",
      "updated": "2024-11-02T14:53:26Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01274v1",
      "landing_url": "https://arxiv.org/abs/2411.01274v1",
      "doi": "https://doi.org/10.1016/j.neucom.2025.132444"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns multi-robot navigation and perception fusion with graph attention; it does not discuss discrete audio tokens, tokenization, or any token-based representation, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns multi-robot navigation and perception fusion with graph attention; it does not discuss discrete audio tokens, tokenization, or any token-based representation, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multi-robot collaborative navigation and planning in unknown environments, with no mention of discrete audio tokens, neural audio codecs, or any form of audio discretization or tokenization related to audio signals. The topic is unrelated to audio processing or discrete audio token modeling, and therefore does not meet any inclusion criteria nor does it fit the scope of the survey on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multi-robot collaborative navigation and planning in unknown environments, with no mention of discrete audio tokens, neural audio codecs, or any form of audio discretization or tokenization related to audio signals. The topic is unrelated to audio processing or discrete audio token modeling, and therefore does not meet any inclusion criteria nor does it fit the scope of the survey on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
    "abstract": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
    "metadata": {
      "arxiv_id": "2411.01407",
      "title": "Reducing Data Fragmentation in Data Deduplication Systems via Partial Repetition and Coding",
      "summary": "Data deduplication, one of the key features of modern Big Data storage devices, is the process of removing replicas of data chunks stored by different users. Despite the importance of deduplication, several drawbacks of the method, such as storage robustness and file fragmentation, have not been previously analyzed from a theoretical point of view. Storage robustness pertains to ensuring that deduplicated data can be used to reconstruct the original files without service disruptions and data loss. Fragmentation pertains to the problems of placing deduplicated data chunks of different user files in a proximity-preserving linear order, since neighboring chunks of the same file may be stored in sectors far apart on the server. This work proposes a new theoretical model for data fragmentation and introduces novel graph- and coding-theoretic approaches for reducing fragmentation via limited duplication (repetition coding) and coded deduplication (e.g., linear coding). In addition to alleviating issues with fragmentation, limited duplication and coded deduplication can also serve the dual purpose of increasing the robusteness of the system design. The contributions of our work are three-fold. First, we describe a new model for file structures in the form of self-avoiding (simple) paths in specialized graphs. Second, we introduce several new metrics for measuring the fragmentation level in deduplication systems on graph-structured files, including the stretch metric that captures the worst-case \"spread\" of adjacent data chunks within a file when deduplicated and placed on the server; and, the jump metric that captures the worst-case number of times during the reconstruction process of a file that one has to change the readout location on the server. For the stretch metric, we establish a connection between the level of fragmentation and the bandwidth of the file-graph. In particular, ...",
      "authors": [
        "Yun-Han Li",
        "Jin Sima",
        "Ilan Shomorony",
        "Olgica Milenkovic"
      ],
      "published": "2024-11-03T02:14:03Z",
      "updated": "2024-11-03T02:14:03Z",
      "categories": [
        "cs.IT",
        "cs.DM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01407v1",
      "landing_url": "https://arxiv.org/abs/2411.01407v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.01407"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss data deduplication and fragmentation in storage systems, with no mention of discrete audio tokens, tokenization methods, or audio-domain evaluation, thus it does not meet the discrete audio tokens criteria and aligns with exclusion aspects.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss data deduplication and fragmentation in storage systems, with no mention of discrete audio tokens, tokenization methods, or audio-domain evaluation, thus it does not meet the discrete audio tokens criteria and aligns with exclusion aspects.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on data deduplication and fragmentation in storage systems, dealing with theoretical and coding-theoretic approaches in data storage robustness, but does not concern discrete audio tokens, their generation, or application in audio modeling or generation tasks as required by the inclusion criteria. It lacks any discussion of audio or discretized tokens for audio-related modeling, thus not meeting the core subject requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on data deduplication and fragmentation in storage systems, dealing with theoretical and coding-theoretic approaches in data storage robustness, but does not concern discrete audio tokens, their generation, or application in audio modeling or generation tasks as required by the inclusion criteria. It lacks any discussion of audio or discretized tokens for audio-related modeling, thus not meeting the core subject requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
    "abstract": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
    "metadata": {
      "arxiv_id": "2411.04257",
      "title": "LSHBloom: Memory-efficient, Extreme-scale Document Deduplication",
      "summary": "Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.",
      "authors": [
        "Arham Khan",
        "Robert Underwood",
        "Carlo Siebenschuh",
        "Yadu Babuji",
        "Aswathy Ajith",
        "Kyle Hippe",
        "Ozan Gokdemir",
        "Alexander Brace",
        "Kyle Chard",
        "Ian Foster"
      ],
      "published": "2024-11-06T21:00:45Z",
      "updated": "2025-12-02T12:52:27Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04257v3",
      "landing_url": "https://arxiv.org/abs/2411.04257v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.04257"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on memory-efficient document deduplication for LLM training data and does not involve discrete audio tokens, tokenizers, codecs, or analysis of token design, thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on memory-efficient document deduplication for LLM training data and does not involve discrete audio tokens, tokenizers, codecs, or analysis of token design, thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on text document deduplication for large language model training datasets and does not address discrete audio tokens, nor does it discuss discretization of audio waveforms or neural audio codecs relevant to audio tokenization, thus not meeting the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on text document deduplication for large language model training datasets and does not address discrete audio tokens, nor does it discuss discretization of audio waveforms or neural audio codecs relevant to audio tokenization, thus not meeting the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
    "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
    "metadata": {
      "arxiv_id": "2411.04530",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "authors": [
        "Crystina Zhang",
        "Jing Lu",
        "Vinh Q. Tran",
        "Tal Schuster",
        "Donald Metzler",
        "Jimmy Lin"
      ],
      "published": "2024-11-07T08:38:32Z",
      "updated": "2025-11-19T00:30:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04530v2",
      "landing_url": "https://arxiv.org/abs/2411.04530v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.04530"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses subword-level semantics in multilingual language models and does not involve discrete audio tokens, tokenizers, or token-based audio representations, hence does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses subword-level semantics in multilingual language models and does not involve discrete audio tokens, tokenizers, or token-based audio representations, hence does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on multilingual language models and semantic understanding based on subword-level tokens in text, rather than discrete audio tokens derived from continuous audio waveforms or neural audio codecs. It does not involve discretization of audio, neural audio codec, or audio token generation, and lacks any reference to audio-related tasks or tokenization relevant to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on multilingual language models and semantic understanding based on subword-level tokens in text, rather than discrete audio tokens derived from continuous audio waveforms or neural audio codecs. It does not involve discretization of audio, neural audio codec, or audio token generation, and lacks any reference to audio-related tasks or tokenization relevant to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
    "abstract": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
    "metadata": {
      "arxiv_id": "2411.06968",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "authors": [
        "Yoshiki Masuyama",
        "Koichi Miyazaki",
        "Masato Murata"
      ],
      "published": "2024-11-11T13:17:24Z",
      "updated": "2024-11-11T13:17:24Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06968v1",
      "landing_url": "https://arxiv.org/abs/2411.06968v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06968"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses a decoder-only ASR model using speech tokens but does not specify the generation or use of discrete audio tokens/tokenizers (e.g., vector-quantized codes, multi-codebook tokens) as its core representation, so it does not satisfy the inclusion criteria focused on discrete audio token design and evaluation; it aligns more with continuous input representations and traditional ASR architectures, thus should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses a decoder-only ASR model using speech tokens but does not specify the generation or use of discrete audio tokens/tokenizers (e.g., vector-quantized codes, multi-codebook tokens) as its core representation, so it does not satisfy the inclusion criteria focused on discrete audio token design and evaluation; it aligns more with continuous input representations and traditional ASR architectures, thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses a Mamba-based decoder-only approach for speech recognition focusing on state space models and transformer-based comparisons, but it does not describe generating or using discrete audio tokens derived from neural audio codec or self-supervised model quantization, nor does it detail tokenization or vocabulary settings related to discrete audio tokens; thus, it does not meet the inclusion criteria related to discrete token-based audio representations or their systematic analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses a Mamba-based decoder-only approach for speech recognition focusing on state space models and transformer-based comparisons, but it does not describe generating or using discrete audio tokens derived from neural audio codec or self-supervised model quantization, nor does it detail tokenization or vocabulary settings related to discrete audio tokens; thus, it does not meet the inclusion criteria related to discrete token-based audio representations or their systematic analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
    "abstract": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item clearly investigates discrete speech tokens and includes a systematic comparison and downstream evaluation with an LLM, aligning with the contribution criteria for Discrete Audio Tokens; token-source details are not fully stated in the abstract, but the paper appears to meet the inclusion criteria overall.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item clearly investigates discrete speech tokens and includes a systematic comparison and downstream evaluation with an LLM, aligning with the contribution criteria for Discrete Audio Tokens; token-source details are not fully stated in the abstract, but the paper appears to meet the inclusion criteria overall.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on comparing discrete speech tokens with continuous features in semantic-related tasks using a Speech Large Language Model. It explicitly discusses discrete speech tokens as representations derived from neural models, their limitations, and explores improvements, aligning with the inclusion criteria emphasizing discrete audio token design, analysis, and downstream evaluation. The title and abstract are in English and detail analytic and experimental evaluations of discrete tokens, meeting language and methodological transparency requirements. Since it addresses semantic understanding tasks relevant to downstream applications and analyzes design trade-offs, it fits the inclusion criteria and does not match the exclusion criteria related to continuous-only features or lack of discrete token details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study focuses on comparing discrete speech tokens with continuous features in semantic-related tasks using a Speech Large Language Model. It explicitly discusses discrete speech tokens as representations derived from neural models, their limitations, and explores improvements, aligning with the inclusion criteria emphasizing discrete audio token design, analysis, and downstream evaluation. The title and abstract are in English and detail analytic and experimental evaluations of discrete tokens, meeting language and methodological transparency requirements. Since it addresses semantic understanding tasks relevant to downstream applications and analyzes design trade-offs, it fits the inclusion criteria and does not match the exclusion criteria related to continuous-only features or lack of discrete token details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data",
    "abstract": "Recent research has shown that state-of-the-art (SotA) Automatic Speech Recognition (ASR) systems, such as Whisper, often exhibit predictive biases that disproportionately affect various demographic groups. This study focuses on identifying the performance disparities of Whisper models on Dutch speech data from the Common Voice dataset and the Dutch National Public Broadcasting organisation. We analyzed the word error rate, character error rate and a BERT-based semantic similarity across gender groups. We used the moral framework of Weerts et al. (2022) to assess quality of service harms and fairness, and to provide a nuanced discussion on the implications of these biases, particularly for automatic subtitling. Our findings reveal substantial disparities in word error rate (WER) among gender groups across all model sizes, with bias identified through statistical testing.",
    "metadata": {
      "arxiv_id": "2411.09431",
      "title": "Everyone deserves their voice to be heard: Analyzing Predictive Gender Bias in ASR Models Applied to Dutch Speech Data",
      "summary": "Recent research has shown that state-of-the-art (SotA) Automatic Speech Recognition (ASR) systems, such as Whisper, often exhibit predictive biases that disproportionately affect various demographic groups. This study focuses on identifying the performance disparities of Whisper models on Dutch speech data from the Common Voice dataset and the Dutch National Public Broadcasting organisation. We analyzed the word error rate, character error rate and a BERT-based semantic similarity across gender groups. We used the moral framework of Weerts et al. (2022) to assess quality of service harms and fairness, and to provide a nuanced discussion on the implications of these biases, particularly for automatic subtitling. Our findings reveal substantial disparities in word error rate (WER) among gender groups across all model sizes, with bias identified through statistical testing.",
      "authors": [
        "Rik Raes",
        "Saskia Lensink",
        "Mykola Pechenizkiy"
      ],
      "published": "2024-11-14T13:29:09Z",
      "updated": "2024-11-14T13:29:09Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09431v1",
      "landing_url": "https://arxiv.org/abs/2411.09431v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09431"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study addresses predictive gender bias in ASR performance on Dutch data and does not involve discrete audio tokens, tokenization processes, or analysis of token-level representations, thus it does not meet inclusion criteria and aligns with exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study addresses predictive gender bias in ASR performance on Dutch data and does not involve discrete audio tokens, tokenization processes, or analysis of token-level representations, thus it does not meet inclusion criteria and aligns with exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on analyzing predictive gender bias in ASR models applied to Dutch speech data, measuring error rates and fairness implications. However, it does not discuss or include any aspect of discrete audio tokens, neural audio codecs, tokenization processes, or related discrete token designs or evaluations that align with the inclusion criteria. Therefore, it does not meet the core topic requirement of discrete audio token representation and use in modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on analyzing predictive gender bias in ASR models applied to Dutch speech data, measuring error rates and fairness implications. However, it does not discuss or include any aspect of discrete audio tokens, neural audio codecs, tokenization processes, or related discrete token designs or evaluations that align with the inclusion criteria. Therefore, it does not meet the core topic requirement of discrete audio token representation and use in modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-shot Voice Conversion with Diffusion Transformers",
    "abstract": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
    "metadata": {
      "arxiv_id": "2411.09943",
      "title": "Zero-shot Voice Conversion with Diffusion Transformers",
      "summary": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker. Traditional approaches struggle with timbre leakage, insufficient timbre representation, and mismatches between training and inference tasks. We propose Seed-VC, a novel framework that addresses these issues by introducing an external timbre shifter during training to perturb the source speech timbre, mitigating leakage and aligning training with inference. Additionally, we employ a diffusion transformer that leverages the entire reference speech context, capturing fine-grained timbre features through in-context learning. Experiments demonstrate that Seed-VC outperforms strong baselines like OpenVoice and CosyVoice, achieving higher speaker similarity and lower word error rates in zero-shot voice conversion tasks. We further extend our approach to zero-shot singing voice conversion by incorporating fundamental frequency (F0) conditioning, resulting in comparative performance to current state-of-the-art methods. Our findings highlight the effectiveness of Seed-VC in overcoming core challenges, paving the way for more accurate and versatile voice conversion systems.",
      "authors": [
        "Songting Liu"
      ],
      "published": "2024-11-15T04:43:44Z",
      "updated": "2024-11-15T04:43:44Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.09943v1",
      "landing_url": "https://arxiv.org/abs/2411.09943v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.09943"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於零-shot語音轉換的連續音訊建模與擴散Transformer，未提出離散音訊 token 的產生流程、碼本/字彙表設定，且未以離散 token 作為核心方法或分析對象，因此不符合本主題的離散音訊 token 要求。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於零-shot語音轉換的連續音訊建模與擴散Transformer，未提出離散音訊 token 的產生流程、碼本/字彙表設定，且未以離散 token 作為核心方法或分析對象，因此不符合本主題的離散音訊 token 要求。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on zero-shot voice conversion using diffusion transformers and timbre shifting, but the abstract does not mention any discrete audio tokenization process such as vector quantization, codebooks, or discrete token generation; it discusses continuous speech transformations rather than discrete token sequences or their design, which is core to the inclusion criteria. Therefore, it does not meet the requirement of using discrete token sequences as core representations for audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on zero-shot voice conversion using diffusion transformers and timbre shifting, but the abstract does not mention any discrete audio tokenization process such as vector quantization, codebooks, or discrete token generation; it discusses continuous speech transformations rather than discrete token sequences or their design, which is core to the inclusion criteria. Therefore, it does not meet the requirement of using discrete token sequences as core representations for audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
    "abstract": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
    "metadata": {
      "arxiv_id": "2411.10293",
      "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
      "summary": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
      "authors": [
        "Ryoma Yataka",
        "Adriano Cardace",
        "Pu Perry Wang",
        "Petros Boufounos",
        "Ryuhei Takahashi"
      ],
      "published": "2024-11-15T15:51:25Z",
      "updated": "2025-01-17T19:06:26Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "math.DG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10293v3",
      "landing_url": "https://arxiv.org/abs/2411.10293v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.10293"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "本篇聚焦室內雷達感知與 DETR 變體，並未討論離散音訊 tokens、tokenization 或與音訊生成/跨模態推理相關內容，因此不符合納入條件，應予以排除。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "本篇聚焦室內雷達感知與 DETR 變體，並未討論離散音訊 tokens、tokenization 或與音訊生成/跨模態推理相關內容，因此不符合納入條件，應予以排除。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a multi-view radar detection transformer for indoor perception, focusing on radar and vision modalities without any mention of discrete audio tokens or discretization of continuous audio waveforms into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens nor discusses any tokenization or quantization related to audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a multi-view radar detection transformer for indoor perception, focusing on radar and vision modalities without any mention of discrete audio tokens or discretization of continuous audio waveforms into token sequences; thus, it does not meet the inclusion criteria related to discrete audio tokens nor discusses any tokenization or quantization related to audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
    "abstract": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly encodes speech into discrete, tokenized representations via a self-supervised Mamba encoder and uses these tokens for Spoken Term Detection, fulfilling the Discrete Audio Tokens criterion and demonstrating downstream token-based modeling and evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly encodes speech into discrete, tokenized representations via a self-supervised Mamba encoder and uses these tokens for Spoken Term Detection, fulfilling the Discrete Audio Tokens criterion and demonstrating downstream token-based modeling and evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes encoding speech into discrete, speaker-agnostic semantic tokens generated via a bidirectional Mamba encoder trained in a self-supervised framework, which aligns with the inclusion criteria of producing discrete audio tokens for downstream tasks; however, the abstract lacks explicit details about the tokenization process specifics such as codebook size, quantization method, or token design choices, making it difficult to fully verify reproducibility and token definition adherence.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes encoding speech into discrete, speaker-agnostic semantic tokens generated via a bidirectional Mamba encoder trained in a self-supervised framework, which aligns with the inclusion criteria of producing discrete audio tokens for downstream tasks; however, the abstract lacks explicit details about the tokenization process specifics such as codebook size, quantization method, or token design choices, making it difficult to fully verify reproducibility and token definition adherence.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly states the work encodes speech into discrete, speaker-agnostic semantic tokens (derived via a self-supervised bidirectional Mamba encoder) and uses those tokens as the core representation for spoken-term-detection, which matches the inclusion group of deriving discrete units from a self-supervised model and applying them in a downstream task; junior reviews agree the study centers on discrete tokens but flagged that the abstract omits low-level tokenizer details (codebook/quantization/bitrate) needed for full reproducibility. Given tokens are the principal contribution and used for downstream evaluation, but the abstract lacks detailed tokenizer specs, I recommend inclusion but note the reproducibility details should be checked in the full paper.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly states the work encodes speech into discrete, speaker-agnostic semantic tokens (derived via a self-supervised bidirectional Mamba encoder) and uses those tokens as the core representation for spoken-term-detection, which matches the inclusion group of deriving discrete units from a self-supervised model and applying them in a downstream task; junior reviews agree the study centers on discrete tokens but flagged that the abstract omits low-level tokenizer details (codebook/quantization/bitrate) needed for full reproducibility. Given tokens are the principal contribution and used for downstream evaluation, but the abstract lacks detailed tokenizer specs, I recommend inclusion but note the reproducibility details should be checked in the full paper.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
    "abstract": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
    "metadata": {
      "arxiv_id": "2411.14642",
      "title": "VQalAttent: a Transparent Speech Generation Pipeline based on Transformer-learned VQ-VAE Latent Space",
      "summary": "Generating high-quality speech efficiently remains a key challenge for generative models in speech synthesis. This paper introduces VQalAttent, a lightweight model designed to generate fake speech with tunable performance and interpretability. Leveraging the AudioMNIST dataset, consisting of human utterances of decimal digits (0-9), our method employs a two-step architecture: first, a scalable vector quantized autoencoder (VQ-VAE) that compresses audio spectrograms into discrete latent representations, and second, a decoder-only transformer that learns the probability model of these latents. Trained transformer generates similar latent sequences, convertible to audio spectrograms by the VQ-VAE decoder, from which we generate fake utterances. Interpreting statistical and perceptual quality of the fakes, depending on the dimension and the extrinsic information of the latent space, enables guided improvements in larger, commercial generative models. As a valuable tool for understanding and refining audio synthesis, our results demonstrate VQalAttent's capacity to generate intelligible speech samples with limited computational resources, while the modularity and transparency of the training pipeline helps easily correlate the analytics with modular modifications, hence providing insights for the more complex models.",
      "authors": [
        "Armani Rodriguez",
        "Silvija Kokalj-Filipovic"
      ],
      "published": "2024-11-22T00:21:39Z",
      "updated": "2024-11-22T00:21:39Z",
      "categories": [
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14642v1",
      "landing_url": "https://arxiv.org/abs/2411.14642v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.14642"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper explicitly uses a neural audio codec (VQ-VAE) to produce discrete latent tokens and trains a transformer to model the token sequence for speech generation, directly addressing discrete audio tokens and their downstream use.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper explicitly uses a neural audio codec (VQ-VAE) to produce discrete latent tokens and trains a transformer to model the token sequence for speech generation, directly addressing discrete audio tokens and their downstream use.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes the use of a vector quantized variational autoencoder (VQ-VAE) to compress audio spectrograms into discrete latent representations, which are essentially discrete audio tokens, then modeled by a transformer. It clearly details the process of discrete token generation through neural audio coding (VQ-VAE), meeting criteria of discrete token production and usage in audio generation. The study focuses on speech synthesis downstream task, analyzing token-based generation quality and model transparency, aligning well with inclusion guidelines while meeting English and methodological detail requirements. Therefore, it fits all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes the use of a vector quantized variational autoencoder (VQ-VAE) to compress audio spectrograms into discrete latent representations, which are essentially discrete audio tokens, then modeled by a transformer. It clearly details the process of discrete token generation through neural audio coding (VQ-VAE), meeting criteria of discrete token production and usage in audio generation. The study focuses on speech synthesis downstream task, analyzing token-based generation quality and model transparency, aligning well with inclusion guidelines while meeting English and methodological detail requirements. Therefore, it fits all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
    "abstract": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
    "metadata": {
      "arxiv_id": "2411.16119",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu"
      ],
      "published": "2024-11-25T06:05:08Z",
      "updated": "2024-11-25T06:05:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16119v1",
      "landing_url": "https://arxiv.org/abs/2411.16119v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16119"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses lattice vector quantizers for end-to-end neural image compression, not discrete audio tokens or audio-domain downstream tasks, thus it does not meet the 'Discrete Audio Tokens' scope or requirements.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses lattice vector quantizers for end-to-end neural image compression, not discrete audio tokens or audio-domain downstream tasks, thus it does not meet the 'Discrete Audio Tokens' scope or requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study focuses on lattice vector quantization methods for neural image compression, discussing rate-distortion optimization for latent feature compression but does not address audio data, nor does it describe discrete audio tokens or tokenization processes related to audio waveform discretization or audio sequence modeling, thus it falls outside the defined scope regarding discrete audio tokens in audio domains.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study focuses on lattice vector quantization methods for neural image compression, discussing rate-distortion optimization for latent feature compression but does not address audio data, nor does it describe discrete audio tokens or tokenization processes related to audio waveform discretization or audio sequence modeling, thus it falls outside the defined scope regarding discrete audio tokens in audio domains.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
    "abstract": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
    "metadata": {
      "arxiv_id": "2411.16156",
      "title": "VideoOrion: Tokenizing Object Dynamics in Videos",
      "summary": "We present VideoOrion, a Video Large Language Model (Video-LLM) that explicitly captures the key semantic information in videos - the spatial-temporal dynamics of objects throughout the videos. VideoOrion employs expert vision models to extract object dynamics through a detect-segment-track pipeline, encoding them into a set of object tokens by aggregating spatial-temporal object features. Our method addresses the persistent challenge in Video-LLMs of efficiently compressing high-dimensional video data into semantic tokens that are comprehensible to LLMs. Compared to prior methods which resort to downsampling the original video or aggregating visual tokens using resamplers, leading to information loss and entangled semantics, VideoOrion not only offers a more natural and efficient way to derive compact, disentangled semantic representations but also enables explicit object modeling of video content with minimal computational cost. Moreover, the introduced object tokens naturally allow VideoOrion to accomplish video-based referring tasks. Experimental results show that VideoOrion can learn to make good use of the object tokens, and achieves competitive results on both general video question answering and video-based referring benchmarks.",
      "authors": [
        "Yicheng Feng",
        "Yijiang Li",
        "Wanpeng Zhang",
        "Hao Luo",
        "Zihao Yue",
        "Sipeng Zheng",
        "Zongqing Lu"
      ],
      "published": "2024-11-25T07:32:02Z",
      "updated": "2025-03-18T08:15:28Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16156v2",
      "landing_url": "https://arxiv.org/abs/2411.16156v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.16156"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study focuses on tokenizing object dynamics in videos (visual tokens) rather than discrete audio tokens, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The study focuses on tokenizing object dynamics in videos (visual tokens) rather than discrete audio tokens, so it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on video understanding by tokenizing object dynamics in videos, without any indication of discretizing audio waveforms or generating discrete audio tokens for audio modeling, generation, or cross-modal reasoning; therefore, it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on video understanding by tokenizing object dynamics in videos, without any indication of discretizing audio waveforms or generating discrete audio tokens for audio modeling, generation, or cross-modal reasoning; therefore, it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Representation Collapsing Problems in Vector Quantization",
    "abstract": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses representation collapse in vector quantization in a general ML context and does not describe discrete audio tokens, tokenizers, or audio-specific token design and evaluation, thus it fails the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses representation collapse in vector quantization in a general ML context and does not describe discrete audio tokens, tokenizers, or audio-specific token design and evaluation, thus it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on representation collapsing in vector quantization, which relates to discrete tokens derived from quantization methods; however, the abstract does not specify that the tokens are derived from audio waveforms or neural audio codecs, nor does it focus on discrete audio tokens for audio generation, understanding, or cross-modal tasks as defined in the inclusion criteria. Hence, while vector quantization is relevant, the domain and application do not align with the discrete audio tokens topic defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on representation collapsing in vector quantization, which relates to discrete tokens derived from quantization methods; however, the abstract does not specify that the tokens are derived from audio waveforms or neural audio codecs, nor does it focus on discrete audio tokens for audio generation, understanding, or cross-modal tasks as defined in the inclusion criteria. Hence, while vector quantization is relevant, the domain and application do not align with the discrete audio tokens topic defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Generative vs. Predictive Models in Massive MIMO Channel Prediction",
    "abstract": "Massive MIMO (mMIMO) systems are essential for 5G/6G networks to meet high throughput and reliability demands, with machine learning (ML)-based techniques, particularly autoencoders (AEs), showing promise for practical deployment. However, standard AEs struggle under noisy channel conditions, limiting their effectiveness. This work introduces a Vector Quantization-based generative AE model (VQ-VAE) for robust mMIMO cross-antenna channel prediction. We compare Generative and Predictive AE-based models, demonstrating that Generative models outperform Predictive ones, especially in noisy environments. The proposed VQ-VAE achieves up to 15 [dB] NMSE gains over standard AEs and about 9 [dB] over VAEs. Additionally, we present a complexity analysis of AE-based models alongside a diffusion model, highlighting the trade-off between accuracy and computational efficiency.",
    "metadata": {
      "arxiv_id": "2411.16971",
      "title": "Generative vs. Predictive Models in Massive MIMO Channel Prediction",
      "summary": "Massive MIMO (mMIMO) systems are essential for 5G/6G networks to meet high throughput and reliability demands, with machine learning (ML)-based techniques, particularly autoencoders (AEs), showing promise for practical deployment. However, standard AEs struggle under noisy channel conditions, limiting their effectiveness. This work introduces a Vector Quantization-based generative AE model (VQ-VAE) for robust mMIMO cross-antenna channel prediction. We compare Generative and Predictive AE-based models, demonstrating that Generative models outperform Predictive ones, especially in noisy environments. The proposed VQ-VAE achieves up to 15 [dB] NMSE gains over standard AEs and about 9 [dB] over VAEs. Additionally, we present a complexity analysis of AE-based models alongside a diffusion model, highlighting the trade-off between accuracy and computational efficiency.",
      "authors": [
        "Ju-Hyung Lee",
        "Joohan Lee",
        "Andreas F. Molisch"
      ],
      "published": "2024-11-25T22:43:22Z",
      "updated": "2024-11-25T22:43:22Z",
      "categories": [
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16971v1",
      "landing_url": "https://arxiv.org/abs/2411.16971v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16971"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses MIMO channel prediction using a Vector Quantization-based VQ-VAE and generative vs predictive models, but it does not center on discrete audio tokens (as used for audio generation/understanding) nor provide tokenization design for audio data; therefore it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses MIMO channel prediction using a Vector Quantization-based VQ-VAE and generative vs predictive models, but it does not center on discrete audio tokens (as used for audio generation/understanding) nor provide tokenization design for audio data; therefore it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses machine learning models for Massive MIMO channel prediction and mentions vector quantization-based autoencoders but focuses on wireless channel modeling and prediction rather than discrete audio tokenization or audio signal processing; it lacks any mention of discretized audio tokens or neural audio codecs for speech or music processing, thus not meeting the inclusion criteria centered on discrete audio token research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses machine learning models for Massive MIMO channel prediction and mentions vector quantization-based autoencoders but focuses on wireless channel modeling and prediction rather than discrete audio tokenization or audio signal processing; it lacks any mention of discretized audio tokens or neural audio codecs for speech or music processing, thus not meeting the inclusion criteria centered on discrete audio token research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
    "abstract": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
    "metadata": {
      "arxiv_id": "2411.17100",
      "title": "k2SSL: A Faster and Better Framework for Self-Supervised Speech Representation Learning",
      "summary": "Self-supervised learning (SSL) has achieved great success in speech-related tasks. While Transformer and Conformer architectures have dominated SSL backbones, encoders like Zipformer, which excel in automatic speech recognition (ASR), remain unexplored in SSL. Concurrently, inefficiencies in data processing within existing SSL training frameworks, such as fairseq, pose challenges in managing the growing volumes of training data. To address these issues, we propose k2SSL, an open-source framework that offers faster, more memory-efficient, and better-performing self-supervised speech representation learning, focusing on downstream ASR tasks. The optimized HuBERT and proposed Zipformer-based SSL systems exhibit substantial reductions in both training time and memory usage during SSL training. Experiments on LibriSpeech demonstrate that Zipformer Base significantly outperforms HuBERT and WavLM, achieving up to a 34.8% relative WER reduction compared to HuBERT Base after fine-tuning, along with a 3.5x pre-training speedup in GPU hours. When scaled to 60k hours of LibriLight data, Zipformer Large exhibits remarkable efficiency, matching HuBERT Large's performance while requiring only 5/8 pre-training steps.",
      "authors": [
        "Yifan Yang",
        "Jianheng Zhuo",
        "Zengrui Jin",
        "Ziyang Ma",
        "Xiaoyu Yang",
        "Zengwei Yao",
        "Liyong Guo",
        "Wei Kang",
        "Fangjun Kuang",
        "Long Lin",
        "Daniel Povey",
        "Xie Chen"
      ],
      "published": "2024-11-26T04:37:11Z",
      "updated": "2025-03-22T04:46:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17100v2",
      "landing_url": "https://arxiv.org/abs/2411.17100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17100"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets faster and better self-supervised speech representation learning (Zipformer/HuBERT/WavLM) with efficiency gains, and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets faster and better self-supervised speech representation learning (Zipformer/HuBERT/WavLM) with efficiency gains, and does not discuss discrete audio tokens, tokenizers, or token-based downstream tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on improving a self-supervised speech representation learning framework (k2SSL) using Transformer/Zipformer architectures for ASR tasks but does not clearly describe the generation or use of discrete audio tokens via discretization or neural audio codecs, nor does it provide detailed tokenization schemes or quantization settings essential for the inclusion criteria; thus, it primarily discusses continuous representations without explicit discrete token modeling or analysis.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article focuses on improving a self-supervised speech representation learning framework (k2SSL) using Transformer/Zipformer architectures for ASR tasks but does not clearly describe the generation or use of discrete audio tokens via discretization or neural audio codecs, nor does it provide detailed tokenization schemes or quantization settings essential for the inclusion criteria; thus, it primarily discusses continuous representations without explicit discrete token modeling or analysis.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
    "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
    "metadata": {
      "arxiv_id": "2411.17607",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "Shengmin Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-11-26T17:19:09Z",
      "updated": "2024-12-02T16:13:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17607v2",
      "landing_url": "https://arxiv.org/abs/2411.17607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17607"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes discrete speech tokens produced by a vector-quantized bottleneck/tokenizer and token-based modeling, with downstream speech-language tasks, aligning with the discrete audio token criteria; no exclusion criteria are evident.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes discrete speech tokens produced by a vector-quantized bottleneck/tokenizer and token-based modeling, with downstream speech-language tasks, aligning with the discrete audio token criteria; no exclusion criteria are evident.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel speech-text pre-training approach leveraging synthetic interleaved data and uses a supervised speech tokenizer derived from an ASR model with vector-quantized bottleneck to produce discrete speech tokens. This meets the inclusion criteria regarding discrete audio tokens generated by neural audio encoding with vector quantization, with clear description of tokenization and application in speech language modeling and generation tasks. The abstract is in English and provides sufficient methodological detail to assess the discrete token generation process and its utility, aligning well with the domain's focus on discrete token usage for audio modeling and generation.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel speech-text pre-training approach leveraging synthetic interleaved data and uses a supervised speech tokenizer derived from an ASR model with vector-quantized bottleneck to produce discrete speech tokens. This meets the inclusion criteria regarding discrete audio tokens generated by neural audio encoding with vector quantization, with clear description of tokenization and application in speech language modeling and generation tasks. The abstract is in English and provides sufficient methodological detail to assess the discrete token generation process and its utility, aligning well with the domain's focus on discrete token usage for audio modeling and generation.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
    "abstract": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
    "metadata": {
      "arxiv_id": "2411.17666",
      "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
      "summary": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
      "authors": [
        "Hyunji Lee",
        "Danni Liu",
        "Supriti Sinhamahapatra",
        "Jan Niehues"
      ],
      "published": "2024-11-26T18:29:11Z",
      "updated": "2025-02-20T18:04:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17666v2",
      "landing_url": "https://arxiv.org/abs/2411.17666v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17666"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses cross-lingual/cross-modal representations in multimodal models, not discrete audio tokens or tokenization; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses cross-lingual/cross-modal representations in multimodal models, not discrete audio tokens or tokenization; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract discuss analysis of multimodal foundation models focusing on text and speech representations and cross-lingual/cross-modal differences, but they do not mention any discrete audio tokenization process, the use of neural audio codec or vector quantization methods, nor do they detail token vocabularies or discrete sequences for audio. The research seems to primarily analyze continuous representations and modality gaps without explicitly involving discrete token sequences or discrete audio tokens crucial to the inclusion criteria. Therefore, it does not meet the key inclusion criteria regarding discrete audio tokens and their role in generative or alignment tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract discuss analysis of multimodal foundation models focusing on text and speech representations and cross-lingual/cross-modal differences, but they do not mention any discrete audio tokenization process, the use of neural audio codec or vector quantization methods, nor do they detail token vocabularies or discrete sequences for audio. The research seems to primarily analyze continuous representations and modality gaps without explicitly involving discrete token sequences or discrete audio tokens crucial to the inclusion criteria. Therefore, it does not meet the key inclusion criteria regarding discrete audio tokens and their role in generative or alignment tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
    "abstract": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
    "metadata": {
      "arxiv_id": "2411.17773",
      "title": "Efficient Multi-modal Large Language Models via Visual Token Grouping",
      "summary": "The development of Multi-modal Large Language Models (MLLMs) enhances Large Language Models (LLMs) with the ability to perceive data formats beyond text, significantly advancing a range of downstream applications, such as visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. While existing methods conduct token reduction in the feature alignment phase. In this paper, we introduce VisToG, a novel grouping mechanism that leverages the capabilities of pre-trained vision encoders to group similar image segments without the need for segmentation masks. Specifically, we concatenate semantic tokens to represent image semantic segments after the linear projection layer before feeding into the vision encoder. Besides, with the isolated attention we adopt, VisToG can identify and eliminate redundant visual tokens utilizing the prior knowledge in the pre-trained vision encoder, which effectively reduces computational demands. Extensive experiments demonstrate the effectiveness of VisToG, maintaining 98.1% of the original performance while achieving a reduction of over 27\\% inference time.",
      "authors": [
        "Minbin Huang",
        "Runhui Huang",
        "Han Shi",
        "Yimeng Chen",
        "Chuanyang Zheng",
        "Xiangguo Sun",
        "Xin Jiang",
        "Zhenguo Li",
        "Hong Cheng"
      ],
      "published": "2024-11-26T09:36:02Z",
      "updated": "2024-12-02T14:55:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17773v2",
      "landing_url": "https://arxiv.org/abs/2411.17773v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17773"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses visual token grouping for multimodal LLMs and does not mention discrete audio tokens or tokenization of audio into a discrete vocabulary; it fails the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses visual token grouping for multimodal LLMs and does not mention discrete audio tokens or tokenization of audio into a discrete vocabulary; it fails the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a multi-modal large language model approach for visual token grouping to reduce computational costs in image processing, with no mention of discretizing audio waveforms into tokens or any audio-related discrete token processing; hence, it does not meet the inclusion criteria for discrete audio tokens and instead falls outside the scope defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a multi-modal large language model approach for visual token grouping to reduce computational costs in image processing, with no mention of discretizing audio waveforms into tokens or any audio-related discrete token processing; hence, it does not meet the inclusion criteria for discrete audio tokens and instead falls outside the scope defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
    "abstract": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
    "metadata": {
      "arxiv_id": "2411.17998",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "authors": [
        "Jia Qi Yip",
        "Chin Yuen Kwok",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2024-11-27T02:31:52Z",
      "updated": "2024-11-27T02:31:52Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17998v1",
      "landing_url": "https://arxiv.org/abs/2411.17998v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.17998"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses neural audio codecs and compressed representations, the abstract does not explicitly describe discrete tokens, tokenization, or codebooks as core inputs or analyzed design choices, making its inclusion under token-centric criteria unclear.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "Although it uses neural audio codecs and compressed representations, the abstract does not explicitly describe discrete tokens, tokenization, or codebooks as core inputs or analyzed design choices, making its inclusion under token-centric criteria unclear.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech separation using neural audio codecs with embedding loss, which implies working directly on compressed audio representations from neural codecs. Given that neural audio codecs typically produce discrete token-like representations (e.g., vector quantization) and the study discusses codec-based models and training losses on these representations, it likely involves discrete audio tokens. However, the abstract does not explicitly describe the tokenization process, token vocabularies, quantization details, or discrete token design, which are critical for confirming inclusion. Given the emphasis on codec-based discrete embeddings and the relevance to speech-related downstream tasks, it aligns well with the inclusion criteria, though the lack of explicit token generation details is a minor uncertainty.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech separation using neural audio codecs with embedding loss, which implies working directly on compressed audio representations from neural codecs. Given that neural audio codecs typically produce discrete token-like representations (e.g., vector quantization) and the study discusses codec-based models and training losses on these representations, it likely involves discrete audio tokens. However, the abstract does not explicitly describe the tokenization process, token vocabularies, quantization details, or discrete token design, which are critical for confirming inclusion. Given the emphasis on codec-based discrete embeddings and the relevance to speech-related downstream tasks, it aligns well with the inclusion criteria, though the lack of explicit token generation details is a minor uncertainty.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged uncertainty: JuniorNano gave a 3 (uncertain) noting absence of explicit tokenization details, and JuniorMini leaned to include (4) but also acknowledged missing explicit descriptions of discrete tokens/codebooks. The abstract describes working in compressed codec representations and an \"embedding loss\" for training speech-separation models without decoding, but it does not explicitly state that the codec outputs discrete token sequences (e.g., VQ/codebook settings, vocabulary, time downsampling, or tokenizer architecture), nor does it provide token-generation/configuration details required by the inclusion criteria. Furthermore, the work focuses on speech separation (a mainly discriminative/regression task) rather than on generation or sequence modeling with discrete token sequences. Given the strict requirement that papers must clearly describe discrete-token production (tokenizer/codec quantization, codebooks/vocab, bitrate/time subsampling) and treat discrete tokens as the core representation, the abstract lacks sufficient evidence to meet ALL inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged uncertainty: JuniorNano gave a 3 (uncertain) noting absence of explicit tokenization details, and JuniorMini leaned to include (4) but also acknowledged missing explicit descriptions of discrete tokens/codebooks. The abstract describes working in compressed codec representations and an \"embedding loss\" for training speech-separation models without decoding, but it does not explicitly state that the codec outputs discrete token sequences (e.g., VQ/codebook settings, vocabulary, time downsampling, or tokenizer architecture), nor does it provide token-generation/configuration details required by the inclusion criteria. Furthermore, the work focuses on speech separation (a mainly discriminative/regression task) rather than on generation or sequence modeling with discrete token sequences. Given the strict requirement that papers must clearly describe discrete-token production (tokenizer/codec quantization, codebooks/vocab, bitrate/time subsampling) and treat discrete tokens as the core representation, the abstract lacks sufficient evidence to meet ALL inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive Generation",
    "abstract": "Image tokenizers play a critical role in shaping the performance of subsequent generative models. Since the introduction of VQ-GAN, discrete image tokenization has undergone remarkable advancements. Improvements in architecture, quantization techniques, and training recipes have significantly enhanced both image reconstruction and the downstream generation quality. In this paper, we present XQ-GAN, an image tokenization framework designed for both image reconstruction and generation tasks. Our framework integrates state-of-the-art quantization techniques, including vector quantization (VQ), residual quantization (RQ), multi-scale residual quantization (MSVQ), product quantization (PQ), lookup-free quantization (LFQ), and binary spherical quantization (BSQ), within a highly flexible and customizable training environment. On the standard ImageNet 256x256 benchmark, our released model achieves an rFID of 0.64, significantly surpassing MAGVIT-v2 (0.9 rFID) and VAR (0.9 rFID). Furthermore, we demonstrate that using XQ-GAN as a tokenizer improves gFID metrics alongside rFID. For instance, with the same VAR architecture, XQ-GAN+VAR achieves a gFID of 2.6, outperforming VAR's 3.3 gFID by a notable margin. To support further research, we provide pre-trained weights of different image tokenizers for the community to directly train the subsequent generative models on it or fine-tune for specialized tasks.",
    "metadata": {
      "arxiv_id": "2412.01762",
      "title": "XQ-GAN: An Open-source Image Tokenization Framework for Autoregressive Generation",
      "summary": "Image tokenizers play a critical role in shaping the performance of subsequent generative models. Since the introduction of VQ-GAN, discrete image tokenization has undergone remarkable advancements. Improvements in architecture, quantization techniques, and training recipes have significantly enhanced both image reconstruction and the downstream generation quality. In this paper, we present XQ-GAN, an image tokenization framework designed for both image reconstruction and generation tasks. Our framework integrates state-of-the-art quantization techniques, including vector quantization (VQ), residual quantization (RQ), multi-scale residual quantization (MSVQ), product quantization (PQ), lookup-free quantization (LFQ), and binary spherical quantization (BSQ), within a highly flexible and customizable training environment. On the standard ImageNet 256x256 benchmark, our released model achieves an rFID of 0.64, significantly surpassing MAGVIT-v2 (0.9 rFID) and VAR (0.9 rFID). Furthermore, we demonstrate that using XQ-GAN as a tokenizer improves gFID metrics alongside rFID. For instance, with the same VAR architecture, XQ-GAN+VAR achieves a gFID of 2.6, outperforming VAR's 3.3 gFID by a notable margin. To support further research, we provide pre-trained weights of different image tokenizers for the community to directly train the subsequent generative models on it or fine-tune for specialized tasks.",
      "authors": [
        "Xiang Li",
        "Kai Qiu",
        "Hao Chen",
        "Jason Kuen",
        "Jiuxiang Gu",
        "Jindong Wang",
        "Zhe Lin",
        "Bhiksha Raj"
      ],
      "published": "2024-12-02T17:58:06Z",
      "updated": "2024-12-02T17:58:06Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01762v1",
      "landing_url": "https://arxiv.org/abs/2412.01762v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.01762"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item addresses image tokenization and autoregressive image generation, not discrete audio tokens; it fails to meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item addresses image tokenization and autoregressive image generation, not discrete audio tokens; it fails to meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on image tokenization using various quantization techniques for image generation tasks and does not pertain to audio signals or discrete audio tokens, which are the core inclusion criteria; thus, it does not meet the thematic requirement related to discrete audio tokens for audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on image tokenization using various quantization techniques for image generation tasks and does not pertain to audio signals or discrete audio tokens, which are the core inclusion criteria; thus, it does not meet the thematic requirement related to discrete audio tokens for audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Tokens in Retrieval Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Retrieval-Augmented Generation in NLP and proposes an evaluator module; there is no discussion of discrete audio tokens, tokenization, or audio token designs, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Retrieval-Augmented Generation in NLP and proposes an evaluator module; there is no discussion of discrete audio tokens, tokenization, or audio token designs, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Retrieval-Augmented Generation systems and their reliability improvements through an evaluator module, without mention of discretizing continuous audio waveforms into discrete audio tokens or using neural audio codecs for token generation. It does not address the core topic of discrete audio tokens, token design, or applications related to audio generation, understanding, or cross-modal modeling with discrete tokens, and thus does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Retrieval-Augmented Generation systems and their reliability improvements through an evaluator module, without mention of discretizing continuous audio waveforms into discrete audio tokens or using neural audio codecs for token generation. It does not address the core topic of discrete audio tokens, token design, or applications related to audio generation, understanding, or cross-modal modeling with discrete tokens, and thus does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot",
    "abstract": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.",
    "metadata": {
      "arxiv_id": "2412.02612",
      "title": "GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot",
      "summary": "We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Kedong Wang",
        "Shengmin Jiang",
        "Lei Zhao",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-12-03T17:41:24Z",
      "updated": "2024-12-03T17:41:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02612v1",
      "landing_url": "https://arxiv.org/abs/2412.02612v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02612"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract clearly describes a speech tokenizer with a vector-quantized bottleneck and a discrete token sequence used for speech modeling, aligning with the discrete audio tokens topic and meeting the inclusion criteria while not triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Abstract clearly describes a speech tokenizer with a vector-quantized bottleneck and a discrete token sequence used for speech modeling, aligning with the discrete audio tokens topic and meeting the inclusion criteria while not triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a neural audio codec-based discrete tokenization approach (ultra-low bitrate, single-codebook speech tokenizer with 12.5Hz frame rate derived from ASR incorporating vector-quantized bottleneck) for end-to-end spoken chatbot modeling and generation, satisfying the core inclusion criteria for discrete audio tokens and providing sufficient method details and evaluation results in English.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a neural audio codec-based discrete tokenization approach (ultra-low bitrate, single-codebook speech tokenizer with 12.5Hz frame rate derived from ASR incorporating vector-quantized bottleneck) for end-to-end spoken chatbot modeling and generation, satisfying the core inclusion criteria for discrete audio tokens and providing sufficient method details and evaluation results in English.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model",
    "abstract": "We examine the text-free speech representations of raw audio obtained from a self-supervised learning (SSL) model by analyzing the synthesized speech using the SSL representations instead of conventional text representations. Since raw audio does not have paired speech representations as transcribed texts do, obtaining speech representations from unpaired speech is crucial for augmenting available datasets for speech synthesis. Specifically, the proposed speech synthesis is conducted using discrete symbol representations from the SSL model in comparison with text representations, and analytical examinations of the synthesized speech have been carried out. The results empirically show that using text representations is advantageous for preserving semantic information, while using discrete symbol representations is superior for preserving acoustic content, including prosodic and intonational information.",
    "metadata": {
      "arxiv_id": "2412.03074",
      "title": "Analytic Study of Text-Free Speech Synthesis for Raw Audio using a Self-Supervised Learning Model",
      "summary": "We examine the text-free speech representations of raw audio obtained from a self-supervised learning (SSL) model by analyzing the synthesized speech using the SSL representations instead of conventional text representations. Since raw audio does not have paired speech representations as transcribed texts do, obtaining speech representations from unpaired speech is crucial for augmenting available datasets for speech synthesis. Specifically, the proposed speech synthesis is conducted using discrete symbol representations from the SSL model in comparison with text representations, and analytical examinations of the synthesized speech have been carried out. The results empirically show that using text representations is advantageous for preserving semantic information, while using discrete symbol representations is superior for preserving acoustic content, including prosodic and intonational information.",
      "authors": [
        "Joonyong Park",
        "Daisuke Saito",
        "Nobuaki Minematsu"
      ],
      "published": "2024-12-04T06:52:03Z",
      "updated": "2024-12-04T06:52:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.03074v1",
      "landing_url": "https://arxiv.org/abs/2412.03074v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.03074"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item clearly focuses on discrete audio tokens derived from a self-supervised model and analyzes their use for text-free speech synthesis, comparing discrete tokens to text representations, addressing token generation/architecture and their impact on acoustic versus semantic content, thus meeting the discrete-audio-token inclusion criteria and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item clearly focuses on discrete audio tokens derived from a self-supervised model and analyzes their use for text-free speech synthesis, comparing discrete tokens to text representations, addressing token generation/architecture and their impact on acoustic versus semantic content, thus meeting the discrete-audio-token inclusion criteria and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study analyzes speech synthesis using discrete symbol representations derived from a self-supervised learning model, which fits the theme of using discrete audio tokens obtained via SSL models for speech generation and analysis; however, details on the token generation process, vocabulary, or codebook settings are not explicitly provided, making reproducibility and token definition unclear.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study analyzes speech synthesis using discrete symbol representations derived from a self-supervised learning model, which fits the theme of using discrete audio tokens obtained via SSL models for speech generation and analysis; however, details on the token generation process, vocabulary, or codebook settings are not explicitly provided, making reproducibility and token definition unclear.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The paper clearly addresses text-free speech synthesis using discrete symbol representations derived from a self-supervised learning (SSL) model, which aligns with the 'Discrete Audio Tokens' theme; JuniorReviewer A rated it highly (5) noting it meets the discrete-token inclusion, while JuniorReviewer B was more cautious (3) because the abstract does not detail how the discrete tokens are produced (tokenizer/codec architecture, quantization, vocabulary/codebook, or rates) needed for reproducibility. Because the abstract asserts use of SSL-derived discrete symbols but omits the explicit token-generation and tokenizer/codebook details required by the inclusion criteria, I cannot confidently confirm it meets all mandatory documentation requirements, so inclusion is uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The paper clearly addresses text-free speech synthesis using discrete symbol representations derived from a self-supervised learning (SSL) model, which aligns with the 'Discrete Audio Tokens' theme; JuniorReviewer A rated it highly (5) noting it meets the discrete-token inclusion, while JuniorReviewer B was more cautious (3) because the abstract does not detail how the discrete tokens are produced (tokenizer/codec architecture, quantization, vocabulary/codebook, or rates) needed for reproducibility. Because the abstract asserts use of SSL-derived discrete symbols but omits the explicit token-generation and tokenizer/codebook details required by the inclusion criteria, I cannot confidently confirm it meets all mandatory documentation requirements, so inclusion is uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
    "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study centers on continuous speech token representations and their integration with LLMs; it does not introduce or analyze discrete audio token vocabularies, encoders producing discrete tokens, or token-based downstream tasks, thus fails to meet inclusion criteria and aligns with exclusion.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study centers on continuous speech token representations and their integration with LLMs; it does not introduce or analyze discrete audio token vocabularies, encoders producing discrete tokens, or token-based downstream tasks, thus fails to meet inclusion criteria and aligns with exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper primarily focuses on continuous speech tokens rather than discrete audio tokens; although it compares continuous tokens to discrete ones, it does not provide detailed description or design of discrete tokenization or codec architecture, nor does it treat discrete tokens as a core element but rather as a baseline. Therefore, it does not meet the inclusion criteria focused on discrete audio token methods and design details, nor does it sufficiently provide reproducible information on discrete tokenization mechanisms.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper primarily focuses on continuous speech tokens rather than discrete audio tokens; although it compares continuous tokens to discrete ones, it does not provide detailed description or design of discrete tokenization or codec architecture, nor does it treat discrete tokens as a core element but rather as a baseline. Therefore, it does not meet the inclusion criteria focused on discrete audio token methods and design details, nor does it sufficiently provide reproducible information on discrete tokenization mechanisms.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
    "abstract": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
    "metadata": {
      "arxiv_id": "2412.07783",
      "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
      "summary": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
      "authors": [
        "Patrick Styll",
        "Dowon Kim",
        "Jiook Cha"
      ],
      "published": "2024-11-25T12:20:07Z",
      "updated": "2025-01-30T10:33:33Z",
      "categories": [
        "q-bio.NC",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.07783v3",
      "landing_url": "https://arxiv.org/abs/2412.07783v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.07783"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item models neonatal fMRI using a transformer for neurodevelopmental outcome prediction and does not involve discrete audio tokens, tokenizers, or token-based modeling, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item models neonatal fMRI using a transformer for neurodevelopmental outcome prediction and does not involve discrete audio tokens, tokenizers, or token-based modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using a Transformer model to predict neurodevelopmental outcomes from neonatal fMRI data, which is related to brain imaging and neuroscientific analysis. It does not involve discrete audio tokens, nor does it discuss discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-based token design or evaluation. Hence, it does not meet the inclusion criteria centered on discrete audio tokens and is unrelated to audio generation, understanding, or cross-modal modeling with discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using a Transformer model to predict neurodevelopmental outcomes from neonatal fMRI data, which is related to brain imaging and neuroscientific analysis. It does not involve discrete audio tokens, nor does it discuss discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-based token design or evaluation. Hence, it does not meet the inclusion criteria centered on discrete audio tokens and is unrelated to audio generation, understanding, or cross-modal modeling with discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
    "abstract": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
    "metadata": {
      "arxiv_id": "2412.08117",
      "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
      "summary": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
      "authors": [
        "Haowei Lou",
        "Helen Paik",
        "Pari Delir Haghighi",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2024-12-11T05:55:06Z",
      "updated": "2024-12-11T05:55:06Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08117v1",
      "landing_url": "https://arxiv.org/abs/2412.08117v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses latent diffusion with continuous latent embeddings for TTS and does not describe discrete audio tokens, tokenizers, codebooks, or a token-based representation, so it fails the inclusion criteria and aligns with the exclusion criteria about non-discrete token approaches.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses latent diffusion with continuous latent embeddings for TTS and does not describe discrete audio tokens, tokenizers, codebooks, or a token-based representation, so it fails the inclusion criteria and aligns with the exclusion criteria about non-discrete token approaches.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a novel Text-to-Speech (TTS) generation model using latent diffusion in a continuous latent space to reduce computational complexity, but it does not mention the use of discrete audio tokens generated by neural audio codecs or self-supervised models, nor the quantization or vocabulary/tokenizer details necessary to confirm the presence and design of discrete audio tokens as per the inclusion criteria. The method focuses on latent embeddings rather than mapping continuous waveform into discrete token sequences for TTS, thus it does not meet the core criteria of discrete audio token representation or its analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a novel Text-to-Speech (TTS) generation model using latent diffusion in a continuous latent space to reduce computational complexity, but it does not mention the use of discrete audio tokens generated by neural audio codecs or self-supervised models, nor the quantization or vocabulary/tokenizer details necessary to confirm the presence and design of discrete audio tokens as per the inclusion criteria. The method focuses on latent embeddings rather than mapping continuous waveform into discrete token sequences for TTS, thus it does not meet the core criteria of discrete audio token representation or its analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
    "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly discusses discrete speech tokens produced via finite-scalar quantization and codebooks, which aligns with discrete audio token representations, and it leverages these tokens for streaming TTS with LLM backbones, meeting the inclusion criteria while not fitting the exclusion criteria (continuous-only representations).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly discusses discrete speech tokens produced via finite-scalar quantization and codebooks, which aligns with discrete audio token representations, and it leverages these tokens for streaming TTS with LLM backbones, meeting the inclusion criteria while not fitting the exclusion criteria (continuous-only representations).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes CosyVoice 2, a streaming speech synthesis model utilizing supervised discrete speech tokens and large language models, indicating the use of discrete audio tokens obtained from neural audio tokenization methods. It mentions codebook utilization and training on large multilingual data, satisfying the inclusion of discrete token design and its application in TTS with evaluation of naturalness and latency. The paper is in English with clear methodology relevant to discrete tokens and their use in speech generation, thus meeting the inclusion criteria without showing any exclusion criteria related to continuous-only modeling or non-discrete token usage.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract describes CosyVoice 2, a streaming speech synthesis model utilizing supervised discrete speech tokens and large language models, indicating the use of discrete audio tokens obtained from neural audio tokenization methods. It mentions codebook utilization and training on large multilingual data, satisfying the inclusion of discrete token design and its application in TTS with evaluation of naturalness and latency. The paper is in English with clear methodology relevant to discrete tokens and their use in speech generation, thus meeting the inclusion criteria without showing any exclusion criteria related to continuous-only modeling or non-discrete token usage.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
    "abstract": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
    "metadata": {
      "arxiv_id": "2412.10261",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "authors": [
        "Shuaiting Li",
        "Chengxuan Wang",
        "Juncan Deng",
        "Zeyu Wang",
        "Zewen Ye",
        "Zongsheng Wang",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "published": "2024-12-13T16:30:35Z",
      "updated": "2024-12-16T08:54:43Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10261v2",
      "landing_url": "https://arxiv.org/abs/2412.10261v2",
      "doi": "https://doi.org/10.1145/3669940.3707268"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses MVQ for DNN compression using vector quantization in CV tasks (image classification, object detection, segmentation) and hardware acceleration, with no discussion of discrete audio tokens, tokenizers, or audio-centric downstream tasks; thus it does not meet the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses MVQ for DNN compression using vector quantization in CV tasks (image classification, object detection, segmentation) and hardware acceleration, with no discussion of discrete audio tokens, tokenizers, or audio-centric downstream tasks; thus it does not meet the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on vector quantization methods for DNN compression and hardware acceleration, without any mention or focus on discrete audio tokens, audio waveforms, or audio-related tasks; the study does not meet the inclusion criteria which require a core focus on discrete audio tokens or neural audio codecs for audio generation, understanding, or multimodal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on vector quantization methods for DNN compression and hardware acceleration, without any mention or focus on discrete audio tokens, audio waveforms, or audio-related tasks; the study does not meet the inclusion criteria which require a core focus on discrete audio tokens or neural audio codecs for audio generation, understanding, or multimodal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
    "abstract": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
    "metadata": {
      "arxiv_id": "2412.11102",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "authors": [
        "Ximing Xing",
        "Juncheng Hu",
        "Guotao Liang",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "published": "2024-12-15T07:49:31Z",
      "updated": "2025-03-25T15:35:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11102v3",
      "landing_url": "https://arxiv.org/abs/2412.11102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.11102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets SVG/vector graphics and semantic tokens for visuals, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets SVG/vector graphics and semantic tokens for visuals, not discrete audio tokens, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on Large Language Models generating and understanding scalable vector graphics, specifically SVGs, using semantic tokens related to graphic components, not discrete audio tokens derived from continuous audio waveforms through discretization. The study does not address audio data, neural audio codecs, vector quantization of audio, or audio token sequences for audio generation or cross-modal tasks. It does not meet the core inclusion criteria regarding discrete audio tokens. Therefore, it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on Large Language Models generating and understanding scalable vector graphics, specifically SVGs, using semantic tokens related to graphic components, not discrete audio tokens derived from continuous audio waveforms through discretization. The study does not address audio data, neural audio codecs, vector quantization of audio, or audio token sequences for audio generation or cross-modal tasks. It does not meet the core inclusion criteria regarding discrete audio tokens. Therefore, it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "abstract": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete audio tokens as part of a hybrid representation for audio large language modeling, discusses tokenization/quantization concepts (discrete tokens derived from neural codecs or self-supervised representations), and reports evaluation on next-token prediction, matching the inclusion criteria and not falling under the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete audio tokens as part of a hybrid representation for audio large language modeling, discusses tokenization/quantization concepts (discrete tokens derived from neural codecs or self-supervised representations), and reports evaluation on next-token prediction, matching the inclusion criteria and not falling under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study WHISPER-GPT proposes a hybrid model combining continuous audio representations and discrete audio tokens derived from neural compression algorithms (e.g., ENCODEC), which aligns with the inclusion criterion about using neural audio codecs to generate discrete tokens as core representations for modeling and generation. The abstract mentions improvements in token prediction metrics and usage of discrete tokens in a generative LLM context, meeting criteria about applying discrete audio tokens in downstream generative tasks. However, the abstract does not explicitly detail the tokenizer settings, codebook configurations, or quantization details needed to fully confirm that the discrete tokens meet the inclusion criteria and support reproducibility. Nevertheless, the integration of discrete token sequences from neural audio codecs as core input fits well within the defined scope, and the study focuses on generative modeling rather than discriminative tasks, suggesting it is relevant. Given these points, the study should be included, albeit the missing method details prevent a higher confidence score.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study WHISPER-GPT proposes a hybrid model combining continuous audio representations and discrete audio tokens derived from neural compression algorithms (e.g., ENCODEC), which aligns with the inclusion criterion about using neural audio codecs to generate discrete tokens as core representations for modeling and generation. The abstract mentions improvements in token prediction metrics and usage of discrete tokens in a generative LLM context, meeting criteria about applying discrete audio tokens in downstream generative tasks. However, the abstract does not explicitly detail the tokenizer settings, codebook configurations, or quantization details needed to fully confirm that the discrete tokens meet the inclusion criteria and support reproducibility. Nevertheless, the integration of discrete token sequences from neural audio codecs as core input fits well within the defined scope, and the study focuses on generative modeling rather than discriminative tasks, suggesting it is relevant. Given these points, the study should be included, albeit the missing method details prevent a higher confidence score.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Understanding Emotional Body Expressions via Large Language Models",
    "abstract": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
    "metadata": {
      "arxiv_id": "2412.12581",
      "title": "Understanding Emotional Body Expressions via Large Language Models",
      "summary": "Emotion recognition based on body movements is vital in human-computer interaction. However, existing emotion recognition methods predominantly focus on enhancing classification accuracy, often neglecting the provision of textual explanations to justify their classifications. In this paper, we propose an Emotion-Action Interpreter powered by Large Language Model (EAI-LLM), which not only recognizes emotions but also generates textual explanations by treating 3D body movement data as unique input tokens within large language models (LLMs). Specifically, we propose a multi-granularity skeleton tokenizer designed for LLMs, which separately extracts spatio-temporal tokens and semantic tokens from the skeleton data. This approach allows LLMs to generate more nuanced classification descriptions while maintaining robust classification performance. Furthermore, we treat the skeleton sequence as a specific language and propose a unified skeleton token module. This module leverages the extensive background knowledge and language processing capabilities of LLMs to address the challenges of joint training on heterogeneous datasets, thereby significantly enhancing recognition accuracy on individual datasets. Experimental results demonstrate that our model achieves recognition accuracy comparable to existing methods. More importantly, with the support of background knowledge from LLMs, our model can generate detailed emotion descriptions based on classification results, even when trained on a limited amount of labeled skeleton data.",
      "authors": [
        "Haifeng Lu",
        "Jiuyi Chen",
        "Feng Liang",
        "Mingkui Tan",
        "Runhao Zeng",
        "Xiping Hu"
      ],
      "published": "2024-12-17T06:20:39Z",
      "updated": "2024-12-20T11:49:07Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12581v2",
      "landing_url": "https://arxiv.org/abs/2412.12581v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.12581"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on emotion recognition from 3D skeletal data using LLMs and a skeleton tokenizer, not on discrete audio tokens or audio-based tokenization; thus it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on emotion recognition from 3D skeletal data using LLMs and a skeleton tokenizer, not on discrete audio tokens or audio-based tokenization; thus it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on emotion recognition from 3D body movement data using large language models, without addressing discrete audio tokens or their generation, design, or application related to audio waveform discretization or audio tokenization techniques, hence it falls outside the stated inclusion criteria focusing on discrete audio tokens derived from audio data.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on emotion recognition from 3D body movement data using large language models, without addressing discrete audio tokens or their generation, design, or application related to audio waveform discretization or audio tokenization techniques, hence it falls outside the stated inclusion criteria focusing on discrete audio tokens derived from audio data.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Video Generation without Vector Quantization",
    "abstract": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
    "metadata": {
      "arxiv_id": "2412.14169",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Haiwen Diao",
        "Zhengxiong Luo",
        "Yufeng Cui",
        "Huchuan Lu",
        "Shiguang Shan",
        "Yonggang Qi",
        "Xinlong Wang"
      ],
      "published": "2024-12-18T18:59:53Z",
      "updated": "2025-03-02T08:09:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14169v2",
      "landing_url": "https://arxiv.org/abs/2412.14169v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.14169"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on continuous video generation without discrete audio tokens (no vector quantization), thus it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on continuous video generation without discrete audio tokens (no vector quantization), thus it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on autoregressive video generation and explicitly states the use of a non-quantized model without vector quantization, which means it does not involve discrete audio tokens or any quantization-based audio tokenization relevant to the inclusion criteria. The study is about video frame prediction, not about discrete audio tokens or audio modeling. No discrete audio token sequence or neural audio codec is described or evaluated, and the topic concerns video rather than audio.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on autoregressive video generation and explicitly states the use of a non-quantized model without vector quantization, which means it does not involve discrete audio tokens or any quantization-based audio tokenization relevant to the inclusion criteria. The study is about video frame prediction, not about discrete audio tokens or audio modeling. No discrete audio token sequence or neural audio codec is described or evaluated, and the topic concerns video rather than audio.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
    "abstract": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
    "metadata": {
      "arxiv_id": "2412.14643",
      "title": "RefHCM: A Unified Model for Referring Perceptions in Human-Centric Scenarios",
      "summary": "Human-centric perceptions play a crucial role in real-world applications. While recent human-centric works have achieved impressive progress, these efforts are often constrained to the visual domain and lack interaction with human instructions, limiting their applicability in broader scenarios such as chatbots and sports analysis. This paper introduces Referring Human Perceptions, where a referring prompt specifies the person of interest in an image. To tackle the new task, we propose RefHCM (Referring Human-Centric Model), a unified framework to integrate a wide range of human-centric referring tasks. Specifically, RefHCM employs sequence mergers to convert raw multimodal data -- including images, text, coordinates, and parsing maps -- into semantic tokens. This standardized representation enables RefHCM to reformulate diverse human-centric referring tasks into a sequence-to-sequence paradigm, solved using a plain encoder-decoder transformer architecture. Benefiting from a unified learning strategy, RefHCM effectively facilitates knowledge transfer across tasks and exhibits unforeseen capabilities in handling complex reasoning. This work represents the first attempt to address referring human perceptions with a general-purpose framework, while simultaneously establishing a corresponding benchmark that sets new standards for the field. Extensive experiments showcase RefHCM's competitive and even superior performance across multiple human-centric referring tasks. The code and data are publicly at https://github.com/JJJYmmm/RefHCM.",
      "authors": [
        "Jie Huang",
        "Ruibing Hou",
        "Jiahe Zhao",
        "Hong Chang",
        "Shiguang Shan"
      ],
      "published": "2024-12-19T08:51:57Z",
      "updated": "2024-12-19T08:51:57Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14643v1",
      "landing_url": "https://arxiv.org/abs/2412.14643v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14643"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses RefHCM with multimodal tokens for referring tasks and cross-modal learning, but it does not address discrete audio token generation/quantization or use audio tokens as core representation, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses RefHCM with multimodal tokens for referring tasks and cross-modal learning, but it does not address discrete audio token generation/quantization or use audio tokens as core representation, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on human-centric perception and referring tasks in images using multimodal data processed by a transformer model, without any mention of discretizing continuous audio waveforms into discrete audio tokens or neural audio codecs, which are core to the inclusion criteria; thus, it does not meet the topic definition and related requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on human-centric perception and referring tasks in images using multimodal data processed by a transformer model, without any mention of discretizing continuous audio waveforms into discrete audio tokens or neural audio codecs, which are core to the inclusion criteria; thus, it does not meet the topic definition and related requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
    "abstract": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale. To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
    "metadata": {
      "arxiv_id": "2412.14802",
      "title": "Stack Trace Deduplication: Faster, More Accurately, and in More Realistic Scenarios",
      "summary": "In large-scale software systems, there are often no fully-fledged bug reports with human-written descriptions when an error occurs. In this case, developers rely on stack traces, i.e., series of function calls that led to the error. Since there can be tens and hundreds of thousands of them describing the same issue from different users, automatic deduplication into categories is necessary to allow for processing. Recent works have proposed powerful deep learning-based approaches for this, but they are evaluated and compared in isolation from real-life workflows, and it is not clear whether they will actually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel model, an industry-based dataset, and a multi-faceted evaluation. Our model consists of two parts - (1) an embedding model with byte-pair encoding and approximate nearest neighbor search to quickly find the most relevant stack traces to the incoming one, and (2) a reranker that re-ranks the most fitting stack traces, taking into account the repeated frames between them. To complement the existing datasets collected from open-source projects, we share with the community SlowOps - a dataset of stack traces from IntelliJ-based products developed by JetBrains, which has an order of magnitude more stack traces per category. Finally, we carry out an evaluation that strives to be realistic: measuring not only the accuracy of categorization, but also the operation time and the ability to create new categories. The evaluation shows that our model strikes a good balance - it outperforms other models on both open-source datasets and SlowOps, while also being faster on time than most. We release all of our code and data, and hope that our work can pave the way to further practice-oriented research in the area.",
      "authors": [
        "Egor Shibaev",
        "Denis Sushentsev",
        "Yaroslav Golubev",
        "Aleksandr Khvorov"
      ],
      "published": "2024-12-19T12:48:17Z",
      "updated": "2024-12-19T12:48:17Z",
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14802v1",
      "landing_url": "https://arxiv.org/abs/2412.14802v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.14802"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on stack trace deduplication in software systems and proposes embedding-based retrieval and reranking, with a dataset from JetBrains; it does not involve discrete audio tokens, tokenization, or audio-centric modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on stack trace deduplication in software systems and proposes embedding-based retrieval and reranking, with a dataset from JetBrains; it does not involve discrete audio tokens, tokenization, or audio-centric modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on stack trace deduplication in software error handling using deep learning, without any indication of dealing with discrete audio tokens or audio waveform discretization; therefore, it does not meet the inclusion criteria related to discrete audio tokens for audio generation, understanding, or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on stack trace deduplication in software error handling using deep learning, without any indication of dealing with discrete audio tokens or audio waveform discretization; therefore, it does not meet the inclusion criteria related to discrete audio tokens for audio generation, understanding, or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
    "abstract": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
    "metadata": {
      "arxiv_id": "2412.15195",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "authors": [
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published": "2024-12-19T18:58:14Z",
      "updated": "2024-12-19T18:58:14Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15195v1",
      "landing_url": "https://arxiv.org/abs/2412.15195v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15195"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract covers a general vector quantization method with image-focused evaluation and does not describe generation/usage of discrete audio tokens, tokenizers/codec vocabularies, or audio-specific downstream modeling, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract covers a general vector quantization method with image-focused evaluation and does not describe generation/usage of discrete audio tokens, tokenizers/codec vocabularies, or audio-specific downstream modeling, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization methods for image reconstruction tasks and proposes an improved local minima issue solution via optimal transport, without any reference to audio domains, neural audio codecs, discrete audio tokens, or related audio token representations and evaluations, thus it does not meet the inclusion criteria for discrete audio tokens research.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization methods for image reconstruction tasks and proposes an improved local minima issue solution via optimal transport, without any reference to audio domains, neural audio codecs, discrete audio tokens, or related audio token representations and evaluations, thus it does not meet the inclusion criteria for discrete audio tokens research.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
    "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the abstract mentions semantic tokens and reducing token sequence length for discrete-token modeling, it does not clearly specify how tokens are generated or provide sufficient token-design details to confirm a discrete-audio-token pipeline.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "Although the abstract mentions semantic tokens and reducing token sequence length for discrete-token modeling, it does not clearly specify how tokens are generated or provide sufficient token-design details to confirm a discrete-audio-token pipeline.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces SLAM-Omni, a timbre-controllable voice interaction system using semantic tokens to model spoken language and decouple speaker information via a vocoder, highlighting zero-shot timbre control and efficient training by grouping speech semantic tokens which reduces sequence length and computational cost. It appears to focus on discrete token sequences as core representations in an end-to-end voice dialogue system, aligns with inclusion criteria emphasizing discrete audio tokens generated by neural methods used in downstream generative and understanding tasks. The abstract mentions semantic tokens, sequence length reduction, and a vocoder which aligns with discretization and neural audio codec usage, indicating relevance. However, detailed tokenization or codec architecture specifics are not explicit in the abstract, making full confirmation uncertain but likely given the terminology and context.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The paper introduces SLAM-Omni, a timbre-controllable voice interaction system using semantic tokens to model spoken language and decouple speaker information via a vocoder, highlighting zero-shot timbre control and efficient training by grouping speech semantic tokens which reduces sequence length and computational cost. It appears to focus on discrete token sequences as core representations in an end-to-end voice dialogue system, aligns with inclusion criteria emphasizing discrete audio tokens generated by neural methods used in downstream generative and understanding tasks. The abstract mentions semantic tokens, sequence length reduction, and a vocoder which aligns with discretization and neural audio codec usage, indicating relevance. However, detailed tokenization or codec architecture specifics are not explicit in the abstract, making full confirmation uncertain but likely given the terminology and context.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
    "abstract": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
    "metadata": {
      "arxiv_id": "2412.16102",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Hui Wang",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yuzhe Liang",
        "Ziyang Ma",
        "Yuxuan Hu",
        "Rui Zhao",
        "Jianwei Yu",
        "Yan Lu",
        "Xie Chen"
      ],
      "published": "2024-12-20T17:43:50Z",
      "updated": "2025-08-09T10:01:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16102v3",
      "landing_url": "https://arxiv.org/abs/2412.16102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16102"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although the title/abstract describe interleaved speech/text tokens for streaming TTS, the abstract does not specify how 'speech tokens' are generated (tokenizer/codec, token vocabulary, discretization), making it unclear whether it meets the discrete audio-token criteria.",
      "evaluation": 3
    },
    "round-A_JuniorNano_reasoning": "Although the title/abstract describe interleaved speech/text tokens for streaming TTS, the abstract does not specify how 'speech tokens' are generated (tokenizer/codec, token vocabulary, discretization), making it unclear whether it meets the discrete audio-token criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a streaming Text-to-Speech system trained on interleaved sequences of text and speech tokens, but it does not explicitly describe the production process or design of discrete audio tokens, such as tokenizer architecture, quantization, codebook settings, or bitrate, nor does it focus on the discrete tokens as a core element for modeling or analysis as per the inclusion criteria. Therefore, it is uncertain if it fits the discrete audio tokens topic precisely.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a streaming Text-to-Speech system trained on interleaved sequences of text and speech tokens, but it does not explicitly describe the production process or design of discrete audio tokens, such as tokenizer architecture, quantization, codebook settings, or bitrate, nor does it focus on the discrete tokens as a core element for modeling or analysis as per the inclusion criteria. Therefore, it is uncertain if it fits the discrete audio tokens topic precisely.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers flagged that the paper describes training on interleaved text and \"speech tokens\" but the abstract omits critical details about how those speech tokens are produced (tokenizer/codec architecture, quantization/codebook settings, bitrate/time downsampling), which the inclusion criteria require to determine whether the work centers on discrete audio tokens; while the method is relevant to TTS using token sequences, the lack of reproducible token-generation details makes it impossible to confirm fit. Therefore: The study is uncertain for inclusion because it may use discrete audio tokens but does not provide the necessary methodological details in the abstract to satisfy the topic's English-evaluable requirements.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers flagged that the paper describes training on interleaved text and \"speech tokens\" but the abstract omits critical details about how those speech tokens are produced (tokenizer/codec architecture, quantization/codebook settings, bitrate/time downsampling), which the inclusion criteria require to determine whether the work centers on discrete audio tokens; while the method is relevant to TTS using token sequences, the lack of reproducible token-generation details makes it impossible to confirm fit. Therefore: The study is uncertain for inclusion because it may use discrete audio tokens but does not provide the necessary methodological details in the abstract to satisfy the topic's English-evaluable requirements.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
    "abstract": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
    "metadata": {
      "arxiv_id": "2412.16626",
      "title": "Mamba-SEUNet: Mamba UNet for Monaural Speech Enhancement",
      "summary": "In recent speech enhancement (SE) research, transformer and its variants have emerged as the predominant methodologies. However, the quadratic complexity of the self-attention mechanism imposes certain limitations on practical deployment. Mamba, as a novel state-space model (SSM), has gained widespread application in natural language processing and computer vision due to its strong capabilities in modeling long sequences and relatively low computational complexity. In this work, we introduce Mamba-SEUNet, an innovative architecture that integrates Mamba with U-Net for SE tasks. By leveraging bidirectional Mamba to model forward and backward dependencies of speech signals at different resolutions, and incorporating skip connections to capture multi-scale information, our approach achieves state-of-the-art (SOTA) performance. Experimental results on the VCTK+DEMAND dataset indicate that Mamba-SEUNet attains a PESQ score of 3.59, while maintaining low computational complexity. When combined with the Perceptual Contrast Stretching technique, Mamba-SEUNet further improves the PESQ score to 3.73.",
      "authors": [
        "Junyu Wang",
        "Zizhen Lin",
        "Tianrui Wang",
        "Meng Ge",
        "Longbiao Wang",
        "Jianwu Dang"
      ],
      "published": "2024-12-21T13:43:51Z",
      "updated": "2025-01-02T10:56:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16626v2",
      "landing_url": "https://arxiv.org/abs/2412.16626v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16626"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a continuous-time speech enhancement architecture (Mamba UNet) without any discrete audio tokens, tokenization, or discrete token-based modeling, thus failing the inclusion criteria and meeting the exclusion criteria for this review.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a continuous-time speech enhancement architecture (Mamba UNet) without any discrete audio tokens, tokenization, or discrete token-based modeling, thus failing the inclusion criteria and meeting the exclusion criteria for this review.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a novel speech enhancement model using Mamba-SEUNet that focuses on modeling speech sequences and improving speech quality, but it does not mention any discretization of audio into tokens, nor does it describe a tokenizer, codec, or discrete codebook/token vocabulary as core components or for downstream generative/understanding tasks. Hence, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a novel speech enhancement model using Mamba-SEUNet that focuses on modeling speech sequences and improving speech quality, but it does not mention any discretization of audio into tokens, nor does it describe a tokenizer, codec, or discrete codebook/token vocabulary as core components or for downstream generative/understanding tasks. Hence, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
    "abstract": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper models continuous speech representations and autoregressive prediction without any discrete tokens or tokenization/codec-based vocabularies, so it does not satisfy the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper models continuous speech representations and autoregressive prediction without any discrete tokens or tokenization/codec-based vocabularies, so it does not satisfy the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces KALL-E, which models continuous speech distributions directly from text using a Flow-VAE latent representation without discretizing speech into discrete tokens; it does not describe any discrete tokenization, quantization, or discrete token vocabulary which are core to the inclusion criteria, and the approach focuses on continuous latent speech representations rather than discrete audio tokens, thus failing to meet the key inclusion requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces KALL-E, which models continuous speech distributions directly from text using a Flow-VAE latent representation without discretizing speech into discrete tokens; it does not describe any discrete tokenization, quantization, or discrete token vocabulary which are core to the inclusion criteria, and the approach focuses on continuous latent speech representations rather than discrete audio tokens, thus failing to meet the key inclusion requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
    "abstract": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
    "metadata": {
      "arxiv_id": "2412.16919",
      "title": "TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction",
      "summary": "We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks",
      "authors": [
        "Xuying Zhang",
        "Yutong Liu",
        "Yangguang Li",
        "Renrui Zhang",
        "Yufei Liu",
        "Kai Wang",
        "Wanli Ouyang",
        "Zhiwei Xiong",
        "Peng Gao",
        "Qibin Hou",
        "Ming-Ming Cheng"
      ],
      "published": "2024-12-22T08:28:20Z",
      "updated": "2025-08-09T03:33:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16919v3",
      "landing_url": "https://arxiv.org/abs/2412.16919v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16919"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The title/abstract describe 3D asset generation using a 3D-aware VQ-VAE and GPT; there is no discrete audio tokenization or token-level analysis, so it fails inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The title/abstract describe 3D asset generation using a 3D-aware VQ-VAE and GPT; there is no discrete audio tokenization or token-level analysis, so it fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on generating 3D assets using neural encoder-decoder and transformer models but does not discuss discrete audio tokens, audio waveform discretization, or token sequences derived from audio data. It neither addresses neural audio codecs, vector quantization for audio tokens, nor any auditory generation or cross-modal audio tasks. Hence, it fails to meet the inclusion criteria centered on discrete audio tokens and their applications in audio modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on generating 3D assets using neural encoder-decoder and transformer models but does not discuss discrete audio tokens, audio waveform discretization, or token sequences derived from audio data. It neither addresses neural audio codecs, vector quantization for audio tokens, nor any auditory generation or cross-modal audio tasks. Hence, it fails to meet the inclusion criteria centered on discrete audio tokens and their applications in audio modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
    "abstract": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
    "metadata": {
      "arxiv_id": "2412.17048",
      "title": "Why Do Speech Language Models Fail to Generate Semantically Coherent Outputs? A Modality Evolving Perspective",
      "summary": "Although text-based large language models exhibit human-level writing ability and remarkable intelligence, speech language models (SLMs) still struggle to generate semantically coherent outputs. There are several potential reasons for this performance degradation: (A) speech tokens mainly provide phonetic information rather than semantic information, (B) the length of speech sequences is much longer than that of text sequences, and (C) paralinguistic information, such as prosody, introduces additional complexity and variability. In this paper, we explore the influence of three key factors separately by transiting the modality from text to speech in an evolving manner. Our findings reveal that the impact of the three factors varies. Factor A has a relatively minor impact, factor B influences syntactical and semantic modeling more obviously, and factor C exerts the most significant impact, particularly in the basic lexical modeling. Based on these findings, we provide insights into the unique challenges of training SLMs and highlight pathways to develop more effective end-to-end SLMs.",
      "authors": [
        "Hankun Wang",
        "Haoran Wang",
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-12-22T14:59:19Z",
      "updated": "2024-12-22T14:59:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17048v1",
      "landing_url": "https://arxiv.org/abs/2412.17048v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17048"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract analyzes factors affecting semantically coherent outputs in speech language models but does not discuss discrete audio tokens, tokenization, or token design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract analyzes factors affecting semantically coherent outputs in speech language models but does not discuss discrete audio tokens, tokenization, or token design, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses challenges in speech language models in terms of semantic coherence but does not mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it describe any tokenization process or discrete token representations central to the study. It primarily focuses on speech tokens providing phonetic rather than semantic information but lacks details on discrete token generation schemes, codebooks, or their application in downstream tasks. Hence, it does not meet the inclusion criteria centered on discrete audio tokens usage and design.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses challenges in speech language models in terms of semantic coherence but does not mention the use or design of discrete audio tokens derived from neural audio codecs or self-supervised models, nor does it describe any tokenization process or discrete token representations central to the study. It primarily focuses on speech tokens providing phonetic rather than semantic information but lacks details on discrete token generation schemes, codebooks, or their application in downstream tasks. Hence, it does not meet the inclusion criteria centered on discrete audio tokens usage and design.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
    "abstract": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses hierarchical vector quantization for unsupervised action segmentation in videos, not discrete audio tokens or tokenization pipelines, and provides no discrete token/codec/tokenizer details or evaluation for audio-related tasks, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses hierarchical vector quantization for unsupervised action segmentation in videos, not discrete audio tokens or tokenization pipelines, and provides no discrete token/codec/tokenizer details or evaluation for audio-related tasks, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on unsupervised temporal action segmentation of videos and proposes Hierarchical Vector Quantization for clustering video segments. It does not deal with discretization of continuous audio waveforms into discrete audio tokens for modeling, generation, or cross-modal reasoning, nor does it discuss neural audio codecs, audio tokenizers, or related audio sequence modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain (video action segmentation).",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on unsupervised temporal action segmentation of videos and proposes Hierarchical Vector Quantization for clustering video segments. It does not deal with discretization of continuous audio waveforms into discrete audio tokens for modeling, generation, or cross-modal reasoning, nor does it discuss neural audio codecs, audio tokenizers, or related audio sequence modeling. Therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on a different domain (video action segmentation).",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
    "abstract": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The study explicitly uses discrete audio tokens by vector-quantizing SSL features into semantic tokens and leverages them in a downstream speech enhancement model, meeting the token-discretization criteria and providing English abstract, so include.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The study explicitly uses discrete audio tokens by vector-quantizing SSL features into semantic tokens and leverages them in a downstream speech enhancement model, meeting the token-discretization criteria and providing English abstract, so include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study uses quantized self-supervised learning features, specifically vector quantization, to represent phonetic characteristics as semantic tokens, which aligns with the discrete audio tokens definition involving discretization of continuous audio into token sequences by vector quantization. It provides details of the quantization method and incorporates these discrete tokens into a speech enhancement model with experimental validation. This matches inclusion criteria of using discrete tokens generated by neural audio encoder/self-supervised models, and the study focuses on token design and its impact on performance. Hence, it meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study uses quantized self-supervised learning features, specifically vector quantization, to represent phonetic characteristics as semantic tokens, which aligns with the discrete audio tokens definition involving discretization of continuous audio into token sequences by vector quantization. It provides details of the quantization method and incorporates these discrete tokens into a speech enhancement model with experimental validation. This matches inclusion criteria of using discrete tokens generated by neural audio encoder/self-supervised models, and the study focuses on token design and its impact on performance. Hence, it meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
    "abstract": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
    "metadata": {
      "arxiv_id": "2501.00018",
      "title": "SECodec: Structural Entropy-based Compressive Speech Representation Codec for Speech Language Models",
      "summary": "With the rapid advancement of large language models (LLMs), discrete speech representations have become crucial for integrating speech into LLMs. Existing methods for speech representation discretization rely on a predefined codebook size and Euclidean distance-based quantization. However, 1) the size of codebook is a critical parameter that affects both codec performance and downstream task training efficiency. 2) The Euclidean distance-based quantization may lead to audio distortion when the size of the codebook is controlled within a reasonable range. In fact, in the field of information compression, structural information and entropy guidance are crucial, but previous methods have largely overlooked these factors. Therefore, we address the above issues from an information-theoretic perspective, we present SECodec, a novel speech representation codec based on structural entropy (SE) for building speech language models. Specifically, we first model speech as a graph, clustering the speech features nodes within the graph and extracting the corresponding codebook by hierarchically and disentangledly minimizing 2D SE. Then, to address the issue of audio distortion, we propose a new quantization method. This method still adheres to the 2D SE minimization principle, adaptively selecting the most suitable token corresponding to the cluster for each incoming original speech node. Furthermore, we develop a Structural Entropy-based Speech Language Model (SESLM) that leverages SECodec. Experimental results demonstrate that SECodec performs comparably to EnCodec in speech reconstruction, and SESLM surpasses VALL-E in zero-shot text-to-speech tasks. Code, demo speeches, speech feature graph, SE codebook, and models are available at https://github.com/wlq2019/SECodec.",
      "authors": [
        "Linqin Wang",
        "Yaping Liu",
        "Zhengtao Yu",
        "Shengxiang Gao",
        "Cunli Mao",
        "Yuxin Huang",
        "Wenjun Wang",
        "Ling Dong"
      ],
      "published": "2024-12-16T03:33:05Z",
      "updated": "2024-12-16T03:33:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.00018v1",
      "landing_url": "https://arxiv.org/abs/2501.00018v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.00018"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper presents SECodec, a discrete speech token codec based on structural entropy with a learned codebook and downstream SESLM for speech-language modeling, aligning with discrete audio token generation/usage and providing methodological detail in EnglishAbstract.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper presents SECodec, a discrete speech token codec based on structural entropy with a learned codebook and downstream SESLM for speech-language modeling, aligning with discrete audio token generation/usage and providing methodological detail in EnglishAbstract.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a novel neural codec (SECodec) that discretizes speech waveforms into discrete tokens via structural entropy minimization, and it clearly describes the token generation process (e.g., codebook extraction, quantization) and downstream applications in speech language models (SESLM) for TTS and zero-shot tasks, fulfilling all inclusion criteria focused on discrete audio tokens for sequence modelling and generative tasks, with sufficient English detail to confirm the approach; it thus matches the inclusion criteria with no exclusion criteria met.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a novel neural codec (SECodec) that discretizes speech waveforms into discrete tokens via structural entropy minimization, and it clearly describes the token generation process (e.g., codebook extraction, quantization) and downstream applications in speech language models (SESLM) for TTS and zero-shot tasks, fulfilling all inclusion criteria focused on discrete audio tokens for sequence modelling and generative tasks, with sufficient English detail to confirm the approach; it thus matches the inclusion criteria with no exclusion criteria met.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
    "abstract": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
    "metadata": {
      "arxiv_id": "2501.01046",
      "title": "FED: Fast and Efficient Dataset Deduplication Framework with GPU Acceleration",
      "summary": "Dataset deduplication plays a crucial role in enhancing data quality, ultimately improving the training performance and efficiency of large language models. A commonly used method for data deduplication is the MinHash LSH algorithm. Recently, NVIDIA introduced a GPU-based MinHash LSH deduplication method, but it remains suboptimal, leaving room for further improvement in processing efficiency. This paper proposes a GPU-accelerated deduplication framework, FED, that optimizes MinHash LSH for GPU clusters and leverages computationally efficient, partially reusable non-cryptographic hash functions. FED significantly outperforms the CPU-based deduplication tool in SlimPajama (using 64 logical CPU cores) by up to 107.2 times and the GPU-based tool in NVIDIA NeMo Curator by up to 6.3 times when processing 30 million documents on a node with four GPUs. Notably, our method dramatically accelerates the previously time-consuming MinHash signature generation phase, achieving speed-ups of up to 260 compared to the CPU baseline. Despite these gains in efficiency, FED maintains high deduplication quality, with the duplicate document sets reaching a Jaccard similarity of over 0.96 compared to those identified by the standard MinHash algorithm. In large-scale experiments, the deduplication of 1.2 trillion tokens is completed in just 6 hours in a four-node, 16-GPU environment. The related code is publicly available on GitHub (\\href{https://github.com/mcrl/FED}{https://github.com/mcrl/FED}).",
      "authors": [
        "Youngjun Son",
        "Chaewon Kim",
        "Jaejin Lee"
      ],
      "published": "2025-01-02T04:11:23Z",
      "updated": "2025-03-12T13:36:32Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01046v3",
      "landing_url": "https://arxiv.org/abs/2501.01046v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.01046"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item is about a GPU-accelerated dataset deduplication framework using MinHash LSH, not about discrete audio tokens or tokenization/design for audio modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item is about a GPU-accelerated dataset deduplication framework using MinHash LSH, not about discrete audio tokens or tokenization/design for audio modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a GPU-accelerated deduplication framework for datasets and does not discuss any discrete audio tokenization or audio representation; it lacks any mention of discretizing continuous audio into token sequences or related audio codec/tokenizer designs, and thus does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a GPU-accelerated deduplication framework for datasets and does not discuss any discrete audio tokenization or audio representation; it lacks any mention of discretizing continuous audio into token sequences or related audio codec/tokenizer designs, and thus does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
    "abstract": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
    "metadata": {
      "arxiv_id": "2501.02293",
      "title": "Optimizing Audio Compression Through Entropy-Controlled Dithering",
      "summary": "This paper explores entropy-controlled dithering techniques in audio compression, examining the application of standard and modified TPDFs, combined with noise shaping and entropy-controlled parameters, across various audio contexts, including pitch, loudness, rhythm, and instrumentation variations. Perceptual quality metrics such as VISQOL and STOI were used to evaluate performance. The results demonstrate that TPDF-based dithering consistently outperforms RPDF, particularly under optimal alpha conditions, while highlighting performance variability based on signal characteristics. These findings suggest the situational appropriateness of using various TPDF distributions. This work emphasizes the trade-off between entropy and perceptual fidelity, offering insights into the potential of entropy-controlled dithering as a foundation for enhanced audio compression algorithms. A practical implementation as a Digital Audio Workstation plugin introduces customizable dithering controls, laying the groundwork for future advancements in audio compression algorithms.",
      "authors": [
        "Ellison Murray",
        "Morriel Kasher",
        "Predrag Spasojevic"
      ],
      "published": "2025-01-04T14:03:56Z",
      "updated": "2025-01-09T20:11:27Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02293v2",
      "landing_url": "https://arxiv.org/abs/2501.02293v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.02293"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on entropy-controlled dithering in audio compression and perceptual quality metrics, with no mention of discrete audio tokens, tokenizers, codebooks, or token-based downstream tasks; thus it does not meet the discrete-audio-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item centers on entropy-controlled dithering in audio compression and perceptual quality metrics, with no mention of discrete audio tokens, tokenizers, codebooks, or token-based downstream tasks; thus it does not meet the discrete-audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on entropy-controlled dithering techniques in audio compression, emphasizing perceptual quality and algorithmic improvements; however, it does not address the production or use of discrete audio tokens, nor does it describe any tokenization, quantization, or discrete codebooks relevant to modeling audio as sequences of tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their role in sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on entropy-controlled dithering techniques in audio compression, emphasizing perceptual quality and algorithmic improvements; however, it does not address the production or use of discrete audio tokens, nor does it describe any tokenization, quantization, or discrete codebooks relevant to modeling audio as sequences of tokens. Therefore, it does not meet the inclusion criteria centered on discrete audio tokens and their role in sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
    "abstract": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
    "metadata": {
      "arxiv_id": "2501.02350",
      "title": "PM-Dedup: Secure Deduplication with Partial Migration from Cloud to Edge Servers",
      "summary": "Currently, an increasing number of users and enterprises are storing their data in the cloud but do not fully trust cloud providers with their data in plaintext form. To address this concern, they encrypt their data before uploading it to the cloud. However, encryption with different keys means that even identical data will become different ciphertexts, making deduplication less effective. Encrypted deduplication avoids this issue by ensuring that identical data chunks generate the same ciphertext with content-based keys, enabling the cloud to efficiently identify and remove duplicates even in encrypted form. Current encrypted data deduplication work can be classified into two types: target-based and source-based. Target-based encrypted deduplication requires clients to upload all encrypted chunks (the basic unit of deduplication) to the cloud with high network bandwidth overhead. Source-based deduplication involves clients uploading fingerprints (hashes) of encrypted chunks for duplicate checking and only uploading unique encrypted chunks, which reduces network transfer but introduces high latency and potential side-channel attacks, which need to be mitigated by Proof of Ownership (PoW), and high computing overhead of the cloud. So, reducing the latency and the overheads of network and cloud while ensuring security has become a significant challenge for secure data deduplication in cloud storage. In response to this challenge, we present PM-Dedup, a novel secure source-based deduplication approach that relocates a portion of the deduplication checking process and PoW tasks from the cloud to the trusted execution environments (TEEs) in the client-side edge servers. We also propose various designs to enhance the security and efficiency of data deduplication.",
      "authors": [
        "Zhaokang Ke",
        "Haoyu Gong",
        "David H. C. Du"
      ],
      "published": "2025-01-04T18:12:23Z",
      "updated": "2025-01-04T18:12:23Z",
      "categories": [
        "cs.CR",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.02350v1",
      "landing_url": "https://arxiv.org/abs/2501.02350v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.02350"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses secure encrypted deduplication for cloud storage using TEEs and PoW, with no discussion of discrete audio tokens, tokenizers, or audio token design, so it does not meet the topic criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses secure encrypted deduplication for cloud storage using TEEs and PoW, with no discussion of discrete audio tokens, tokenizers, or audio token design, so it does not meet the topic criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on secure data deduplication techniques involving cloud and edge servers, without any mention of discrete audio tokens, audio waveforms, or related tokenization methods for audio data. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens and their applications and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on secure data deduplication techniques involving cloud and edge servers, without any mention of discrete audio tokens, audio waveforms, or related tokenization methods for audio data. Therefore, this study does not meet the inclusion criteria related to discrete audio tokens and their applications and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
    "abstract": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
    "metadata": {
      "arxiv_id": "2501.04644",
      "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
      "summary": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
      "authors": [
        "Hanzhao Li",
        "Yuke Li",
        "Xinsheng Wang",
        "Jingbin Hu",
        "Qicong Xie",
        "Shan Yang",
        "Lei Xie"
      ],
      "published": "2025-01-08T17:52:35Z",
      "updated": "2025-04-30T09:30:49Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04644v2",
      "landing_url": "https://arxiv.org/abs/2501.04644v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.04644"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents FleSpeech, a multi-stage speech generation framework using multimodal prompts, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item presents FleSpeech, a multi-stage speech generation framework using multimodal prompts, but it does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a flexible speech generation framework (FleSpeech) driven by multimodal prompts including text, audio, and visual inputs but does not explicitly mention the use or design of discrete audio tokens (e.g., vector quantized tokens or discrete tokenizers) which are essential for inclusion; no details about tokenization, codebooks, or discretization process for continuous audio are provided, and the study primarily seems to address continuous speech generation with multimodal control rather than discrete token-based modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a flexible speech generation framework (FleSpeech) driven by multimodal prompts including text, audio, and visual inputs but does not explicitly mention the use or design of discrete audio tokens (e.g., vector quantized tokens or discrete tokenizers) which are essential for inclusion; no details about tokenization, codebooks, or discretization process for continuous audio are provided, and the study primarily seems to address continuous speech generation with multimodal control rather than discrete token-based modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FreeSVC: Towards Zero-shot Multilingual Singing Voice Conversion",
    "abstract": "This work presents FreeSVC, a promising multilingual singing voice conversion approach that leverages an enhanced VITS model with Speaker-invariant Clustering (SPIN) for better content representation and the State-of-the-Art (SOTA) speaker encoder ECAPA2. FreeSVC incorporates trainable language embeddings to handle multiple languages and employs an advanced speaker encoder to disentangle speaker characteristics from linguistic content. Designed for zero-shot learning, FreeSVC enables cross-lingual singing voice conversion without extensive language-specific training. We demonstrate that a multilingual content extractor is crucial for optimal cross-language conversion. Our source code and models are publicly available.",
    "metadata": {
      "arxiv_id": "2501.05586",
      "title": "FreeSVC: Towards Zero-shot Multilingual Singing Voice Conversion",
      "summary": "This work presents FreeSVC, a promising multilingual singing voice conversion approach that leverages an enhanced VITS model with Speaker-invariant Clustering (SPIN) for better content representation and the State-of-the-Art (SOTA) speaker encoder ECAPA2. FreeSVC incorporates trainable language embeddings to handle multiple languages and employs an advanced speaker encoder to disentangle speaker characteristics from linguistic content. Designed for zero-shot learning, FreeSVC enables cross-lingual singing voice conversion without extensive language-specific training. We demonstrate that a multilingual content extractor is crucial for optimal cross-language conversion. Our source code and models are publicly available.",
      "authors": [
        "Alef Iury Siqueira Ferreira",
        "Lucas Rafael Gris",
        "Augusto Seben da Rosa",
        "Frederico Santos de Oliveira",
        "Edresson Casanova",
        "Rafael Teixeira Sousa",
        "Arnaldo Candido Junior",
        "Anderson da Silva Soares",
        "Arlindo Galvão Filho"
      ],
      "published": "2025-01-09T21:39:09Z",
      "updated": "2025-01-09T21:39:09Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05586v1",
      "landing_url": "https://arxiv.org/abs/2501.05586v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890068"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on zero-shot multilingual singing voice conversion using VITS/SPIN with no mention of discrete audio tokens, tokenization, or neural codec-based token representations; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on zero-shot multilingual singing voice conversion using VITS/SPIN with no mention of discrete audio tokens, tokenization, or neural codec-based token representations; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a multilingual singing voice conversion model but does not mention generating or using discrete audio tokens derived from discretization processes like neural audio codecs or self-supervised representation learning; thus, it lacks core content related to discrete token generation, representation, or design which is critical for inclusion under the specified criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study presents a multilingual singing voice conversion model but does not mention generating or using discrete audio tokens derived from discretization processes like neural audio codecs or self-supervised representation learning; thus, it lacks core content related to discrete token generation, representation, or design which is critical for inclusion under the specified criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
    "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a codec-based TTS model that uses discrete 'speech tokens' processed in a hierarchical decoder, implying discrete audio token generation—aligning with the discrete audio tokens focus, though explicit token design details are limited.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a codec-based TTS model that uses discrete 'speech tokens' processed in a hierarchical decoder, implying discrete audio token generation—aligning with the discrete audio tokens focus, though explicit token design details are limited.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes MARS6 as a codec-based TTS model built on spoken language modeling improvements, using hierarchical speech tokens processed at 12 Hz which implies some form of tokenization of speech signals; although the abstract does not explicitly detail the discrete token generation process or provide specifics on tokenizer/codec architecture, the mention of codec-based methods aligns with the use of neural audio codecs generating discrete tokens, fulfilling inclusion criteria on token source and downstream usage in TTS; the abstract is in English and provides sufficient methodological clues to consider the tokens as discrete audio tokens used in synthesis, justifying inclusion.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The abstract describes MARS6 as a codec-based TTS model built on spoken language modeling improvements, using hierarchical speech tokens processed at 12 Hz which implies some form of tokenization of speech signals; although the abstract does not explicitly detail the discrete token generation process or provide specifics on tokenizer/codec architecture, the mention of codec-based methods aligns with the use of neural audio codecs generating discrete tokens, fulfilling inclusion criteria on token source and downstream usage in TTS; the abstract is in English and provides sufficient methodological clues to consider the tokens as discrete audio tokens used in synthesis, justifying inclusion.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing",
    "abstract": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model.",
    "metadata": {
      "arxiv_id": "2501.13831",
      "title": "Predicting Compact Phrasal Rewrites with Large Language Models for ASR Post Editing",
      "summary": "Large Language Models (LLMs) excel at rewriting tasks such as text style transfer and grammatical error correction. While there is considerable overlap between the inputs and outputs in these tasks, the decoding cost still increases with output length, regardless of the amount of overlap. By leveraging the overlap between the input and the output, Kaneko and Okazaki (2023) proposed model-agnostic edit span representations to compress the rewrites to save computation. They reported an output length reduction rate of nearly 80% with minimal accuracy impact in four rewriting tasks. In this paper, we propose alternative edit phrase representations inspired by phrase-based statistical machine translation. We systematically compare our phrasal representations with their span representations. We apply the LLM rewriting model to the task of Automatic Speech Recognition (ASR) post editing and show that our target-phrase-only edit representation has the best efficiency-accuracy trade-off. On the LibriSpeech test set, our method closes 50-60% of the WER gap between the edit span model and the full rewrite model while losing only 10-20% of the length reduction rate of the edit span model.",
      "authors": [
        "Hao Zhang",
        "Felix Stahlberg",
        "Shankar Kumar"
      ],
      "published": "2025-01-23T16:54:27Z",
      "updated": "2025-01-23T16:54:27Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13831v1",
      "landing_url": "https://arxiv.org/abs/2501.13831v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.13831"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on LLM-based rewrites for ASR post-editing and efficiency via edit-span representations, not on discrete audio tokens, tokenizers, or token-based modeling of audio; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on LLM-based rewrites for ASR post-editing and efficiency via edit-span representations, not on discrete audio tokens, tokenizers, or token-based modeling of audio; thus it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on using Large Language Models for post-editing Automatic Speech Recognition outputs, emphasizing rewriting and efficiency improvements rather than discretizing continuous audio waveforms into discrete audio tokens. There is no indication of producing discrete token sequences from audio via neural audio codecs or vector quantization, nor is there discussion of token vocabularies or quantization details, which are central to the inclusion criteria. Therefore, it does not meet the criteria for research involving discrete audio tokens as core representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on using Large Language Models for post-editing Automatic Speech Recognition outputs, emphasizing rewriting and efficiency improvements rather than discretizing continuous audio waveforms into discrete audio tokens. There is no indication of producing discrete token sequences from audio via neural audio codecs or vector quantization, nor is there discussion of token vocabularies or quantization details, which are central to the inclusion criteria. Therefore, it does not meet the criteria for research involving discrete audio tokens as core representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task Learning",
    "abstract": "Voice conversion (VC) modifies voice characteristics while preserving linguistic content. This paper presents the Stepback network, a novel model for converting speaker identity using non-parallel data. Unlike traditional VC methods that rely on parallel data, our approach leverages deep learning techniques to enhance disentanglement completion and linguistic content preservation. The Stepback network incorporates a dual flow of different domain data inputs and uses constraints with self-destructive amendments to optimize the content encoder. Extensive experiments show that our model significantly improves VC performance, reducing training costs while achieving high-quality voice conversion. The Stepback network's design offers a promising solution for advanced voice conversion tasks.",
    "metadata": {
      "arxiv_id": "2501.15613",
      "title": "Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task Learning",
      "summary": "Voice conversion (VC) modifies voice characteristics while preserving linguistic content. This paper presents the Stepback network, a novel model for converting speaker identity using non-parallel data. Unlike traditional VC methods that rely on parallel data, our approach leverages deep learning techniques to enhance disentanglement completion and linguistic content preservation. The Stepback network incorporates a dual flow of different domain data inputs and uses constraints with self-destructive amendments to optimize the content encoder. Extensive experiments show that our model significantly improves VC performance, reducing training costs while achieving high-quality voice conversion. The Stepback network's design offers a promising solution for advanced voice conversion tasks.",
      "authors": [
        "Qian Yang",
        "Calbert Graham"
      ],
      "published": "2025-01-26T17:43:32Z",
      "updated": "2025-01-26T17:43:32Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15613v1",
      "landing_url": "https://arxiv.org/abs/2501.15613v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.15613"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a voice-conversion model with non-parallel data and disentanglement, with no mention of discrete audio tokens or tokenization schemes, so it does not meet the inclusion criteria and aligns with the exclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a voice-conversion model with non-parallel data and disentanglement, with no mention of discrete audio tokens or tokenization schemes, so it does not meet the inclusion criteria and aligns with the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on voice conversion using deep learning, aiming at speaker identity conversion with non-parallel data, but does not mention the use or design of discrete audio tokens, neural audio codecs with quantization, tokenizers, or any discrete representation mapping continuous audio into token sequences as required by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on voice conversion using deep learning, aiming at speaker identity conversion with non-parallel data, but does not mention the use or design of discrete audio tokens, neural audio codecs with quantization, tokenizers, or any discrete representation mapping continuous audio into token sequences as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
    "abstract": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
    "metadata": {
      "arxiv_id": "2501.15907",
      "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
      "summary": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2025-01-27T09:59:20Z",
      "updated": "2025-10-08T06:46:48Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15907v2",
      "landing_url": "https://arxiv.org/abs/2501.15907v2",
      "doi": "https://doi.org/10.1109/TASLPRO.2025.3612835"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on large-scale multilingual speech datasets and a preprocessing pipeline, but provides no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio token inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on large-scale multilingual speech datasets and a preprocessing pipeline, but provides no discussion of discrete audio tokens, tokenizers, or token-based modeling, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article introduces a large-scale multilingual dataset for speech generation but does not mention the creation, use, or analysis of discrete audio tokens or tokenization methods that transform continuous audio into discrete token sequences as required by the inclusion criteria. It focuses on dataset collection and speech model training rather than discrete token design or evaluation associated with neural audio codecs or self-supervised models.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The article introduces a large-scale multilingual dataset for speech generation but does not mention the creation, use, or analysis of discrete audio tokens or tokenization methods that transform continuous audio into discrete token sequences as required by the inclusion criteria. It focuses on dataset collection and speech model training rather than discrete token design or evaluation associated with neural audio codecs or self-supervised models.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Fixed-sized clusters $k$-Means",
    "abstract": "We present a $k$-means-based clustering algorithm, which optimizes the mean square error, for given cluster sizes. A straightforward application is balanced clustering, where the sizes of each cluster are equal. In the $k$-means assignment phase, the algorithm solves an assignment problem using the Hungarian algorithm. This makes the assignment phase time complexity $O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
    "metadata": {
      "arxiv_id": "2501.16113",
      "title": "Fixed-sized clusters $k$-Means",
      "summary": "We present a $k$-means-based clustering algorithm, which optimizes the mean square error, for given cluster sizes. A straightforward application is balanced clustering, where the sizes of each cluster are equal. In the $k$-means assignment phase, the algorithm solves an assignment problem using the Hungarian algorithm. This makes the assignment phase time complexity $O(n^3)$. This enables clustering of datasets of size more than 5000 points.",
      "authors": [
        "Mikko I. Malinen",
        "Pasi Fränti"
      ],
      "published": "2025-01-27T15:04:35Z",
      "updated": "2025-01-27T15:04:35Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.16113v1",
      "landing_url": "https://arxiv.org/abs/2501.16113v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.16113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a k-means clustering algorithm with fixed-size clusters and a Hungarian-assignment step, addressing generic clustering performance rather than discrete audio tokens, tokenization schemes, or downstream audio generation/understanding tasks; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses a k-means clustering algorithm with fixed-size clusters and a Hungarian-assignment step, addressing generic clustering performance rather than discrete audio tokens, tokenization schemes, or downstream audio generation/understanding tasks; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a clustering algorithm based on fixed-sized k-Means focusing on balanced clustering and computational complexity. There is no mention of discrete audio tokens, neural audio codecs, vector quantization, or any audio-related tokenization or modeling. The work appears unrelated to the discrete audio token topic focusing on audio waveform discretization and token sequence modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a clustering algorithm based on fixed-sized k-Means focusing on balanced clustering and computational complexity. There is no mention of discrete audio tokens, neural audio codecs, vector quantization, or any audio-related tokenization or modeling. The work appears unrelated to the discrete audio token topic focusing on audio waveform discretization and token sequence modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimized Self-supervised Training with BEST-RQ for Speech Recognition",
    "abstract": "Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.",
    "metadata": {
      "arxiv_id": "2501.16131",
      "title": "Optimized Self-supervised Training with BEST-RQ for Speech Recognition",
      "summary": "Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.",
      "authors": [
        "Ilja Baumann",
        "Dominik Wagner",
        "Korbinian Riedhammer",
        "Tobias Bocklet"
      ],
      "published": "2025-01-27T15:20:50Z",
      "updated": "2025-01-27T15:20:50Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.16131v1",
      "landing_url": "https://arxiv.org/abs/2501.16131v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.16131"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes BERT-based Speech pre-Training with a Random-projection Quantizer (BEST-RQ) and a multi-codebook extension, implying discrete tokens produced by a neural quantizer/codec and used as inputs for downstream speech modeling, which aligns with the discrete-audio-token criteria (token generation, codebooks, multi-codebooks, and evaluation on ASR).",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract describes BERT-based Speech pre-Training with a Random-projection Quantizer (BEST-RQ) and a multi-codebook extension, implying discrete tokens produced by a neural quantizer/codec and used as inputs for downstream speech modeling, which aligns with the discrete-audio-token criteria (token generation, codebooks, multi-codebooks, and evaluation on ASR).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optimizing self-supervised training for speech recognition using BEST-RQ, which involves a random-projection quantizer and multi-codebook extensions, implying the use of discrete token representations for audio; however, the abstract does not explicitly describe the details of the tokenizer, vocabulary, or quantization settings, which are necessary for confirming alignment with the inclusion criteria emphasizing detailed token design and reproducibility. Therefore, while the paper appears relevant in the use of discrete audio tokens for speech tasks, the lack of explicit technical detail on token generation leads to some uncertainty.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optimizing self-supervised training for speech recognition using BEST-RQ, which involves a random-projection quantizer and multi-codebook extensions, implying the use of discrete token representations for audio; however, the abstract does not explicitly describe the details of the tokenizer, vocabulary, or quantization settings, which are necessary for confirming alignment with the inclusion criteria emphasizing detailed token design and reproducibility. Therefore, while the paper appears relevant in the use of discrete audio tokens for speech tasks, the lack of explicit technical detail on token generation leads to some uncertainty.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly describes using a random-projection quantizer (BEST-RQ) and a multi-codebook extension derived from low-level feature clustering, which indicates the study derives discrete audio tokens (multi-codebook quantization) and uses them in self-supervised pretraining and downstream ASR—fulfilling the token-source/mechanism inclusion requirements; junior reviewers were split (one included, one unsure due to missing tokenizer/configuration details). However, the abstract does not report full tokenizer/codebook/vocabulary or bitrate details needed for full reproducibility, so while it meets the core discrete-token scope, some methodological detail is lacking. Score justification (one sentence): 4 — better to include, because discrete multi-codebook quantization is central to the method and used for downstream modeling, though the abstract omits some tokenizer/codebook specifics required for complete reproducibility.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly describes using a random-projection quantizer (BEST-RQ) and a multi-codebook extension derived from low-level feature clustering, which indicates the study derives discrete audio tokens (multi-codebook quantization) and uses them in self-supervised pretraining and downstream ASR—fulfilling the token-source/mechanism inclusion requirements; junior reviewers were split (one included, one unsure due to missing tokenizer/configuration details). However, the abstract does not report full tokenizer/codebook/vocabulary or bitrate details needed for full reproducibility, so while it meets the core discrete-token scope, some methodological detail is lacking. Score justification (one sentence): 4 — better to include, because discrete multi-codebook quantization is central to the method and used for downstream modeling, though the abstract omits some tokenizer/codebook specifics required for complete reproducibility.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
    "abstract": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
    "metadata": {
      "arxiv_id": "2502.00250",
      "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
      "summary": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
      "authors": [
        "Takumu Fujioka",
        "Gouhei Tanaka"
      ],
      "published": "2025-02-01T01:16:27Z",
      "updated": "2025-02-01T01:16:27Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00250v1",
      "landing_url": "https://arxiv.org/abs/2502.00250v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00250"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract discuss transformer-based vector font classification using outline formats (TrueType vs PostScript), with no mention of discrete audio tokens or tokenization/codec design; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The title/abstract discuss transformer-based vector font classification using outline formats (TrueType vs PostScript), with no mention of discrete audio tokens or tokenization/codec design; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on Transformer-based classification of vector fonts using TrueType and PostScript outlines, which is unrelated to discrete audio tokens or audio waveform discretization; it does not discuss audio tokenization, discrete audio representations, or audio generation tasks, thus failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on Transformer-based classification of vector fonts using TrueType and PostScript outlines, which is unrelated to discrete audio tokens or audio waveform discretization; it does not discuss audio tokenization, discrete audio representations, or audio generation tasks, thus failing the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
    "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The abstract explicitly uses discrete semantic and acoustic tokens produced by a self-supervised model and a neural codec, aligning with the discrete audio token criteria and providing methodological details suitable for reproducibility.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The abstract explicitly uses discrete semantic and acoustic tokens produced by a self-supervised model and a neural codec, aligning with the discrete audio token criteria and providing methodological details suitable for reproducibility.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study employs a neural codec model with a single quantizer for tokenizing speech signals into acoustic tokens and uses a pre-trained self-supervised model for semantic tokens, both of which align with the inclusion criteria of discrete audio tokens obtained via neural audio codecs or self-supervised models; the work focuses on generation and enhancement using these discrete tokens modeled by language models, providing details on tokenization and hierarchical modeling, thus meeting the systematic design and evaluation expectations, and includes an English abstract and title with methodological details sufficient for assessment.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study employs a neural codec model with a single quantizer for tokenizing speech signals into acoustic tokens and uses a pre-trained self-supervised model for semantic tokens, both of which align with the inclusion criteria of discrete audio tokens obtained via neural audio codecs or self-supervised models; the work focuses on generation and enhancement using these discrete tokens modeled by language models, providing details on tokenization and hierarchical modeling, thus meeting the systematic design and evaluation expectations, and includes an English abstract and title with methodological details sufficient for assessment.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
    "abstract": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
    "metadata": {
      "arxiv_id": "2502.03128",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "authors": [
        "Yuancheng Wang",
        "Jiachen Zheng",
        "Junan Zhang",
        "Xueyao Zhang",
        "Huan Liao",
        "Zhizheng Wu"
      ],
      "published": "2025-02-05T12:36:21Z",
      "updated": "2025-02-05T12:36:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03128v1",
      "landing_url": "https://arxiv.org/abs/2502.03128v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item centers on discrete speech representations (SSL tokens and acoustic tokens quantized from waveforms) and uses masked generative pre-training on these tokens for unified speech generation, directly addressing token design/usage for downstream generation tasks, thus matching the Discrete Audio Tokens topic. English title/abstract are provided and describe the token-generation process sufficiently for assessment.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item centers on discrete speech representations (SSL tokens and acoustic tokens quantized from waveforms) and uses masked generative pre-training on these tokens for unified speech generation, directly addressing token design/usage for downstream generation tasks, thus matching the Discrete Audio Tokens topic. English title/abstract are provided and describe the token-generation process sufficiently for assessment.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Metis, a speech generation foundation model using masked generative pre-training with two types of discrete speech representations: SSL tokens from self-supervised learning and acoustic tokens directly quantized from waveforms. These tokens are explicitly described as discrete and derived through quantization methods, aligning well with the theme of discrete audio tokens. The work addresses multiple speech generation tasks, offering empirical evaluation and comprehensive details, fulfilling the inclusion criteria. The abstract is provided in English with sufficient methodological clarity, supporting reproducibility assessment. Hence, the paper meets all inclusion criteria and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents Metis, a speech generation foundation model using masked generative pre-training with two types of discrete speech representations: SSL tokens from self-supervised learning and acoustic tokens directly quantized from waveforms. These tokens are explicitly described as discrete and derived through quantization methods, aligning well with the theme of discrete audio tokens. The work addresses multiple speech generation tasks, offering empirical evaluation and comprehensive details, fulfilling the inclusion criteria. The abstract is provided in English with sufficient methodological clarity, supporting reproducibility assessment. Hence, the paper meets all inclusion criteria and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
    "abstract": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
    "metadata": {
      "arxiv_id": "2502.03605",
      "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
      "summary": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
      "authors": [
        "Subhadip Ghosh",
        "Endalk Y. Gebru",
        "Chandramouli V. Kashyap",
        "Ramesh Harjani",
        "Sachin S. Sapatnekar"
      ],
      "published": "2025-02-05T20:48:27Z",
      "updated": "2025-02-05T20:48:27Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03605v1",
      "landing_url": "https://arxiv.org/abs/2502.03605v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03605"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses OTA transistor sizing using a Transformer model and lookup tables, with no discussion of discrete audio tokens or tokenization; it does not meet the topic criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item discusses OTA transistor sizing using a Transformer model and lookup tables, with no discussion of discrete audio tokens or tokenization; it does not meet the topic criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on transistor sizing for OTA circuit design using a transformer model and lookup tables, which is related to electronic circuit design rather than audio signal processing or discrete audio token modeling. There is no mention of discretizing audio waveforms into token sequences or neural audio codecs, nor any discussion about audio generation, understanding, or cross-modal reasoning. Therefore, this study does not meet the inclusion criteria focused on discrete audio tokens and is unrelated to the specified research topic.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on transistor sizing for OTA circuit design using a transformer model and lookup tables, which is related to electronic circuit design rather than audio signal processing or discrete audio token modeling. There is no mention of discretizing audio waveforms into token sequences or neural audio codecs, nor any discussion about audio generation, understanding, or cross-modal reasoning. Therefore, this study does not meet the inclusion criteria focused on discrete audio tokens and is unrelated to the specified research topic.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
    "abstract": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦于通过扩散- Transformer 的自回归建模来处理连续音频表示，并强调在推理阶段不使用离散音频 token，未提供离散化的 token/码本/字彙表设定，也未以离散 token 作为核心方法，因此不符合“Discrete Audio Tokens”主题的纳入条件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦于通过扩散- Transformer 的自回归建模来处理连续音频表示，并强调在推理阶段不使用离散音频 token，未提供离散化的 token/码本/字彙表设定，也未以离散 token 作为核心方法，因此不符合“Discrete Audio Tokens”主题的纳入条件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on autoregressive speech generation combining diffusion and transformer models but does not mention the use or design of discrete audio tokens, neural audio codecs, or quantization mechanisms that produce a finite token vocabulary; the method appears to work on continuous speech representations without discrete tokenization, which does not meet the inclusion criteria and aligns with exclusion criteria regarding continuous audio representation modeling without discrete tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on autoregressive speech generation combining diffusion and transformer models but does not mention the use or design of discrete audio tokens, neural audio codecs, or quantization mechanisms that produce a finite token vocabulary; the method appears to work on continuous speech representations without discrete tokenization, which does not meet the inclusion criteria and aligns with exclusion criteria regarding continuous audio representation modeling without discrete tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
    "abstract": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
    "metadata": {
      "arxiv_id": "2502.04519",
      "title": "GenVC: Self-Supervised Zero-Shot Voice Conversion",
      "summary": "Most current zero-shot voice conversion methods rely on externally supervised components, particularly speaker encoders, for training. To explore alternatives that eliminate this dependency, this paper introduces GenVC, a novel framework that disentangles speaker identity and linguistic content from speech signals in a self-supervised manner. GenVC leverages speech tokenizers and an autoregressive, Transformer-based language model as its backbone for speech generation. This design supports large-scale training while enhancing both source speaker privacy protection and target speaker cloning fidelity. Experimental results demonstrate that GenVC achieves notably higher speaker similarity, with naturalness on par with leading zero-shot approaches. Moreover, due to its autoregressive formulation, GenVC introduces flexibility in temporal alignment, reducing the preservation of source prosody and speaker-specific traits, and making it highly effective for voice anonymization.",
      "authors": [
        "Zexin Cai",
        "Henry Li Xinyuan",
        "Ashi Garg",
        "Leibny Paola García-Perera",
        "Kevin Duh",
        "Sanjeev Khudanpur",
        "Matthew Wiesner",
        "Nicholas Andrews"
      ],
      "published": "2025-02-06T21:40:09Z",
      "updated": "2025-08-20T17:34:21Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04519v2",
      "landing_url": "https://arxiv.org/abs/2502.04519v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.04519"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly mentions ‘speech tokenizers’ and using an autoregressive Transformer LM as the backbone for speech generation in a self-supervised setup, indicating discrete audio token representations and downstream generation tasks, which aligns with the inclusion criteria for discrete audio tokens and token-based modeling; while detailed token design is not exhaustively described, the core emphasis on discrete tokens suggests eligibility.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly mentions ‘speech tokenizers’ and using an autoregressive Transformer LM as the backbone for speech generation in a self-supervised setup, indicating discrete audio token representations and downstream generation tasks, which aligns with the inclusion criteria for discrete audio tokens and token-based modeling; while detailed token design is not exhaustively described, the core emphasis on discrete tokens suggests eligibility.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study describes a voice conversion framework using self-supervised learning and Transformer-based models, but it does not detail the use of discrete audio tokens derived from neural audio codecs or clustering-based tokenizers, nor does it describe the tokenization process or vocabulary/codebook settings as required by inclusion criteria. It focuses on voice conversion effectiveness rather than on discrete audio token design, evaluation, or generation aspects central to the theme. Therefore, it does not meet the inclusion criteria of using discrete audio tokens as core representations for audio modeling/generation and lacks sufficient detail on tokenization for reproducibility assessment.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study describes a voice conversion framework using self-supervised learning and Transformer-based models, but it does not detail the use of discrete audio tokens derived from neural audio codecs or clustering-based tokenizers, nor does it describe the tokenization process or vocabulary/codebook settings as required by inclusion criteria. It focuses on voice conversion effectiveness rather than on discrete audio token design, evaluation, or generation aspects central to the theme. Therefore, it does not meet the inclusion criteria of using discrete audio tokens as core representations for audio modeling/generation and lacks sufficient detail on tokenization for reproducibility assessment.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract indicates a token-based approach by referencing \"speech tokenizers\" and an autoregressive Transformer language model for speech generation, and JuniorNano judged this as aligning with discrete-token modeling; however, JuniorMini correctly notes the abstract provides no details on how discrete tokens are produced (no tokenizer/codec architecture, quantization or codebook/vocabulary/rate settings) as required by the inclusion criteria. Because the inclusion rules require clear, reproducible description of discrete token generation or that the paper’s main contribution centers on discrete audio tokens, and the provided abstract lacks those specifics, this item should likely be excluded. Final one-sentence explanation: Abstract lacks the necessary methodological details about discrete token generation (tokenizer/codec/codebook settings), so it does not meet the inclusion requirements.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract indicates a token-based approach by referencing \"speech tokenizers\" and an autoregressive Transformer language model for speech generation, and JuniorNano judged this as aligning with discrete-token modeling; however, JuniorMini correctly notes the abstract provides no details on how discrete tokens are produced (no tokenizer/codec architecture, quantization or codebook/vocabulary/rate settings) as required by the inclusion criteria. Because the inclusion rules require clear, reproducible description of discrete token generation or that the paper’s main contribution centers on discrete audio tokens, and the provided abstract lacks those specifics, this item should likely be excluded. Final one-sentence explanation: Abstract lacks the necessary methodological details about discrete token generation (tokenizer/codec/codebook settings), so it does not meet the inclusion requirements.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
    "abstract": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract indicates Koel-TTS maps text and context audio to acoustic tokens and uses token-based representations for generation, aligning with the discrete audio tokens topic, though it does not provide detailed tokenization architecture or token-level evaluation in the abstract.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract indicates Koel-TTS maps text and context audio to acoustic tokens and uses token-based representations for generation, aligning with the discrete audio tokens topic, though it does not provide detailed tokenization architecture or token-level evaluation in the abstract.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses Koel-TTS, a text-to-speech model that synthesizes speech by mapping text and context audio to acoustic tokens. However, it does not explicitly mention the use, design, or generation details of discrete audio tokens as defined by the inclusion criteria (e.g., neural audio codec-based vector quantization, multi-codebook tokenization, or clustering-based discrete units). The abstract focuses on synthesis quality improvements and guidance methods but lacks clarity on whether discrete token sequences are the core representation, and does not specify the tokenization process or architecture details necessary for reproducibility and verification. Therefore, it does not fully meet the inclusion criteria focused on discrete audio tokens and their systematized evaluation, nor does it suffer from any explicit exclusion criteria beyond insufficient detail on discrete tokens.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses Koel-TTS, a text-to-speech model that synthesizes speech by mapping text and context audio to acoustic tokens. However, it does not explicitly mention the use, design, or generation details of discrete audio tokens as defined by the inclusion criteria (e.g., neural audio codec-based vector quantization, multi-codebook tokenization, or clustering-based discrete units). The abstract focuses on synthesis quality improvements and guidance methods but lacks clarity on whether discrete token sequences are the core representation, and does not specify the tokenization process or architecture details necessary for reproducibility and verification. Therefore, it does not fully meet the inclusion criteria focused on discrete audio tokens and their systematized evaluation, nor does it suffer from any explicit exclusion criteria beyond insufficient detail on discrete tokens.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviews are considered: JuniorNano noted the paper maps text and context audio to acoustic tokens and treats token-based representations (leaning toward inclusion), while JuniorMini pointed out the abstract lacks explicit tokenizer/codec/quantization details required by the Discrete Audio Tokens inclusion criteria (leaning toward exclusion). The abstract indeed claims Koel-TTS “maps text and context audio to acoustic tokens” and uses token-based generation, but it does not describe the discrete-token generation process (e.g., neural codec or vector quantization, codebook settings, bitrate/time downsampling) or provide token-level design/evaluation details required by the topic’s eligibility rules. Because the inclusion criteria require clear English description of how discrete tokens are produced and used (to support reproducibility and confirm tokens are the core representation) and the abstract fails to provide those specifics, the item should not be included based on the available information.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviews are considered: JuniorNano noted the paper maps text and context audio to acoustic tokens and treats token-based representations (leaning toward inclusion), while JuniorMini pointed out the abstract lacks explicit tokenizer/codec/quantization details required by the Discrete Audio Tokens inclusion criteria (leaning toward exclusion). The abstract indeed claims Koel-TTS “maps text and context audio to acoustic tokens” and uses token-based generation, but it does not describe the discrete-token generation process (e.g., neural codec or vector quantization, codebook settings, bitrate/time downsampling) or provide token-level design/evaluation details required by the topic’s eligibility rules. Because the inclusion criteria require clear English description of how discrete tokens are produced and used (to support reproducibility and confirm tokens are the core representation) and the abstract fails to provide those specifics, the item should not be included based on the available information.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
    "abstract": "Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.",
    "metadata": {
      "arxiv_id": "2502.05512",
      "title": "IndexTTS: An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System",
      "summary": "Recently, large language model (LLM) based text-to-speech (TTS) systems have gradually become the mainstream in the industry due to their high naturalness and powerful zero-shot voice cloning capabilities.Here, we introduce the IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid modeling method that combines characters and pinyin, making the pronunciations of polyphonic characters and long-tail characters controllable. We also performed a comparative analysis of the Vector Quantization (VQ) with Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech tokens. To further enhance the effect and stability of voice cloning, we introduce a conformer-based speech conditional encoder and replace the speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved significant improvements in naturalness, content consistency, and zero-shot voice cloning. As for the popular TTS systems in the open-source, such as Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively simple training process, more controllable usage, and faster inference speed. Moreover, its performance surpasses that of these systems. Our demos are available at https://index-tts.github.io.",
      "authors": [
        "Wei Deng",
        "Siyi Zhou",
        "Jingchen Shu",
        "Jinchao Wang",
        "Lu Wang"
      ],
      "published": "2025-02-08T10:23:20Z",
      "updated": "2025-02-08T10:23:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05512v1",
      "landing_url": "https://arxiv.org/abs/2502.05512v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05512"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses discrete audio tokens via vector quantization and codebooks for acoustic tokens, and provides an English abstract, aligning with the discrete-token criteria (token origin via neural codecs and token usage in generation/analysis).",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The item discusses discrete audio tokens via vector quantization and codebooks for acoustic tokens, and provides an English abstract, aligning with the discrete-token criteria (token origin via neural codecs and token usage in generation/analysis).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents IndexTTS, a zero-shot TTS system that introduces improvements in codebook utilization (Vector Quantization vs. Finite-Scalar Quantization) for acoustic speech tokens and discusses neural audio codec components such as conformer-based speech encoders and BigVGAN2 speechcode decoders. It involves discrete token representations of speech that are used for generation and modeling, fitting the theme of discrete audio tokens. The abstract provides details on tokenization approaches and comparative analysis, which align with the inclusion criteria concerning discrete token generation methods and their application in TTS systems.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents IndexTTS, a zero-shot TTS system that introduces improvements in codebook utilization (Vector Quantization vs. Finite-Scalar Quantization) for acoustic speech tokens and discusses neural audio codec components such as conformer-based speech encoders and BigVGAN2 speechcode decoders. It involves discrete token representations of speech that are used for generation and modeling, fitting the theme of discrete audio tokens. The abstract provides details on tokenization approaches and comparative analysis, which align with the inclusion criteria concerning discrete token generation methods and their application in TTS systems.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
    "abstract": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
    "metadata": {
      "arxiv_id": "2502.05713",
      "title": "4D VQ-GAN: Synthesising Medical Scans at Any Time Point for Personalised Disease Progression Modelling of Idiopathic Pulmonary Fibrosis",
      "summary": "Understanding the progression trajectories of diseases is crucial for early diagnosis and effective treatment planning. This is especially vital for life-threatening conditions such as Idiopathic Pulmonary Fibrosis (IPF), a chronic, progressive lung disease with a prognosis comparable to many cancers. Computed tomography (CT) imaging has been established as a reliable diagnostic tool for IPF. Accurately predicting future CT scans of early-stage IPF patients can aid in developing better treatment strategies, thereby improving survival outcomes. In this paper, we propose 4D Vector Quantised Generative Adversarial Networks (4D-VQ-GAN), a model capable of generating realistic CT volumes of IPF patients at any time point. The model is trained using a two-stage approach. In the first stage, a 3D-VQ-GAN is trained to reconstruct CT volumes. In the second stage, a Neural Ordinary Differential Equation (ODE) based temporal model is trained to capture the temporal dynamics of the quantised embeddings generated by the encoder in the first stage. We evaluate different configurations of our model for generating longitudinal CT scans and compare the results against ground truth data, both quantitatively and qualitatively. For validation, we conduct survival analysis using imaging biomarkers derived from generated CT scans and achieve a C-index comparable to that of biomarkers derived from the real CT scans. The survival analysis results demonstrate the potential clinical utility inherent to generated longitudinal CT scans, showing that they can reliably predict survival outcomes.",
      "authors": [
        "An Zhao",
        "Moucheng Xu",
        "Ahmed H. Shahin",
        "Wim Wuyts",
        "Mark G. Jones",
        "Joseph Jacob",
        "Daniel C. Alexander"
      ],
      "published": "2025-02-08T22:25:53Z",
      "updated": "2025-02-08T22:25:53Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05713v1",
      "landing_url": "https://arxiv.org/abs/2502.05713v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05713"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on 4D-VQ-GAN for generating longitudinal CT scans in IPF, a medical imaging application; it does not center on discrete audio tokens or tokenization for audio, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on 4D-VQ-GAN for generating longitudinal CT scans in IPF, a medical imaging application; it does not center on discrete audio tokens or tokenization for audio, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on synthesizing medical CT scans using 4D VQ-GAN and Neural ODEs to model disease progression in Idiopathic Pulmonary Fibrosis, which involves image data rather than audio signals. The inclusion criteria require research centered on discrete audio tokens derived from discretized audio waveforms for audio generation, understanding, alignment, or cross-modal reasoning, which this study does not address. There is no discussion of neural audio codecs, discrete audio tokens, or related tokenizer/codebook details; hence it does not meet the inclusion criteria and conflicts with the exclusion criteria about the domain focus and token usage.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on synthesizing medical CT scans using 4D VQ-GAN and Neural ODEs to model disease progression in Idiopathic Pulmonary Fibrosis, which involves image data rather than audio signals. The inclusion criteria require research centered on discrete audio tokens derived from discretized audio waveforms for audio generation, understanding, alignment, or cross-modal reasoning, which this study does not address. There is no discussion of neural audio codecs, discrete audio tokens, or related tokenizer/codebook details; hence it does not meet the inclusion criteria and conflicts with the exclusion criteria about the domain focus and token usage.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models",
    "abstract": "Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impact of combining KD loss with alternative pruning techniques, including Low-Rank Factorization (LRF) and l0 regularization, on a conformer-based pre-trained network under the paradigm of Self-Supervised Learning (SSL). We also propose a strategy to jointly prune and train an RNN-T-based ASR model, demonstrating that this approach yields superior performance compared to pruning a pre-trained network first and then using it for ASR training. This approach led to a significant reduction in word error rate: l0 and KD combination achieves the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER) improvement over the baseline, while LRF and KD combination yields the best results for streaming ASR, improving RWER by 13.4%.",
    "metadata": {
      "arxiv_id": "2502.05837",
      "title": "Synergistic Effects of Knowledge Distillation and Structured Pruning for Self-Supervised Speech Models",
      "summary": "Traditionally, Knowledge Distillation (KD) is used for model compression, often leading to suboptimal performance. In this paper, we evaluate the impact of combining KD loss with alternative pruning techniques, including Low-Rank Factorization (LRF) and l0 regularization, on a conformer-based pre-trained network under the paradigm of Self-Supervised Learning (SSL). We also propose a strategy to jointly prune and train an RNN-T-based ASR model, demonstrating that this approach yields superior performance compared to pruning a pre-trained network first and then using it for ASR training. This approach led to a significant reduction in word error rate: l0 and KD combination achieves the best non-streaming performance, with a 8.9% Relative Word Error Rate (RWER) improvement over the baseline, while LRF and KD combination yields the best results for streaming ASR, improving RWER by 13.4%.",
      "authors": [
        "Shiva Kumar C",
        "Jitendra Kumar Dhiman",
        "Nagaraj Adiga",
        "Shatrughan Singh"
      ],
      "published": "2025-02-09T10:17:25Z",
      "updated": "2025-02-09T10:17:25Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05837v1",
      "landing_url": "https://arxiv.org/abs/2502.05837v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05837"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on knowledge distillation and structured pruning for self-supervised speech models and ASR, with no discussion of discrete audio tokens, tokenization, codebooks, or token-based modeling; therefore it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on knowledge distillation and structured pruning for self-supervised speech models and ASR, with no discussion of discrete audio tokens, tokenization, codebooks, or token-based modeling; therefore it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on knowledge distillation and pruning techniques for self-supervised speech models, aiming to improve ASR performance, but it does not mention any discrete audio tokens or the process of discretizing continuous audio into token sequences, nor does it provide details on tokenizers or codebooks; therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on continuous model compression rather than discrete representation learning.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on knowledge distillation and pruning techniques for self-supervised speech models, aiming to improve ASR performance, but it does not mention any discrete audio tokens or the process of discretizing continuous audio into token sequences, nor does it provide details on tokenizers or codebooks; therefore, it does not meet the inclusion criteria related to discrete audio tokens and focuses on continuous model compression rather than discrete representation learning.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
    "abstract": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
    "metadata": {
      "arxiv_id": "2502.09520",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "authors": [
        "Francesco Pezone",
        "Sergio Barbarossa",
        "Giuseppe Caire"
      ],
      "published": "2025-02-13T17:35:57Z",
      "updated": "2025-10-10T10:21:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09520v2",
      "landing_url": "https://arxiv.org/abs/2502.09520v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09520"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on semantic image communications using masked vector quantization for images, not on discrete audio tokens or token-based audio representations, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on semantic image communications using masked vector quantization for images, not on discrete audio tokens or token-based audio representations, so it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study SQ-GAN focuses on semantic image communications using masked vector quantization, specifically involving image compression and semantic segmentation maps; it does not address discrete audio tokens or audio waveform discretization for audio modeling, generation, or cross-modal tasks as defined by the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study SQ-GAN focuses on semantic image communications using masked vector quantization, specifically involving image compression and semantic segmentation maps; it does not address discrete audio tokens or audio waveform discretization for audio modeling, generation, or cross-modal tasks as defined by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Construction A Lattice Design Based on the Truncated Union Bound",
    "abstract": "This paper considers $n= 128$ dimensional construction A lattice design, using binary codes with known minimum Hamming distance and codeword multiplicity, the number of minimum weight codeword. A truncated theta series of the lattice is explicitly given to obtain the truncated union bound to estimate the word error rate under maximum likelihood decoding. The best component code is selected by minimizing the required volume-to-noise ratio (VNR) for a target word error rate $P_e$. The estimate becomes accurate for $P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH codes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is achieved compared to that by the classic balanced distance rule and the equal error probability rule. The $(128, 106, 8)$ EBCH code gives the best-known $n=128$ construction A lattice at $P_e= 10^{-5}$.",
    "metadata": {
      "arxiv_id": "2502.10728",
      "title": "Construction A Lattice Design Based on the Truncated Union Bound",
      "summary": "This paper considers $n= 128$ dimensional construction A lattice design, using binary codes with known minimum Hamming distance and codeword multiplicity, the number of minimum weight codeword. A truncated theta series of the lattice is explicitly given to obtain the truncated union bound to estimate the word error rate under maximum likelihood decoding. The best component code is selected by minimizing the required volume-to-noise ratio (VNR) for a target word error rate $P_e$. The estimate becomes accurate for $P_e \\leq 10^{-4}$, and design examples are given with the best extended BCH codes and polar codes for $P_e= 10^{-4}$ to $10^{-8}$. A lower error rate is achieved compared to that by the classic balanced distance rule and the equal error probability rule. The $(128, 106, 8)$ EBCH code gives the best-known $n=128$ construction A lattice at $P_e= 10^{-5}$.",
      "authors": [
        "Jiajie Xue",
        "Brian M. Kurkoski",
        "Emanuele Viterbo"
      ],
      "published": "2025-02-15T08:39:42Z",
      "updated": "2025-02-15T08:39:42Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.10728v1",
      "landing_url": "https://arxiv.org/abs/2502.10728v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.10728"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper addresses lattice design and error-rate analysis for coded communications, not discrete audio tokens or tokenization schemes, and therefore does not meet the 'Discrete Audio Tokens' inclusion criteria (no tokenization design or downstream audio modeling discussed).",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper addresses lattice design and error-rate analysis for coded communications, not discrete audio tokens or tokenization schemes, and therefore does not meet the 'Discrete Audio Tokens' inclusion criteria (no tokenization design or downstream audio modeling discussed).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on lattice design using binary codes to estimate word error rate under maximum likelihood decoding, without any mention or indication of discrete audio tokens, neural audio codecs, or audio-related tokenization methods relevant to discrete audio token representation for audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on lattice design using binary codes to estimate word error rate under maximum likelihood decoding, without any mention or indication of discrete audio tokens, neural audio codecs, or audio-related tokenization methods relevant to discrete audio token representation for audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
    "abstract": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
    "metadata": {
      "arxiv_id": "2502.11094",
      "title": "SyncSpeech: Low-Latency and Efficient Dual-Stream Text-to-Speech based on Temporal Masked Transformer",
      "summary": "This paper presents a dual-stream text-to-speech (TTS) model, SyncSpeech, capable of receiving streaming text input from upstream models while simultaneously generating streaming speech, facilitating seamless interaction with large language models. SyncSpeech has the following advantages: Low latency, as it begins generating streaming speech upon receiving the second text token; High efficiency, as it decodes all speech tokens corresponding to the each arrived text token in one step. To achieve this, we propose a temporal masked transformer as the backbone of SyncSpeech, combined with token-level duration prediction to predict speech tokens and the duration for the next step. Additionally, we design a two-stage training strategy to improve training efficiency and the quality of generated speech. We evaluated the SyncSpeech on both English and Mandarin datasets. Compared to the recent dual-stream TTS models, SyncSpeech significantly reduces the first packet delay of speech tokens and accelerates the real-time factor. Moreover, with the same data scale, SyncSpeech achieves performance comparable to that of traditional autoregressive-based TTS models in terms of both speech quality and robustness. Speech samples are available at https://SyncSpeech.github.io/}{https://SyncSpeech.github.io/.",
      "authors": [
        "Zhengyan Sheng",
        "Zhihao Du",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Yexin Yang",
        "Zhenhua Ling"
      ],
      "published": "2025-02-16T12:14:17Z",
      "updated": "2025-02-16T12:14:17Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11094v1",
      "landing_url": "https://arxiv.org/abs/2502.11094v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11094"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes a low-latency dual-stream TTS system with ‘speech tokens,’ but it does not clearly specify or analyze discrete audio token production (e.g., neural codecs, tokenizers, codebooks) or treat discrete tokens as the core representation, so it likely does not meet the discrete audio token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a low-latency dual-stream TTS system with ‘speech tokens,’ but it does not clearly specify or analyze discrete audio token production (e.g., neural codecs, tokenizers, codebooks) or treat discrete tokens as the core representation, so it likely does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a dual-stream TTS model focusing on low-latency and efficient speech synthesis from streaming text input. However, the abstract does not describe the use or generation of discrete audio tokens—such as neural audio codec-based vector quantization or self-supervised discrete units—or any tokenizer/codec architecture details, token vocabulary/codebook settings, or discrete token-based modeling as required by the inclusion criteria. It discusses temporal masked transformers and token-level duration prediction for speech token decoding but without clarifying these speech tokens as discrete audio tokens derived via discretization of audio waveforms. Therefore, it does not meet the key inclusion criterion regarding discrete audio tokens and focuses primarily on continuous audio or general TTS modeling rather than discrete token-based audio representation or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a dual-stream TTS model focusing on low-latency and efficient speech synthesis from streaming text input. However, the abstract does not describe the use or generation of discrete audio tokens—such as neural audio codec-based vector quantization or self-supervised discrete units—or any tokenizer/codec architecture details, token vocabulary/codebook settings, or discrete token-based modeling as required by the inclusion criteria. It discusses temporal masked transformers and token-level duration prediction for speech token decoding but without clarifying these speech tokens as discrete audio tokens derived via discretization of audio waveforms. Therefore, it does not meet the key inclusion criterion regarding discrete audio tokens and focuses primarily on continuous audio or general TTS modeling rather than discrete token-based audio representation or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Alias4SBML: A Python Package for Generating Alias Nodes in SBML Models",
    "abstract": "Interpreting biological networks becomes challenging when molecular components, such as genes or proteins, participate in numerous interactions, resulting in densely connected regions and overlapping interactions that obscure functional relationships and biological insights. To address this, we introduce Alias4SBML, a Python package that enhances SBML model visualizations by generating alias nodes-duplicate representations of highly connected molecular components-to redistribute interactions and reduce visual congestion. Applying Alias4SBML to the SBML models, including one with 59 species and 41 reactions and another with 701 species and 505 reactions, demonstrated significant improvements in readability, with edge length reductions of up to 50.88 %. Our approach preserves the structural integrity of the network while facilitating clearer interpretation of complex biological systems, offering a flexible and scalable solution for visualizing biological models more efficiently.",
    "metadata": {
      "arxiv_id": "2502.11318",
      "title": "Alias4SBML: A Python Package for Generating Alias Nodes in SBML Models",
      "summary": "Interpreting biological networks becomes challenging when molecular components, such as genes or proteins, participate in numerous interactions, resulting in densely connected regions and overlapping interactions that obscure functional relationships and biological insights. To address this, we introduce Alias4SBML, a Python package that enhances SBML model visualizations by generating alias nodes-duplicate representations of highly connected molecular components-to redistribute interactions and reduce visual congestion. Applying Alias4SBML to the SBML models, including one with 59 species and 41 reactions and another with 701 species and 505 reactions, demonstrated significant improvements in readability, with edge length reductions of up to 50.88 %. Our approach preserves the structural integrity of the network while facilitating clearer interpretation of complex biological systems, offering a flexible and scalable solution for visualizing biological models more efficiently.",
      "authors": [
        "Adel Heydarabadipour",
        "Herbert M Sauro"
      ],
      "published": "2025-02-16T23:32:58Z",
      "updated": "2025-02-16T23:32:58Z",
      "categories": [
        "q-bio.MN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11318v1",
      "landing_url": "https://arxiv.org/abs/2502.11318v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.11318"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on SBML visualization and alias nodes, not on discrete audio tokens or tokenization, so it does not meet inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on SBML visualization and alias nodes, not on discrete audio tokens or tokenization, so it does not meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a Python package to improve visualization of biological networks in SBML models by generating alias nodes to reduce visual congestion; it does not discuss discrete audio tokens, neural audio codecs, or audio tokenization essential to the inclusion criteria, nor does it address any audio-related discrete token schemes or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a Python package to improve visualization of biological networks in SBML models by generating alias nodes to reduce visual congestion; it does not discuss discrete audio tokens, neural audio codecs, or audio tokenization essential to the inclusion criteria, nor does it address any audio-related discrete token schemes or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Robust Approximation of ASR Metrics",
    "abstract": "Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\\%.",
    "metadata": {
      "arxiv_id": "2502.12408",
      "title": "On the Robust Approximation of ASR Metrics",
      "summary": "Recent advances in speech foundation models are largely driven by scaling both model size and data, enabling them to perform a wide range of tasks, including speech recognition. Traditionally, ASR models are evaluated using metrics like Word Error Rate (WER) and Character Error Rate (CER), which depend on ground truth labels. As a result of limited labeled data from diverse domains and testing conditions, the true generalization capabilities of these models beyond standard benchmarks remain unclear. Moreover, labeling data is both costly and time-consuming. To address this, we propose a novel label-free approach for approximating ASR performance metrics, eliminating the need for ground truth labels. Our method utilizes multimodal embeddings in a unified space for speech and transcription representations, combined with a high-quality proxy model to compute proxy metrics. These features are used to train a regression model to predict key ASR metrics like Word Error Rate (WER) and Character Error Rate (CER). We experiment with over 40 models across 14 datasets representing both standard and in-the-wild testing conditions. Our results show that we approximate the metrics within a single-digit absolute difference across all experimental configurations, outperforming the most recent baseline by more than 50\\%.",
      "authors": [
        "Abdul Waheed",
        "Hanin Atwany",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "published": "2025-02-18T01:10:17Z",
      "updated": "2025-06-04T22:29:37Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12408v2",
      "landing_url": "https://arxiv.org/abs/2502.12408v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.12408"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a label-free method to approximate ASR metrics (WER/CER) using multimodal embeddings and a proxy/regression model, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a label-free method to approximate ASR metrics (WER/CER) using multimodal embeddings and a proxy/regression model, with no discussion of discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a label-free approach to approximate ASR metrics without ground truth labels, focusing on metric prediction rather than on discrete audio tokens or their generation, design, or use in generative or cross-modal tasks. The abstract does not mention any discretization process of continuous audio into discrete tokens, token vocabulary, or codecs, nor does it discuss downstream tasks involving discrete token sequences. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and falls under the exclusion criteria for lacking discrete token processing as the core research subject.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a label-free approach to approximate ASR metrics without ground truth labels, focusing on metric prediction rather than on discrete audio tokens or their generation, design, or use in generative or cross-modal tasks. The abstract does not mention any discretization process of continuous audio into discrete tokens, token vocabulary, or codecs, nor does it discuss downstream tasks involving discrete token sequences. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and falls under the exclusion criteria for lacking discrete token processing as the core research subject.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
    "abstract": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
    "metadata": {
      "arxiv_id": "2502.12448",
      "title": "From Principles to Applications: A Comprehensive Survey of Discrete Tokenizers in Generation, Comprehension, Recommendation, and Information Retrieval",
      "summary": "Discrete tokenizers have emerged as indispensable components in modern machine learning systems, particularly within the context of autoregressive modeling and large language models (LLMs). These tokenizers serve as the critical interface that transforms raw, unstructured data from diverse modalities into discrete tokens, enabling LLMs to operate effectively across a wide range of tasks. Despite their central role in generation, comprehension, and recommendation systems, a comprehensive survey dedicated to discrete tokenizers remains conspicuously absent in the literature. This paper addresses this gap by providing a systematic review of the design principles, applications, and challenges of discrete tokenizers. We begin by dissecting the sub-modules of tokenizers and systematically demonstrate their internal mechanisms to provide a comprehensive understanding of their functionality and design. Building on this foundation, we synthesize state-of-the-art methods, categorizing them into multimodal generation and comprehension tasks, and semantic tokens for personalized recommendations. Furthermore, we critically analyze the limitations of existing tokenizers and outline promising directions for future research. By presenting a unified framework for understanding discrete tokenizers, this survey aims to guide researchers and practitioners in addressing open challenges and advancing the field, ultimately contributing to the development of more robust and versatile AI systems.",
      "authors": [
        "Jian Jia",
        "Jingtong Gao",
        "Ben Xue",
        "Junhao Wang",
        "Qingpeng Cai",
        "Quan Chen",
        "Xiangyu Zhao",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-02-18T02:29:51Z",
      "updated": "2025-02-18T02:29:51Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12448v1",
      "landing_url": "https://arxiv.org/abs/2502.12448v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12448"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item is a general survey of discrete tokenizers across modalities, not specifically focused on discrete audio tokens or their generation/codec/tokenization details, and thus does not meet the audio-token-specific inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item is a general survey of discrete tokenizers across modalities, not specifically focused on discrete audio tokens or their generation/codec/tokenization details, and thus does not meet the audio-token-specific inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper is a comprehensive survey of discrete tokenizers across various modalities and tasks, but it does not specifically focus on or provide detailed information about discrete audio tokens, their generation processes, or their application in audio-related tasks as defined by the inclusion criteria; thus, it does not meet the specific focus on discrete audio tokens required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper is a comprehensive survey of discrete tokenizers across various modalities and tasks, but it does not specifically focus on or provide detailed information about discrete audio tokens, their generation processes, or their application in audio-related tasks as defined by the inclusion criteria; thus, it does not meet the specific focus on discrete audio tokens required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization",
    "abstract": "Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.",
    "metadata": {
      "arxiv_id": "2502.12672",
      "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization",
      "summary": "Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.",
      "authors": [
        "Tzu-Quan Lin",
        "Wei-Ping Huang",
        "Hao Tang",
        "Hung-yi Lee"
      ],
      "published": "2025-02-18T09:23:42Z",
      "updated": "2025-12-18T04:50:21Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12672v3",
      "landing_url": "https://arxiv.org/abs/2502.12672v3",
      "doi": "https://doi.org/10.1109/TASLPRO.2025.3635827"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on cross-task generalization in pre-trained/fine-tuned speech representation models and weight-space interpolation, with no discussion of discrete audio tokens, tokenizers, or codebooks.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on cross-task generalization in pre-trained/fine-tuned speech representation models and weight-space interpolation, with no discussion of discrete audio tokens, tokenizers, or codebooks.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on fine-tuning speech representation models for cross-task generalization rather than generating or utilizing discrete audio tokens derived from discretization processes like neural audio codecs or self-supervised discrete units. It does not explicitly mention producing or using discrete token sequences, codebooks, or vocabularies as core representations for modeling or generation, which is central to the inclusion criteria. Therefore, it does not meet the inclusion requirements related to discrete audio tokens and their detailed design, evaluation, and use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on fine-tuning speech representation models for cross-task generalization rather than generating or utilizing discrete audio tokens derived from discretization processes like neural audio codecs or self-supervised discrete units. It does not explicitly mention producing or using discrete token sequences, codebooks, or vocabularies as core representations for modeling or generation, which is central to the inclusion criteria. Therefore, it does not meet the inclusion requirements related to discrete audio tokens and their detailed design, evaluation, and use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration",
    "abstract": "In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.",
    "metadata": {
      "arxiv_id": "2502.16142",
      "title": "Understanding Zero-shot Rare Word Recognition Improvements Through LLM Integration",
      "summary": "In this study, we investigate the integration of a large language model (LLM) with an automatic speech recognition (ASR) system, specifically focusing on enhancing rare word recognition performance. Using a 190,000-hour dataset primarily sourced from YouTube, pre-processed with Whisper V3 pseudo-labeling, we demonstrate that the LLM-ASR architecture outperforms traditional Zipformer-Transducer models in the zero-shot rare word recognition task, after training on a large dataset. Our analysis reveals that the LLM contributes significantly to improvements in rare word error rate (R-WER), while the speech encoder primarily determines overall transcription performance (Orthographic Word Error Rate, O-WER, and Normalized Word Error Rate, N-WER). Through extensive ablation studies, we highlight the importance of adapter integration in aligning speech encoder outputs with the LLM's linguistic capabilities. Furthermore, we emphasize the critical role of high-quality labeled data in achieving optimal performance. These findings provide valuable insights into the synergy between LLM-based ASR architectures, paving the way for future advancements in large-scale LLM-based speech recognition systems.",
      "authors": [
        "Haoxuan Wang"
      ],
      "published": "2025-02-22T08:30:38Z",
      "updated": "2025-02-22T08:30:38Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16142v1",
      "landing_url": "https://arxiv.org/abs/2502.16142v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16142"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses LLM-integrated ASR and rare word recognition but does not describe any discrete audio token generation, tokenizers, or token-level modeling; it fails to address the core topic of 'Discrete Audio Tokens' as defined.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses LLM-integrated ASR and rare word recognition but does not describe any discrete audio token generation, tokenizers, or token-level modeling; it fails to address the core topic of 'Discrete Audio Tokens' as defined.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on integrating a large language model with an ASR system to improve zero-shot rare word recognition, without discussing any discretization of continuous audio waveforms into discrete token sequences or providing details on tokenizers, codebooks, or vector quantization methods; thus, it does not meet the core inclusion criteria related to discrete audio tokens and their use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on integrating a large language model with an ASR system to improve zero-shot rare word recognition, without discussing any discretization of continuous audio waveforms into discrete token sequences or providing details on tokenizers, codebooks, or vector quantization methods; thus, it does not meet the core inclusion criteria related to discrete audio tokens and their use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
    "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The paper targets continuous embeddings from a neural audio codec for speech enhancement and performs processing in the continuous embedding space, explicitly avoiding discrete tokens; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper targets continuous embeddings from a neural audio codec for speech enhancement and performs processing in the continuous embedding space, explicitly avoiding discrete tokens; thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on speech enhancement using the continuous embeddings from a pretrained neural audio codec encoder rather than discrete tokens derived from vector quantization or similar discretization processes. The abstract does not mention the production or use of discrete audio tokens, token vocabularies, or tokenization details, failing to meet the inclusion criteria requiring clear discrete token derivation and usage. Instead, it leverages continuous embeddings, which falls into the exclusion criteria of continuous audio representation modeling without discrete token usage as the core method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on speech enhancement using the continuous embeddings from a pretrained neural audio codec encoder rather than discrete tokens derived from vector quantization or similar discretization processes. The abstract does not mention the production or use of discrete audio tokens, token vocabularies, or tokenization details, failing to meet the inclusion criteria requiring clear discrete token derivation and usage. Instead, it leverages continuous embeddings, which falls into the exclusion criteria of continuous audio representation modeling without discrete token usage as the core method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
    "abstract": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on Unified Semantic and ID representations for deep recommender systems and does not address discrete audio tokens, tokenizers, or audio token generation/quantization; it lacks the required audio-token design/experiments, so it should be excluded under the specified criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on Unified Semantic and ID representations for deep recommender systems and does not address discrete audio tokens, tokenizers, or audio token generation/quantization; it lacks the required audio-token design/experiments, so it should be excluded under the specified criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on recommender systems using semantic and ID tokens to improve recommendation performance and does not deal with discretizing continuous audio waveforms into discrete audio tokens for audio generation, understanding, or cross-modal reasoning. It also lacks details on neural audio codec architecture, vector quantization, or audio tokenization processes required by the inclusion criteria. Therefore, it does not meet the thematic requirements related to discrete audio tokens as defined.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on recommender systems using semantic and ID tokens to improve recommendation performance and does not deal with discretizing continuous audio waveforms into discrete audio tokens for audio generation, understanding, or cross-modal reasoning. It also lacks details on neural audio codec architecture, vector quantization, or audio tokenization processes required by the inclusion criteria. Therefore, it does not meet the thematic requirements related to discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
    "abstract": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses codec-discretized speech with neural codec tokens as the core input representation for a unified speech understanding and generation model, satisfying the discrete audio token criterion and not falling under any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses codec-discretized speech with neural codec tokens as the core input representation for a unified speech understanding and generation model, satisfying the discrete audio token criterion and not falling under any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses the use of a neural codec-based approach to generate discrete speech tokens that enable both speech understanding and generation via a continual pre-training framework on language models, aligning well with the inclusion criteria emphasizing discrete audio tokens from neural audio codecs used for downstream modeling tasks like TTS and speech generation; it also provides an English title and abstract with sufficient detail on the approach, and is directly relevant to modeling discrete audio tokens for speech tasks without excluding any required aspects.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study discusses the use of a neural codec-based approach to generate discrete speech tokens that enable both speech understanding and generation via a continual pre-training framework on language models, aligning well with the inclusion criteria emphasizing discrete audio tokens from neural audio codecs used for downstream modeling tasks like TTS and speech generation; it also provides an English title and abstract with sufficient detail on the approach, and is directly relevant to modeling discrete audio tokens for speech tasks without excluding any required aspects.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
    "abstract": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
    "metadata": {
      "arxiv_id": "2502.17239",
      "title": "Baichuan-Audio: A Unified Framework for End-to-End Speech Interaction",
      "summary": "We introduce Baichuan-Audio, an end-to-end audio large language model that seamlessly integrates audio understanding and generation. It features a text-guided aligned speech generation mechanism, enabling real-time speech interaction with both comprehension and generation capabilities. Baichuan-Audio leverages a pre-trained ASR model, followed by multi-codebook discretization of speech at a frame rate of 12.5 Hz. This multi-codebook setup ensures that speech tokens retain both semantic and acoustic information. To further enhance modeling, an independent audio head is employed to process audio tokens, effectively capturing their unique characteristics. To mitigate the loss of intelligence during pre-training and preserve the original capabilities of the LLM, we propose a two-stage pre-training strategy that maintains language understanding while enhancing audio modeling. Following alignment, the model excels in real-time speech-based conversation and exhibits outstanding question-answering capabilities, demonstrating its versatility and efficiency. The proposed model demonstrates superior performance in real-time spoken dialogue and exhibits strong question-answering abilities. Our code, model and training data are available at https://github.com/baichuan-inc/Baichuan-Audio",
      "authors": [
        "Tianpeng Li",
        "Jun Liu",
        "Tao Zhang",
        "Yuanbo Fang",
        "Da Pan",
        "Mingrui Wang",
        "Zheng Liang",
        "Zehuan Li",
        "Mingan Lin",
        "Guosheng Dong",
        "Jianhua Xu",
        "Haoze Sun",
        "Zenan Zhou",
        "Weipeng Chen"
      ],
      "published": "2025-02-24T15:16:34Z",
      "updated": "2025-02-24T15:16:34Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.17239v1",
      "landing_url": "https://arxiv.org/abs/2502.17239v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.17239"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes discrete audio tokens produced via a multi-codebook discretization (token-based representation) used for modeling and downstream tasks, aligning with the topic of discrete audio tokens and token-based audio-LMM frameworks; it provides an English title and abstract and discusses token-level processing and evaluation in real-time dialogue, meeting the inclusion criteria and not presenting a purely continuous or non-token-centric approach.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item describes discrete audio tokens produced via a multi-codebook discretization (token-based representation) used for modeling and downstream tasks, aligning with the topic of discrete audio tokens and token-based audio-LMM frameworks; it provides an English title and abstract and discusses token-level processing and evaluation in real-time dialogue, meeting the inclusion criteria and not presenting a purely continuous or non-token-centric approach.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract details the use of a multi-codebook discretization method for speech at a frame rate of 12.5 Hz, which fits the definition of discrete audio tokens produced by a neural audio codec; it also discusses semantic and acoustic information preservation, pre-training strategies, and applications in real-time speech interaction and question answering, aligning with the inclusion criteria of using discrete tokens for audio modeling, generation, and multimodal LLM usage, with sufficient English details to evaluate. Therefore, this study meets the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The abstract details the use of a multi-codebook discretization method for speech at a frame rate of 12.5 Hz, which fits the definition of discrete audio tokens produced by a neural audio codec; it also discusses semantic and acoustic information preservation, pre-training strategies, and applications in real-time speech interaction and question answering, aligning with the inclusion criteria of using discrete tokens for audio modeling, generation, and multimodal LLM usage, with sufficient English details to evaluate. Therefore, this study meets the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
    "abstract": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
    "metadata": {
      "arxiv_id": "2502.18200",
      "title": "Zero-Shot Semantic Communication with Multimodal Foundation Models",
      "summary": "Most existing semantic communication (SemCom) systems use deep joint source-channel coding (DeepJSCC) to encode task-specific semantics in a goal-oriented manner. However, their reliance on predefined tasks and datasets significantly limits their flexibility and generalizability in practical deployments. Multi-modal foundation models provide a promising solution by generating universal semantic tokens. Inspired by this, we introduce SemCLIP, a zero-shot SemCom framework leveraging the contrastive language-image pre-training (CLIP) model. By transmitting CLIP-generated image tokens instead of raw images, SemCLIP enables efficient SemCom under low bandwidth and challenging channel conditions, facilitating diverse downstream tasks and zero-shot applications. Specifically, we propose a DeepJSCC scheme for efficient CLIP token encoding. To mitigate potential degradation caused by compression and channel noise, a multi-modal transmission-aware prompt learning mechanism is designed at the receiver, which adapts prompts based on transmission quality, enhancing system robustness and channel adaptability. Simulation results demonstrate that SemCLIP outperforms the baselines, achieving a $41\\%$ improvement in zero-shot performance at low signal-to-noise ratios. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-fold compared to alternative image transmission methods, demonstrating the potential of foundation models towards a generalized, task-agnostic SemCom solution.",
      "authors": [
        "Jiangjing Hu",
        "Haotian Wu",
        "Wenjing Zhang",
        "Fengyu Wang",
        "Wenjun Xu",
        "Hui Gao",
        "Deniz Gündüz"
      ],
      "published": "2025-02-25T13:41:06Z",
      "updated": "2025-05-29T05:24:20Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.18200v2",
      "landing_url": "https://arxiv.org/abs/2502.18200v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.18200"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on zero-shot semantic communication using CLIP-generated image tokens for multimodal data, not on discrete audio tokens, tokenization vocabularies, or audio-centric tokenization/codec design; thus it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on zero-shot semantic communication using CLIP-generated image tokens for multimodal data, not on discrete audio tokens, tokenization vocabularies, or audio-centric tokenization/codec design; thus it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The article focuses on semantic communication using multimodal foundation models, specifically image tokens generated by the CLIP model, rather than discrete audio tokens derived from neural audio codecs or self-supervised audio models. There is no mention of discretized audio token design, evaluation, or use for audio generation or crossmodal audio modeling, which are central to the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The article focuses on semantic communication using multimodal foundation models, specifically image tokens generated by the CLIP model, rather than discrete audio tokens derived from neural audio codecs or self-supervised audio models. There is no mention of discretized audio token design, evaluation, or use for audio generation or crossmodal audio modeling, which are central to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
    "abstract": "Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",
    "metadata": {
      "arxiv_id": "2503.00493",
      "title": "LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech Enhancement",
      "summary": "Recent advancements in language models (LMs) have demonstrated strong capabilities in semantic understanding and contextual modeling, which have flourished in generative speech enhancement (SE). However, many LM-based SE approaches primarily focus on semantic information, often neglecting the critical role of acoustic information, which leads to acoustic inconsistency after enhancement and limited generalization across diverse SE tasks. In this paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes generalization capabilities for speech enhancement. LLaSE-G1 offers the following key contributions: First, to mitigate acoustic inconsistency, LLaSE-G1 employs continuous representations from WavLM as input and predicts speech tokens from X-Codec2, maximizing acoustic preservation. Second, to promote generalization capability, LLaSE-G1 introduces dual-channel inputs and outputs, unifying multiple SE tasks without requiring task-specific IDs. Third, LLaSE-G1 outperforms prior task-specific discriminative and generative SE models, demonstrating scaling effects at test time and emerging capabilities for unseen SE tasks. Additionally, we release our code and models to support further research in this area.",
      "authors": [
        "Boyi Kang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "Zhen Ye",
        "Mingshuai Liu",
        "Ziqian Wang",
        "Yike Zhu",
        "Guobin Ma",
        "Jun Chen",
        "Longshuai Xiao",
        "Chao Weng",
        "Wei Xue",
        "Lei Xie"
      ],
      "published": "2025-03-01T13:44:50Z",
      "updated": "2025-06-10T06:55:05Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00493v4",
      "landing_url": "https://arxiv.org/abs/2503.00493v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.00493"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item explicitly uses discrete audio tokens: it predicts speech tokens from X-Codec2 and uses token-based representations to unify SE tasks, satisfying the ‘Discrete Audio Tokens’ focus; it provides an English abstract and discusses token-driven modeling rather than purely continuous representations; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item explicitly uses discrete audio tokens: it predicts speech tokens from X-Codec2 and uses token-based representations to unify SE tasks, satisfying the ‘Discrete Audio Tokens’ focus; it provides an English abstract and discusses token-driven modeling rather than purely continuous representations; thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents LLaSE-G1, a LLaMA-based model for speech enhancement that uses continuous representations from WavLM as input and predicts speech tokens from X-Codec2 to better preserve acoustic details, aiming to generalize across various SE tasks. However, while it mentions predicting speech tokens from X-Codec2, the abstract does not elaborate on the discrete tokenization process in detail, such as tokenizer architecture, quantization methods, codebook setup, or token generation procedures necessary for evaluating it as a discrete audio token approach per the inclusion criteria. Moreover, the focus is on speech enhancement rather than token design, evaluation, or systematic comparison. Hence, it only partially meets the inclusion criteria and lacks sufficient detail to confirm it as a core discrete audio token method, suggesting an ambiguous fit in the scope.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The paper presents LLaSE-G1, a LLaMA-based model for speech enhancement that uses continuous representations from WavLM as input and predicts speech tokens from X-Codec2 to better preserve acoustic details, aiming to generalize across various SE tasks. However, while it mentions predicting speech tokens from X-Codec2, the abstract does not elaborate on the discrete tokenization process in detail, such as tokenizer architecture, quantization methods, codebook setup, or token generation procedures necessary for evaluating it as a discrete audio token approach per the inclusion criteria. Moreover, the focus is on speech enhancement rather than token design, evaluation, or systematic comparison. Hence, it only partially meets the inclusion criteria and lacks sufficient detail to confirm it as a core discrete audio token method, suggesting an ambiguous fit in the scope.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raise valid points: JuniorNano notes the paper explicitly predicts speech tokens from X-Codec2 (indicating use of discrete audio tokens and alignment with the topic), while JuniorMini correctly points out the abstract lacks concrete tokenizer/quantization/codebook details required by the inclusion’s English-evaluability requirement. The abstract does explicitly state predicting speech tokens from X-Codec2 and using them to unify SE tasks, which meets the ‘‘token source’’ criterion (neural codec-derived tokens used as core representation), but it does not provide the reproducibility-level details (tokenizer/codec architecture, codebook settings, bitrate/time downsampling) that would fully satisfy the strict documentation requirement—details that likely exist in the full paper. Given the explicit mention of X-Codec2 tokens and the paper’s use of token-based modeling for generation/SE, this item is likely relevant and merits inclusion for further inspection, but it should be verified against the full text to confirm the discrete-token generation details before final dataset entry.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raise valid points: JuniorNano notes the paper explicitly predicts speech tokens from X-Codec2 (indicating use of discrete audio tokens and alignment with the topic), while JuniorMini correctly points out the abstract lacks concrete tokenizer/quantization/codebook details required by the inclusion’s English-evaluability requirement. The abstract does explicitly state predicting speech tokens from X-Codec2 and using them to unify SE tasks, which meets the ‘‘token source’’ criterion (neural codec-derived tokens used as core representation), but it does not provide the reproducibility-level details (tokenizer/codec architecture, codebook settings, bitrate/time downsampling) that would fully satisfy the strict documentation requirement—details that likely exist in the full paper. Given the explicit mention of X-Codec2 tokens and the paper’s use of token-based modeling for generation/SE, this item is likely relevant and merits inclusion for further inspection, but it should be verified against the full text to confirm the discrete-token generation details before final dataset entry.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
    "abstract": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
    "metadata": {
      "arxiv_id": "2503.00733",
      "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
      "summary": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing. Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, UniWav achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training.",
      "authors": [
        "Alexander H. Liu",
        "Sang-gil Lee",
        "Chao-Han Huck Yang",
        "Yuan Gong",
        "Yu-Chiang Frank Wang",
        "James R. Glass",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "published": "2025-03-02T05:15:40Z",
      "updated": "2025-03-02T05:15:40Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00733v1",
      "landing_url": "https://arxiv.org/abs/2503.00733v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00733"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses unified pre-training for speech representations and generation and only mentions ‘speech tokenization’ as a downstream application, but it does not present or analyze a discrete audio tokenization/codec design (e.g., VQ/VQ-VAE, multi-codebooks) or treat discrete tokens as the core modeling input; hence it does not meet the core criteria for discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses unified pre-training for speech representations and generation and only mentions ‘speech tokenization’ as a downstream application, but it does not present or analyze a discrete audio tokenization/codec design (e.g., VQ/VQ-VAE, multi-codebooks) or treat discrete tokens as the core modeling input; hence it does not meet the core criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents a unified pre-training framework for speech representation and generation but does not explicitly mention the use or design of discrete audio tokens, neural audio codecs, quantization strategies, or vocabularies critical to discrete token generation and use; thus, it mainly deals with continuous speech representations and encoder-decoder modelling without clear evidence of discrete tokenization as per inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper presents a unified pre-training framework for speech representation and generation but does not explicitly mention the use or design of discrete audio tokens, neural audio codecs, quantization strategies, or vocabularies critical to discrete token generation and use; thus, it mainly deals with continuous speech representations and encoder-decoder modelling without clear evidence of discrete tokenization as per inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving",
    "abstract": "In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel method designed to enhance data efficiency in learning-based autonomous driving systems. CAPS addresses the challenge of imbalanced training datasets in imitation learning by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data representation, which helps reveal meaningful patterns in the data. These patterns are used to group the data into clusters, with each sample being assigned a cluster ID. The cluster IDs are then used to re-balance the dataset, ensuring that rare yet valuable samples receive higher priority during training. By ensuring a more diverse and informative training set, CAPS improves the generalization of the trained planner across a wide range of driving scenarios. We evaluate our method through closed-loop simulations in the CARLA environment. The results on Bench2Drive scenarios demonstrate that our framework outperforms state-of-the-art methods, leading to notable improvements in model performance.",
    "metadata": {
      "arxiv_id": "2503.01650",
      "title": "CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in Autonomous Driving",
      "summary": "In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel method designed to enhance data efficiency in learning-based autonomous driving systems. CAPS addresses the challenge of imbalanced training datasets in imitation learning by leveraging Vector Quantized Variational Autoencoders (VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data representation, which helps reveal meaningful patterns in the data. These patterns are used to group the data into clusters, with each sample being assigned a cluster ID. The cluster IDs are then used to re-balance the dataset, ensuring that rare yet valuable samples receive higher priority during training. By ensuring a more diverse and informative training set, CAPS improves the generalization of the trained planner across a wide range of driving scenarios. We evaluate our method through closed-loop simulations in the CARLA environment. The results on Bench2Drive scenarios demonstrate that our framework outperforms state-of-the-art methods, leading to notable improvements in model performance.",
      "authors": [
        "Hamidreza Mirkhani",
        "Behzad Khamidehi",
        "Ehsan Ahmadi",
        "Fazel Arasteh",
        "Mohammed Elmahgiubi",
        "Weize Zhang",
        "Umar Rajguru",
        "Kasra Rezaee"
      ],
      "published": "2025-03-03T15:27:11Z",
      "updated": "2025-03-03T15:27:11Z",
      "categories": [
        "cs.LG",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01650v1",
      "landing_url": "https://arxiv.org/abs/2503.01650v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01650"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on context-aware priority sampling for imitation learning in autonomous driving and uses VQ-VAE for data rebalancing, but it does not address discrete audio tokens or tokenization as its core representation; therefore it does not meet the ‘Discrete Audio Tokens’ inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on context-aware priority sampling for imitation learning in autonomous driving and uses VQ-VAE for data rebalancing, but it does not address discrete audio tokens or tokenization as its core representation; therefore it does not meet the ‘Discrete Audio Tokens’ inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on imitation learning for autonomous driving using Vector Quantized Variational Autoencoders (VQ-VAEs) for data representation and sampling, which involves discrete representations through vector quantization. However, the study's main subject is autonomous driving rather than audio processing, and the discrete tokens are used for driving data clustering and sampling, not for audio waveform discretization or audio token modeling as defined in the inclusion criteria. There's no indication that the tokens or quantization pertain to audio signals, speech, music, or related audio tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related tasks, nor is it relevant per the defined theme.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on imitation learning for autonomous driving using Vector Quantized Variational Autoencoders (VQ-VAEs) for data representation and sampling, which involves discrete representations through vector quantization. However, the study's main subject is autonomous driving rather than audio processing, and the discrete tokens are used for driving data clustering and sampling, not for audio waveform discretization or audio token modeling as defined in the inclusion criteria. There's no indication that the tokens or quantization pertain to audio signals, speech, music, or related audio tasks. Therefore, it does not meet the inclusion criteria focused on discrete audio tokens and related tasks, nor is it relevant per the defined theme.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
    "abstract": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
    "metadata": {
      "arxiv_id": "2503.01710",
      "title": "Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens",
      "summary": "Recent advancements in large language models (LLMs) have driven significant progress in zero-shot text-to-speech (TTS) synthesis. However, existing foundation models rely on multi-stage processing or complex architectures for predicting multiple codebooks, limiting efficiency and integration flexibility. To overcome these challenges, we introduce Spark-TTS, a novel system powered by BiCodec, a single-stream speech codec that decomposes speech into two complementary token types: low-bitrate semantic tokens for linguistic content and fixed-length global tokens for speaker attributes. This disentangled representation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT) generation approach, enables both coarse-grained control (e.g., gender, speaking style) and fine-grained adjustments (e.g., precise pitch values, speaking rate). To facilitate research in controllable TTS, we introduce VoxBox, a meticulously curated 100,000-hour dataset with comprehensive attribute annotations. Extensive experiments demonstrate that Spark-TTS not only achieves state-of-the-art zero-shot voice cloning but also generates highly customizable voices that surpass the limitations of reference-based synthesis. Source code, pre-trained models, and audio samples are available at https://github.com/SparkAudio/Spark-TTS.",
      "authors": [
        "Xinsheng Wang",
        "Mingqi Jiang",
        "Ziyang Ma",
        "Ziyu Zhang",
        "Songxiang Liu",
        "Linqin Li",
        "Zheng Liang",
        "Qixi Zheng",
        "Rui Wang",
        "Xiaoqin Feng",
        "Weizhen Bian",
        "Zhen Ye",
        "Sitong Cheng",
        "Ruibin Yuan",
        "Zhixian Zhao",
        "Xinfa Zhu",
        "Jiahao Pan",
        "Liumeng Xue",
        "Pengcheng Zhu",
        "Yunlin Chen",
        "Zhifei Li",
        "Xie Chen",
        "Lei Xie",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2025-03-03T16:23:10Z",
      "updated": "2025-03-03T16:23:10Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.01710v1",
      "landing_url": "https://arxiv.org/abs/2503.01710v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.01710"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item introduces Spark-TTS with BiCodec that decomposes speech into discrete token types (low-bitrate semantic tokens and fixed-length global tokens) used to drive an LLM-based TTS system, addressing token generation, design, and downstream controllable speech synthesis, which aligns with the criteria for discrete audio tokens and token-based modeling/analysis; thus it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item introduces Spark-TTS with BiCodec that decomposes speech into discrete token types (low-bitrate semantic tokens and fixed-length global tokens) used to drive an LLM-based TTS system, addressing token generation, design, and downstream controllable speech synthesis, which aligns with the criteria for discrete audio tokens and token-based modeling/analysis; thus it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents Spark-TTS, which uses BiCodec, a neural codec that decomposes speech into discrete token types (semantic and speaker attribute tokens), aligning well with the inclusion criterion of discrete audio tokens generated by neural audio codecs with multi-codebook and multi-granularity properties. The paper provides clear English abstract and details about tokenization and codec architecture, addresses speech generation (TTS), and offers empirical evaluation, thus satisfying both inclusion and exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents Spark-TTS, which uses BiCodec, a neural codec that decomposes speech into discrete token types (semantic and speaker attribute tokens), aligning well with the inclusion criterion of discrete audio tokens generated by neural audio codecs with multi-codebook and multi-granularity properties. The paper provides clear English abstract and details about tokenization and codec architecture, addresses speech generation (TTS), and offers empirical evaluation, thus satisfying both inclusion and exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
    "abstract": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
    "metadata": {
      "arxiv_id": "2503.02862",
      "title": "Privacy and Accuracy-Aware AI/ML Model Deduplication",
      "summary": "With the growing adoption of privacy-preserving machine learning algorithms, such as Differentially Private Stochastic Gradient Descent (DP-SGD), training or fine-tuning models on private datasets has become increasingly prevalent. This shift has led to the need for models offering varying privacy guarantees and utility levels to satisfy diverse user requirements. However, managing numerous versions of large models introduces significant operational challenges, including increased inference latency, higher resource consumption, and elevated costs. Model deduplication is a technique widely used by many model serving and database systems to support high-performance and low-cost inference queries and model diagnosis queries. However, none of the existing model deduplication works has considered privacy, leading to unbounded aggregation of privacy costs for certain deduplicated models and inefficiencies when applied to deduplicate DP-trained models. We formalize the problems of deduplicating DP-trained models for the first time and propose a novel privacy- and accuracy-aware deduplication mechanism to address the problems. We developed a greedy strategy to select and assign base models to target models to minimize storage and privacy costs. When deduplicating a target model, we dynamically schedule accuracy validations and apply the Sparse Vector Technique to reduce the privacy costs associated with private validation data. Compared to baselines that do not provide privacy guarantees, our approach improved the compression ratio by up to $35\\times$ for individual models (including large language models and vision transformers). We also observed up to $43\\times$ inference speedup due to the reduction of I/O operations.",
      "authors": [
        "Hong Guan",
        "Lei Yu",
        "Lixi Zhou",
        "Li Xiong",
        "Kanchan Chowdhury",
        "Lulu Xie",
        "Xusheng Xiao",
        "Jia Zou"
      ],
      "published": "2025-03-04T18:40:38Z",
      "updated": "2025-03-04T18:40:38Z",
      "categories": [
        "cs.CR",
        "cs.DB"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02862v1",
      "landing_url": "https://arxiv.org/abs/2503.02862v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.02862"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item discusses privacy-preserving ML model deduplication and DP techniques, with no discussion of discrete audio tokens, tokenization, or audio-domain modeling, so it does not meet the Discrete Audio Tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item discusses privacy-preserving ML model deduplication and DP techniques, with no discussion of discrete audio tokens, tokenization, or audio-domain modeling, so it does not meet the Discrete Audio Tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on privacy-aware AI/ML model deduplication related to managing large models trained with differential privacy, which is unrelated to discrete audio tokens, neural audio codecs, or audio tokenization; thus, it does not meet the inclusion criteria centered on discrete audio token representation or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on privacy-aware AI/ML model deduplication related to managing large models trained with differential privacy, which is unrelated to discrete audio tokens, neural audio codecs, or audio tokenization; thus, it does not meet the inclusion criteria centered on discrete audio token representation or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
    "metadata": {
      "arxiv_id": "2503.04606",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "published": "2025-03-06T16:53:14Z",
      "updated": "2025-04-29T10:34:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04606v3",
      "landing_url": "https://arxiv.org/abs/2503.04606v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04606"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on discrete tokens for video/visual semantics (semantic tokenizer for 3D visual features) and a hybrid LLM-diffusion video generation framework, not on discrete audio tokens or audio tokenization; thus it fails the inclusion criteria for discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on discrete tokens for video/visual semantics (semantic tokenizer for 3D visual features) and a hybrid LLM-diffusion video generation framework, not on discrete audio tokens or audio tokenization; thus it fails the inclusion criteria for discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "This study focuses on video generation by integrating language and diffusion models, compressing 3D visual features into semantic tokens for videos; however, it does not address audio tokens or the discretization of continuous audio waveforms into discrete audio token sequences as specified in the inclusion criteria, nor does it discuss neural audio codecs, vector quantization, or audio-related multi-codebook or multi-granularity features, and is therefore outside the scope of \"Discrete Audio Tokens.\"",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "This study focuses on video generation by integrating language and diffusion models, compressing 3D visual features into semantic tokens for videos; however, it does not address audio tokens or the discretization of continuous audio waveforms into discrete audio token sequences as specified in the inclusion criteria, nor does it discuss neural audio codecs, vector quantization, or audio-related multi-codebook or multi-granularity features, and is therefore outside the scope of \"Discrete Audio Tokens.\"",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
    "abstract": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
    "metadata": {
      "arxiv_id": "2503.04713",
      "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
      "summary": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
      "authors": [
        "Anuj Diwan",
        "Zhisheng Zheng",
        "David Harwath",
        "Eunsol Choi"
      ],
      "published": "2025-03-06T18:57:40Z",
      "updated": "2025-09-24T19:42:38Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04713v2",
      "landing_url": "https://arxiv.org/abs/2503.04713v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.04713"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes a dataset of rich style captions for TTS and its impact on model performance, with no mention of discrete audio tokens, tokenizers, or token-based representations; it does not meet the inclusion criteria focused on discrete audio tokens and instead aligns with excluded continuous or non-tokenized approaches.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes a dataset of rich style captions for TTS and its impact on model performance, with no mention of discrete audio tokens, tokenizers, or token-based representations; it does not meet the inclusion criteria focused on discrete audio tokens and instead aligns with excluded continuous or non-tokenized approaches.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a large-scale dataset with style annotations for text-to-speech (TTS) and shows improvements in style consistency and speech quality when fine-tuning a style-prompted TTS model. However, the abstract does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process creating token sequences from continuous audio waveforms, which is required by the inclusion criteria. Since the main contribution is dataset scaling and style prompting, not discrete token design or usage, it does not meet the inclusion criteria focusing on discrete audio tokens for generative or cross-modal modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a large-scale dataset with style annotations for text-to-speech (TTS) and shows improvements in style consistency and speech quality when fine-tuning a style-prompted TTS model. However, the abstract does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization process creating token sequences from continuous audio waveforms, which is required by the inclusion criteria. Since the main contribution is dataset scaling and style prompting, not discrete token design or usage, it does not meet the inclusion criteria focusing on discrete audio tokens for generative or cross-modal modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Task Vector Quantization for Memory-Efficient Model Merging",
    "abstract": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
    "metadata": {
      "arxiv_id": "2503.06921",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "authors": [
        "Youngeun Kim",
        "Seunghwan Lee",
        "Aecheon Jung",
        "Bogon Ryu",
        "Sungeun Hong"
      ],
      "published": "2025-03-10T05:00:24Z",
      "updated": "2025-08-07T10:57:05Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06921v2",
      "landing_url": "https://arxiv.org/abs/2503.06921v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06921"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses memory-efficient model merging by quantizing task vectors, with no mention of discrete audio tokens, tokenization, or token-based representations; thus it does not meet the Discrete Audio Tokens criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses memory-efficient model merging by quantizing task vectors, with no mention of discrete audio tokens, tokenization, or token-based representations; thus it does not meet the Discrete Audio Tokens criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on model merging using vector quantization for task vectors in multi-task models related to image classification and dense prediction, without discussing discrete audio tokens, neural audio codecs, or any audio-related continuous-to-discrete tokenization process relevant to audio modeling tasks per the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on model merging using vector quantization for task vectors in multi-task models related to image classification and dense prediction, without discussing discrete audio tokens, neural audio codecs, or any audio-related continuous-to-discrete tokenization process relevant to audio modeling tasks per the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
    "abstract": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
    "metadata": {
      "arxiv_id": "2503.09509",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Kedong Xu",
        "Hong Gu",
        "Kejie Huang"
      ],
      "published": "2025-03-12T16:18:45Z",
      "updated": "2025-07-30T16:58:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09509v2",
      "landing_url": "https://arxiv.org/abs/2503.09509v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.09509"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns ViM-VQ for visual models and post-training vector quantization, not discrete audio tokens or audio tokenization as defined in the criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns ViM-VQ for visual models and post-training vector quantization, not discrete audio tokens or audio tokenization as defined in the criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization techniques for Visual Mamba networks applied to computer vision tasks, with no indication of audio waveform discretization or use of discrete audio tokens related to speech, music, or environmental sounds, nor any relevance to neural audio codecs or audio tokenization frameworks as defined in the inclusion criteria. Therefore, it does not meet the specific focus on discrete audio tokens for audio modeling, generation, or alignment and falls outside the research scope.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization techniques for Visual Mamba networks applied to computer vision tasks, with no indication of audio waveform discretization or use of discrete audio tokens related to speech, music, or environmental sounds, nor any relevance to neural audio codecs or audio tokenization frameworks as defined in the inclusion criteria. Therefore, it does not meet the specific focus on discrete audio tokens for audio modeling, generation, or alignment and falls outside the research scope.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
    "abstract": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
    "metadata": {
      "arxiv_id": "2503.11315",
      "title": "MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens",
      "summary": "Audio-Visual Speech Recognition (AVSR) achieves robust speech recognition in noisy environments by combining auditory and visual information. However, recent Large Language Model (LLM) based AVSR systems incur high computational costs due to the high temporal resolution of audio-visual speech processed by LLMs. In this work, we introduce an efficient multimodal speech LLM framework that minimizes token length while preserving essential linguistic content. Our approach employs an early AV-fusion module for streamlined feature integration, an audio-visual speech Q-Former that dynamically allocates tokens based on input duration, and a refined query allocation strategy with a speech rate predictor to adjust token allocation according to speaking speed of each audio sample. Extensive experiments on the LRS3 dataset show that our method achieves state-of-the-art performance with a WER of 0.72% while using only 3.5 tokens per second. Moreover, our approach not only reduces token usage by 86% compared to the previous multimodal speech LLM framework, but also improves computational efficiency by reducing FLOPs by 35.7%.",
      "authors": [
        "Jeong Hun Yeo",
        "Hyeongseop Rha",
        "Se Jin Park",
        "Yong Man Ro"
      ],
      "published": "2025-03-14T11:31:30Z",
      "updated": "2025-06-05T05:58:37Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11315v2",
      "landing_url": "https://arxiv.org/abs/2503.11315v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.11315"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Reasoning: The paper discusses token-efficient AVSR using 'speech tokens' but does not describe discrete audio token generation (e.g., neural codecs, vector quantization, token vocabularies) or a token-based discrete representation as core methodology, so it does not clearly meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper discusses token-efficient AVSR using 'speech tokens' but does not describe discrete audio token generation (e.g., neural codecs, vector quantization, token vocabularies) or a token-based discrete representation as core methodology, so it does not clearly meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes an efficient Large Language Model based system for audio-visual speech recognition that minimizes token length but does not explicitly mention the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or self-supervised discrete units. There is no clear description of tokenization methods, vocabulary settings, or quantization mechanisms indicating discrete token design or usage core to the approach. Thus, it likely focuses on continuous audio-visual feature fusion rather than discrete audio token discrete representations required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract describes an efficient Large Language Model based system for audio-visual speech recognition that minimizes token length but does not explicitly mention the use or generation of discrete audio tokens through neural audio codecs, vector quantization, or self-supervised discrete units. There is no clear description of tokenization methods, vocabulary settings, or quantization mechanisms indicating discrete token design or usage core to the approach. Thus, it likely focuses on continuous audio-visual feature fusion rather than discrete audio token discrete representations required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
    "abstract": "Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
    "metadata": {
      "arxiv_id": "2503.11513",
      "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models",
      "summary": "Text-to-video generation poses significant challenges due to the inherent complexity of video data, which spans both temporal and spatial dimensions. It introduces additional redundancy, abrupt variations, and a domain gap between language and vision tokens while generation. Addressing these challenges requires an effective video tokenizer that can efficiently encode video data while preserving essential semantic and spatiotemporal information, serving as a critical bridge between text and vision. Inspired by the observation in VQ-VAE-2 and workflows of traditional animation, we propose HiTVideo for text-to-video generation with hierarchical tokenizers. It utilizes a 3D causal VAE with a multi-layer discrete token framework, encoding video content into hierarchically structured codebooks. Higher layers capture semantic information with higher compression, while lower layers focus on fine-grained spatiotemporal details, striking a balance between compression efficiency and reconstruction quality. Our approach efficiently encodes longer video sequences (e.g., 8 seconds, 64 frames), reducing bits per pixel (bpp) by approximately 70\\% compared to baseline tokenizers, while maintaining competitive reconstruction quality. We explore the trade-offs between compression and reconstruction, while emphasizing the advantages of high-compressed semantic tokens in text-to-video tasks. HiTVideo aims to address the potential limitations of existing video tokenizers in text-to-video generation tasks, striving for higher compression ratios and simplify LLMs modeling under language guidance, offering a scalable and promising framework for advancing text to video generation. Demo page: https://ziqinzhou66.github.io/project/HiTVideo.",
      "authors": [
        "Ziqin Zhou",
        "Yifan Yang",
        "Yuqing Yang",
        "Tianyu He",
        "Houwen Peng",
        "Kai Qiu",
        "Qi Dai",
        "Lili Qiu",
        "Chong Luo",
        "Lingqiao Liu"
      ],
      "published": "2025-03-14T15:36:39Z",
      "updated": "2025-03-14T15:36:39Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11513v1",
      "landing_url": "https://arxiv.org/abs/2503.11513v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.11513"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on hierarchical video tokenizers for text-to-video generation, not on discrete audio tokens or audio tokenization methods; thus it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on hierarchical video tokenizers for text-to-video generation, not on discrete audio tokens or audio tokenization methods; thus it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on hierarchical tokenizers for text-to-video generation using discrete tokens encoding video content, not audio data. There is no mention or evidence that the tokens represent discretized audio waveforms or discrete audio tokens for speech/music/environmental sound modeling, generation, or alignment. Hence, it does not meet the key inclusion criteria of dealing with discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on hierarchical tokenizers for text-to-video generation using discrete tokens encoding video content, not audio data. There is no mention or evidence that the tokens represent discretized audio waveforms or discrete audio tokens for speech/music/environmental sound modeling, generation, or alignment. Hence, it does not meet the key inclusion criteria of dealing with discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract introduces discrete speech tokens learned via a low-bitrate neural codec and self-supervised representations, discusses token generation/disentanglement, and evaluates downstream speech tasks, directly matching the Discrete Audio Tokens inclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract introduces discrete speech tokens learned via a low-bitrate neural codec and self-supervised representations, discusses token generation/disentanglement, and evaluates downstream speech tasks, directly matching the Discrete Audio Tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "該論文標題與摘要明確描述了一種利用低碼率神經編碼器與自監督表示學習來獲取離散化且語義解耦的統一語音 token，符合以神經音訊編碼器產生離散 token 並應用於下游生成與理解任務的要求，且詳細說明了 token 的設計與評估，具備良好的英文可評估性，完全符合收錄標準且不違反排除條件。",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "該論文標題與摘要明確描述了一種利用低碼率神經編碼器與自監督表示學習來獲取離散化且語義解耦的統一語音 token，符合以神經音訊編碼器產生離散 token 並應用於下游生成與理解任務的要求，且詳細說明了 token 的設計與評估，具備良好的英文可評估性，完全符合收錄標準且不違反排除條件。",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
    "abstract": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
    "metadata": {
      "arxiv_id": "2503.14928",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "published": "2025-03-19T06:28:17Z",
      "updated": "2025-03-19T06:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14928v1",
      "landing_url": "https://arxiv.org/abs/2503.14928v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14928"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete speech tokens generated from lip/video input (discrete lip aligner, tokenized speech tokens, and token-based refinement/modeling), aligning with the Discrete Audio Tokens criteria and not being a continuous-only study.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete speech tokens generated from lip/video input (discrete lip aligner, tokenized speech tokens, and token-based refinement/modeling), aligning with the Discrete Audio Tokens criteria and not being a continuous-only study.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a novel cross-modal diffusion framework (ImaginTalk) that predicts discrete speech tokens from lip videos to generate speech, which suggests the use of discrete audio tokens derived from visual input. It specifically mentions a discrete lip aligner predicting discrete speech tokens and refining them, aligning well with the requirement of discrete token generation for audio representation. The abstract is in English and provides method details about token prediction and refinement, supporting reproducibility. The work focuses on using discrete token sequences as core representations for speech generation from silent video, fitting the inclusion criteria related to discrete audio tokens and cross-modal generation tasks. Hence, it meets the inclusion criteria without showing characteristics matching the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes a novel cross-modal diffusion framework (ImaginTalk) that predicts discrete speech tokens from lip videos to generate speech, which suggests the use of discrete audio tokens derived from visual input. It specifically mentions a discrete lip aligner predicting discrete speech tokens and refining them, aligning well with the requirement of discrete token generation for audio representation. The abstract is in English and provides method details about token prediction and refinement, supporting reproducibility. The work focuses on using discrete token sequences as core representations for speech generation from silent video, fitting the inclusion criteria related to discrete audio tokens and cross-modal generation tasks. Hence, it meets the inclusion criteria without showing characteristics matching the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
    "abstract": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
    "metadata": {
      "arxiv_id": "2503.18769",
      "title": "AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning",
      "summary": "This paper presents AlphaSpace, a novel methodology designed to enhance the spatial reasoning capabilities of language models for robotic manipulation in 3D Cartesian space. AlphaSpace employs a hierarchical semantics-based tokenization strategy that encodes spatial information at both coarse and fine-grained levels. Our approach represents objects with their attributes, positions, and height information through structured tokens, enabling precise spatial reasoning without relying on traditional vision-based embeddings. This approach enables LLMs to accurately manipulate objects by positioning them at specific (x, y, z) coordinates. Experimental results suggest that AlphaSpace demonstrates promising potential for improving manipulation tasks, achieving a total accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude 3.5 Sonnet. These results demonstrate the potential of structured spatial encoding for manipulation tasks and warrant further exploration.",
      "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Bui Quang Huy"
      ],
      "published": "2025-03-24T15:16:51Z",
      "updated": "2025-03-27T06:39:47Z",
      "categories": [
        "cs.CL",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.18769v2",
      "landing_url": "https://arxiv.org/abs/2503.18769v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.18769"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on spatial tokenization for robotic manipulation and symbolic reasoning, not on discrete audio tokens or audio tokenizers; it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on spatial tokenization for robotic manipulation and symbolic reasoning, not on discrete audio tokens or audio tokenizers; it does not meet the inclusion criteria for Discrete Audio Tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on spatial reasoning and robotic manipulation using hierarchical semantic tokenization and symbolic reasoning, without mention of discretizing continuous audio waveforms into tokens for audio modeling or generation, which is the core inclusion criteria for the topic 'Discrete Audio Tokens'. There is no evidence of audio tokenization, neural audio codecs, or related audio discrete token mechanisms discussed. Therefore, it does not meet the key thematic inclusion criteria of discrete audio tokens derived from continuous audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on spatial reasoning and robotic manipulation using hierarchical semantic tokenization and symbolic reasoning, without mention of discretizing continuous audio waveforms into tokens for audio modeling or generation, which is the core inclusion criteria for the topic 'Discrete Audio Tokens'. There is no evidence of audio tokenization, neural audio codecs, or related audio discrete token mechanisms discussed. Therefore, it does not meet the key thematic inclusion criteria of discrete audio tokens derived from continuous audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
    "abstract": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly describes a semantic tokenizer that maps speech into discrete tokens used for autoregressive, streaming TTS and semantic-to-acoustic decoding, aligning with the discrete audio token design/use criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly describes a semantic tokenizer that maps speech into discrete tokens used for autoregressive, streaming TTS and semantic-to-acoustic decoding, aligning with the discrete audio token design/use criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses FireRedTTS-1S, a streaming text-to-speech system that converts text into speech via semantic tokens, suggesting some form of semantic tokenization. However, the abstract lacks explicit details on discrete token generation process, such as tokenizer architecture, quantization method, codebook settings, or how these semantic tokens are discretized and used as core representations in the system. The description implies semantic tokens but does not clarify if these tokens are discrete audio tokens derived via neural codec or self-supervised learning models, with design choices impact, evaluation, or reproducibility details that meet the strict inclusion criteria. Therefore, it likely involves some tokenization but insufficiently satisfies the inclusion criteria regarding discrete audio tokens as defined, making its inclusion questionable.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper discusses FireRedTTS-1S, a streaming text-to-speech system that converts text into speech via semantic tokens, suggesting some form of semantic tokenization. However, the abstract lacks explicit details on discrete token generation process, such as tokenizer architecture, quantization method, codebook settings, or how these semantic tokens are discretized and used as core representations in the system. The description implies semantic tokens but does not clarify if these tokens are discrete audio tokens derived via neural codec or self-supervised learning models, with design choices impact, evaluation, or reproducibility details that meet the strict inclusion criteria. Therefore, it likely involves some tokenization but insufficiently satisfies the inclusion criteria regarding discrete audio tokens as defined, making its inclusion questionable.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract explicitly states the system uses a “semantic-aware speech tokenizer” to convert speech into semantic tokens which are then autoregressively generated by a language model and used for semantic-to-acoustic decoding—this places discrete semantic tokens as the core representation for TTS, matching the inclusion group of using discrete audio tokens for downstream generation; JuniorNano favored inclusion for that reason. JuniorMini’s concern that the abstract lacks low-level tokenizer/quantization/codebook details is valid (the abstract does not show exact tokenization/codec settings required by the strict reproducibility criterion), but the presence of a tokenizer as a central component and empirical evaluations (zero-shot cloning, objective/subjective comparisons) make it likely this work belongs to the “Discrete Audio Tokens” topic. Given the remaining uncertainty from the abstract alone but the clear alignment with the token-as-core-representation requirement, I recommend inclusion (better to include).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "The abstract explicitly states the system uses a “semantic-aware speech tokenizer” to convert speech into semantic tokens which are then autoregressively generated by a language model and used for semantic-to-acoustic decoding—this places discrete semantic tokens as the core representation for TTS, matching the inclusion group of using discrete audio tokens for downstream generation; JuniorNano favored inclusion for that reason. JuniorMini’s concern that the abstract lacks low-level tokenizer/quantization/codebook details is valid (the abstract does not show exact tokenization/codec settings required by the strict reproducibility criterion), but the presence of a tokenizer as a central component and empirical evaluations (zero-shot cloning, objective/subjective comparisons) make it likely this work belongs to the “Discrete Audio Tokens” topic. Given the remaining uncertainty from the abstract alone but the clear alignment with the token-as-core-representation requirement, I recommend inclusion (better to include).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Molecular Quantum Transformer",
    "abstract": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
    "metadata": {
      "arxiv_id": "2503.21686",
      "title": "Molecular Quantum Transformer",
      "summary": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
      "authors": [
        "Yuichi Kamata",
        "Quoc Hoan Tran",
        "Yasuhiro Endo",
        "Hirotaka Oshima"
      ],
      "published": "2025-03-27T16:54:15Z",
      "updated": "2025-05-16T02:38:13Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.21686v2",
      "landing_url": "https://arxiv.org/abs/2503.21686v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.21686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns a Molecular Quantum Transformer for quantum chemistry, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns a Molecular Quantum Transformer for quantum chemistry, with no discussion of discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on a quantum Transformer model for molecular quantum systems, specifically for calculating ground-state energies, which pertains to quantum chemistry and quantum machine learning rather than discrete audio token representations or audio-related tasks. There is no mention of audio waveform discretization, neural audio codecs, discrete tokens derived from audio, or audio generation/understanding tasks, which are core to the inclusion criteria. Therefore, this study fails to meet the inclusion criteria centered on discrete audio tokens and instead belongs to an unrelated research domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on a quantum Transformer model for molecular quantum systems, specifically for calculating ground-state energies, which pertains to quantum chemistry and quantum machine learning rather than discrete audio token representations or audio-related tasks. There is no mention of audio waveform discretization, neural audio codecs, discrete tokens derived from audio, or audio generation/understanding tasks, which are core to the inclusion criteria. Therefore, this study fails to meet the inclusion criteria centered on discrete audio tokens and instead belongs to an unrelated research domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA",
    "abstract": "Transcription of aviation communications has several applications, from assisting air traffic controllers in identifying the accuracy of read-back errors to search and rescue operations. Recent advances in artificial intelligence have provided unprecedented opportunities for improving aviation communication transcription tasks. OpenAI's Whisper is one of the leading automatic speech recognition models. However, fine-tuning Whisper for aviation communication transcription is not computationally efficient. Thus, this paper aims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation to fine-tune a more computationally efficient version of Whisper, distil-Whisper. To perform the fine-tuning, we used the Air Traffic Control Corpus dataset from the Linguistic Data Consortium, which contains approximately 70 hours of controller and pilot transmissions near three major airports in the US. The objective was to reduce the word error rate to enhance accuracy in the transcription of aviation communication. First, starting with an initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we performed a grid search. We applied a 5-fold cross-validation to find the best combination of distil-Whisper hyperparameters. Then, we fine-tuned the model for LoRA hyperparameters, achieving an impressive average word error rate of 3.86% across five folds. This result highlights the model's potential for use in the cockpit.",
    "metadata": {
      "arxiv_id": "2503.22692",
      "title": "Enhancing Aviation Communication Transcription: Fine-Tuning Distil-Whisper with LoRA",
      "summary": "Transcription of aviation communications has several applications, from assisting air traffic controllers in identifying the accuracy of read-back errors to search and rescue operations. Recent advances in artificial intelligence have provided unprecedented opportunities for improving aviation communication transcription tasks. OpenAI's Whisper is one of the leading automatic speech recognition models. However, fine-tuning Whisper for aviation communication transcription is not computationally efficient. Thus, this paper aims to use a Parameter-Efficient Fine-tuning method called Low-Rank Adaptation to fine-tune a more computationally efficient version of Whisper, distil-Whisper. To perform the fine-tuning, we used the Air Traffic Control Corpus dataset from the Linguistic Data Consortium, which contains approximately 70 hours of controller and pilot transmissions near three major airports in the US. The objective was to reduce the word error rate to enhance accuracy in the transcription of aviation communication. First, starting with an initial set of hyperparameters for LoRA (Alpha = 64 and Rank = 32), we performed a grid search. We applied a 5-fold cross-validation to find the best combination of distil-Whisper hyperparameters. Then, we fine-tuned the model for LoRA hyperparameters, achieving an impressive average word error rate of 3.86% across five folds. This result highlights the model's potential for use in the cockpit.",
      "authors": [
        "Shokoufeh Mirzaei",
        "Jesse Arzate",
        "Yukti Vijay"
      ],
      "published": "2025-03-13T22:12:45Z",
      "updated": "2025-03-13T22:12:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.22692v1",
      "landing_url": "https://arxiv.org/abs/2503.22692v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.22692"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on fine-tuning ASR models (Distil-Whisper) for aviation transcription using LoRA; it does not introduce or analyze discrete audio tokens, tokenizers, or token-based representations, thus failing the discrete-token inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on fine-tuning ASR models (Distil-Whisper) for aviation transcription using LoRA; it does not introduce or analyze discrete audio tokens, tokenizers, or token-based representations, thus failing the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on fine-tuning a speech recognition model (distil-Whisper) for aviation communication transcription, emphasizing word error rate improvements. However, it does not describe any process of discretizing continuous audio signals into discrete token sequences, nor does it mention neural audio codecs, vector quantization, or token vocabularies, which are central to the discrete audio tokens topic. Therefore, it fails to meet the inclusion criteria centered on discrete audio token generation and usage and aligns more with continuous audio representation for speech transcription tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on fine-tuning a speech recognition model (distil-Whisper) for aviation communication transcription, emphasizing word error rate improvements. However, it does not describe any process of discretizing continuous audio signals into discrete token sequences, nor does it mention neural audio codecs, vector quantization, or token vocabularies, which are central to the discrete audio tokens topic. Therefore, it fails to meet the inclusion criteria centered on discrete audio token generation and usage and aligns more with continuous audio representation for speech transcription tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
    "abstract": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
    "metadata": {
      "arxiv_id": "2503.24164",
      "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
      "summary": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
      "authors": [
        "Ngoc Dung Huynh",
        "Mohamed Reda Bouadjenek",
        "Imran Razzak",
        "Hakim Hacid",
        "Sunil Aryal"
      ],
      "published": "2025-03-31T14:46:34Z",
      "updated": "2025-07-07T14:41:48Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.24164v2",
      "landing_url": "https://arxiv.org/abs/2503.24164v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.24164"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on a unified speech-vision-language model with multimodal reasoning and speech generation, but provides no details on discrete audio tokens, tokenizers, or neural codecs, so it does not meet the discrete-audio-token criteria and aligns with the exclusion criteria of non-discrete audio representations.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on a unified speech-vision-language model with multimodal reasoning and speech generation, but provides no details on discrete audio tokens, tokenizers, or neural codecs, so it does not meet the discrete-audio-token criteria and aligns with the exclusion criteria of non-discrete audio representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a unified speech-vision-language model (SVLA) with multimodal reasoning and speech generation capabilities; however, the abstract does not clearly describe the use or generation of discrete audio tokens, nor does it provide details about tokenization processes, codec architectures, or quantization methods necessary for discrete token representation and reproducibility, which are essential according to the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a unified speech-vision-language model (SVLA) with multimodal reasoning and speech generation capabilities; however, the abstract does not clearly describe the use or generation of discrete audio tokens, nor does it provide details about tokenization processes, codec architectures, or quantization methods necessary for discrete token representation and reproducibility, which are essential according to the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment",
    "abstract": "Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.",
    "metadata": {
      "arxiv_id": "2504.03669",
      "title": "Self-Learning-Based Optimization for Free-form Pipe Routing in Aeroengine with Dynamic Design Environment",
      "summary": "Pipe routing is a highly complex, time-consuming, and no-deterministic polynomial-time hard (NP-hard) problem in aeroengine design. Despite extensive research efforts in optimizing constant-curvature pipe routing, the growing demand for free-form pipes poses new challenges. Dynamic design environments and fuzzy layout rules further impact the optimization performance and efficiency. To tackle these challenges, this study proposes a self-learning-based method (SLPR) for optimizing free-form pipe routing in aeroengines. The SLPR is based on the proximal policy optimization (PPO) algorithm and integrates a unified rule modeling framework for efficient obstacle detection and fuzzy rule modeling in continuous space. Additionally, a potential energy table is constructed to enable rapid queries of layout tendencies and interference. The agent within SLPR iteratively refines pipe routing and accumulates the design knowledge through interaction with the environment. Once the design environment shifts, the agent can swiftly adapt by fine-tuning network parameters. Comparative tests reveal that SLPR ensures smooth pipe routing through cubic non-uniform B-spline (NURBS) curves, avoiding redundant pipe segments found in constant-curvature pipe routing. Results in both static and dynamic design environments demonstrate that SLPR outperforms three representative baselines in terms of the pipe length reduction, the adherence to layout rules, the path complexity, and the computational efficiency. Furthermore, tests in dynamic environments indicate that SLPR eliminates labor-intensive searches from scratch and even yields superior solutions compared to the retrained model. These results highlight the practical value of SLPR for real-world pipe routing, meeting lightweight, precision, and sustainability requirements of the modern aeroengine design.",
      "authors": [
        "Caicheng Wang",
        "Zili Wang",
        "Shuyou Zhang",
        "Yongzhe Xiang",
        "Zheyi Li",
        "Jianrong Tan"
      ],
      "published": "2025-03-20T13:45:13Z",
      "updated": "2025-03-20T13:45:13Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03669v1",
      "landing_url": "https://arxiv.org/abs/2504.03669v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.03669"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on self-learning optimization for aeroengine pipe routing and has no content on discrete audio tokens or token design, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on self-learning optimization for aeroengine pipe routing and has no content on discrete audio tokens or token design, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on optimizing pipe routing in aeroengines using a self-learning-based method and does not discuss any form of discrete audio tokens, neural audio codecs, tokenization of audio waveforms, or related audio generation, understanding, or multimodal modeling topics. It fails to meet any inclusion criteria related to discrete audio tokens and instead fits an entirely different research domain (aeroengine engineering), so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on optimizing pipe routing in aeroengines using a self-learning-based method and does not discuss any form of discrete audio tokens, neural audio codecs, tokenization of audio waveforms, or related audio generation, understanding, or multimodal modeling topics. It fails to meet any inclusion criteria related to discrete audio tokens and instead fits an entirely different research domain (aeroengine engineering), so it should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
    "abstract": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
    "metadata": {
      "arxiv_id": "2504.05197",
      "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
      "summary": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Tao Wang",
        "Jianhua Tao",
        "Zheng Lian",
        "Zhengqi Wen",
        "Chenxing Li",
        "Ruibo Fu",
        "Ye Bai",
        "Xiaohui Zhang"
      ],
      "published": "2025-04-07T15:47:09Z",
      "updated": "2025-05-05T16:34:37Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05197v2",
      "landing_url": "https://arxiv.org/abs/2504.05197v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.05197"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on plug-and-play watermarking of neural speech generation model parameters for open-source protection and does not discuss discrete audio tokens, tokenizers, or token-level representations, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on plug-and-play watermarking of neural speech generation model parameters for open-source protection and does not discuss discrete audio tokens, tokenizers, or token-level representations, thus it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on watermarking neural speech generation models at the parameter level to protect copyrights rather than on discretizing continuous audio waveforms into discrete token sequences for modeling or generative tasks. It does not describe any process of generating discrete audio tokens, such as vector quantization, multi-codebooks, or other forms of discretization related to neural audio codecs or self-supervised models, nor does it analyze or work directly with discrete audio tokens as per the inclusion criteria. Therefore, it does not meet the core thematic requirements of \"Discrete Audio Tokens.\"",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on watermarking neural speech generation models at the parameter level to protect copyrights rather than on discretizing continuous audio waveforms into discrete token sequences for modeling or generative tasks. It does not describe any process of generating discrete audio tokens, such as vector quantization, multi-codebooks, or other forms of discretization related to neural audio codecs or self-supervised models, nor does it analyze or work directly with discrete audio tokens as per the inclusion criteria. Therefore, it does not meet the core thematic requirements of \"Discrete Audio Tokens.\"",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item introduces TASTE, a discrete speech tokenization and embedding method designed for joint spoken language modeling, explicitly focusing on transforming continuous speech into discrete tokens and analyzing their use in downstream models, with an English title/abstract and clear token-level design emphasis, fulfilling inclusion criteria and not triggering any exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item introduces TASTE, a discrete speech tokenization and embedding method designed for joint spoken language modeling, explicitly focusing on transforming continuous speech into discrete tokens and analyzing their use in downstream models, with an English title/abstract and clear token-level design emphasis, fulfilling inclusion criteria and not triggering any exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes TASTE, an end-to-end method for text-aligned speech tokenization and embedding designed for spoken language modeling, which includes a tokenization stage aligning speech tokens with corresponding text transcriptions. While it addresses reducing token sequence length and preserving paralinguistic information, the abstract does not clearly specify the use or design details of discrete audio tokens such as neural audio codec structures, quantization methods, vocabulary or codebook settings, or discrete token generation mechanisms. Without explicit description of discrete token generation or representation, it's unclear if it meets the core criterion of using discrete audio tokens for spoken language modeling as defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study proposes TASTE, an end-to-end method for text-aligned speech tokenization and embedding designed for spoken language modeling, which includes a tokenization stage aligning speech tokens with corresponding text transcriptions. While it addresses reducing token sequence length and preserving paralinguistic information, the abstract does not clearly specify the use or design details of discrete audio tokens such as neural audio codec structures, quantization methods, vocabulary or codebook settings, or discrete token generation mechanisms. Without explicit description of discrete token generation or representation, it's unclear if it meets the core criterion of using discrete audio tokens for spoken language modeling as defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagree: one judged it includable (claims of text-aligned speech tokenization and reduced token sequence length imply discrete tokens), the other flagged that the abstract lacks explicit details of discrete token generation (e.g., vector quantization/codebook/codec, vocabulary/bitrate) required by the inclusion criteria. The abstract uses terms like “speech tokens” and “tokenization” but does not clearly state a discrete-token generation mechanism or quantization/codec details, so it is ambiguous whether this work meets the strict ‘discrete audio tokens’ requirements—hence uncertain. (Score explanation: ambiguous evidence of true discretization in the abstract, so neither clearly includable nor excludable.)",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagree: one judged it includable (claims of text-aligned speech tokenization and reduced token sequence length imply discrete tokens), the other flagged that the abstract lacks explicit details of discrete token generation (e.g., vector quantization/codebook/codec, vocabulary/bitrate) required by the inclusion criteria. The abstract uses terms like “speech tokens” and “tokenization” but does not clearly state a discrete-token generation mechanism or quantization/codec details, so it is ambiguous whether this work meets the strict ‘discrete audio tokens’ requirements—hence uncertain. (Score explanation: ambiguous evidence of true discretization in the abstract, so neither clearly includable nor excludable.)",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
    "abstract": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
    "metadata": {
      "arxiv_id": "2504.08274",
      "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
      "summary": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
      "authors": [
        "Haowei Lou",
        "Hye-young Paik",
        "Sheng Li",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2025-04-11T06:12:57Z",
      "updated": "2025-04-11T06:12:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08274v1",
      "landing_url": "https://arxiv.org/abs/2504.08274v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08274"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper addresses multilingual TTS with phoneme-level representations and latent features but does not present discrete audio tokens, tokenizers/codecs, or token-based modeling as its core method, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper addresses multilingual TTS with phoneme-level representations and latent features but does not present discrete audio tokens, tokenizers/codecs, or token-based modeling as its core method, so it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes a multilingual TTS framework focusing on phoneme representation standardization and style adaptation but does not mention discretization of audio waveforms into discrete tokens or use of neural audio codecs/vector quantization to generate discrete token sequences, which is the core requirement for inclusion. Hence, it deals with continuous speech synthesis rather than discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes a multilingual TTS framework focusing on phoneme representation standardization and style adaptation but does not mention discretization of audio waveforms into discrete tokens or use of neural audio codecs/vector quantization to generate discrete token sequences, which is the core requirement for inclusion. Hence, it deals with continuous speech synthesis rather than discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
    "abstract": "In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.",
    "metadata": {
      "arxiv_id": "2504.09101",
      "title": "Synthetic Aircraft Trajectory Generation Using Time-Based VQ-VAE",
      "summary": "In modern air traffic management, generating synthetic flight trajectories has emerged as a promising solution for addressing data scarcity, protecting sensitive information, and supporting large-scale analyses. In this paper, we propose a novel method for trajectory synthesis by adapting the Time-Based Vector Quantized Variational Autoencoder (TimeVQVAE). Our approach leverages time-frequency domain processing, vector quantization, and transformer-based priors to capture both global and local dynamics in flight data. By discretizing the latent space and integrating transformer priors, the model learns long-range spatiotemporal dependencies and preserves coherence across entire flight paths. We evaluate the adapted TimeVQVAE using an extensive suite of quality, statistical, and distributional metrics, as well as a flyability assessment conducted in an open-source air traffic simulator. Results indicate that TimeVQVAE outperforms a temporal convolutional VAE baseline, generating synthetic trajectories that mirror real flight data in terms of spatial accuracy, temporal consistency, and statistical properties. Furthermore, the simulator-based assessment shows that most generated trajectories maintain operational feasibility, although occasional outliers underscore the potential need for additional domain-specific constraints. Overall, our findings underscore the importance of multi-scale representation learning for capturing complex flight behaviors and demonstrate the promise of TimeVQVAE in producing representative synthetic trajectories for downstream tasks such as model training, airspace design, and air traffic forecasting.",
      "authors": [
        "Abdulmajid Murad",
        "Massimiliano Ruocco"
      ],
      "published": "2025-04-12T06:46:51Z",
      "updated": "2025-04-12T06:46:51Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09101v1",
      "landing_url": "https://arxiv.org/abs/2504.09101v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.09101"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item describes synthetic aircraft trajectory generation using TimeVQVAE; while it mentions vector quantization, it does not address discrete audio tokens or tokenization design for audio data, thus does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item describes synthetic aircraft trajectory generation using TimeVQVAE; while it mentions vector quantization, it does not address discrete audio tokens or tokenization design for audio data, thus does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on generating synthetic aircraft trajectories using a Time-Based Vector Quantized Variational Autoencoder which employs vector quantization, yet this is applied to flight trajectory data rather than audio waveforms. The inclusion criteria require discrete audio tokens derived from neural audio codecs or self-supervised audio models representing discretized audio waveforms, which is not met here. Since the study does not involve discrete audio tokens for speech, music, or environmental sounds but instead deals with synthetic trajectory data, it does not fit the inclusion criteria and falls under exclusion for non-audio domain.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on generating synthetic aircraft trajectories using a Time-Based Vector Quantized Variational Autoencoder which employs vector quantization, yet this is applied to flight trajectory data rather than audio waveforms. The inclusion criteria require discrete audio tokens derived from neural audio codecs or self-supervised audio models representing discretized audio waveforms, which is not met here. Since the study does not involve discrete audio tokens for speech, music, or environmental sounds but instead deals with synthetic trajectory data, it does not fit the inclusion criteria and falls under exclusion for non-audio domain.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
    "abstract": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
    "metadata": {
      "arxiv_id": "2504.09862",
      "title": "RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-Wave Point Cloud Sequence",
      "summary": "Millimeter-wave radar offers a privacy-preserving and environment-robust alternative to vision-based sensing, enabling human motion analysis in challenging conditions such as low light, occlusions, rain, or smoke. However, its sparse point clouds pose significant challenges for semantic understanding. We present RadarLLM, the first framework that leverages large language models (LLMs) for human motion understanding from radar signals. RadarLLM introduces two key innovations: (1) a motion-guided radar tokenizer based on our Aggregate VQ-VAE architecture, integrating deformable body templates and masked trajectory modeling to convert spatial-temporal radar sequences into compact semantic tokens; and (2) a radar-aware language model that establishes cross-modal alignment between radar and text in a shared embedding space. To overcome the scarcity of paired radar-text data, we generate a realistic radar-text dataset from motion-text datasets with a physics-aware synthesis pipeline. Extensive experiments on both synthetic and real-world benchmarks show that RadarLLM achieves state-of-the-art performance, enabling robust and interpretable motion understanding under privacy and visibility constraints, even in adverse environments. This paper has been accepted for presentation at AAAI 2026. This is an extended version with supplementary materials.",
      "authors": [
        "Zengyuan Lai",
        "Jiarui Yang",
        "Songpengcheng Xia",
        "Lizhou Lin",
        "Lan Sun",
        "Renwen Wang",
        "Jianran Liu",
        "Qi Wu",
        "Ling Pei"
      ],
      "published": "2025-04-14T04:18:25Z",
      "updated": "2025-11-17T04:20:12Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.09862v2",
      "landing_url": "https://arxiv.org/abs/2504.09862v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.09862"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Although it uses a discrete tokenizer (VQ-VAE) for radar data, the paper is about radar motion understanding rather than discrete audio tokens derived from audio signals, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Although it uses a discrete tokenizer (VQ-VAE) for radar data, the paper is about radar motion understanding rather than discrete audio tokens derived from audio signals, so it does not meet the Discrete Audio Tokens inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on using millimeter-wave radar point clouds and large language models for human motion understanding, without involving discrete audio tokens derived from continuous audio waveforms; it does not describe any process of discretizing audio signals into tokens or any tokenization related to audio, thus it does not meet the inclusion criteria focused on discrete audio token generation and modeling from audio signals.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on using millimeter-wave radar point clouds and large language models for human motion understanding, without involving discrete audio tokens derived from continuous audio waveforms; it does not describe any process of discretizing audio signals into tokens or any tokenization related to audio, thus it does not meet the inclusion criteria focused on discrete audio token generation and modeling from audio signals.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
    "abstract": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
    "metadata": {
      "arxiv_id": "2504.10352",
      "title": "Pseudo-Autoregressive Neural Codec Language Models for Efficient Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recent zero-shot text-to-speech (TTS) systems face a common dilemma: autoregressive (AR) models suffer from slow generation and lack duration controllability, while non-autoregressive (NAR) models lack temporal modeling and typically require complex designs. In this paper, we introduce a novel pseudo-autoregressive (PAR) codec language modeling approach that unifies AR and NAR modeling. Combining explicit temporal modeling from AR with parallel generation from NAR, PAR generates dynamic-length spans at fixed time steps. Building on PAR, we propose PALLE, a two-stage TTS system that leverages PAR for initial generation followed by NAR refinement. In the first stage, PAR progressively generates speech tokens along the time dimension, with each step predicting all positions in parallel but only retaining the left-most span. In the second stage, low-confidence tokens are iteratively refined in parallel, leveraging the global contextual information. Experiments demonstrate that PALLE, trained on LibriTTS, outperforms state-of-the-art systems trained on large-scale data, including F5-TTS, E2-TTS, and MaskGCT, on the LibriSpeech test-clean set in terms of speech quality, speaker similarity, and intelligibility, while achieving up to ten times faster inference speed. Audio samples are available at https://microsoft.com/research/project/vall-e-x/palle.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Yuxuan Hu",
        "Haibin Wu",
        "Hui Wang",
        "Jianwei Yu",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yanqing Liu",
        "Yan Lu",
        "Kai Yu",
        "Xie Chen"
      ],
      "published": "2025-04-14T16:03:21Z",
      "updated": "2025-08-05T15:33:39Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10352v3",
      "landing_url": "https://arxiv.org/abs/2504.10352v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.10352"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item presents a neural codec language modeling approach for TTS that uses speech tokens produced by a neural codec, i.e., discrete token representations, and designs token-level generation/refinement stages, aligning with the discrete audio token criteria and providing empirical evaluation, thus should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item presents a neural codec language modeling approach for TTS that uses speech tokens produced by a neural codec, i.e., discrete token representations, and designs token-level generation/refinement stages, aligning with the discrete audio token criteria and providing empirical evaluation, thus should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a pseudo-autoregressive codec language modeling approach for zero-shot text-to-speech synthesis, mentioning neural codec language models, but the abstract does not explicitly describe using discrete audio tokens, tokenizer architecture, quantization methods, or codebook settings to confirm the presence of discrete token sequences as per the inclusion criteria. While it leverages neural codecs, it lacks sufficient details about the discrete token generation pipeline necessary for a definite inclusion decision.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study proposes a pseudo-autoregressive codec language modeling approach for zero-shot text-to-speech synthesis, mentioning neural codec language models, but the abstract does not explicitly describe using discrete audio tokens, tokenizer architecture, quantization methods, or codebook settings to confirm the presence of discrete token sequences as per the inclusion criteria. While it leverages neural codecs, it lacks sufficient details about the discrete token generation pipeline necessary for a definite inclusion decision.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted relevant points: one flagged that the paper uses a neural-codec-based language model that generates speech tokens, while the other was cautious that the abstract lacks low-level tokenizer/codebook details. The abstract nevertheless explicitly frames a ‘codec language modeling’ approach that generates and refines speech tokens for TTS, i.e., discrete audio tokens produced/used by a neural codec and applied as the core sequence representation—so it meets the main inclusion condition even if implementation details are not described in the abstract (these can be checked in the full paper).",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted relevant points: one flagged that the paper uses a neural-codec-based language model that generates speech tokens, while the other was cautious that the abstract lacks low-level tokenizer/codebook details. The abstract nevertheless explicitly frames a ‘codec language modeling’ approach that generates and refines speech tokens for TTS, i.e., discrete audio tokens produced/used by a neural codec and applied as the core sequence representation—so it meets the main inclusion condition even if implementation details are not described in the abstract (these can be checked in the full paper).",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
    "abstract": "Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.",
    "metadata": {
      "arxiv_id": "2504.12005",
      "title": "Voice Conversion with Diverse Intonation using Conditional Variational Auto-Encoder",
      "summary": "Voice conversion is a task of synthesizing an utterance with target speaker's voice while maintaining linguistic information of the source utterance. While a speaker can produce varying utterances from a single script with different intonations, conventional voice conversion models were limited to producing only one result per source input. To overcome this limitation, we propose a novel approach for voice conversion with diverse intonations using conditional variational autoencoder (CVAE). Experiments have shown that the speaker's style feature can be mapped into a latent space with Gaussian distribution. We have also been able to convert voices with more diverse intonation by making the posterior of the latent space more complex with inverse autoregressive flow (IAF). As a result, the converted voice not only has a diversity of intonations, but also has better sound quality than the model without CVAE.",
      "authors": [
        "Soobin Suh",
        "Dabi Ahn",
        "Heewoong Park",
        "Jonghun Park"
      ],
      "published": "2025-04-16T11:59:56Z",
      "updated": "2025-04-16T11:59:56Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12005v1",
      "landing_url": "https://arxiv.org/abs/2504.12005v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12005"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on CVAE-based voice conversion with latent continuous representations and does not describe discrete audio token generation or a tokenization/codec scheme, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on CVAE-based voice conversion with latent continuous representations and does not describe discrete audio token generation or a tokenization/codec scheme, so it does not meet the 'Discrete Audio Tokens' criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using conditional variational auto-encoders to enhance intonation diversity, but it does not mention the use or generation of discrete audio tokens, such as neural audio codecs, vector quantization, or tokenization processes; thus, it does not meet the inclusion criteria related to discrete token representation and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using conditional variational auto-encoders to enhance intonation diversity, but it does not mention the use or generation of discrete audio tokens, such as neural audio codecs, vector quantization, or tokenization processes; thus, it does not meet the inclusion criteria related to discrete token representation and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
    "abstract": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
    "metadata": {
      "arxiv_id": "2504.12339",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "authors": [
        "Yaodong Song",
        "Hongjie Chen",
        "Jie Lian",
        "Yuxin Zhang",
        "Guangmin Xia",
        "Zehan Li",
        "Genliang Zhao",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "published": "2025-04-15T01:44:56Z",
      "updated": "2025-05-28T14:24:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12339v2",
      "landing_url": "https://arxiv.org/abs/2504.12339v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.12339"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes GOAT-TTS using continuous acoustic embeddings and LLM-based speech token prediction, but it does not define or rely on discrete audio tokens, tokenizers, or quantization as the core representation, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes GOAT-TTS using continuous acoustic embeddings and LLM-based speech token prediction, but it does not define or rely on discrete audio tokens, tokenizers, or quantization as the core representation, so it does not meet the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract describes a text-to-speech (TTS) model that uses a large language model with a dual-branch architecture, including a speech encoder and fine-tuned LLM layers for speech token prediction; however, it does not explicitly mention the use or design details of discrete audio tokens, such as vector quantization, multi-codebooks, or discrete token vocabularies, nor does it describe the tokenization or codebook settings essential to confirm adherence to the discrete audio token theme. Therefore, it lacks clear evidence of defining or using discrete audio tokens as core representations, making it uncertain whether it meets the inclusion criteria fully.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract describes a text-to-speech (TTS) model that uses a large language model with a dual-branch architecture, including a speech encoder and fine-tuned LLM layers for speech token prediction; however, it does not explicitly mention the use or design details of discrete audio tokens, such as vector quantization, multi-codebooks, or discrete token vocabularies, nor does it describe the tokenization or codebook settings essential to confirm adherence to the discrete audio token theme. Therefore, it lacks clear evidence of defining or using discrete audio tokens as core representations, making it uncertain whether it meets the inclusion criteria fully.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The abstract proposes GOAT-TTS with a dual-branch LLM: a modality-alignment branch using continuous acoustic embeddings (explicitly avoiding quantization-related loss) and a speech-generation branch that performs \"speech token prediction.\" However, the abstract never describes any discrete audio tokenization pipeline, tokenizer/codec architecture, quantization or codebook/vocabulary settings—details which the inclusion criteria require to identify the work as using discrete audio tokens. Both junior reviews similarly noted the absence or ambiguity around discrete-token design. Therefore the item does not meet the inclusion requirements. (Score explanation: Abstract lacks explicit discrete audio token/tokenizer/codebook/quantization details required for inclusion.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "The abstract proposes GOAT-TTS with a dual-branch LLM: a modality-alignment branch using continuous acoustic embeddings (explicitly avoiding quantization-related loss) and a speech-generation branch that performs \"speech token prediction.\" However, the abstract never describes any discrete audio tokenization pipeline, tokenizer/codec architecture, quantization or codebook/vocabulary settings—details which the inclusion criteria require to identify the work as using discrete audio tokens. Both junior reviews similarly noted the absence or ambiguity around discrete-token design. Therefore the item does not meet the inclusion requirements. (Score explanation: Abstract lacks explicit discrete audio token/tokenizer/codebook/quantization details required for inclusion.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
    "abstract": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
    "metadata": {
      "arxiv_id": "2504.12715",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "authors": [
        "Long Zeng",
        "Jianxiang Yu",
        "Jiapeng Zhu",
        "Qingsong Zhong",
        "Xiang Li"
      ],
      "published": "2025-04-17T07:43:52Z",
      "updated": "2025-04-17T07:43:52Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12715v1",
      "landing_url": "https://arxiv.org/abs/2504.12715v1",
      "doi": "https://doi.org/10.1145/3696410.3714656"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item concerns discrete representations via vector-quantized codebooks in graph autoencoders, not discrete audio tokens or audio tokenization architectures, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item concerns discrete representations via vector-quantized codebooks in graph autoencoders, not discrete audio tokens or audio tokenization architectures, thus it does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper deals with vector quantized graph autoencoders focused on graph topology representation and tasks like link prediction and node classification; it does not address discrete audio tokens derived from audio waveforms nor neural audio codec design or audio tokenization methods, and thus falls outside the defined inclusion criteria related to audio token representation and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper deals with vector quantized graph autoencoders focused on graph topology representation and tasks like link prediction and node classification; it does not address discrete audio tokens derived from audio waveforms nor neural audio codec design or audio tokenization methods, and thus falls outside the defined inclusion criteria related to audio token representation and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
    "abstract": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
    "metadata": {
      "arxiv_id": "2504.14075",
      "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
      "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
      "authors": [
        "Wei Dong",
        "Yan Min",
        "Han Zhou",
        "Jun Chen"
      ],
      "published": "2025-04-18T20:57:16Z",
      "updated": "2025-04-18T20:57:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14075v1",
      "landing_url": "https://arxiv.org/abs/2504.14075v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14075"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper is about low-light image enhancement using a CNN-Transformer architecture with structure priors, and it does not discuss discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper is about low-light image enhancement using a CNN-Transformer architecture with structure priors, and it does not discuss discrete audio tokens or tokenization, thus it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on low-light image enhancement using a CNN-Transformer hybrid model guided by structural priors, with no mention of audio processing, discrete audio tokens, or tokenization methods related to audio signals. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and their application.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on low-light image enhancement using a CNN-Transformer hybrid model guided by structural priors, with no mention of audio processing, discrete audio tokens, or tokenization methods related to audio signals. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens and their application.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
    "abstract": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
    "metadata": {
      "arxiv_id": "2504.14092",
      "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
      "summary": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
      "authors": [
        "Wei Dong",
        "Han Zhou",
        "Seyed Amirreza Mousavi",
        "Jun Chen"
      ],
      "published": "2025-04-18T22:19:40Z",
      "updated": "2025-04-18T22:19:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14092v1",
      "landing_url": "https://arxiv.org/abs/2504.14092v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14092"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper concerns image-based shadow removal using Retinex theory and CNN-Transformer architectures; it has nothing to do with discrete audio tokens or tokenization, so it should be excluded under the given criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper concerns image-based shadow removal using Retinex theory and CNN-Transformer architectures; it has nothing to do with discrete audio tokens or tokenization, so it should be excluded under the given criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a mask-free shadow removal method based on a hybrid CNN-Transformer architecture and Retinex theory, targeting image processing and computer vision tasks related to shadow removal. It does not discuss discrete audio tokens, nor does it involve discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-related tasks such as speech or music generation, understanding, or cross-modal alignment. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria related to lack of focus on discrete audio tokens or audio tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a mask-free shadow removal method based on a hybrid CNN-Transformer architecture and Retinex theory, targeting image processing and computer vision tasks related to shadow removal. It does not discuss discrete audio tokens, nor does it involve discretization of continuous audio waveforms into token sequences, neural audio codecs, or any audio-related tasks such as speech or music generation, understanding, or cross-modal alignment. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria related to lack of focus on discrete audio tokens or audio tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Road Environment Segmentation using Vector Quantization",
    "abstract": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets vector quantization for vision-based road segmentation, not discrete audio tokens; it does not meet the inclusion criteria for Discrete Audio Tokens and instead falls under modalities/contexts outside the scope, so it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets vector quantization for vision-based road segmentation, not discrete audio tokens; it does not meet the inclusion criteria for Discrete Audio Tokens and instead falls under modalities/contexts outside the scope, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on road environment segmentation for autonomous driving using vector quantization to discretize visual features, which is unrelated to discrete audio tokens formed via discretizing continuous audio waveforms for audio-based sequence modeling. It does not involve audio data, nor does it discuss neural audio codecs or discrete audio token generation relevant to the inclusion criteria, thus not meeting the topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on road environment segmentation for autonomous driving using vector quantization to discretize visual features, which is unrelated to discrete audio tokens formed via discretizing continuous audio waveforms for audio-based sequence modeling. It does not involve audio data, nor does it discuss neural audio codecs or discrete audio token generation relevant to the inclusion criteria, thus not meeting the topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
    "abstract": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
    "metadata": {
      "arxiv_id": "2504.15509",
      "title": "SimulS2S-LLM: Unlocking Simultaneous Inference of Speech LLMs for Speech-to-Speech Translation",
      "summary": "Simultaneous speech translation (SST) outputs translations in parallel with streaming speech input, balancing translation quality and latency. While large language models (LLMs) have been extended to handle the speech modality, streaming remains challenging as speech is prepended as a prompt for the entire generation process. To unlock LLM streaming capability, this paper proposes SimulS2S-LLM, which trains speech LLMs offline and employs a test-time policy to guide simultaneous inference. SimulS2S-LLM alleviates the mismatch between training and inference by extracting boundary-aware speech prompts that allows it to be better matched with text input data. SimulS2S-LLM achieves simultaneous speech-to-speech translation (Simul-S2ST) by predicting discrete output speech tokens and then synthesising output speech using a pre-trained vocoder. An incremental beam search is designed to expand the search space of speech token prediction without increasing latency. Experiments on the CVSS speech data show that SimulS2S-LLM offers a better translation quality-latency trade-off than existing methods that use the same training data, such as improving ASR-BLEU scores by 3 points at similar latency.",
      "authors": [
        "Keqi Deng",
        "Wenxi Chen",
        "Xie Chen",
        "Philip C. Woodland"
      ],
      "published": "2025-04-22T01:05:32Z",
      "updated": "2025-04-22T01:05:32Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15509v1",
      "landing_url": "https://arxiv.org/abs/2504.15509v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.15509"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper uses discrete output speech tokens as a core representation for speech-to-speech translation and syntheses output speech via a vocoder, aligning with the discrete audio tokens criteria and not appearing to fall into any exclusion category.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper uses discrete output speech tokens as a core representation for speech-to-speech translation and syntheses output speech via a vocoder, aligning with the discrete audio tokens criteria and not appearing to fall into any exclusion category.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The article discusses simultaneous speech-to-speech translation using discrete output speech tokens predicted by an LLM and synthesized by a vocoder, indicating the use of discrete audio tokens from neural audio codec representations, which aligns with the inclusion criteria focused on discrete audio token representations and generation for audio understanding and cross-modal tasks with sufficient English details; thus, it meets the inclusion criteria without exclusion factors.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The article discusses simultaneous speech-to-speech translation using discrete output speech tokens predicted by an LLM and synthesized by a vocoder, indicating the use of discrete audio tokens from neural audio codec representations, which aligns with the inclusion criteria focused on discrete audio token representations and generation for audio understanding and cross-modal tasks with sufficient English details; thus, it meets the inclusion criteria without exclusion factors.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion",
    "abstract": "Using a multi-accented corpus of parallel utterances for use with commercial speech devices, we present a case study to show that it is possible to quantify a degree of confidence about a source speaker's identity in the case of one-to-one voice conversion. Following voice conversion using a HiFi-GAN vocoder, we compare information leakage for a range speaker characteristics; assuming a \"worst-case\" white-box scenario, we quantify our confidence to perform inference and narrow the pool of likely source speakers, reinforcing the regulatory obligation and moral duty that providers of synthetic voices have to ensure the privacy of their speakers' data.",
    "metadata": {
      "arxiv_id": "2504.15822",
      "title": "Quantifying Source Speaker Leakage in One-to-One Voice Conversion",
      "summary": "Using a multi-accented corpus of parallel utterances for use with commercial speech devices, we present a case study to show that it is possible to quantify a degree of confidence about a source speaker's identity in the case of one-to-one voice conversion. Following voice conversion using a HiFi-GAN vocoder, we compare information leakage for a range speaker characteristics; assuming a \"worst-case\" white-box scenario, we quantify our confidence to perform inference and narrow the pool of likely source speakers, reinforcing the regulatory obligation and moral duty that providers of synthetic voices have to ensure the privacy of their speakers' data.",
      "authors": [
        "Scott Wellington",
        "Xuechen Liu",
        "Junichi Yamagishi"
      ],
      "published": "2025-04-22T12:09:03Z",
      "updated": "2025-04-22T12:09:03Z",
      "categories": [
        "cs.SD",
        "cs.CR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.15822v1",
      "landing_url": "https://arxiv.org/abs/2504.15822v1",
      "doi": "https://doi.org/10.1109/BIOSIG61931.2024.10786731"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not discuss discrete audio tokens, tokenization, or token-based representations; it focuses on privacy leakage in one-to-one voice conversion, not on token design or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not discuss discrete audio tokens, tokenization, or token-based representations; it focuses on privacy leakage in one-to-one voice conversion, not on token design or analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study discusses quantifying identity leakage in one-to-one voice conversion using a HiFi-GAN vocoder but does not mention the use or generation of discrete audio tokens or any discretization process to produce token sequences as per the inclusion criteria. The focus is on speaker identity and privacy, rather than on discrete token representations or their design, generation, or application in modeling/generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study discusses quantifying identity leakage in one-to-one voice conversion using a HiFi-GAN vocoder but does not mention the use or generation of discrete audio tokens or any discretization process to produce token sequences as per the inclusion criteria. The focus is on speaker identity and privacy, rather than on discrete token representations or their design, generation, or application in modeling/generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
    "abstract": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.",
    "metadata": {
      "arxiv_id": "2504.18539",
      "title": "Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation",
      "summary": "Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.",
      "authors": [
        "Sungnyun Kim",
        "Sungwoo Cho",
        "Sangmin Bae",
        "Kangwook Jang",
        "Se-Young Yun"
      ],
      "published": "2025-01-23T05:11:19Z",
      "updated": "2025-04-30T05:16:51Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.MM",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.18539v2",
      "landing_url": "https://arxiv.org/abs/2504.18539v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.18539"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on robust audio-visual speech representation using continuous self-supervised learning (CAV2vec) with corrupted modalities, and does not discuss discrete audio tokens, tokenizers, or any token-based encoding as part of its core method, thus it does not meet the discrete audio token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on robust audio-visual speech representation using continuous self-supervised learning (CAV2vec) with corrupted modalities, and does not discuss discrete audio tokens, tokenizers, or any token-based encoding as part of its core method, thus it does not meet the discrete audio token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes a self-supervised learning framework for robust audio-visual speech recognition that focuses on handling corrupted audiovisual inputs but does not discuss or present discrete audio tokens generated via neural audio codecs, vector quantization, or similar methods; it mainly deals with continuous audio-visual representation learning rather than discretization or tokenization of audio signals as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study proposes a self-supervised learning framework for robust audio-visual speech recognition that focuses on handling corrupted audiovisual inputs but does not discuss or present discrete audio tokens generated via neural audio codecs, vector quantization, or similar methods; it mainly deals with continuous audio-visual representation learning rather than discretization or tokenization of audio signals as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
    "abstract": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
    "metadata": {
      "arxiv_id": "2504.19046",
      "title": "Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention",
      "summary": "Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.",
      "authors": [
        "Billel Essaid",
        "Hamza Kheddar",
        "Noureddine Batel"
      ],
      "published": "2025-04-26T22:49:08Z",
      "updated": "2025-04-26T22:49:08Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.19046v1",
      "landing_url": "https://arxiv.org/abs/2504.19046v1",
      "doi": "https://doi.org/10.1109/ICTIS62692.2024.10894163"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此研究聚焦於使用深度學習生成聽覺神經刺激的電圖（electrodograms）以改善人工耳蝸植入編碼，未涉及離散音訊 token、碼本或字彙表的生成與分析，因此不符合離散音訊 token 的納入條件。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此研究聚焦於使用深度學習生成聽覺神經刺激的電圖（electrodograms）以改善人工耳蝸植入編碼，未涉及離散音訊 token、碼本或字彙表的生成與分析，因此不符合離散音訊 token 的納入條件。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on enhancing cochlear implant signal coding using deep learning techniques but does not mention discretizing continuous audio waveforms into discrete token sequences, nor does it describe neural audio codecs, vector quantization, codebook settings, or similar mechanisms which are crucial for meeting the inclusion criteria related to \"Discrete Audio Tokens.\" The abstract also lacks details on token generation processes or their use for audio generation, understanding, or cross-modal modeling, indicating it primarily addresses continuous signal processing rather than discrete token representation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on enhancing cochlear implant signal coding using deep learning techniques but does not mention discretizing continuous audio waveforms into discrete token sequences, nor does it describe neural audio codecs, vector quantization, codebook settings, or similar mechanisms which are crucial for meeting the inclusion criteria related to \"Discrete Audio Tokens.\" The abstract also lacks details on token generation processes or their use for audio generation, understanding, or cross-modal modeling, indicating it primarily addresses continuous signal processing rather than discrete token representation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
    "abstract": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
    "metadata": {
      "arxiv_id": "2504.20629",
      "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
      "summary": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
      "authors": [
        "Jeongsoo Choi",
        "Ji-Hoon Kim",
        "Kim Sung-Bin",
        "Tae-Hyun Oh",
        "Joon Son Chung"
      ],
      "published": "2025-04-29T10:56:24Z",
      "updated": "2025-10-03T10:54:34Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.20629v2",
      "landing_url": "https://arxiv.org/abs/2504.20629v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.20629"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses multimodal-to-speech generation using aligned inputs (text, video, audio) and diffusion; it does not mention discrete audio tokens, tokenizers, or neural audio codecs, so it does not satisfy the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses multimodal-to-speech generation using aligned inputs (text, video, audio) and diffusion; it does not mention discrete audio tokens, tokenizers, or neural audio codecs, so it does not satisfy the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multimodal-to-speech generation using a diffusion transformer model that aligns multiple continuous input modalities (text, video, audio) for synchronized speech synthesis. However, the abstract does not mention any discretization process of audio into discrete tokens, neural audio codecs, or vector quantization methods, nor does it specify use or design of discrete audio tokens as required by the inclusion criteria. It mainly addresses synthesis quality, synchronization, and speaker similarity without describing discrete token generation or token-based modeling details. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and token-based approaches.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multimodal-to-speech generation using a diffusion transformer model that aligns multiple continuous input modalities (text, video, audio) for synchronized speech synthesis. However, the abstract does not mention any discretization process of audio into discrete tokens, neural audio codecs, or vector quantization methods, nor does it specify use or design of discrete audio tokens as required by the inclusion criteria. It mainly addresses synthesis quality, synchronization, and speaker similarity without describing discrete token generation or token-based modeling details. Therefore, it does not meet the core inclusion criteria related to discrete audio tokens and token-based approaches.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs",
    "abstract": "Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.",
    "metadata": {
      "arxiv_id": "2505.00127",
      "title": "Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs",
      "summary": "Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.",
      "authors": [
        "Jinyan Su",
        "Jennifer Healey",
        "Preslav Nakov",
        "Claire Cardie"
      ],
      "published": "2025-04-30T18:48:06Z",
      "updated": "2025-04-30T18:48:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.00127v1",
      "landing_url": "https://arxiv.org/abs/2505.00127v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.00127"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item analyzes the effect of reasoning length on LLM performance and does not discuss discrete audio tokens, tokenization, neural audio codecs, or any token-based audio representation, thus it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic and fails exclusion as well.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item analyzes the effect of reasoning length on LLM performance and does not discuss discrete audio tokens, tokenization, neural audio codecs, or any token-based audio representation, thus it does not meet the inclusion criteria for the 'Discrete Audio Tokens' topic and fails exclusion as well.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on the length of reasoning in large language models and its impact on answer correctness, which is unrelated to discrete audio tokens, audio waveforms discretization, or audio token generation; it does not discuss any neural audio codecs, vector quantization, or discrete tokens related to audio, nor does it address audio generation, understanding, or cross-modal modeling involving discrete audio tokens. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria of not focusing on discrete audio token modeling or analysis.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on the length of reasoning in large language models and its impact on answer correctness, which is unrelated to discrete audio tokens, audio waveforms discretization, or audio token generation; it does not discuss any neural audio codecs, vector quantization, or discrete tokens related to audio, nor does it address audio generation, understanding, or cross-modal modeling involving discrete audio tokens. Therefore, it does not meet the inclusion criteria and falls under the exclusion criteria of not focusing on discrete audio token modeling or analysis.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Optimal Transport and Voice Conversion",
    "abstract": "In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.",
    "metadata": {
      "arxiv_id": "2505.04382",
      "title": "Discrete Optimal Transport and Voice Conversion",
      "summary": "In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.",
      "authors": [
        "Anton Selitskiy",
        "Maitreya Kocharekar"
      ],
      "published": "2025-05-07T13:04:29Z",
      "updated": "2025-11-30T10:52:57Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.04382v3",
      "landing_url": "https://arxiv.org/abs/2505.04382v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.04382"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work uses discrete optimal transport for voice conversion but does not describe discrete token generation or a token vocabulary; it centers on embedding alignment rather than discrete audio tokens, thus failing the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work uses discrete optimal transport for voice conversion but does not describe discrete token generation or a token vocabulary; it centers on embedding alignment rather than discrete audio tokens, thus failing the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses voice conversion using discrete optimal transport mapping between audio embeddings but does not explicitly mention discretization processes producing discrete audio tokens, token vocabularies, neural audio codecs, vector quantization, or token design and evaluation details; thus, it lacks evidence of using discrete token sequences as defined by the inclusion criteria and seems more focused on continuous embedding alignment and generation quality evaluation rather than discrete token-based modeling or generation.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study addresses voice conversion using discrete optimal transport mapping between audio embeddings but does not explicitly mention discretization processes producing discrete audio tokens, token vocabularies, neural audio codecs, vector quantization, or token design and evaluation details; thus, it lacks evidence of using discrete token sequences as defined by the inclusion criteria and seems more focused on continuous embedding alignment and generation quality evaluation rather than discrete token-based modeling or generation.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
    "abstract": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
    "metadata": {
      "arxiv_id": "2505.05159",
      "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
      "summary": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
      "authors": [
        "Linhan Ma",
        "Dake Guo",
        "He Wang",
        "Jin Xu",
        "Lei Xie"
      ],
      "published": "2025-05-08T11:55:19Z",
      "updated": "2025-05-15T08:28:37Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05159v3",
      "landing_url": "https://arxiv.org/abs/2505.05159v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.05159"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item addresses stable/controllable/expressive TTS with AR/NAR architecture and duration predictor, but there is no focus on discrete audio token generation, tokenizer/codec, or token-based modeling as described in the inclusion criteria. Therefore it does not meet the 'Discrete Audio Tokens' core requirements and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item addresses stable/controllable/expressive TTS with AR/NAR architecture and duration predictor, but there is no focus on discrete audio token generation, tokenizer/codec, or token-based modeling as described in the inclusion criteria. Therefore it does not meet the 'Discrete Audio Tokens' core requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a TTS model that improves stability and naturalness by combining autoregressive duration prediction and non-autoregressive acoustic modeling, but it does not mention the use or design of discrete audio tokens (e.g., vector quantization, discrete token vocabularies, or neural audio codecs) as core components for sequence modeling or generation, which is the main inclusion criterion for the \"Discrete Audio Tokens\" topic. Therefore, it lacks critical information regarding discrete token generation, representation, or evaluation necessary to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a TTS model that improves stability and naturalness by combining autoregressive duration prediction and non-autoregressive acoustic modeling, but it does not mention the use or design of discrete audio tokens (e.g., vector quantization, discrete token vocabularies, or neural audio codecs) as core components for sequence modeling or generation, which is the main inclusion criterion for the \"Discrete Audio Tokens\" topic. Therefore, it lacks critical information regarding discrete token generation, representation, or evaluation necessary to meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
    "abstract": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
    "metadata": {
      "arxiv_id": "2505.05738",
      "title": "Accurate and Efficient Multivariate Time Series Forecasting via Offline Clustering",
      "summary": "Accurate and efficient multivariate time series (MTS) forecasting is essential for applications such as traffic management and weather prediction, which depend on capturing long-range temporal dependencies and interactions between entities. Existing methods, particularly those based on Transformer architectures, compute pairwise dependencies across all time steps, leading to a computational complexity that scales quadratically with the length of the input. To overcome these challenges, we introduce the Forecaster with Offline Clustering Using Segments (FOCUS), a novel approach to MTS forecasting that simplifies long-range dependency modeling through the use of prototypes extracted via offline clustering. These prototypes encapsulate high-level events in the real-world system underlying the data, summarizing the key characteristics of similar time segments. In the online phase, FOCUS dynamically adapts these patterns to the current input and captures dependencies between the input segment and high-level events, enabling both accurate and efficient forecasting. By identifying prototypes during the offline clustering phase, FOCUS reduces the computational complexity of modeling long-range dependencies in the online phase to linear scaling. Extensive experiments across diverse benchmarks demonstrate that FOCUS achieves state-of-the-art accuracy while significantly reducing computational costs.",
      "authors": [
        "Yiming Niu",
        "Jinliang Deng",
        "Lulu Zhang",
        "Zimu Zhou",
        "Yongxin Tong"
      ],
      "published": "2025-05-09T02:34:06Z",
      "updated": "2025-05-25T07:48:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05738v2",
      "landing_url": "https://arxiv.org/abs/2505.05738v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.05738"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper targets multivariate time series forecasting via offline clustering and prototypes, with no discrete audio tokenization or token design, so it does not meet the discrete audio tokens inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper targets multivariate time series forecasting via offline clustering and prototypes, with no discrete audio tokenization or token design, so it does not meet the discrete audio tokens inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a method for multivariate time series forecasting using offline clustering and prototypes, with no mention of discrete audio tokens, neural audio codecs, or tokenization processes related to audio signals, which are core to the inclusion criteria. The study also does not address audio generation, understanding, or cross-modal modeling involving discrete tokens, falling outside the scope defined. Therefore, it fails to meet the inclusion criteria and aligns with exclusion criteria related to lack of discrete audio token focus.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a method for multivariate time series forecasting using offline clustering and prototypes, with no mention of discrete audio tokens, neural audio codecs, or tokenization processes related to audio signals, which are core to the inclusion criteria. The study also does not address audio generation, understanding, or cross-modal modeling involving discrete tokens, falling outside the scope defined. Therefore, it fails to meet the inclusion criteria and aligns with exclusion criteria related to lack of discrete audio token focus.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
    "abstract": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
    "metadata": {
      "arxiv_id": "2505.06252",
      "title": "ZipLLM: Efficient LLM Storage via Model-Aware Synergistic Data Deduplication and Compression",
      "summary": "Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM-oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication is better aligned with model storage workloads, achieving high data reduction with low metadata overhead. Building on these insights, we design BitX, an effective, fast, lossless delta compression algorithm that compresses XORed difference between fine-tuned and base LLMs. We build ZipLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, ZipLLM reduces model storage consumption by 54%, over 20% higher than state-of-the-art deduplication and compression approaches.",
      "authors": [
        "Zirui Wang",
        "Tingfeng Lan",
        "Zhaoyuan Su",
        "Juncheng Yang",
        "Yue Cheng"
      ],
      "published": "2025-04-30T04:16:32Z",
      "updated": "2025-11-08T18:45:50Z",
      "categories": [
        "cs.DB",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.06252v3",
      "landing_url": "https://arxiv.org/abs/2505.06252v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.06252"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item focuses on model storage compression for LLMs (delta compression BitX/ZipLLM) and does not address discrete audio tokens, tokenization, or any audio-related token design; thus it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item focuses on model storage compression for LLMs (delta compression BitX/ZipLLM) and does not address discrete audio tokens, tokenization, or any audio-related token design; thus it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on efficient storage of large language models (LLMs) via deduplication and compression techniques; there is no mention of discrete audio tokens, neural audio codecs, or any audio-related tokenization mechanisms. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens, nor does it address the relevant topic of audio tokenization or modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on efficient storage of large language models (LLMs) via deduplication and compression techniques; there is no mention of discrete audio tokens, neural audio codecs, or any audio-related tokenization mechanisms. Therefore, the study does not meet the inclusion criteria related to discrete audio tokens, nor does it address the relevant topic of audio tokenization or modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating self-supervised features for expressive, multilingual voice conversion",
    "abstract": "Voice conversion (VC) systems are widely used for several applications, from speaker anonymisation to personalised speech synthesis. Supervised approaches learn a mapping between different speakers using parallel data, which is expensive to produce. Unsupervised approaches are typically trained to reconstruct the input signal, which is composed of the content and the speaker information. Disentangling these components is a challenge and often leads to speaker leakage or prosodic information removal. In this paper, we explore voice conversion by leveraging the potential of self-supervised learning (SSL). A combination of the latent representations of SSL models, concatenated with speaker embeddings, is fed to a vocoder which is trained to reconstruct the input. Zero-shot voice conversion results show that this approach allows to keep the prosody and content of the source speaker while matching the speaker similarity of a VC system based on phonetic posteriorgrams (PPGs).",
    "metadata": {
      "arxiv_id": "2505.08278",
      "title": "Investigating self-supervised features for expressive, multilingual voice conversion",
      "summary": "Voice conversion (VC) systems are widely used for several applications, from speaker anonymisation to personalised speech synthesis. Supervised approaches learn a mapping between different speakers using parallel data, which is expensive to produce. Unsupervised approaches are typically trained to reconstruct the input signal, which is composed of the content and the speaker information. Disentangling these components is a challenge and often leads to speaker leakage or prosodic information removal. In this paper, we explore voice conversion by leveraging the potential of self-supervised learning (SSL). A combination of the latent representations of SSL models, concatenated with speaker embeddings, is fed to a vocoder which is trained to reconstruct the input. Zero-shot voice conversion results show that this approach allows to keep the prosody and content of the source speaker while matching the speaker similarity of a VC system based on phonetic posteriorgrams (PPGs).",
      "authors": [
        "Álvaro Martín-Cortinas",
        "Daniel Sáez-Trigueros",
        "Grzegorz Beringer",
        "Iván Vallés-Pérez",
        "Roberto Barra-Chicote",
        "Biel Tura-Vecino",
        "Adam Gabryś",
        "Piotr Bilinski",
        "Thomas Merritt",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2025-05-13T06:44:03Z",
      "updated": "2025-05-13T06:44:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.08278v1",
      "landing_url": "https://arxiv.org/abs/2505.08278v1",
      "doi": "https://doi.org/10.1109/ICASSPW62465.2024.10627128"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on self-supervised latent representations for voice conversion but does not describe discrete audio token generation, tokenizers, codebooks, or discretized tokens, which are central to the inclusion criteria; therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on self-supervised latent representations for voice conversion but does not describe discrete audio token generation, tokenizers, codebooks, or discretized tokens, which are central to the inclusion criteria; therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on voice conversion using self-supervised learning features, but the abstract does not describe the production or use of discrete audio tokens derived from discretization processes such as vector quantization or clustering; it mainly discusses latent representations and embeddings without detailing discrete token generation, vocabulary, or codebook settings, thus not meeting the core definition of discrete audio tokens required by the inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on voice conversion using self-supervised learning features, but the abstract does not describe the production or use of discrete audio tokens derived from discretization processes such as vector quantization or clustering; it mainly discusses latent representations and embeddings without detailing discrete token generation, vocabulary, or codebook settings, thus not meeting the core definition of discrete audio tokens required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
    "abstract": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
    "metadata": {
      "arxiv_id": "2505.11225",
      "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization",
      "summary": "While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.",
      "authors": [
        "Chengyu Huang",
        "Zhengxin Zhang",
        "Claire Cardie"
      ],
      "published": "2025-05-16T13:21:28Z",
      "updated": "2025-11-11T05:07:45Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11225v2",
      "landing_url": "https://arxiv.org/abs/2505.11225v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.11225"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets training LLMs for concise reasoning using history-aware policy optimization, with no discussion of discrete audio tokens, so it does not meet the Inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets training LLMs for concise reasoning using history-aware policy optimization, with no discussion of discrete audio tokens, so it does not meet the Inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on training language models to reason concisely through a policy optimization method, targeting improvements in text-based reasoning efficiency and accuracy. It does not discuss discrete audio tokens, audio waveform discretization, or any aspects related to audio signal processing or neural audio codecs, which are central to the inclusion criteria. Therefore, the study is not relevant to the topic of discrete audio tokens and does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on training language models to reason concisely through a policy optimization method, targeting improvements in text-based reasoning efficiency and accuracy. It does not discuss discrete audio tokens, audio waveform discretization, or any aspects related to audio signal processing or neural audio codecs, which are central to the inclusion criteria. Therefore, the study is not relevant to the topic of discrete audio tokens and does not meet the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
    "abstract": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
    "metadata": {
      "arxiv_id": "2505.11391",
      "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
      "summary": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
      "authors": [
        "Julius Richter",
        "Danilo de Oliveira",
        "Tal Peer",
        "Timo Gerkmann"
      ],
      "published": "2025-05-16T15:56:07Z",
      "updated": "2025-10-24T13:26:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11391v3",
      "landing_url": "https://arxiv.org/abs/2505.11391v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.11391"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This work focuses on generating speech from silent video using diffusion models and mel-spectrograms, with no discrete audio tokenization, thus it does not meet the discrete-audio-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This work focuses on generating speech from silent video using diffusion models and mel-spectrograms, with no discrete audio tokenization, thus it does not meet the discrete-audio-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes LipDiffuser, a conditional diffusion model for generating speech from silent video, focusing on visual features and vocoder-based waveform reconstruction. However, it does not mention any discretization of audio waveforms into token sequences, neural audio codecs, vector quantization, or design and evaluation of discrete audio tokens, which are essential for inclusion according to the criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio token representation and generation and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes LipDiffuser, a conditional diffusion model for generating speech from silent video, focusing on visual features and vocoder-based waveform reconstruction. However, it does not mention any discretization of audio waveforms into token sequences, neural audio codecs, vector quantization, or design and evaluation of discrete audio tokens, which are essential for inclusion according to the criteria. Therefore, the study does not meet the inclusion criteria related to discrete audio token representation and generation and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
    "abstract": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
    "metadata": {
      "arxiv_id": "2505.11918",
      "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
      "summary": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
      "authors": [
        "Zhiheng Chen",
        "Ruofan Wu",
        "Guanhua Fang"
      ],
      "published": "2025-05-17T09:02:18Z",
      "updated": "2025-05-17T09:02:18Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11918v1",
      "landing_url": "https://arxiv.org/abs/2505.11918v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.11918"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns transformers for Gaussian Mixture Models with unsupervised learning and does not address discrete audio tokens, tokenization, or audio token-based modeling, thus failing the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns transformers for Gaussian Mixture Models with unsupervised learning and does not address discrete audio tokens, tokenization, or audio token-based modeling, thus failing the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses transformers in the context of Gaussian Mixture Models and unsupervised learning but does not address discrete audio tokens, neural audio codecs, or any form of discretization of continuous audio into token sequences relevant to the topic of \"Discrete Audio Tokens.\" The study does not involve audio waveform discretization, token design, or applications in audio generation, understanding, or cross-modal reasoning. Hence, it does not meet the inclusion criteria focused on discrete audio tokens and related system aspects.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper discusses transformers in the context of Gaussian Mixture Models and unsupervised learning but does not address discrete audio tokens, neural audio codecs, or any form of discretization of continuous audio into token sequences relevant to the topic of \"Discrete Audio Tokens.\" The study does not involve audio waveform discretization, token design, or applications in audio generation, understanding, or cross-modal reasoning. Hence, it does not meet the inclusion criteria focused on discrete audio tokens and related system aspects.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
    "abstract": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
    "metadata": {
      "arxiv_id": "2505.13000",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "authors": [
        "Jiaqi Li",
        "Xiaolong Lin",
        "Zhekai Li",
        "Shixi Huang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Zhenpeng Zhan",
        "Zhizheng Wu"
      ],
      "published": "2025-05-19T11:41:08Z",
      "updated": "2025-10-01T15:01:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13000v2",
      "landing_url": "https://arxiv.org/abs/2505.13000v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13000"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "DualCodec explicitly references first-layer 'codec tokens' and a discrete-token-like aspect within a neural audio codec, proposing semantic-enhanced tokens and a low-frame-rate framework, which aligns with the topic of discrete audio tokens, and compares to token-based codecs; however, the abstract does not detail the token generation process.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "DualCodec explicitly references first-layer 'codec tokens' and a discrete-token-like aspect within a neural audio codec, proposing semantic-enhanced tokens and a low-frame-rate framework, which aligns with the topic of discrete audio tokens, and compares to token-based codecs; however, the abstract does not detail the token generation process.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces DualCodec, a neural audio codec system that integrates semantic and waveform representations to produce low-frame-rate discrete audio tokens suitable for speech generation, describing codec architecture and providing experimental validation; this aligns well with inclusion criteria emphasizing discrete token schemes from neural codecs used for audio generation and provides clear design details for reproducibility.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study introduces DualCodec, a neural audio codec system that integrates semantic and waveform representations to produce low-frame-rate discrete audio tokens suitable for speech generation, describing codec architecture and providing experimental validation; this aligns well with inclusion criteria emphasizing discrete token schemes from neural codecs used for audio generation and provides clear design details for reproducibility.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
    "abstract": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Contains a neural codec-based approach producing discrete acoustic tokens and using them as the primary input for TTS, with token-level denoising and downstream speech synthesis, addressing discrete token design and evaluation; no exclusion criteria evident.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "Contains a neural codec-based approach producing discrete acoustic tokens and using them as the primary input for TTS, with token-level denoising and downstream speech synthesis, addressing discrete token design and evaluation; no exclusion criteria evident.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses using a neural codec-based speech denoiser integrated with an LLM-based TTS system, where discrete acoustic tokens derived from a neural audio codec are central to the method. The approach involves predicting clean acoustic tokens from noisy ones, explicitly referencing the use of codec tokens and token denoising, which aligns well with the inclusion criteria about discrete audio tokens derived from neural audio codecs and their use in downstream generation tasks like TTS. The abstract provides sufficient detail about the token-based method and the codec framework, supporting reproducibility and meeting the English assessability requirement. Therefore, the paper meets all inclusion criteria and no exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper discusses using a neural codec-based speech denoiser integrated with an LLM-based TTS system, where discrete acoustic tokens derived from a neural audio codec are central to the method. The approach involves predicting clean acoustic tokens from noisy ones, explicitly referencing the use of codec tokens and token denoising, which aligns well with the inclusion criteria about discrete audio tokens derived from neural audio codecs and their use in downstream generation tasks like TTS. The abstract provides sufficient detail about the token-based method and the codec framework, supporting reproducibility and meeting the English assessability requirement. Therefore, the paper meets all inclusion criteria and no exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
    "abstract": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
    "metadata": {
      "arxiv_id": "2505.14470",
      "title": "PAST: Phonetic-Acoustic Speech Tokenizer",
      "summary": "We present PAST, a novel end-to-end framework that jointly models phonetic information alongside signal reconstruction, eliminating the need for external pretrained models. Unlike previous approaches that rely on pretrained self-supervised models, PAST employs supervised phonetic data, directly integrating domain knowledge into the tokenization process via auxiliary tasks. Additionally, we introduce a streamable, causal variant of PAST, enabling real-time speech applications. Results demonstrate that PAST surpasses existing evaluated baseline tokenizers across common evaluation metrics, including phonetic representation and speech reconstruction. Notably, PAST also achieves superior performance when serving as a speech representation for speech language models, further highlighting its effectiveness as a foundation for spoken language generation. To foster further research, we release the full implementation. For code, model checkpoints, and samples see: https://pages.cs.huji.ac.il/adiyoss-lab/PAST",
      "authors": [
        "Nadav Har-Tuv",
        "Or Tal",
        "Yossi Adi"
      ],
      "published": "2025-05-20T15:05:14Z",
      "updated": "2025-06-04T08:23:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14470v2",
      "landing_url": "https://arxiv.org/abs/2505.14470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14470"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on a discrete-tokenization framework (PAST) for speech, focusing on producing and utilizing token sequences for phonetic-acoustic representation and downstream models, and it discusses tokenization architecture, downstream tasks, and reproducibility, aligning with the discrete audio token criteria.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This item centers on a discrete-tokenization framework (PAST) for speech, focusing on producing and utilizing token sequences for phonetic-acoustic representation and downstream models, and it discusses tokenization architecture, downstream tasks, and reproducibility, aligning with the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes PAST, an end-to-end speech tokenizer that models phonetic information jointly with signal reconstruction, providing a tokenizer that appears to produce discrete phonetic tokens without reliance on pretrained self-supervised models. The abstract mentions tokenization, phonetic representation, and its use as speech representation for speech language models, aligning with the theme of discrete audio tokens mapped from continuous speech waveforms. While the abstract lacks detailed descriptions about the exact tokenizer architecture, quantization methods, codebooks, or token vocab size, it does claim superiority in tokenization and speech reconstruction, and code/model release implies reproducibility. It fits the inclusion criteria since it focuses on discrete token sequences for speech modeling and generation, not mere continuous representations or discriminative tasks. Thus, it is appropriate to include but with a slight reservation due to limited technical detail in the abstract alone.",
      "evaluation": 4
    },
    "round-A_JuniorMini_reasoning": "The study proposes PAST, an end-to-end speech tokenizer that models phonetic information jointly with signal reconstruction, providing a tokenizer that appears to produce discrete phonetic tokens without reliance on pretrained self-supervised models. The abstract mentions tokenization, phonetic representation, and its use as speech representation for speech language models, aligning with the theme of discrete audio tokens mapped from continuous speech waveforms. While the abstract lacks detailed descriptions about the exact tokenizer architecture, quantization methods, codebooks, or token vocab size, it does claim superiority in tokenization and speech reconstruction, and code/model release implies reproducibility. It fits the inclusion criteria since it focuses on discrete token sequences for speech modeling and generation, not mere continuous representations or discriminative tasks. Thus, it is appropriate to include but with a slight reservation due to limited technical detail in the abstract alone.",
    "round-A_JuniorMini_evaluation": 4,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Discrete Audio Representations for Automated Audio Captioning",
    "abstract": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item centers on discrete audio tokens and their generation (tokenizers/codec concepts) and applies them to automated audio captioning with downstream evaluation, satisfying the inclusion criteria and not meeting any exclusion criteria.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "This item centers on discrete audio tokens and their generation (tokenizers/codec concepts) and applies them to automated audio captioning with downstream evaluation, satisfying the inclusion criteria and not meeting any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses discrete audio tokens generated both through unsupervised and supervised tokenization of continuous audio, with a focus on their application to automated audio captioning. It specifically investigates audio tokens as discrete representations, compares tokenization methods, and proposes a supervised tokenizer with explicit semantic information, aligning with the inclusion criteria around token generation, evaluation, and application in downstream tasks. The abstract provides sufficient detail indicating tokenization method and downstream use, meeting the English information requirement and core focus on discrete audio tokens rather than continuous representations alone or purely discriminative tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper discusses discrete audio tokens generated both through unsupervised and supervised tokenization of continuous audio, with a focus on their application to automated audio captioning. It specifically investigates audio tokens as discrete representations, compares tokenization methods, and proposes a supervised tokenizer with explicit semantic information, aligning with the inclusion criteria around token generation, evaluation, and application in downstream tasks. The abstract provides sufficient detail indicating tokenization method and downstream use, meeting the English information requirement and core focus on discrete audio tokens rather than continuous representations alone or purely discriminative tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
    "abstract": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
    "metadata": {
      "arxiv_id": "2505.16616",
      "title": "Performance of Objective Speech Quality Metrics on Languages Beyond Validation Data: A Study of Turkish and Korean",
      "summary": "Objective speech quality measures are widely used to assess the performance of video conferencing platforms and telecommunication systems. They predict human-rated speech quality and are crucial for assessing the systems quality of experience. Despite the widespread use, the quality measures are developed on a limited set of languages. This can be problematic since the performance on unseen languages is consequently not guaranteed or even studied. Here we raise awareness to this issue by investigating the performance of two objective speech quality measures (PESQ and ViSQOL) on Turkish and Korean. Using English as baseline, we show that Turkish samples have significantly higher ViSQOL scores and that for Turkish male speakers the correlation between PESQ and ViSQOL is highest. These results highlight the need to explore biases across metrics and to develop a labeled speech quality dataset with a variety of languages.",
      "authors": [
        "Javier Perez",
        "Dimme de Groot",
        "Jorge Martinez"
      ],
      "published": "2025-05-22T12:50:32Z",
      "updated": "2025-05-22T12:50:32Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16616v1",
      "landing_url": "https://arxiv.org/abs/2505.16616v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16616"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item analyzes cross-language performance of traditional objective speech quality metrics (PESQ, ViSQOL) and does not involve discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item analyzes cross-language performance of traditional objective speech quality metrics (PESQ, ViSQOL) and does not involve discrete audio tokens, tokenization, or token-based modeling, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study investigates the performance of objective speech quality metrics on different languages, focusing on evaluation of speech quality measures rather than producing or analyzing discrete audio tokens that serve as core representations for generative or multimodal tasks. The abstract does not mention discretization processes, neural audio codecs, or discrete token sequences that align with the inclusion criteria around discrete audio tokens. Therefore, it does not meet the specific topic focus and technical details required for inclusion.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study investigates the performance of objective speech quality metrics on different languages, focusing on evaluation of speech quality measures rather than producing or analyzing discrete audio tokens that serve as core representations for generative or multimodal tasks. The abstract does not mention discretization processes, neural audio codecs, or discrete token sequences that align with the inclusion criteria around discrete audio tokens. Therefore, it does not meet the specific topic focus and technical details required for inclusion.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion",
    "abstract": "Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/",
    "metadata": {
      "arxiv_id": "2505.16691",
      "title": "EZ-VC: Easy Zero-shot Any-to-Any Voice Conversion",
      "summary": "Voice Conversion research in recent times has increasingly focused on improving the zero-shot capabilities of existing methods. Despite remarkable advancements, current architectures still tend to struggle in zero-shot cross-lingual settings. They are also often unable to generalize for speakers of unseen languages and accents. In this paper, we adopt a simple yet effective approach that combines discrete speech representations from self-supervised models with a non-autoregressive Diffusion-Transformer based conditional flow matching speech decoder. We show that this architecture allows us to train a voice-conversion model in a purely textless, self-supervised fashion. Our technique works without requiring multiple encoders to disentangle speech features. Our model also manages to excel in zero-shot cross-lingual settings even for unseen languages. For Demo: https://ez-vc.github.io/EZ-VC-Demo/",
      "authors": [
        "Advait Joglekar",
        "Divyanshu Singh",
        "Rooshil Rohit Bhatia",
        "S. Umesh"
      ],
      "published": "2025-05-22T13:57:02Z",
      "updated": "2025-05-23T05:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16691v2",
      "landing_url": "https://arxiv.org/abs/2505.16691v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.16691"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper explicitly uses discrete speech representations from self-supervised models as input tokens for a voice-conversion system and discusses token-based modeling, meeting the discrete-audio-token criteria, with an English abstract and accessible methodology, so it should be included.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly uses discrete speech representations from self-supervised models as input tokens for a voice-conversion system and discusses token-based modeling, meeting the discrete-audio-token criteria, with an English abstract and accessible methodology, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents a voice conversion model trained in a purely textless, self-supervised fashion that combines discrete speech representations from self-supervised models with a diffusion-transformer decoder. It clearly involves discrete tokens derived from self-supervised models used as core representations for downstream generation tasks, matching the inclusion criteria on token source and research contribution. The paper provides sufficient English information with the tokenization mechanism and focuses on audio generation with discrete token sequences rather than continuous representations or traditional coding. Therefore, it meets all inclusion and none of the exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents a voice conversion model trained in a purely textless, self-supervised fashion that combines discrete speech representations from self-supervised models with a diffusion-transformer decoder. It clearly involves discrete tokens derived from self-supervised models used as core representations for downstream generation tasks, matching the inclusion criteria on token source and research contribution. The paper provides sufficient English information with the tokenization mechanism and focuses on audio generation with discrete token sequences rather than continuous representations or traditional coding. Therefore, it meets all inclusion and none of the exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
    "abstract": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract centers on discrete audio tokens produced by a speech tokenizer, analyzes frame-rate effects on tokenization across Mandarin and English, and discusses downstream ASR/TTS implications, with an English abstract provided; it meets the token-discretization criteria without evident exclusion.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on discrete audio tokens produced by a speech tokenizer, analyzes frame-rate effects on tokenization across Mandarin and English, and discusses downstream ASR/TTS implications, with an English abstract provided; it meets the token-discretization criteria without evident exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study explores how varying frame rates affect speech tokenization, focusing on semantic tokens derived from speech encoding, which aligns with the topic of discrete audio tokens generated via neural audio codec approaches. It addresses the influence of token design (frame rates) on downstream tasks (speech recognition), fulfilling the inclusion criteria of analyzing discrete audio tokens and their impact on tasks like speech understanding. The abstract provides English title and abstract with emphasis on token generation and evaluation, making it evaluable. There is no indication that the study focuses solely on continuous representations, traditional codecs, or non-discrete token based methods, so it does not meet any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study explores how varying frame rates affect speech tokenization, focusing on semantic tokens derived from speech encoding, which aligns with the topic of discrete audio tokens generated via neural audio codec approaches. It addresses the influence of token design (frame rates) on downstream tasks (speech recognition), fulfilling the inclusion criteria of analyzing discrete audio tokens and their impact on tasks like speech understanding. The abstract provides English title and abstract with emphasis on token generation and evaluation, making it evaluable. There is no indication that the study focuses solely on continuous representations, traditional codecs, or non-discrete token based methods, so it does not meet any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
    "abstract": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
    "metadata": {
      "arxiv_id": "2505.17446",
      "title": "Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models",
      "summary": "The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.",
      "authors": [
        "Shunsuke Kando",
        "Yusuke Miyao",
        "Shinnosuke Takamichi"
      ],
      "published": "2025-05-23T04:03:27Z",
      "updated": "2025-05-31T13:32:13Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17446v2",
      "landing_url": "https://arxiv.org/abs/2505.17446v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17446"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper explicitly investigates discrete audio tokens via segmentation and clustering to produce token vocabularies and studies their impact on speech language models, meeting the inclusion criteria for discrete token design and downstream task evaluation.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "This paper explicitly investigates discrete audio tokens via segmentation and clustering to produce token vocabularies and studies their impact on speech language models, meeting the inclusion criteria for discrete token design and downstream task evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper explores the impact of segmentation width and vocabulary size on speech tokenization for speech language models, focusing on discrete token sequences derived from speech signals through clustering (K-means), which fits within the discrete audio tokens topic involving discretization of continuous audio into token sequences for modeling. It discusses token design choices and their effects on downstream tasks, and provides evaluation results, aligning well with inclusion criteria emphasizing discrete token design and analysis with clear English abstracts and methods. There is no indication of exclusion criteria being met, such as continuous representations only or lack of discrete token details.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper explores the impact of segmentation width and vocabulary size on speech tokenization for speech language models, focusing on discrete token sequences derived from speech signals through clustering (K-means), which fits within the discrete audio tokens topic involving discretization of continuous audio into token sequences for modeling. It discusses token design choices and their effects on downstream tasks, and provides evaluation results, aligning well with inclusion criteria emphasizing discrete token design and analysis with clear English abstracts and methods. There is no indication of exclusion criteria being met, such as continuous representations only or lack of discrete token details.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
    "abstract": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
    "metadata": {
      "arxiv_id": "2505.17477",
      "title": "Reverse-Speech-Finder: A Neural Network Backtracking Architecture for Generating Alzheimer's Disease Speech Samples and Improving Diagnosis Performance",
      "summary": "This study introduces Reverse-Speech-Finder (RSF), a groundbreaking neural network backtracking architecture designed to enhance Alzheimer's Disease (AD) diagnosis through speech analysis. Leveraging the power of pre-trained large language models, RSF identifies and utilizes the most probable AD-specific speech markers, addressing both the scarcity of real AD speech samples and the challenge of limited interpretability in existing models. RSF's unique approach consists of three core innovations: Firstly, it exploits the observation that speech markers most probable of predicting AD, defined as the most probable speech-markers (MPMs), must have the highest probability of activating those neurons (in the neural network) with the highest probability of predicting AD, defined as the most probable neurons (MPNs). Secondly, it utilizes a speech token representation at the input layer, allowing backtracking from MPNs to identify the most probable speech-tokens (MPTs) of AD. Lastly, it develops an innovative backtracking method to track backwards from the MPNs to the input layer, identifying the MPTs and the corresponding MPMs, and ingeniously uncovering novel speech markers for AD detection. Experimental results demonstrate RSF's superiority over traditional methods such as SHAP and Integrated Gradients, achieving a 3.5% improvement in accuracy and a 3.2% boost in F1-score. By generating speech data that encapsulates novel markers, RSF not only mitigates the limitations of real data scarcity but also significantly enhances the robustness and accuracy of AD diagnostic models. These findings underscore RSF's potential as a transformative tool in speech-based AD detection, offering new insights into AD-related linguistic deficits and paving the way for more effective non-invasive early intervention strategies.",
      "authors": [
        "Victor OK Li",
        "Yang Han",
        "Jacqueline CK Lam",
        "Lawrence YL Cheung"
      ],
      "published": "2025-05-23T04:59:27Z",
      "updated": "2025-05-23T04:59:27Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17477v1",
      "landing_url": "https://arxiv.org/abs/2505.17477v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17477"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract mentions a 'speech token representation' and backtracking from neurons to input tokens, but it does not clearly describe the use or design of discrete audio tokens (e.g., via neural codecs, tokenizers, or codebooks) as the core representation or evaluation focus. The study centers on a backtracking architecture to identify AD-related speech markers and generate samples, not on discrete token design or token-based downstream tasks. Therefore it likely does not meet the 'Discrete Audio Tokens' inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions a 'speech token representation' and backtracking from neurons to input tokens, but it does not clearly describe the use or design of discrete audio tokens (e.g., via neural codecs, tokenizers, or codebooks) as the core representation or evaluation focus. The study centers on a backtracking architecture to identify AD-related speech markers and generate samples, not on discrete token design or token-based downstream tasks. Therefore it likely does not meet the 'Discrete Audio Tokens' inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Alzheimer's disease speech analysis using a neural network backtracking architecture and large language models to identify speech markers, but it does not indicate the use or generation of discrete audio tokens derived from discretizing continuous audio waveforms into token sequences via neural audio codecs or self-supervised learning models, nor does it provide details on tokenizer architecture, quantization, or codebook settings necessary to assess the discrete token methodology; thus it does not meet the inclusion criteria requiring discrete audio tokens as core representations.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Alzheimer's disease speech analysis using a neural network backtracking architecture and large language models to identify speech markers, but it does not indicate the use or generation of discrete audio tokens derived from discretizing continuous audio waveforms into token sequences via neural audio codecs or self-supervised learning models, nor does it provide details on tokenizer architecture, quantization, or codebook settings necessary to assess the discrete token methodology; thus it does not meet the inclusion criteria requiring discrete audio tokens as core representations.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
    "abstract": "In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at https://funaudiollm.github.io/cosyvoice3.",
    "metadata": {
      "arxiv_id": "2505.17589",
      "title": "CosyVoice 3: Towards In-the-wild Speech Generation via Scaling-up and Post-training",
      "summary": "In our prior works, we introduced a scalable streaming speech synthesis model, CosyVoice 2, which integrates a large language model (LLM) and a chunk-aware flow matching (FM) model, and achieves low-latency bi-streaming speech synthesis and human-parity quality. Despite these advancements, CosyVoice 2 exhibits limitations in language coverage, domain diversity, data volume, text formats, and post-training techniques. In this paper, we present CosyVoice 3, an improved model designed for zero-shot multilingual speech synthesis in the wild, surpassing its predecessor in content consistency, speaker similarity, and prosody naturalness. Key features of CosyVoice 3 include: 1) A novel speech tokenizer to improve prosody naturalness, developed via supervised multi-task training, including automatic speech recognition, speech emotion recognition, language identification, audio event detection, and speaker analysis. 2) A new differentiable reward model for post-training applicable not only to CosyVoice 3 but also to other LLM-based speech synthesis models. 3) Dataset Size Scaling: Training data is expanded from ten thousand hours to one million hours, encompassing 9 languages and 18 Chinese dialects across various domains and text formats. 4) Model Size Scaling: Model parameters are increased from 0.5 billion to 1.5 billion, resulting in enhanced performance on our multilingual benchmark due to the larger model capacity. These advancements contribute significantly to the progress of speech synthesis in the wild. We encourage readers to listen to the demo at https://funaudiollm.github.io/cosyvoice3.",
      "authors": [
        "Zhihao Du",
        "Changfeng Gao",
        "Yuxuan Wang",
        "Fan Yu",
        "Tianyu Zhao",
        "Hao Wang",
        "Xiang Lv",
        "Hui Wang",
        "Chongjia Ni",
        "Xian Shi",
        "Keyu An",
        "Guanrou Yang",
        "Yabin Li",
        "Yanni Chen",
        "Zhifu Gao",
        "Qian Chen",
        "Yue Gu",
        "Mengzhe Chen",
        "Yafeng Chen",
        "Shiliang Zhang",
        "Wen Wang",
        "Jieping Ye"
      ],
      "published": "2025-05-23T07:55:21Z",
      "updated": "2025-05-27T07:48:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17589v2",
      "landing_url": "https://arxiv.org/abs/2505.17589v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17589"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "CosyVoice 3 explicitly mentions a novel speech tokenizer to improve prosody and discusses token-based modeling in the context of a large-scale speech generation system, which aligns with the discrete audio token criterion, though the abstract emphasizes scaling and post-training alongside tokens rather than detailing token design.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "CosyVoice 3 explicitly mentions a novel speech tokenizer to improve prosody and discusses token-based modeling in the context of a large-scale speech generation system, which aligns with the discrete audio token criterion, though the abstract emphasizes scaling and post-training alongside tokens rather than detailing token design.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract discusses improvements in a speech synthesis model (CosyVoice 3) including a novel speech tokenizer developed via supervised multi-task training that encompasses tasks like ASR, emotion recognition, language identification, and speaker analysis. However, it lacks detailed description of a discrete tokenization process, such as vector quantization, codebook design, or explicit mention of mapping continuous audio to discrete token sequences. The tokenization mentioned seems to focus on speech features rather than producing discrete audio tokens as per the inclusion criteria. Therefore, it does not clearly meet the rigorous requirements regarding discrete audio tokens as core representations, nor does it specify critical details for reproducibility like tokenizer architecture, quantization method, or vocabulary details. Hence, the study does not clearly fulfill the criteria for inclusion targeting discrete audio tokens for audio generation or modeling.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The abstract discusses improvements in a speech synthesis model (CosyVoice 3) including a novel speech tokenizer developed via supervised multi-task training that encompasses tasks like ASR, emotion recognition, language identification, and speaker analysis. However, it lacks detailed description of a discrete tokenization process, such as vector quantization, codebook design, or explicit mention of mapping continuous audio to discrete token sequences. The tokenization mentioned seems to focus on speech features rather than producing discrete audio tokens as per the inclusion criteria. Therefore, it does not clearly meet the rigorous requirements regarding discrete audio tokens as core representations, nor does it specify critical details for reproducibility like tokenizer architecture, quantization method, or vocabulary details. Hence, the study does not clearly fulfill the criteria for inclusion targeting discrete audio tokens for audio generation or modeling.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers noted the abstract mentions a “novel speech tokenizer” (one regarded this as potentially aligning with discrete audio tokens, the other flagged insufficient detail). After reviewing, the abstract does not explicitly describe a discrete-token generation pipeline (e.g., vector quantization/codebooks/vocabulary, time downsampling or bitrate, how tokens are used as core sequence representations) nor provide reproducible tokenizer details required by the inclusion criteria—so it fails to demonstrate that discrete audio tokens are the core representation. Therefore I recommend exclusion. (Brief explanation: The paper references a speech tokenizer but lacks essential technical details to confirm it produces and uses discrete audio tokens as defined by the inclusion criteria.)",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers noted the abstract mentions a “novel speech tokenizer” (one regarded this as potentially aligning with discrete audio tokens, the other flagged insufficient detail). After reviewing, the abstract does not explicitly describe a discrete-token generation pipeline (e.g., vector quantization/codebooks/vocabulary, time downsampling or bitrate, how tokens are used as core sequence representations) nor provide reproducible tokenizer details required by the inclusion criteria—so it fails to demonstrate that discrete audio tokens are the core representation. Therefore I recommend exclusion. (Brief explanation: The paper references a speech tokenizer but lacks essential technical details to confirm it produces and uses discrete audio tokens as defined by the inclusion criteria.)",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
    "abstract": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper discusses adaptive semantic tokens for transformer-based edge inference but does not describe discrete audio token generation (tokenizers/codebooks/vocabularies) or use discrete audio tokens as the core input, so it does not meet the 'Discrete Audio Tokens' criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper discusses adaptive semantic tokens for transformer-based edge inference but does not describe discrete audio token generation (tokenizers/codebooks/vocabularies) or use discrete audio tokens as the core input, so it does not meet the 'Discrete Audio Tokens' criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adaptive semantic token communication for transformer-based edge inference, dealing with semantic tokens for object detection transmitted over wireless channels. However, the tokens are semantic features from images (object detection) transmitted for edge inference, not discrete audio tokens derived from continuous audio waveforms as required by the inclusion criteria. There is no mention of discretization of audio signals into discrete audio tokens, neural audio codecs, or audio-related tokenization processes. Therefore, the study does not meet the core inclusion criteria for discrete audio tokens in audio generation or understanding tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adaptive semantic token communication for transformer-based edge inference, dealing with semantic tokens for object detection transmitted over wireless channels. However, the tokens are semantic features from images (object detection) transmitted for edge inference, not discrete audio tokens derived from continuous audio waveforms as required by the inclusion criteria. There is no mention of discretization of audio signals into discrete audio tokens, neural audio codecs, or audio-related tokenization processes. Therefore, the study does not meet the core inclusion criteria for discrete audio tokens in audio generation or understanding tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
    "metadata": {
      "arxiv_id": "2505.18231",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "published": "2025-05-23T12:40:07Z",
      "updated": "2025-12-14T08:17:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "landing_url": "https://arxiv.org/abs/2505.18231v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.18231"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on calibration-free vector quantization for KV caches in LLM inference; there is no discrete audio tokenization method or audio-focused token design, so it doesn't meet the discrete audio tokens criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on calibration-free vector quantization for KV caches in LLM inference; there is no discrete audio tokenization method or audio-focused token design, so it doesn't meet the discrete audio tokens criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper proposes NSNQuant, a method for low-bit vector quantization of KV cache used in large language model inference, focusing on compression and throughput improvement. However, it does not address discrete audio tokens obtained from neural audio codecs or self-supervised audio units, nor does it discuss audio waveform discretization, token vocabularies, or their use in audio generation/understanding or multimodal reasoning. Therefore, it does not meet the inclusion criteria related to discrete audio token research and instead focuses on general model compression techniques unrelated to audio token discretization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper proposes NSNQuant, a method for low-bit vector quantization of KV cache used in large language model inference, focusing on compression and throughput improvement. However, it does not address discrete audio tokens obtained from neural audio codecs or self-supervised audio units, nor does it discuss audio waveform discretization, token vocabularies, or their use in audio generation/understanding or multimodal reasoning. Therefore, it does not meet the inclusion criteria related to discrete audio token research and instead focuses on general model compression techniques unrelated to audio token discretization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards",
    "abstract": "Large language models (LLMs) have demonstrated strong reasoning abilities in mathematical tasks, often enhanced through reinforcement learning (RL). However, RL-trained models frequently produce unnecessarily long reasoning traces -- even for simple queries -- leading to increased inference costs and latency. While recent approaches attempt to control verbosity by adding length penalties to the reward function, these methods rely on fixed penalty terms that are hard to tune and cannot adapt as the model's reasoning capability evolves, limiting their effectiveness. In this work, we propose an adaptive reward-shaping method that enables LLMs to \"think fast and right\" -- producing concise outputs without sacrificing correctness. Our method dynamically adjusts the reward trade-off between accuracy and response length based on model performance: when accuracy is high, the length penalty increases to encourage faster length reduction; when accuracy drops, the penalty is relaxed to preserve correctness. This adaptive reward accelerates early-stage length reduction while avoiding over-compression in later stages. Experiments across multiple datasets show that our approach consistently and dramatically reduces reasoning length while largely maintaining accuracy, offering a new direction for cost-efficient adaptive reasoning in large-scale language models.",
    "metadata": {
      "arxiv_id": "2505.18298",
      "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards",
      "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in mathematical tasks, often enhanced through reinforcement learning (RL). However, RL-trained models frequently produce unnecessarily long reasoning traces -- even for simple queries -- leading to increased inference costs and latency. While recent approaches attempt to control verbosity by adding length penalties to the reward function, these methods rely on fixed penalty terms that are hard to tune and cannot adapt as the model's reasoning capability evolves, limiting their effectiveness. In this work, we propose an adaptive reward-shaping method that enables LLMs to \"think fast and right\" -- producing concise outputs without sacrificing correctness. Our method dynamically adjusts the reward trade-off between accuracy and response length based on model performance: when accuracy is high, the length penalty increases to encourage faster length reduction; when accuracy drops, the penalty is relaxed to preserve correctness. This adaptive reward accelerates early-stage length reduction while avoiding over-compression in later stages. Experiments across multiple datasets show that our approach consistently and dramatically reduces reasoning length while largely maintaining accuracy, offering a new direction for cost-efficient adaptive reasoning in large-scale language models.",
      "authors": [
        "Jinyan Su",
        "Claire Cardie"
      ],
      "published": "2025-05-23T18:44:46Z",
      "updated": "2025-05-23T18:44:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18298v1",
      "landing_url": "https://arxiv.org/abs/2505.18298v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18298"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The input item's topic is about adaptive reward shaping to reduce LLM reasoning length; it does not involve discrete audio tokens, tokenization methods, or related token-based modeling, so it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The input item's topic is about adaptive reward shaping to reduce LLM reasoning length; it does not involve discrete audio tokens, tokenization methods, or related token-based modeling, so it fails the 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on improving reasoning efficiency in large language models through adaptive reward shaping, with no mention of discrete audio tokens, neural audio codecs, or audio tokenization processes. The study does not address audio discretization, token design, or audio-related downstream tasks, thus it does not meet any of the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on improving reasoning efficiency in large language models through adaptive reward shaping, with no mention of discrete audio tokens, neural audio codecs, or audio tokenization processes. The study does not address audio discretization, token design, or audio-related downstream tasks, thus it does not meet any of the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
    "metadata": {
      "arxiv_id": "2505.18864",
      "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework",
      "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.",
      "authors": [
        "Binhao Ma",
        "Hanqing Guo",
        "Zhengping Jay Luo",
        "Rui Duan"
      ],
      "published": "2025-05-24T20:46:36Z",
      "updated": "2025-05-24T20:46:36Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18864v1",
      "landing_url": "https://arxiv.org/abs/2505.18864v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18864"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work focuses on adversarial attacks exploiting speech tokenization in a white-box setting and does not present or analyze discrete audio token design, vocabularies, or token-based downstream tasks, thus it fails to meet the inclusion criteria centered on discrete audio tokens.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The work focuses on adversarial attacks exploiting speech tokenization in a white-box setting and does not present or analyze discrete audio token design, vocabularies, or token-based downstream tasks, thus it fails to meet the inclusion criteria centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on adversarial attacks on SpeechGPT by generating adversarial token sequences synthesized into audio prompts, leveraging token-level manipulation of speech input; however, the abstract does not clearly describe the generation process of discrete audio tokens, their tokenization mechanism details, or aspects like neural audio codecs or quantization defining discrete token sets. Without explicit information about the discrete tokenization process or its design, it is unclear if the token sequences align with the inclusion criteria centered on discrete audio tokens and their systemic roles in generation, understanding, or alignment. Therefore, the study seems related to speech tokenization but lacks sufficient detail to confirm it meets the inclusion criteria based on discrete audio tokens as defined.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study focuses on adversarial attacks on SpeechGPT by generating adversarial token sequences synthesized into audio prompts, leveraging token-level manipulation of speech input; however, the abstract does not clearly describe the generation process of discrete audio tokens, their tokenization mechanism details, or aspects like neural audio codecs or quantization defining discrete token sets. Without explicit information about the discrete tokenization process or its design, it is unclear if the token sequences align with the inclusion criteria centered on discrete audio tokens and their systemic roles in generation, understanding, or alignment. Therefore, the study seems related to speech tokenization but lacks sufficient detail to confirm it meets the inclusion criteria based on discrete audio tokens as defined.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
    "abstract": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
    "metadata": {
      "arxiv_id": "2505.19043",
      "title": "Offline Clustering of Linear Bandits: The Power of Clusters under Limited Data",
      "summary": "Contextual multi-armed bandit is a fundamental learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case where we continually learn from online data, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C2LUB, which we show analytically and experimentally outperforms existing methods under limited offline user data, and Off-CLUB, which may incur bias when data is sparse but performs well and nearly matches the lower bound when data is sufficient. We experimentally validate these results on both real and synthetic datasets.",
      "authors": [
        "Jingyuan Liu",
        "Zeyu Zhang",
        "Xuchuang Wang",
        "Xutong Liu",
        "John C. S. Lui",
        "Mohammad Hajiesmaili",
        "Carlee Joe-Wong"
      ],
      "published": "2025-05-25T08:43:40Z",
      "updated": "2025-10-25T08:29:46Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19043v2",
      "landing_url": "https://arxiv.org/abs/2505.19043v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19043"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns offline clustering for contextual bandits and has no relation to discrete audio tokens or tokenization; it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns offline clustering for contextual bandits and has no relation to discrete audio tokens or tokenization; it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract focus on offline clustering in linear bandits, a framework for sequential decision-making, without any mention of discrete audio tokens, neural audio codecs, quantization, or any audio-related tokenization processes; therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls outside the scope entirely.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract focus on offline clustering in linear bandits, a framework for sequential decision-making, without any mention of discrete audio tokens, neural audio codecs, quantization, or any audio-related tokenization processes; therefore, it does not meet the inclusion criteria related to discrete audio tokens and instead falls outside the scope entirely.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
    "abstract": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
    "metadata": {
      "arxiv_id": "2505.19462",
      "title": "VoiceStar: Robust Zero-Shot Autoregressive TTS with Duration Control and Extrapolation",
      "summary": "We present VoiceStar, the first zero-shot TTS model that achieves both output duration control and extrapolation. VoiceStar is an autoregressive encoder-decoder neural codec language model, that leverages a novel Progress-Monitoring Rotary Position Embedding (PM-RoPE) and is trained with Continuation-Prompt Mixed (CPM) training. PM-RoPE enables the model to better align text and speech tokens, indicates the target duration for the generated speech, and also allows the model to generate speech waveforms much longer in duration than those seen during. CPM training also helps to mitigate the training/inference mismatch, and significantly improves the quality of the generated speech in terms of speaker similarity and intelligibility. VoiceStar outperforms or is on par with current state-of-the-art models on short-form benchmarks such as Librispeech and Seed-TTS, and significantly outperforms these models on long-form/extrapolation benchmarks (20-50s) in terms of intelligibility and naturalness. Code and models: https://github.com/jasonppy/VoiceStar. Audio samples: https://jasonppy.github.io/VoiceStar_web",
      "authors": [
        "Puyuan Peng",
        "Shang-Wen Li",
        "Abdelrahman Mohamed",
        "David Harwath"
      ],
      "published": "2025-05-26T03:35:44Z",
      "updated": "2025-05-31T22:36:04Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19462v2",
      "landing_url": "https://arxiv.org/abs/2505.19462v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19462"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract focuses on a zero-shot autoregressive TTS system with duration control/extrapolation and does not describe generation or analysis based on discrete audio tokens, tokenizers, or neural codecs that produce discrete token vocabularies, thus it does not meet the 'Discrete Audio Tokens' guidance.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a zero-shot autoregressive TTS system with duration control/extrapolation and does not describe generation or analysis based on discrete audio tokens, tokenizers, or neural codecs that produce discrete token vocabularies, thus it does not meet the 'Discrete Audio Tokens' guidance.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study presents VoiceStar, a zero-shot TTS model based on an autoregressive encoder-decoder neural codec language model, which leverages neural audio codecs and aligns text with speech tokens. It focuses on generation and control of speech duration through discrete audio tokens derived via neural codec mechanisms, meeting key inclusion criteria regarding discrete token usage for audio generation and providing English abstract and details supporting reproducibility. Hence, it aligns well with the inclusion criteria and does not fall under any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study presents VoiceStar, a zero-shot TTS model based on an autoregressive encoder-decoder neural codec language model, which leverages neural audio codecs and aligns text with speech tokens. It focuses on generation and control of speech duration through discrete audio tokens derived via neural codec mechanisms, meeting key inclusion criteria regarding discrete token usage for audio generation and providing English abstract and details supporting reproducibility. Hence, it aligns well with the inclusion criteria and does not fall under any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Both junior reviewers raised valid points: JuniorNano excluded it because the abstract lacks explicit tokenizer/codebook details, while JuniorMini included it because the paper explicitly describes an autoregressive encoder–decoder neural codec language model that aligns text and speech tokens for TTS generation and duration control. My judgment: the abstract clearly states the use of a neural audio codec and speech tokens as the core representation for generation and control (matching the inclusion group that uses neural audio codecs to produce discrete tokens for downstream modeling), but it does not provide low-level tokenizer/quantization/codebook/bitrate details in the abstract to fully satisfy the English reproducibility requirement — however those details are likely present in the full paper. Therefore I recommend inclusion but not with maximal confidence.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Both junior reviewers raised valid points: JuniorNano excluded it because the abstract lacks explicit tokenizer/codebook details, while JuniorMini included it because the paper explicitly describes an autoregressive encoder–decoder neural codec language model that aligns text and speech tokens for TTS generation and duration control. My judgment: the abstract clearly states the use of a neural audio codec and speech tokens as the core representation for generation and control (matching the inclusion group that uses neural audio codecs to produce discrete tokens for downstream modeling), but it does not provide low-level tokenizer/quantization/codebook/bitrate details in the abstract to fully satisfy the English reproducibility requirement — however those details are likely present in the full paper. Therefore I recommend inclusion but not with maximal confidence.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
    "abstract": "Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.",
    "metadata": {
      "arxiv_id": "2505.19470",
      "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
      "summary": "Latent variables (LVs) play a crucial role in encoder-decoder models by enabling effective data compression, prediction, and generation. Although their theoretical properties, such as generalization, have been extensively studied in supervised learning, similar analyses for unsupervised models such as variational autoencoders (VAEs) remain insufficiently underexplored. In this work, we extend information-theoretic generalization analysis to vector-quantized (VQ) VAEs with discrete latent spaces, introducing a novel data-dependent prior to rigorously analyze the relationship among LVs, generalization, and data generation. We derive a novel generalization error bound of the reconstruction loss of VQ-VAEs, which depends solely on the complexity of LVs and the encoder, independent of the decoder. Additionally, we provide the upper bound of the 2-Wasserstein distance between the distributions of the true data and the generated data, explaining how the regularization of the LVs contributes to the data generation performance.",
      "authors": [
        "Futoshi Futami",
        "Masahiro Fujisawa"
      ],
      "published": "2025-05-26T03:51:44Z",
      "updated": "2025-11-06T07:57:00Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19470v2",
      "landing_url": "https://arxiv.org/abs/2505.19470v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19470"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper analyzes discrete latent variables produced by vector-quantized VAEs (discrete audio tokens) and studies their information-theoretic generalization, fitting the discrete-token/neural-audio-codec criteria, with an English abstract and content that supports reproducibility.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes discrete latent variables produced by vector-quantized VAEs (discrete audio tokens) and studies their information-theoretic generalization, fitting the discrete-token/neural-audio-codec criteria, with an English abstract and content that supports reproducibility.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on information-theoretic generalization analysis of vector-quantized variational autoencoders (VQ-VAEs) with discrete latent spaces, which involves vector quantization, a known neural audio codec method producing discrete latent variables. However, the abstract does not clarify whether these discrete latent variables correspond to discrete audio tokens derived from audio waveforms and used as sequences for tasks such as audio generation, alignment, or multimodal reasoning, nor does it provide details on token vocabularies, codec architectures, or how the discrete tokens are designed, evaluated, or applied to downstream audio tasks. Thus, the paper discusses a theoretical generalization analysis of VQ-VAEs rather than directly addressing discrete audio token design or application in audio generation, multimodal modeling, or related systematic evaluations required by the inclusion criteria. Accordingly, it lacks explicit evidence meeting the core requirements around discrete audio tokens for audio waveform discretization and subsequent usage in the relevant audio modeling or multimodal tasks. Given this, the study does not sufficiently meet the inclusion criteria and aligns more with exclusion criteria about lacking clear discrete audio token definitions and applications.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on information-theoretic generalization analysis of vector-quantized variational autoencoders (VQ-VAEs) with discrete latent spaces, which involves vector quantization, a known neural audio codec method producing discrete latent variables. However, the abstract does not clarify whether these discrete latent variables correspond to discrete audio tokens derived from audio waveforms and used as sequences for tasks such as audio generation, alignment, or multimodal reasoning, nor does it provide details on token vocabularies, codec architectures, or how the discrete tokens are designed, evaluated, or applied to downstream audio tasks. Thus, the paper discusses a theoretical generalization analysis of VQ-VAEs rather than directly addressing discrete audio token design or application in audio generation, multimodal modeling, or related systematic evaluations required by the inclusion criteria. Accordingly, it lacks explicit evidence meeting the core requirements around discrete audio tokens for audio waveform discretization and subsequent usage in the relevant audio modeling or multimodal tasks. Given this, the study does not sufficiently meet the inclusion criteria and aligns more with exclusion criteria about lacking clear discrete audio token definitions and applications.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": {
      "reasoning": "I reviewed both junior reviewers: JuniorNano recommended inclusion (5) likely because VQ-VAE uses discrete latents, but JuniorMini correctly flagged that the abstract is theoretical and does not specify audio data, tokenizer/codec architecture, codebook/vocabulary settings, bitrate/downsampling, or any downstream audio generation/alignment/multimodal evaluations. The paper studies information-theoretic generalization of VQ-VAEs with discrete latent variables in a general setting and provides theoretical bounds, but it does not demonstrate or describe discrete audio token generation from waveforms, codec details, or use of tokens in downstream audio or multimodal tasks—therefore it fails the inclusion criteria and meets exclusion criteria about lacking discrete audio-token definitions and applications. I score 1 because the abstract lacks the required audio-specific discrete-token details and empirical or application evidence needed for inclusion.",
      "evaluation": 1
    },
    "round-B_SeniorLead_reasoning": "I reviewed both junior reviewers: JuniorNano recommended inclusion (5) likely because VQ-VAE uses discrete latents, but JuniorMini correctly flagged that the abstract is theoretical and does not specify audio data, tokenizer/codec architecture, codebook/vocabulary settings, bitrate/downsampling, or any downstream audio generation/alignment/multimodal evaluations. The paper studies information-theoretic generalization of VQ-VAEs with discrete latent variables in a general setting and provides theoretical bounds, but it does not demonstrate or describe discrete audio token generation from waveforms, codec details, or use of tokens in downstream audio or multimodal tasks—therefore it fails the inclusion criteria and meets exclusion criteria about lacking discrete audio-token definitions and applications. I score 1 because the abstract lacks the required audio-specific discrete-token details and empirical or application evidence needed for inclusion.",
    "round-B_SeniorLead_evaluation": 1,
    "final_verdict": "exclude (senior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:1)"
  },
  {
    "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
    "abstract": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
    "metadata": {
      "arxiv_id": "2505.19595",
      "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
      "summary": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
      "authors": [
        "Jeongsoo Choi",
        "Zhikang Niu",
        "Ji-Hoon Kim",
        "Chunhui Wang",
        "Joon Son Chung",
        "Xie Chen"
      ],
      "published": "2025-05-26T07:07:16Z",
      "updated": "2025-05-30T16:52:09Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19595v2",
      "landing_url": "https://arxiv.org/abs/2505.19595v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19595"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets accelerating diffusion-based TTS training via dual modality alignment and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it fails the token/discrete-token criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets accelerating diffusion-based TTS training via dual modality alignment and does not discuss discrete audio tokens, tokenization, or token-based modeling, thus it fails the token/discrete-token criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on accelerating the training of diffusion-based text-to-speech models using dual modality alignment but does not mention the use of discrete audio tokens, tokenization, or any form of neural audio codec or vector quantization to produce discrete tokens representation of audio. The abstract lacks details on discrete token generation, vocabulary, or quantization methods essential for inclusion, and instead centers on training efficiency and model alignment, which falls outside the discrete audio token scope. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on accelerating the training of diffusion-based text-to-speech models using dual modality alignment but does not mention the use of discrete audio tokens, tokenization, or any form of neural audio codec or vector quantization to produce discrete tokens representation of audio. The abstract lacks details on discrete token generation, vocabulary, or quantization methods essential for inclusion, and instead centers on training efficiency and model alignment, which falls outside the discrete audio token scope. Therefore, it does not meet the inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
    "abstract": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
    "metadata": {
      "arxiv_id": "2505.19669",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "authors": [
        "Haiyang Sun",
        "Shujie Hu",
        "Shujie Liu",
        "Lingwei Meng",
        "Hui Wang",
        "Bing Han",
        "Yifan Yang",
        "Yanqing Liu",
        "Sheng Zhao",
        "Yan Lu",
        "Yanmin Qian"
      ],
      "published": "2025-05-26T08:25:01Z",
      "updated": "2025-06-02T10:03:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19669v2",
      "landing_url": "https://arxiv.org/abs/2505.19669v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19669"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract describes zero-shot streaming TTS using a Transducer to produce semantic tokens and an autoregressive model to generate mel-spectrograms, but it does not clearly describe discrete audio tokens produced by a neural audio codec or self-supervised tokenization, nor a focus on token design/impact for discrete audio tokens across tasks. Therefore it likely does not meet the core 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract describes zero-shot streaming TTS using a Transducer to produce semantic tokens and an autoregressive model to generate mel-spectrograms, but it does not clearly describe discrete audio tokens produced by a neural audio codec or self-supervised tokenization, nor a focus on token design/impact for discrete audio tokens across tasks. Therefore it likely does not meet the core 'Discrete Audio Tokens' inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The study addresses zero-shot streaming text-to-speech synthesis using a combination of Transducer and autoregressive modeling but does not clearly indicate the use or generation of discrete audio tokens mapping continuous audio waveforms into token sequences as defined by the inclusion criteria. The abstract mentions semantic tokens and duration alignment information but lacks detailed descriptions about tokenization processes, codebooks, quantization, or discrete token design essential for inclusion. Moreover, the focus is on TTS framework performance and latency rather than on discrete token design, analysis, or systematic evaluation. Thus, it does not sufficiently meet the criteria for discrete audio tokens research inclusion.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The study addresses zero-shot streaming text-to-speech synthesis using a combination of Transducer and autoregressive modeling but does not clearly indicate the use or generation of discrete audio tokens mapping continuous audio waveforms into token sequences as defined by the inclusion criteria. The abstract mentions semantic tokens and duration alignment information but lacks detailed descriptions about tokenization processes, codebooks, quantization, or discrete token design essential for inclusion. Moreover, the focus is on TTS framework performance and latency rather than on discrete token design, analysis, or systematic evaluation. Thus, it does not sufficiently meet the criteria for discrete audio tokens research inclusion.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
    "abstract": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
    "metadata": {
      "arxiv_id": "2505.19760",
      "title": "Navigating PESQ: Up-to-Date Versions and Open Implementations",
      "summary": "Perceptual Evaluation of Speech Quality (PESQ) is an objective quality measure that remains widely used despite its withdrawal by the International Telecommunication Union (ITU). PESQ has evolved over two decades, with multiple versions and publicly available implementations emerging during this time. Different versions and their updates can be overwhelming, especially for new PESQ users. This work provides practical guidance on the different versions and implementations of PESQ. We show that differences can be significant, especially between PESQ versions. We stress the importance of specifying the exact version and implementation that is used to compute PESQ, and possibly to detail how multi-channel signals are handled. These practices would facilitate the interpretation of results and allow comparisons of PESQ scores between different studies. We also provide a repository that implements the latest corrections to PESQ, i.e., Corrigendum 2, which is not implemented by any other openly available distribution: https://github.com/audiolabs/PESQ.",
      "authors": [
        "Matteo Torcoli",
        "Mhd Modar Halimeh",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-05-26T09:43:09Z",
      "updated": "2025-08-14T11:53:22Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19760v2",
      "landing_url": "https://arxiv.org/abs/2505.19760v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19760"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item concerns PESQ versions/implementations and does not address discrete audio tokens, tokenization, or token-based modeling, thus failing inclusion criteria and triggering exclusion criteria for discrete-token-focused reviews.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item concerns PESQ versions/implementations and does not address discrete audio tokens, tokenization, or token-based modeling, thus failing inclusion criteria and triggering exclusion criteria for discrete-token-focused reviews.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on navigating and clarifying different versions and implementations of PESQ, an objective speech quality measure, rather than on discrete audio tokens or their generation, design, or application in downstream tasks; it does not discuss any form of discretization or token sequence modeling as defined in the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on navigating and clarifying different versions and implementations of PESQ, an objective speech quality measure, rather than on discrete audio tokens or their generation, design, or application in downstream tasks; it does not discuss any form of discretization or token sequence modeling as defined in the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
    "abstract": "Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most research has focused on either causal or full-context speech encoders, there's limited exploration to effectively handle both streaming and non-streaming applications, while achieving state-of-the-art performance. We introduce DuRep, a Dual-mode Speech Representation learning setup, which enables a single speech encoder to function efficiently in both offline and online modes without additional parameters or mode-specific adjustments, across downstream tasks. DuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6% improvements in streaming and non-streaming modes, over baseline encoders on Multilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new performance benchmarks across ASR and non-ASR tasks. Our analysis reveals interesting trade-offs between acoustic and semantic information across encoder layers.",
    "metadata": {
      "arxiv_id": "2505.19774",
      "title": "DuRep: Dual-Mode Speech Representation Learning via ASR-Aware Distillation",
      "summary": "Recent advancements in speech encoders have drawn attention due to their integration with Large Language Models for various speech tasks. While most research has focused on either causal or full-context speech encoders, there's limited exploration to effectively handle both streaming and non-streaming applications, while achieving state-of-the-art performance. We introduce DuRep, a Dual-mode Speech Representation learning setup, which enables a single speech encoder to function efficiently in both offline and online modes without additional parameters or mode-specific adjustments, across downstream tasks. DuRep-200M, our 200M parameter dual-mode encoder, achieves 12% and 11.6% improvements in streaming and non-streaming modes, over baseline encoders on Multilingual ASR. Scaling this approach to 2B parameters, DuRep-2B sets new performance benchmarks across ASR and non-ASR tasks. Our analysis reveals interesting trade-offs between acoustic and semantic information across encoder layers.",
      "authors": [
        "Prabash Reddy Male",
        "Swayambhu Nath Ray",
        "Harish Arsikere",
        "Akshat Jaiswal",
        "Prakhar Swarup",
        "Prantik Sen",
        "Debmalya Chakrabarty",
        "K V Vijay Girish",
        "Nikhil Bhave",
        "Frederick Weber",
        "Sambuddha Bhattacharya",
        "Sri Garimella"
      ],
      "published": "2025-05-26T09:57:59Z",
      "updated": "2025-05-26T09:57:59Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19774v1",
      "landing_url": "https://arxiv.org/abs/2505.19774v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.19774"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper centers on dual-mode continuous speech representation learning and ASR-aware distillation without discussing discrete audio tokens, tokenizers, codebooks, or token-level analyses.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper centers on dual-mode continuous speech representation learning and ASR-aware distillation without discussing discrete audio tokens, tokenizers, codebooks, or token-level analyses.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on dual-mode speech representation learning for ASR tasks using speech encoders but does not mention discrete audio tokenization, neural audio codecs, vector quantization, or explicitly producing discrete token sequences from continuous audio waveforms. The abstract centers on continuous speech encoding improvements rather than discrete token generation or analysis, lacking detailed description of tokenizer design or discrete token vocabulary settings, which are essential inclusion criteria. Therefore, it does not meet the core inclusion criteria requiring discrete audio tokens as the main representation or method.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on dual-mode speech representation learning for ASR tasks using speech encoders but does not mention discrete audio tokenization, neural audio codecs, vector quantization, or explicitly producing discrete token sequences from continuous audio waveforms. The abstract centers on continuous speech encoding improvements rather than discrete token generation or analysis, lacking detailed description of tokenizer design or discrete token vocabulary settings, which are essential inclusion criteria. Therefore, it does not meet the core inclusion criteria requiring discrete audio tokens as the main representation or method.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
    "abstract": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
    "metadata": {
      "arxiv_id": "2505.20741",
      "title": "Uni-VERSA: Versatile Speech Assessment with a Unified Network",
      "summary": "Subjective listening tests remain the golden standard for speech quality assessment, but are costly, variable, and difficult to scale. In contrast, existing objective metrics, such as PESQ, F0 correlation, and DNSMOS, typically capture only specific aspects of speech quality. To address these limitations, we introduce Uni-VERSA, a unified network that simultaneously predicts various objective metrics, encompassing naturalness, intelligibility, speaker characteristics, prosody, and noise, for a comprehensive evaluation of speech signals. We formalize its framework, evaluation protocol, and applications in speech enhancement, synthesis, and quality control. A benchmark based on the URGENT24 challenge, along with a baseline leveraging self-supervised representations, demonstrates that Uni-VERSA provides a viable alternative to single-aspect evaluation methods. Moreover, it aligns closely with human perception, making it a promising approach for future speech quality assessment.",
      "authors": [
        "Jiatong Shi",
        "Hye-Jin Shim",
        "Shinji Watanabe"
      ],
      "published": "2025-05-27T05:31:19Z",
      "updated": "2025-05-27T05:31:19Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.20741v1",
      "landing_url": "https://arxiv.org/abs/2505.20741v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.20741"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "Abstract describes a unified network for predicting multiple speech quality metrics; it does not discuss discrete audio tokens or tokenization, so it does not meet the token-discrete criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a unified network for predicting multiple speech quality metrics; it does not discuss discrete audio tokens or tokenization, so it does not meet the token-discrete criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study introduces Uni-VERSA, a unified network for objective speech assessment metrics, but does not discuss discrete audio tokens, their production, tokenization processes, or usage as core representations for sequence modeling or generation; thus, it does not meet the inclusion criteria focused on discrete token-based audio representation and modeling.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study introduces Uni-VERSA, a unified network for objective speech assessment metrics, but does not discuss discrete audio tokens, their production, tokenization processes, or usage as core representations for sequence modeling or generation; thus, it does not meet the inclusion criteria focused on discrete token-based audio representation and modeling.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
    "abstract": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions. We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
    "metadata": {
      "arxiv_id": "2505.21194",
      "title": "Vectorized Sequence-Based Chunking for Data Deduplication",
      "summary": "Data deduplication has gained wide acclaim as a mechanism to improve storage efficiency and conserve network bandwidth. Its most critical phase, data chunking, is responsible for the overall space savings achieved via the deduplication process. However, modern data chunking algorithms are slow and compute-intensive because they scan large amounts of data while simultaneously making data-driven boundary decisions.\n  We present SeqCDC, a novel chunking algorithm that leverages lightweight boundary detection, content-defined skipping, and SSE/AVX acceleration to improve chunking throughput for large chunk sizes. Our evaluation shows that SeqCDC achieves 15x higher throughput than unaccelerated and 1.2x-1.35x higher throughput than vector-accelerated data chunking algorithms while minimally affecting deduplication space savings.",
      "authors": [
        "Sreeharsha Udayashankar",
        "Samer Al-Kiswany"
      ],
      "published": "2025-05-27T13:42:33Z",
      "updated": "2025-05-27T13:42:33Z",
      "categories": [
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.21194v1",
      "landing_url": "https://arxiv.org/abs/2505.21194v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.21194"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item targets vectorized data chunking for deduplication and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item targets vectorized data chunking for deduplication and does not discuss discrete audio tokens, tokenizers, or token-based modeling, thus it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on a data chunking algorithm for data deduplication to improve storage efficiency; it does not address discrete audio tokens, audio tokenization, or related themes such as neural audio codecs or token design for audio generation or understanding, thus not meeting the inclusion criteria focused on discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on a data chunking algorithm for data deduplication to improve storage efficiency; it does not address discrete audio tokens, audio tokenization, or related themes such as neural audio codecs or token design for audio generation or understanding, thus not meeting the inclusion criteria focused on discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
    "abstract": "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.",
    "metadata": {
      "arxiv_id": "2505.24291",
      "title": "Discl-VC: Disentangled Discrete Tokens and In-Context Learning for Controllable Zero-Shot Voice Conversion",
      "summary": "Currently, zero-shot voice conversion systems are capable of synthesizing the voice of unseen speakers. However, most existing approaches struggle to accurately replicate the speaking style of the source speaker or mimic the distinctive speaking style of the target speaker, thereby limiting the controllability of voice conversion. In this work, we propose Discl-VC, a novel voice conversion framework that disentangles content and prosody information from self-supervised speech representations and synthesizes the target speaker's voice through in-context learning with a flow matching transformer. To enable precise control over the prosody of generated speech, we introduce a mask generative transformer that predicts discrete prosody tokens in a non-autoregressive manner based on prompts. Experimental results demonstrate the superior performance of Discl-VC in zero-shot voice conversion and its remarkable accuracy in prosody control for synthesized speech.",
      "authors": [
        "Kaidi Wang",
        "Wenhao Guan",
        "Ziyue Jiang",
        "Hukai Huang",
        "Peijie Chen",
        "Weijie Wu",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:04:23Z",
      "updated": "2025-05-30T07:04:23Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24291v1",
      "landing_url": "https://arxiv.org/abs/2505.24291v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24291"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly centers on using discrete tokens (disentangled content and discrete prosody tokens) and a transformer-based model to predict them for controllable zero-shot voice conversion, aligning with the discrete audio tokens topic and token-based generation/analysis.",
      "evaluation": 4
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly centers on using discrete tokens (disentangled content and discrete prosody tokens) and a transformer-based model to predict them for controllable zero-shot voice conversion, aligning with the discrete audio tokens topic and token-based generation/analysis.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": {
      "reasoning": "The abstract mentions the use of self-supervised speech representations and proposes a mask generative transformer to predict discrete prosody tokens for voice conversion, indicating involvement of discrete tokens derived from learned representations; however, the details on discrete token generation, tokenizer architecture, or vocabulary/codebook settings are not sufficiently described, making it unclear whether the study focuses on discrete audio tokens as core representations or provides enough reproducible technical details as required by the inclusion criteria.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The abstract mentions the use of self-supervised speech representations and proposes a mask generative transformer to predict discrete prosody tokens for voice conversion, indicating involvement of discrete tokens derived from learned representations; however, the details on discrete token generation, tokenizer architecture, or vocabulary/codebook settings are not sufficiently described, making it unclear whether the study focuses on discrete audio tokens as core representations or provides enough reproducible technical details as required by the inclusion criteria.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "The two junior reviewers converge that the paper is relevant (mentions disentangled discrete tokens and a mask generative transformer predicting discrete prosody tokens), but they differ on confidence because the abstract does not describe how discrete tokens are produced (no tokenizer/codec/quantization/codebook/rate details) nor whether discrete token sequences are the core representation beyond prosody—information required by the inclusion criteria; therefore I cannot confidently confirm it meets all inclusion requirements. Brief explanation (1 sentence): I rate this a 3 (Not sure) because the abstract indicates use of \"discrete prosody tokens\" and token-based modeling but lacks the explicit tokenizer/codec and codebook/quantization details required to deterministically include it under the Discrete Audio Tokens topic.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "The two junior reviewers converge that the paper is relevant (mentions disentangled discrete tokens and a mask generative transformer predicting discrete prosody tokens), but they differ on confidence because the abstract does not describe how discrete tokens are produced (no tokenizer/codec/quantization/codebook/rate details) nor whether discrete token sequences are the core representation beyond prosody—information required by the inclusion criteria; therefore I cannot confidently confirm it meets all inclusion requirements. Brief explanation (1 sentence): I rate this a 3 (Not sure) because the abstract indicates use of \"discrete prosody tokens\" and token-based modeling but lacks the explicit tokenizer/codec and codebook/quantization details required to deterministically include it under the Discrete Audio Tokens topic.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
    "abstract": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
    "metadata": {
      "arxiv_id": "2505.24314",
      "title": "DS-Codec: Dual-Stage Training with Mirror-to-NonMirror Architecture Switching for Speech Codec",
      "summary": "Neural speech codecs are essential for advancing text-to-speech (TTS) systems. With the recent success of large language models in text generation, developing high-quality speech tokenizers has become increasingly important. This paper introduces DS-Codec, a novel neural speech codec featuring a dual-stage training framework with mirror and non-mirror architectures switching, designed to achieve superior speech reconstruction. We conduct extensive experiments and ablation studies to evaluate the effectiveness of our training strategy and compare the performance of the two architectures. Our results show that the mirrored structure significantly enhances the robustness of the learned codebooks, and the training strategy balances the advantages between mirrored and non-mirrored structures, leading to improved high-fidelity speech reconstruction.",
      "authors": [
        "Peijie Chen",
        "Wenhao Guan",
        "Kaidi Wang",
        "Weijie Wu",
        "Hukai Huang",
        "Qingyang Hong",
        "Lin Li"
      ],
      "published": "2025-05-30T07:53:01Z",
      "updated": "2025-05-30T07:53:01Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24314v1",
      "landing_url": "https://arxiv.org/abs/2505.24314v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24314"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item discusses a neural speech codec with codebooks, implying discretized representations, but the abstract does not explicitly define or evaluate discrete audio tokens, tokenization process, or downstream tasks; thus it does not clearly satisfy the discrete-token inclusion criteria.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The item discusses a neural speech codec with codebooks, implying discretized representations, but the abstract does not explicitly define or evaluate discrete audio tokens, tokenization process, or downstream tasks; thus it does not clearly satisfy the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper presents DS-Codec, a neural speech codec with dual-stage training targeting high-fidelity speech reconstruction, which involves a neural audio codec producing tokenized representations that can be used in TTS systems. It discusses mirror and non-mirror architectures and examines robustness of learned codebooks, indicating design details about discrete token generation via neural codec and vector quantization, satisfying the inclusion criteria regarding discrete audio tokens and their applications in speech generation; the abstract is in English with sufficient technical detail for evaluation. Therefore, it meets the inclusion criteria and does not fall into any exclusion criteria.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper presents DS-Codec, a neural speech codec with dual-stage training targeting high-fidelity speech reconstruction, which involves a neural audio codec producing tokenized representations that can be used in TTS systems. It discusses mirror and non-mirror architectures and examines robustness of learned codebooks, indicating design details about discrete token generation via neural codec and vector quantization, satisfying the inclusion criteria regarding discrete audio tokens and their applications in speech generation; the abstract is in English with sufficient technical detail for evaluation. Therefore, it meets the inclusion criteria and does not fall into any exclusion criteria.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "JuniorNano: recommends exclusion (score 2) because, while the abstract mentions codebooks, it does not explicitly define discrete audio tokens, tokenization process, or provide quantization/vocabulary details. JuniorMini: recommends inclusion (score 5), interpreting ‘codebooks’ and ‘neural speech codec’ as implying vector-quantized discrete tokens used for TTS tokenization. My assessment: the abstract clearly describes a neural speech codec and learned codebooks and situates work in TTS/tokenizer context, which suggests discrete-token/vector-quantization methods; however, it lacks explicit, reproducible details about how discrete tokens are generated (quantization method, codebook/vocab settings, time downsampling or bitrate) and does not state that the tokens are used as sequence-model inputs for downstream generation/LLM-style modeling. Because inclusion requires clear evidence of discrete-token generation and sufficient methodological detail to verify that the work centers on discrete audio tokens, the item is ambiguous from the abstract alone and cannot be confidently included or excluded without the full paper. Score explanation: Mentions codebooks (suggesting discretization) but omits explicit token-generation and configuration details required by the inclusion criteria, so the decision is uncertain.",
      "evaluation": 3
    },
    "round-B_SeniorLead_reasoning": "JuniorNano: recommends exclusion (score 2) because, while the abstract mentions codebooks, it does not explicitly define discrete audio tokens, tokenization process, or provide quantization/vocabulary details. JuniorMini: recommends inclusion (score 5), interpreting ‘codebooks’ and ‘neural speech codec’ as implying vector-quantized discrete tokens used for TTS tokenization. My assessment: the abstract clearly describes a neural speech codec and learned codebooks and situates work in TTS/tokenizer context, which suggests discrete-token/vector-quantization methods; however, it lacks explicit, reproducible details about how discrete tokens are generated (quantization method, codebook/vocab settings, time downsampling or bitrate) and does not state that the tokens are used as sequence-model inputs for downstream generation/LLM-style modeling. Because inclusion requires clear evidence of discrete-token generation and sufficient methodological detail to verify that the work centers on discrete audio tokens, the item is ambiguous from the abstract alone and cannot be confidently included or excluded without the full paper. Score explanation: Mentions codebooks (suggesting discretization) but omits explicit token-generation and configuration details required by the inclusion criteria, so the decision is uncertain.",
    "round-B_SeniorLead_evaluation": 3,
    "final_verdict": "maybe (senior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
    "abstract": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on discrete audio tokens produced by neural audio codecs and uses those tokens as the primary input for speech generation via language modeling, discussing tokenization, compression of long-range context, and downstream LM evaluation, which aligns with the ‘Discrete Audio Tokens’ topic and meets the inclusion criteria without falling into exclusion cases.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The item focuses on discrete audio tokens produced by neural audio codecs and uses those tokens as the primary input for speech generation via language modeling, discussing tokenization, compression of long-range context, and downstream LM evaluation, which aligns with the ‘Discrete Audio Tokens’ topic and meets the inclusion criteria without falling into exclusion cases.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study clearly focuses on neural audio codecs that discretize continuous speech into token sequences, addressing token sequence modeling challenges by proposing a compressed-to-fine language modeling approach using these discrete tokens, which fits the inclusion criteria regarding neural codec-based discrete audio tokens, their design, and application in speech generation tasks.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study clearly focuses on neural audio codecs that discretize continuous speech into token sequences, addressing token sequence modeling challenges by proposing a compressed-to-fine language modeling approach using these discrete tokens, which fits the inclusion criteria regarding neural codec-based discrete audio tokens, their design, and application in speech generation tasks.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
    "abstract": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
    "metadata": {
      "arxiv_id": "2505.24717",
      "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
      "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
      "authors": [
        "Benjamin Holzschuh",
        "Qiang Liu",
        "Georg Kohl",
        "Nils Thuerey"
      ],
      "published": "2025-05-30T15:39:54Z",
      "updated": "2025-05-30T15:39:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24717v1",
      "landing_url": "https://arxiv.org/abs/2505.24717v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24717"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "此項目聚焦於物理模擬的 Transformer 架構與預訓練模型，未涉及離散音訊 token 的產生、編碼或以離散 token 作為核心任務，因此不符合離散音訊 token 的定義與評估範圍。",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "此項目聚焦於物理模擬的 Transformer 架構與預訓練模型，未涉及離散音訊 token 的產生、編碼或以離散 token 作為核心任務，因此不符合離散音訊 token 的定義與評估範圍。",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on transformer architectures for physics simulations and does not involve discretization of continuous audio waveforms into discrete audio tokens, neural audio codecs, or any related audio tokenization processes. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is unrelated to audio modeling or generation tasks.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on transformer architectures for physics simulations and does not involve discretization of continuous audio waveforms into discrete audio tokens, neural audio codecs, or any related audio tokenization processes. Therefore, it does not meet any of the inclusion criteria related to discrete audio tokens and is unrelated to audio modeling or generation tasks.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
    "abstract": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
    "metadata": {
      "arxiv_id": "2506.00809",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2025-06-01T03:23:27Z",
      "updated": "2025-06-01T03:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00809v1",
      "landing_url": "https://arxiv.org/abs/2506.00809v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00809"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract explicitly uses discrete acoustic tokens derived from a neural audio codec and trains with a token-based (masked language modeling) objective, directly aligning with the discrete audio tokens topic and downstream modeling goals.",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The abstract explicitly uses discrete acoustic tokens derived from a neural audio codec and trains with a token-based (masked language modeling) objective, directly aligning with the discrete audio tokens topic and downstream modeling goals.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on a multi-stage speech enhancement system utilizing a Sparse Compression Network and a neural audio codec-derived acoustic tokens for refinement through a generative model, aligning with the use of neural audio codec tokens; however, the abstract does not explicitly clarify the discrete token design details such as quantization, vocabulary settings, or tokenizer training necessary to confirm for inclusion criteria. Given the thematic relevance but lack of detailed tokenization process in the abstract, inclusion is uncertain.",
      "evaluation": 3
    },
    "round-A_JuniorMini_reasoning": "The study focuses on a multi-stage speech enhancement system utilizing a Sparse Compression Network and a neural audio codec-derived acoustic tokens for refinement through a generative model, aligning with the use of neural audio codec tokens; however, the abstract does not explicitly clarify the discrete token design details such as quantization, vocabulary settings, or tokenizer training necessary to confirm for inclusion criteria. Given the thematic relevance but lack of detailed tokenization process in the abstract, inclusion is uncertain.",
    "round-A_JuniorMini_evaluation": 3,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers: Reviewer A rated 5 noting the abstract explicitly uses discrete acoustic tokens from a neural audio codec and an MLM objective; Reviewer B rated 3 noting the abstract lacks explicit tokenizer/quantization/codebook details. The abstract does explicitly state use of acoustic tokens derived from a neural audio codec and a masked-language-modeling objective—this directly matches the core ‘‘discrete audio tokens’’ scope (token-as-core generative modeling) and satisfies the token-source/group criterion—however the abstract does not provide the detailed tokenizer/codec/quantization/codebook settings required by the reproducibility/detail clause, so full confirmation from the abstract alone is not possible. I therefore give a cautious positive inclusion recommendation: 4 — better to include: the work clearly centers on neural-codec-derived discrete tokens and token-based generative modeling (in-scope), but the abstract lacks method-level tokenization specifics needed for absolute certainty.",
      "evaluation": 4
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers: Reviewer A rated 5 noting the abstract explicitly uses discrete acoustic tokens from a neural audio codec and an MLM objective; Reviewer B rated 3 noting the abstract lacks explicit tokenizer/quantization/codebook details. The abstract does explicitly state use of acoustic tokens derived from a neural audio codec and a masked-language-modeling objective—this directly matches the core ‘‘discrete audio tokens’’ scope (token-as-core generative modeling) and satisfies the token-source/group criterion—however the abstract does not provide the detailed tokenizer/codec/quantization/codebook settings required by the reproducibility/detail clause, so full confirmation from the abstract alone is not possible. I therefore give a cautious positive inclusion recommendation: 4 — better to include: the work clearly centers on neural-codec-derived discrete tokens and token-based generative modeling (in-scope), but the abstract lacks method-level tokenization specifics needed for absolute certainty.",
    "round-B_SeniorLead_evaluation": 4,
    "final_verdict": "include (senior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
    "abstract": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The title/abstract clearly describe generating discrete audio tokens via hierarchical codebooks for semantic and acoustic residuals, a tokenizer-like discretization produced by a neural codec/self-supervised model, with analyses of downstream ASR and reconstruction—matching the Inclusion criteria about discrete token design, tokenizer/codec details, and applications to speech generation/understanding.\n",
      "evaluation": 5
    },
    "round-A_JuniorNano_reasoning": "The title/abstract clearly describe generating discrete audio tokens via hierarchical codebooks for semantic and acoustic residuals, a tokenizer-like discretization produced by a neural codec/self-supervised model, with analyses of downstream ASR and reconstruction—matching the Inclusion criteria about discrete token design, tokenizer/codec details, and applications to speech generation/understanding.\n",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": {
      "reasoning": "The study proposes HASRD, a hierarchical method that disentangles speech representations into discrete semantic and acoustic tokens using self-supervised learning and multi-codebook quantization, satisfying the requirement for discrete audio tokens derived from neural audio codec-like architectures. It provides detailed design including token assignment across codebooks, quantification of improvements in reconstruction quality, ASR performance, and bitrate reduction, meeting the inclusion criteria of token production mechanisms, design evaluation, and offering systematic analysis suitable for the discrete audio token thematic scope.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The study proposes HASRD, a hierarchical method that disentangles speech representations into discrete semantic and acoustic tokens using self-supervised learning and multi-codebook quantization, satisfying the requirement for discrete audio tokens derived from neural audio codec-like architectures. It provides detailed design including token assignment across codebooks, quantification of improvements in reconstruction quality, ASR performance, and bitrate reduction, meeting the inclusion criteria of token production mechanisms, design evaluation, and offering systematic analysis suitable for the discrete audio token thematic scope.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Zero-Shot Text-to-Speech for Vietnamese",
    "abstract": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
    "metadata": {
      "arxiv_id": "2506.01322",
      "title": "Zero-Shot Text-to-Speech for Vietnamese",
      "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
      "authors": [
        "Thi Vu",
        "Linh The Nguyen",
        "Dat Quoc Nguyen"
      ],
      "published": "2025-06-02T05:07:06Z",
      "updated": "2025-06-02T05:07:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01322v1",
      "landing_url": "https://arxiv.org/abs/2506.01322v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01322"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The work centers on dataset release and zero-shot TTS performance rather than discrete audio token representations or tokenizer-based modeling, thus it fails to meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The work centers on dataset release and zero-shot TTS performance rather than discrete audio token representations or tokenizer-based modeling, thus it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper introduces a dataset for Vietnamese zero-shot TTS and reports on the performance of some TTS models, but it does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or related tokenization mechanisms essential to the inclusion criteria. The abstract lacks any description of discrete token generation or token-based modeling details required to evaluate it on the topic of \"Discrete Audio Tokens,\" and thus it probably focuses on continuous audio representations or end-to-end TTS without discrete audio tokenization.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper introduces a dataset for Vietnamese zero-shot TTS and reports on the performance of some TTS models, but it does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or related tokenization mechanisms essential to the inclusion criteria. The abstract lacks any description of discrete token generation or token-based modeling details required to evaluate it on the topic of \"Discrete Audio Tokens,\" and thus it probably focuses on continuous audio representations or end-to-end TTS without discrete audio tokenization.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion",
    "abstract": "We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations. Samples and code: https://www.kamperh.com/linearvc/.",
    "metadata": {
      "arxiv_id": "2506.01510",
      "title": "LinearVC: Linear transformations of self-supervised features through the lens of voice conversion",
      "summary": "We introduce LinearVC, a simple voice conversion method that sheds light on the structure of self-supervised representations. First, we show that simple linear transformations of self-supervised features effectively convert voices. Next, we probe the geometry of the feature space by constraining the set of allowed transformations. We find that just rotating the features is sufficient for high-quality voice conversion. This suggests that content information is embedded in a low-dimensional subspace which can be linearly transformed to produce a target voice. To validate this hypothesis, we finally propose a method that explicitly factorizes content and speaker information using singular value decomposition; the resulting linear projection with a rank of just 100 gives competitive conversion results. Our work has implications for both practical voice conversion and a broader understanding of self-supervised speech representations. Samples and code: https://www.kamperh.com/linearvc/.",
      "authors": [
        "Herman Kamper",
        "Benjamin van Niekerk",
        "Julian Zaïdi",
        "Marc-André Carbonneau"
      ],
      "published": "2025-06-02T10:18:02Z",
      "updated": "2025-06-02T10:18:02Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01510v1",
      "landing_url": "https://arxiv.org/abs/2506.01510v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01510"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item studies linear transformations of self-supervised speech representations for voice conversion and does not discuss discrete audio tokens, tokenization, or any discretized codebooks. It fails the core inclusion criterion of discrete audio tokens and does not address token design, evaluation, or downstream tasks involving discrete tokens. Therefore it should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item studies linear transformations of self-supervised speech representations for voice conversion and does not discuss discrete audio tokens, tokenization, or any discretized codebooks. It fails the core inclusion criterion of discrete audio tokens and does not address token design, evaluation, or downstream tasks involving discrete tokens. Therefore it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on linear transformations of self-supervised speech features for voice conversion, but it does not describe discretization of continuous audio waveforms into discrete token sequences or the use/design of discrete audio tokens as defined. There is no mention of neural audio codecs, vector quantization, or discrete token vocabulary, thus it does not meet the inclusion criteria regarding discrete audio token generation or use.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on linear transformations of self-supervised speech features for voice conversion, but it does not describe discretization of continuous audio waveforms into discrete token sequences or the use/design of discrete audio tokens as defined. There is no mention of neural audio codecs, vector quantization, or discrete token vocabulary, thus it does not meet the inclusion criteria regarding discrete audio token generation or use.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
    "abstract": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
    "metadata": {
      "arxiv_id": "2506.01731",
      "title": "Benchmarking Neural Speech Codec Intelligibility with SITool",
      "summary": "Speech intelligibility assessment is essential for evaluating neural speech codecs, yet most evaluation efforts focus on overall quality rather than intelligibility. Only a few publicly available tools exist for conducting standardized intelligibility tests, like the Diagnostic Rhyme Test (DRT) and Modified Rhyme Test (MRT). We introduce the Speech Intelligibility Toolkit for Subjective Evaluation (SITool), a Flask-based web application for conducting DRT and MRT in laboratory and crowdsourcing settings. We use SITool to benchmark 13 neural and traditional speech codecs, analyzing phoneme-level degradations and comparing subjective DRT results with objective intelligibility metrics. Our findings show that, while neural speech codecs can outperform traditional ones in subjective intelligibility, only STOI and ESTOI - not WER - significantly correlate with subjective results, although they struggle to capture gender and wordlist-specific variations observed in subjective evaluations.",
      "authors": [
        "Anna Leschanowsky",
        "Kishor Kayyar Lakshminarayana",
        "Anjana Rajasekhar",
        "Lyonel Behringer",
        "Ibrahim Kilinc",
        "Guillaume Fuchs",
        "Emanuël A. P. Habets"
      ],
      "published": "2025-06-02T14:42:50Z",
      "updated": "2025-06-02T14:42:50Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01731v1",
      "landing_url": "https://arxiv.org/abs/2506.01731v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01731"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item evaluates neural speech codec intelligibility using SITool and standard tests, but it does not discuss discrete audio tokens, tokenizers, codebooks, or token-based modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item evaluates neural speech codec intelligibility using SITool and standard tests, but it does not discuss discrete audio tokens, tokenizers, codebooks, or token-based modeling, thus it does not meet the inclusion criteria for Discrete Audio Tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper benchmarks neural and traditional speech codecs focusing on speech intelligibility, using subjective and objective metrics, but does not clearly describe discretization into token sequences or provide specifics about discrete token generation (e.g., tokenizer/codec architecture, quantization, codebooks), nor does it emphasize use of discrete audio tokens for downstream generative or modeling tasks as required by the inclusion criteria. Therefore, it does not align with the core theme of \"Discrete Audio Tokens\" as defined, focusing instead on codec intelligibility evaluation rather than token design or application.",
      "evaluation": 2
    },
    "round-A_JuniorMini_reasoning": "The paper benchmarks neural and traditional speech codecs focusing on speech intelligibility, using subjective and objective metrics, but does not clearly describe discretization into token sequences or provide specifics about discrete token generation (e.g., tokenizer/codec architecture, quantization, codebooks), nor does it emphasize use of discrete audio tokens for downstream generative or modeling tasks as required by the inclusion criteria. Therefore, it does not align with the core theme of \"Discrete Audio Tokens\" as defined, focusing instead on codec intelligibility evaluation rather than token design or application.",
    "round-A_JuniorMini_evaluation": 2,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
    "abstract": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
    "metadata": {
      "arxiv_id": "2506.02863",
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "summary": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dading Chong",
        "Karan Thakkar",
        "Tiantian Feng",
        "Dongchao Yang",
        "Junhyeok Lee",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Zengyi Qin",
        "Shrikanth Narayanan",
        "Mounya Elhiali",
        "Najim Dehak"
      ],
      "published": "2025-06-03T13:28:55Z",
      "updated": "2025-09-26T13:07:51Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02863v2",
      "landing_url": "https://arxiv.org/abs/2506.02863v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.02863"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item focuses on CapSpeech benchmark datasets for CapTTS tasks and does not discuss discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item focuses on CapSpeech benchmark datasets for CapTTS tasks and does not discuss discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on style-captioned text-to-speech synthesis and introduces benchmark datasets and tasks related to CapTTS; however, it does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization processes to convert continuous audio waveforms into token sequences. Furthermore, there is no clear description of tokenizer architecture, vocabulary/codebook settings, or quantization methods essential for discrete audio token representation. Therefore, it does not fulfill the inclusion criteria related to discrete audio tokens as core representations for modeling or generation.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on style-captioned text-to-speech synthesis and introduces benchmark datasets and tasks related to CapTTS; however, it does not mention the use or design of discrete audio tokens, neural audio codecs, vector quantization, or any discretization processes to convert continuous audio waveforms into token sequences. Furthermore, there is no clear description of tokenizer architecture, vocabulary/codebook settings, or quantization methods essential for discrete audio token representation. Therefore, it does not fulfill the inclusion criteria related to discrete audio tokens as core representations for modeling or generation.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
    "abstract": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
    "metadata": {
      "arxiv_id": "2506.04134",
      "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
      "summary": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
      "authors": [
        "Jinting Wang",
        "Shan Yang",
        "Chenxing Li",
        "Dong Yu",
        "Li Liu"
      ],
      "published": "2025-06-04T16:26:49Z",
      "updated": "2025-11-11T03:49:45Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04134v4",
      "landing_url": "https://arxiv.org/abs/2506.04134v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.04134"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper targets end-to-end CS video-to-speech generation without discussing discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the discrete audio token criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper targets end-to-end CS video-to-speech generation without discussing discrete audio tokens, tokenizers, or neural audio codecs, so it does not meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on direct speech generation from Chinese Cued Speech videos without relying on text as an intermediate, but the abstract does not mention the use or generation of discrete audio tokens derived from neural audio codecs or self-supervised learning models, nor does it detail any tokenization or quantization processes relevant to discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete token representations and their explicit design, evaluation, or use in audio generation or understanding.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on direct speech generation from Chinese Cued Speech videos without relying on text as an intermediate, but the abstract does not mention the use or generation of discrete audio tokens derived from neural audio codecs or self-supervised learning models, nor does it detail any tokenization or quantization processes relevant to discrete audio tokens; thus, it does not meet the inclusion criteria centered on discrete token representations and their explicit design, evaluation, or use in audio generation or understanding.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Kernel $k$-Medoids as General Vector Quantization",
    "abstract": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
    "metadata": {
      "arxiv_id": "2506.04786",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "authors": [
        "Thore Gerlach",
        "Sascha Mücke",
        "Christian Bauckhage"
      ],
      "published": "2025-06-05T09:14:25Z",
      "updated": "2025-09-04T20:11:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04786v2",
      "landing_url": "https://arxiv.org/abs/2506.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04786"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This paper is about theoretical vector quantization methods (k-medoids, KDE) and QUBO, with no focus on discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This paper is about theoretical vector quantization methods (k-medoids, KDE) and QUBO, with no focus on discrete audio tokens or audio tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on kernel k-medoids and vector quantization techniques in a machine learning context, but it does not address discrete audio tokens, neural audio codecs, or any audio-based tokenization process. It lacks any discussion of discretizing continuous audio waveforms into token sequences for audio generation, understanding, or cross-modal reasoning, which are core scopes of the inclusion criteria. Therefore, it does not meet the inclusion criteria for studies on discrete audio tokens and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on kernel k-medoids and vector quantization techniques in a machine learning context, but it does not address discrete audio tokens, neural audio codecs, or any audio-based tokenization process. It lacks any discussion of discretizing continuous audio waveforms into token sequences for audio generation, understanding, or cross-modal reasoning, which are core scopes of the inclusion criteria. Therefore, it does not meet the inclusion criteria for studies on discrete audio tokens and should be excluded.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
    "abstract": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
    "metadata": {
      "arxiv_id": "2506.05432",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "authors": [
        "Yuxuan Yue",
        "Zukang Xu",
        "Zhihang Yuan",
        "Dawei Yang",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "published": "2025-06-05T08:58:58Z",
      "updated": "2025-06-26T06:17:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
      "landing_url": "https://arxiv.org/abs/2506.05432v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05432"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The article focuses on vector quantization for large language model weights (PCDVQ) rather than discrete audio tokens or neural audio codecs, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The article focuses on vector quantization for large language model weights (PCDVQ) rather than discrete audio tokens or neural audio codecs, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on vector quantization techniques for compressing large language models, specifically addressing quantization of weight vectors, without involving audio waveform discretization or discrete audio tokens. It does not discuss generating or modeling discrete audio tokens from continuous audio signals nor related audio-specific applications or evaluations, thus it does not meet the inclusion criteria about discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on vector quantization techniques for compressing large language models, specifically addressing quantization of weight vectors, without involving audio waveform discretization or discrete audio tokens. It does not discuss generating or modeling discrete audio tokens from continuous audio signals nor related audio-specific applications or evaluations, thus it does not meet the inclusion criteria about discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Modified K-means Algorithm with Local Optimality Guarantees",
    "abstract": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
    "metadata": {
      "arxiv_id": "2506.06990",
      "title": "Modified K-means Algorithm with Local Optimality Guarantees",
      "summary": "The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.",
      "authors": [
        "Mingyi Li",
        "Michael R. Metel",
        "Akiko Takeda"
      ],
      "published": "2025-06-08T04:37:28Z",
      "updated": "2025-06-11T06:52:53Z",
      "categories": [
        "cs.LG",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06990v2",
      "landing_url": "https://arxiv.org/abs/2506.06990v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.06990"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The paper focuses on K-means clustering and local optima; it does not discuss discrete audio tokens, tokenization, or token-based audio modeling, so it does not meet the inclusion criteria.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on K-means clustering and local optima; it does not discuss discrete audio tokens, tokenization, or token-based audio modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The title and abstract describe a modification of the K-means clustering algorithm focusing on local optimality guarantees in machine learning, without any mention of discretizing audio waveforms into tokens, neural audio codecs, vector quantization, or applications related to audio generation, understanding, or multimodal modeling. Therefore, the study does not meet the inclusion criteria and falls under the exclusion criterion of focusing on a general machine learning clustering method rather than discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The title and abstract describe a modification of the K-means clustering algorithm focusing on local optimality guarantees in machine learning, without any mention of discretizing audio waveforms into tokens, neural audio codecs, vector quantization, or applications related to audio generation, understanding, or multimodal modeling. Therefore, the study does not meet the inclusion criteria and falls under the exclusion criterion of focusing on a general machine learning clustering method rather than discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
    "abstract": "A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses.",
    "metadata": {
      "arxiv_id": "2506.08686",
      "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths",
      "summary": "A significant portion of the energy consumed by Large Language Models (LLMs) arises from their inference processes; hence developing energy-efficient methods for inference is crucial. While several techniques exist for inference optimization, output compression remains relatively unexplored, with only a few preliminary efforts addressing this aspect. In this work, we first benchmark 12 decoder-only LLMs across 5 datasets, revealing that these models often produce responses that are substantially longer than necessary. We then conduct a comprehensive quality assessment of LLM responses, formally defining six information categories present in LLM responses. We show that LLMs often tend to include redundant or additional information besides the minimal answer. To address this issue of long responses by LLMs, we explore several simple and intuitive prompt-engineering strategies. Empirical evaluation shows that appropriate prompts targeting length reduction and controlling information content can achieve significant energy optimization between 25-60\\% by reducing the response length while preserving the quality of LLM responses.",
      "authors": [
        "Soham Poddar",
        "Paramita Koley",
        "Janardan Misra",
        "Sanjay Podder",
        "Navveen Balani",
        "Niloy Ganguly",
        "Saptarshi Ghosh"
      ],
      "published": "2025-06-10T10:52:04Z",
      "updated": "2025-06-29T08:48:15Z",
      "categories": [
        "cs.CL",
        "cs.CY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.08686v2",
      "landing_url": "https://arxiv.org/abs/2506.08686v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.08686"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This item deals with reducing LLM response lengths to save energy and does not discuss discrete audio tokens or token design, so it does not meet the inclusion criteria and should be excluded.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This item deals with reducing LLM response lengths to save energy and does not discuss discrete audio tokens or token design, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The study focuses on Large Language Models' response length and energy efficiency, with no mention of discrete audio tokens, tokenization of audio data, or discretization processes related to audio waveforms; hence, it does not meet the defined inclusion criteria related to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The study focuses on Large Language Models' response length and energy efficiency, with no mention of discrete audio tokens, tokenization of audio data, or discretization processes related to audio waveforms; hence, it does not meet the defined inclusion criteria related to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
    "abstract": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The abstract discusses a parallel speech-text model with dual-resolution representations but does not specify discrete audio tokens, tokenizers, or neural codecs, so it does not fulfill the 'Discrete Audio Tokens' focus and should be excluded.",
      "evaluation": 2
    },
    "round-A_JuniorNano_reasoning": "The abstract discusses a parallel speech-text model with dual-resolution representations but does not specify discrete audio tokens, tokenizers, or neural codecs, so it does not fulfill the 'Discrete Audio Tokens' focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper discusses a speech-text voice conversation model using dual-resolution speech representations, focusing on autoregressive modeling involving discrete speech tokens that are aligned with text tokens handled by LLMs. It clearly involves discretization of continuous speech signals into discrete tokens (speech tokens), employs neural model-based tokenization, discusses token design for computational efficiency, and applies these tokens in downstream generation tasks, fulfilling the inclusion criteria related to token origin, design, and usage. The abstract is in English and provides sufficient details to verify the use and generation of discrete tokens. Hence, it meets the inclusion criteria and none of the exclusion criteria apply.",
      "evaluation": 5
    },
    "round-A_JuniorMini_reasoning": "The paper discusses a speech-text voice conversation model using dual-resolution speech representations, focusing on autoregressive modeling involving discrete speech tokens that are aligned with text tokens handled by LLMs. It clearly involves discretization of continuous speech signals into discrete tokens (speech tokens), employs neural model-based tokenization, discusses token design for computational efficiency, and applies these tokens in downstream generation tasks, fulfilling the inclusion criteria related to token origin, design, and usage. The abstract is in English and provides sufficient details to verify the use and generation of discrete tokens. Hence, it meets the inclusion criteria and none of the exclusion criteria apply.",
    "round-A_JuniorMini_evaluation": 5,
    "round-B_SeniorLead_output": {
      "reasoning": "Junior reviewers disagreed (one excluded due to missing discrete-token details; one included interpreting 'discrete speech tokens' as sufficient). My assessment: although the abstract repeatedly mentions 'discrete speech tokens' and dual-resolution speech representations, it fails to describe how those tokens are produced (no tokenizer/codec architecture, quantization, vocab/codebook, bitrate or explicit discretization pipeline), so it does not meet the inclusion requirement to clearly document discrete audio token generation and settings—therefore better to exclude. Score explanation (1-sentence): I assign 2 because the abstract lacks the necessary, explicit methodological details to confirm it uses and defines discrete audio tokens per the inclusion criteria.",
      "evaluation": 2
    },
    "round-B_SeniorLead_reasoning": "Junior reviewers disagreed (one excluded due to missing discrete-token details; one included interpreting 'discrete speech tokens' as sufficient). My assessment: although the abstract repeatedly mentions 'discrete speech tokens' and dual-resolution speech representations, it fails to describe how those tokens are produced (no tokenizer/codec architecture, quantization, vocab/codebook, bitrate or explicit discretization pipeline), so it does not meet the inclusion requirement to clearly document discrete audio token generation and settings—therefore better to exclude. Score explanation (1-sentence): I assign 2 because the abstract lacks the necessary, explicit methodological details to confirm it uses and defines discrete audio tokens per the inclusion criteria.",
    "round-B_SeniorLead_evaluation": 2,
    "final_verdict": "exclude (senior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (senior:2)"
  },
  {
    "title": "A Study on Speech Assessment with Visual Cues",
    "abstract": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
    "metadata": {
      "arxiv_id": "2506.09549",
      "title": "A Study on Speech Assessment with Visual Cues",
      "summary": "Non-intrusive assessment of speech quality and intelligibility is essential when clean reference signals are unavailable. In this work, we propose a multimodal framework that integrates audio features and visual cues to predict PESQ and STOI scores. It employs a dual-branch architecture, where spectral features are extracted using STFT, and visual embeddings are obtained via a visual encoder. These features are then fused and processed by a CNN-BLSTM with attention, followed by multi-task learning to simultaneously predict PESQ and STOI. Evaluations on the LRS3-TED dataset, augmented with noise from the DEMAND corpus, show that our model outperforms the audio-only baseline. Under seen noise conditions, it improves LCC by 9.61% (0.8397->0.9205) for PESQ and 11.47% (0.7403->0.8253) for STOI. These results highlight the effectiveness of incorporating visual cues in enhancing the accuracy of non-intrusive speech assessment.",
      "authors": [
        "Shafique Ahmed",
        "Ryandhimas E. Zezario",
        "Nasir Saleem",
        "Amir Hussain",
        "Hsin-Min Wang",
        "Yu Tsao"
      ],
      "published": "2025-06-11T09:32:12Z",
      "updated": "2025-06-11T09:32:12Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09549v1",
      "landing_url": "https://arxiv.org/abs/2506.09549v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09549"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "This study focuses on multimodal speech quality assessment using audio and visual cues to predict PESQ/STOI; it does not introduce or analyze discrete audio tokens, tokenizers, or token-based modeling.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "This study focuses on multimodal speech quality assessment using audio and visual cues to predict PESQ/STOI; it does not introduce or analyze discrete audio tokens, tokenizers, or token-based modeling.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper focuses on multimodal speech quality assessment using audio and visual features to predict PESQ and STOI scores, but it does not describe or employ discrete audio tokens, neural audio codecs, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria; thus, it does not meet the core topic requirements.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper focuses on multimodal speech quality assessment using audio and visual features to predict PESQ and STOI scores, but it does not describe or employ discrete audio tokens, neural audio codecs, or any form of discretization mapping continuous audio into token sequences as required by the inclusion criteria; thus, it does not meet the core topic requirements.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Training-Free Voice Conversion with Factorized Optimal Transport",
    "abstract": "This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline. In contrast with original pipeline, our algorithm performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio. MKL-VC replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution. Factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets show MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. MKL-VC achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain.",
    "metadata": {
      "arxiv_id": "2506.09709",
      "title": "Training-Free Voice Conversion with Factorized Optimal Transport",
      "summary": "This paper introduces Factorized MKL-VC, a training-free modification for kNN-VC pipeline. In contrast with original pipeline, our algorithm performs high quality any-to-any cross-lingual voice conversion with only 5 second of reference audio. MKL-VC replaces kNN regression with a factorized optimal transport map in WavLM embedding subspaces, derived from Monge-Kantorovich Linear solution. Factorization addresses non-uniform variance across dimensions, ensuring effective feature transformation. Experiments on LibriSpeech and FLEURS datasets show MKL-VC significantly improves content preservation and robustness with short reference audio, outperforming kNN-VC. MKL-VC achieves performance comparable to FACodec, especially in cross-lingual voice conversion domain.",
      "authors": [
        "Alexander Lobashev",
        "Assel Yermekova",
        "Maria Larchenko"
      ],
      "published": "2025-06-11T13:23:03Z",
      "updated": "2025-06-11T13:23:03Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09709v1",
      "landing_url": "https://arxiv.org/abs/2506.09709v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.09709"
    },
    "round-A_JuniorNano_output": {
      "reasoning": "The item does not discuss discrete audio tokens or tokenization; it focuses on training-free voice conversion using factorized optimal transport, which is outside the discrete audio tokens topic and its required tokenization/codec mechanisms.",
      "evaluation": 1
    },
    "round-A_JuniorNano_reasoning": "The item does not discuss discrete audio tokens or tokenization; it focuses on training-free voice conversion using factorized optimal transport, which is outside the discrete audio tokens topic and its required tokenization/codec mechanisms.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "The paper describes a training-free method for voice conversion using factorized optimal transport but does not mention any discretization of audio into tokens or the use of discrete audio tokens as defined in the inclusion criteria. It focuses on continuous representation transformations rather than discrete token generation, and lacks details on tokenizer or quantization schemes relevant to discrete audio tokens.",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "The paper describes a training-free method for voice conversion using factorized optimal transport but does not mention any discretization of audio into tokens or the use of discrete audio tokens as defined in the inclusion criteria. It focuses on continuous representation transformations rather than discrete token generation, and lacks details on tokenizer or quantization schemes relevant to discrete audio tokens.",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  }
]