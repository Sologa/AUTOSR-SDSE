[
  {
    "title": "Towards audio language modeling -- an overview",
    "abstract": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.",
    "metadata": {
      "arxiv_id": "2402.13236",
      "title": "Towards audio language modeling -- an overview",
      "summary": "Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kai-wei Chang",
        "Ho-Lam Chung",
        "Alexander H. Liu",
        "Hung-yi Lee"
      ],
      "published": "2024-02-20T18:50:25Z",
      "updated": "2024-02-20T18:50:25Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13236v1",
      "landing_url": "https://arxiv.org/abs/2402.13236v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.13236"
    },
    "final_verdict": "include (seed_filter)",
    "review_skipped": true,
    "discard_reason": null,
    "force_include_reason": "seed_filter_selected"
  },
  {
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "abstract": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
    "metadata": {
      "arxiv_id": "2502.06490",
      "title": "Recent Advances in Discrete Speech Tokens: A Review",
      "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Hankun Wang",
        "Bohan Li",
        "Chongtian Shao",
        "Hanglei Zhang",
        "Chenpeng Du",
        "Xie Chen",
        "Shujie Liu",
        "Kai Yu"
      ],
      "published": "2025-02-10T14:08:25Z",
      "updated": "2025-12-12T05:18:11Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.MM",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.06490v4",
      "landing_url": "https://arxiv.org/abs/2502.06490v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.06490"
    },
    "final_verdict": "include (seed_filter)",
    "review_skipped": true,
    "discard_reason": null,
    "force_include_reason": "seed_filter_selected"
  },
  {
    "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure",
    "abstract": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
    "metadata": {
      "arxiv_id": "2101.00387",
      "title": "What all do audio transformer models hear? Probing Acoustic Representations for Language Delivery and its Structure",
      "summary": "In recent times, BERT based transformer models have become an inseparable part of the 'tech stack' of text processing models. Similar progress is being observed in the speech domain with a multitude of models observing state-of-the-art results by using audio transformer models to encode speech. This begs the question of what are these audio transformer models learning. Moreover, although the standard methodology is to choose the last layer embedding for any downstream task, but is it the optimal choice? We try to answer these questions for the two recent audio transformer models, Mockingjay and wave2vec2.0. We compare them on a comprehensive set of language delivery and structure features including audio, fluency and pronunciation features. Additionally, we probe the audio models' understanding of textual surface, syntax, and semantic features and compare them to BERT. We do this over exhaustive settings for native, non-native, synthetic, read and spontaneous speech datasets",
      "authors": [
        "Jui Shah",
        "Yaman Kumar Singla",
        "Changyou Chen",
        "Rajiv Ratn Shah"
      ],
      "published": "2021-01-02T06:29:12Z",
      "updated": "2021-07-12T22:46:37Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2101.00387v2",
      "landing_url": "https://arxiv.org/abs/2101.00387v2",
      "doi": "https://doi.org/10.48550/arXiv.2101.00387"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on probing audio transformer representations (Mockingjay/wave2vec2) for language features, with no mention of discrete-token quantizers/codebooks or token-level modeling, so it fails to meet the discrete audio token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on probing audio transformer representations (Mockingjay/wave2vec2) for language features, with no mention of discrete-token quantizers/codebooks or token-level modeling, so it fails to meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
    "abstract": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
    "metadata": {
      "arxiv_id": "2206.07860",
      "title": "EPG2S: Speech Generation and Speech Enhancement based on Electropalatography and Audio Signals using Multimodal Learning",
      "summary": "Speech generation and enhancement based on articulatory movements facilitate communication when the scope of verbal communication is absent, e.g., in patients who have lost the ability to speak. Although various techniques have been proposed to this end, electropalatography (EPG), which is a monitoring technique that records contact between the tongue and hard palate during speech, has not been adequately explored. Herein, we propose a novel multimodal EPG-to-speech (EPG2S) system that utilizes EPG and speech signals for speech generation and enhancement. Different fusion strategies based on multiple combinations of EPG and noisy speech signals are examined, and the viability of the proposed method is investigated. Experimental results indicate that EPG2S achieves desirable speech generation outcomes based solely on EPG signals. Further, the addition of noisy speech signals is observed to improve quality and intelligibility. Additionally, EPG2S is observed to achieve high-quality speech enhancement based solely on audio signals, with the addition of EPG signals further improving the performance. The late fusion strategy is deemed to be the most effective approach for simultaneous speech generation and enhancement.",
      "authors": [
        "Li-Chin Chen",
        "Po-Hsun Chen",
        "Richard Tzong-Han Tsai",
        "Yu Tsao"
      ],
      "published": "2022-06-16T00:33:20Z",
      "updated": "2022-06-16T00:33:20Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.07860v1",
      "landing_url": "https://arxiv.org/abs/2206.07860v1",
      "doi": "https://doi.org/10.1109/LSP.2022.3184636"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on multimodal speech generation/enhancement using EPG and audio signals without addressing discrete audio tokenization, quantization, or finite vocabulary representations, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on multimodal speech generation/enhancement using EPG and audio signals without addressing discrete audio tokenization, quantization, or finite vocabulary representations, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Raycasting of Volumetric Depth Images for Remote Visualization of Large Volumes at High Frame Rates",
    "abstract": "We present an efficient raycasting algorithm for rendering Volumetric Depth Images (VDIs), and we show how it can be used in a remote visualization setting with VDIs generated and streamed from a remote server. VDIs are compact view-dependent volume representations that enable interactive visualization of large volumes at high frame rates by decoupling viewpoint changes from expensive rendering calculations. However, current rendering approaches for VDIs struggle with achieving interactive frame rates at high image resolutions. Here, we exploit the properties of perspective projection to simplify intersections of rays with the view-dependent frustums in a VDI and leverage spatial smoothness in the volume data to minimize memory accesses. Benchmarks show that responsive frame rates can be achieved close to the viewpoint of generation for HD display resolutions, providing high-fidelity approximate renderings of Gigabyte-sized volumes. We also propose a method to subsample the VDI for preview rendering, maintaining high frame rates even for large viewpoint deviations. We provide our implementation as an extension of an established open-source visualization library.",
    "metadata": {
      "arxiv_id": "2206.08660",
      "title": "Efficient Raycasting of Volumetric Depth Images for Remote Visualization of Large Volumes at High Frame Rates",
      "summary": "We present an efficient raycasting algorithm for rendering Volumetric Depth Images (VDIs), and we show how it can be used in a remote visualization setting with VDIs generated and streamed from a remote server. VDIs are compact view-dependent volume representations that enable interactive visualization of large volumes at high frame rates by decoupling viewpoint changes from expensive rendering calculations. However, current rendering approaches for VDIs struggle with achieving interactive frame rates at high image resolutions. Here, we exploit the properties of perspective projection to simplify intersections of rays with the view-dependent frustums in a VDI and leverage spatial smoothness in the volume data to minimize memory accesses. Benchmarks show that responsive frame rates can be achieved close to the viewpoint of generation for HD display resolutions, providing high-fidelity approximate renderings of Gigabyte-sized volumes. We also propose a method to subsample the VDI for preview rendering, maintaining high frame rates even for large viewpoint deviations. We provide our implementation as an extension of an established open-source visualization library.",
      "authors": [
        "Aryaman Gupta",
        "Ulrik Günther",
        "Pietro Incardona",
        "Guido Reina",
        "Steffen Frey",
        "Stefan Gumhold",
        "Ivo F. Sbalzarini"
      ],
      "published": "2022-06-17T09:45:22Z",
      "updated": "2023-07-27T09:53:30Z",
      "categories": [
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.08660v3",
      "landing_url": "https://arxiv.org/abs/2206.08660v3",
      "doi": "https://doi.org/10.1109/PacificVis56936.2023.00014"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe volumetric raycasting visualization, with no mention of discrete audio tokenization or quantized audio modeling, so it fails to meet the audio-oriented inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe volumetric raycasting visualization, with no mention of discrete audio tokenization or quantized audio modeling, so it fails to meet the audio-oriented inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Misspelling Semantics In Thai",
    "abstract": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
    "metadata": {
      "arxiv_id": "2206.09680",
      "title": "Misspelling Semantics In Thai",
      "summary": "User-generated content is full of misspellings. Rather than being just random noise, we hypothesise that many misspellings contain hidden semantics that can be leveraged for language understanding tasks. This paper presents a fine-grained annotated corpus of misspelling in Thai, together with an analysis of misspelling intention and its possible semantics to get a better understanding of the misspelling patterns observed in the corpus. In addition, we introduce two approaches to incorporate the semantics of misspelling: Misspelling Average Embedding (MAE) and Misspelling Semantic Tokens (MST). Experiments on a sentiment analysis task confirm our overall hypothesis: additional semantics from misspelling can boost the micro F1 score up to 0.4-2%, while blindly normalising misspelling is harmful and suboptimal.",
      "authors": [
        "Pakawat Nakwijit",
        "Matthew Purver"
      ],
      "published": "2022-06-20T09:42:50Z",
      "updated": "2022-06-20T09:42:50Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.09680v1",
      "landing_url": "https://arxiv.org/abs/2206.09680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.09680"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: Study focuses on Thai text misspellings rather than discrete audio tokenization and so fails to meet any inclusion criteria while falling under exclusion; score:1."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: Study focuses on Thai text misspellings rather than discrete audio tokenization and so fails to meet any inclusion criteria while falling under exclusion; score:1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vicinity Vision Transformer",
    "abstract": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
    "metadata": {
      "arxiv_id": "2206.10552",
      "title": "Vicinity Vision Transformer",
      "summary": "Vision transformers have shown great success on numerous computer vision tasks. However, its central component, softmax attention, prohibits vision transformers from scaling up to high-resolution images, due to both the computational complexity and memory footprint being quadratic. Although linear attention was introduced in natural language processing (NLP) tasks to mitigate a similar issue, directly applying existing linear attention to vision transformers may not lead to satisfactory results. We investigate this problem and find that computer vision tasks focus more on local information compared with NLP tasks. Based on this observation, we present a Vicinity Attention that introduces a locality bias to vision transformers with linear complexity. Specifically, for each image patch, we adjust its attention weight based on its 2D Manhattan distance measured by its neighbouring patches. In this case, the neighbouring patches will receive stronger attention than far-away patches. Moreover, since our Vicinity Attention requires the token length to be much larger than the feature dimension to show its efficiency advantages, we further propose a new Vicinity Vision Transformer (VVT) structure to reduce the feature dimension without degenerating the accuracy. We perform extensive experiments on the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectiveness of our method. Our method has a slower growth rate of GFlops than previous transformer-based and convolution-based networks when the input resolution increases. In particular, our approach achieves state-of-the-art image classification accuracy with 50% fewer parameters than previous methods.",
      "authors": [
        "Weixuan Sun",
        "Zhen Qin",
        "Hui Deng",
        "Jianyuan Wang",
        "Yi Zhang",
        "Kaihao Zhang",
        "Nick Barnes",
        "Stan Birchfield",
        "Lingpeng Kong",
        "Yiran Zhong"
      ],
      "published": "2022-06-21T17:33:53Z",
      "updated": "2023-07-20T08:57:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.10552v2",
      "landing_url": "https://arxiv.org/abs/2206.10552v2",
      "doi": "https://doi.org/10.48550/arXiv.2206.10552"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The title/abstract describe vision transformers and vicinity attention for high-resolution images, which does not involve discrete audio tokens or tokenizer/quantization design, so it clearly falls outside the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The title/abstract describe vision transformers and vicinity attention for high-resolution images, which does not involve discrete audio tokens or tokenizer/quantization design, so it clearly falls outside the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
    "abstract": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
    "metadata": {
      "arxiv_id": "2206.13680",
      "title": "Attention-based conditioning methods using variable frame rate for style-robust speaker verification",
      "summary": "We propose an approach to extract speaker embeddings that are robust to speaking style variations in text-independent speaker verification. Typically, speaker embedding extraction includes training a DNN for speaker classification and using the bottleneck features as speaker representations. Such a network has a pooling layer to transform frame-level to utterance-level features by calculating statistics over all utterance frames, with equal weighting. However, self-attentive embeddings perform weighted pooling such that the weights correspond to the importance of the frames in a speaker classification task. Entropy can capture acoustic variability due to speaking style variations. Hence, an entropy-based variable frame rate vector is proposed as an external conditioning vector for the self-attention layer to provide the network with information that can address style effects. This work explores five different approaches to conditioning. The best conditioning approach, concatenation with gating, provided statistically significant improvements over the x-vector baseline in 12/23 tasks and was the same as the baseline in 11/23 tasks when using the UCLA speaker variability database. It also significantly outperformed self-attention without conditioning in 9/23 tasks and was worse in 1/23. The method also showed significant improvements in multi-speaker scenarios of SITW.",
      "authors": [
        "Amber Afshan",
        "Abeer Alwan"
      ],
      "published": "2022-06-28T01:14:09Z",
      "updated": "2022-06-28T01:14:09Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.13680v1",
      "landing_url": "https://arxiv.org/abs/2206.13680v1",
      "doi": "https://doi.org/10.48550/arXiv.2206.13680"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work targets speaker verification with self-attention pooling and entropy-based conditioning on continuous embeddings, not research on discrete audio tokenization/quantization, so it fails the inclusion focus."
    },
    "round-A_JuniorNano_reasoning": "The work targets speaker verification with self-attention pooling and entropy-based conditioning on continuous embeddings, not research on discrete audio tokenization/quantization, so it fails the inclusion focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Parallel Compositing of Volumetric Depth Images for Interactive Visualization of Distributed Volumes at High Frame Rates",
    "abstract": "We present a parallel compositing algorithm for Volumetric Depth Images (VDIs) of large three-dimensional volume data. Large distributed volume data are routinely produced in both numerical simulations and experiments, yet it remains challenging to visualize them at smooth, interactive frame rates. VDIs are view-dependent piecewise constant representations of volume data that offer a potential solution. They are more compact and less expensive to render than the original data. So far, however, there is no method for generating VDIs from distributed data. We propose an algorithm that enables this by sort-last parallel generation and compositing of VDIs with automatically chosen content-adaptive parameters. The resulting composited VDI can then be streamed for remote display, providing responsive visualization of large, distributed volume data.",
    "metadata": {
      "arxiv_id": "2206.14503",
      "title": "Parallel Compositing of Volumetric Depth Images for Interactive Visualization of Distributed Volumes at High Frame Rates",
      "summary": "We present a parallel compositing algorithm for Volumetric Depth Images (VDIs) of large three-dimensional volume data. Large distributed volume data are routinely produced in both numerical simulations and experiments, yet it remains challenging to visualize them at smooth, interactive frame rates. VDIs are view-dependent piecewise constant representations of volume data that offer a potential solution. They are more compact and less expensive to render than the original data. So far, however, there is no method for generating VDIs from distributed data. We propose an algorithm that enables this by sort-last parallel generation and compositing of VDIs with automatically chosen content-adaptive parameters. The resulting composited VDI can then be streamed for remote display, providing responsive visualization of large, distributed volume data.",
      "authors": [
        "Aryaman Gupta",
        "Pietro Incardona",
        "Anton Brock",
        "Guido Reina",
        "Steffen Frey",
        "Stefan Gumhold",
        "Ulrik Günther",
        "Ivo F. Sbalzarini"
      ],
      "published": "2022-06-29T09:52:41Z",
      "updated": "2024-07-31T11:40:16Z",
      "categories": [
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2206.14503v3",
      "landing_url": "https://arxiv.org/abs/2206.14503v3",
      "doi": "https://doi.org/10.2312/pgv.20231082"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Not related to discrete audio token generation/value; focuses on volume visualization rather than quantized audio representations, so it fails inclusion and meets exclusion."
    },
    "round-A_JuniorNano_reasoning": "Not related to discrete audio token generation/value; focuses on volume visualization rather than quantized audio representations, so it fails inclusion and meets exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Error-Resilient Neural Speech Coding",
    "abstract": "Neural audio coding has shown very promising results recently in the literature to largely outperform traditional codecs but limited attention has been paid on its error resilience. Neural codecs trained considering only source coding tend to be extremely sensitive to channel noises, especially in wireless channels with high error rate. In this paper, we investigate how to elevate the error resilience of neural audio codecs for packet losses that often occur during real-time communications. We propose a feature-domain packet loss concealment algorithm (FD-PLC) for real-time neural speech coding. Specifically, we introduce a self-attention-based module on the received latent features to recover lost frames in the feature domain before the decoder. A hybrid segment-level and frame-level frequency-domain discriminator is employed to guide the network to focus on both the generative quality of lost frames and the continuity with neighbouring frames. Experimental results on several error patterns show that the proposed scheme can achieve better robustness compared with the corresponding error-free and error-resilient baselines. We also show that feature-domain concealment is superior to waveform-domain counterpart as post-processing.",
    "metadata": {
      "arxiv_id": "2207.00993",
      "title": "Towards Error-Resilient Neural Speech Coding",
      "summary": "Neural audio coding has shown very promising results recently in the literature to largely outperform traditional codecs but limited attention has been paid on its error resilience. Neural codecs trained considering only source coding tend to be extremely sensitive to channel noises, especially in wireless channels with high error rate. In this paper, we investigate how to elevate the error resilience of neural audio codecs for packet losses that often occur during real-time communications. We propose a feature-domain packet loss concealment algorithm (FD-PLC) for real-time neural speech coding. Specifically, we introduce a self-attention-based module on the received latent features to recover lost frames in the feature domain before the decoder. A hybrid segment-level and frame-level frequency-domain discriminator is employed to guide the network to focus on both the generative quality of lost frames and the continuity with neighbouring frames. Experimental results on several error patterns show that the proposed scheme can achieve better robustness compared with the corresponding error-free and error-resilient baselines. We also show that feature-domain concealment is superior to waveform-domain counterpart as post-processing.",
      "authors": [
        "Huaying Xue",
        "Xiulian Peng",
        "Xue Jiang",
        "Yan Lu"
      ],
      "published": "2022-07-03T09:38:30Z",
      "updated": "2022-07-03T09:38:30Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.00993v1",
      "landing_url": "https://arxiv.org/abs/2207.00993v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.00993"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on improving error resilience of neural speech codecs via latent feature concealment and adversarial losses, without discussing any discrete-token/codebook/tokenizer design or quantized-vocabulary modeling, so it falls outside the discrete audio token scope."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on improving error resilience of neural speech codecs via latent feature concealment and adversarial losses, without discussing any discrete-token/codebook/tokenizer design or quantized-vocabulary modeling, so it falls outside the discrete audio token scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
    "abstract": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
    "metadata": {
      "arxiv_id": "2207.01893",
      "title": "ASR-Generated Text for Language Model Pre-training Applied to Speech Tasks",
      "summary": "We aim at improving spoken language modeling (LM) using very large amount of automatically transcribed speech. We leverage the INA (French National Audiovisual Institute) collection and obtain 19GB of text after applying ASR on 350,000 hours of diverse TV shows. From this, spoken language models are trained either by fine-tuning an existing LM (FlauBERT) or through training a LM from scratch. New models (FlauBERT-Oral) are shared with the community and evaluated for 3 downstream tasks: spoken language understanding, classification of TV shows and speech syntactic parsing. Results show that FlauBERT-Oral can be beneficial compared to its initial FlauBERT version demonstrating that, despite its inherent noisy nature, ASR-generated text can be used to build spoken language models.",
      "authors": [
        "Valentin Pelloin",
        "Franck Dary",
        "Nicolas Herve",
        "Benoit Favre",
        "Nathalie Camelin",
        "Antoine Laurent",
        "Laurent Besacier"
      ],
      "published": "2022-07-05T08:47:51Z",
      "updated": "2022-07-05T08:47:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.01893v1",
      "landing_url": "https://arxiv.org/abs/2207.01893v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.01893"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on training spoken language models from ASR-generated text without proposing or analyzing discrete audio tokens or quantized vocabularies, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on training spoken language models from ASR-generated text without proposing or analyzing discrete audio tokens or quantized vocabularies, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
    "abstract": "Speech coding facilitates the transmission of speech over low-bandwidth networks with minimal distortion. Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches. While this new generation of codecs is capable of synthesizing high-fidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently. We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias. As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder. Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of $600\\,\\mathrm{bps}$ that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate. Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.",
    "metadata": {
      "arxiv_id": "2207.02262",
      "title": "Ultra-Low-Bitrate Speech Coding with Pretrained Transformers",
      "summary": "Speech coding facilitates the transmission of speech over low-bandwidth networks with minimal distortion. Neural-network based speech codecs have recently demonstrated significant improvements in quality over traditional approaches. While this new generation of codecs is capable of synthesizing high-fidelity speech, their use of recurrent or convolutional layers often restricts their effective receptive fields, which prevents them from compressing speech efficiently. We propose to further reduce the bitrate of neural speech codecs through the use of pretrained Transformers, capable of exploiting long-range dependencies in the input signal due to their inductive bias. As such, we use a pretrained Transformer in tandem with a convolutional encoder, which is trained end-to-end with a quantizer and a generative adversarial net decoder. Our numerical experiments show that supplementing the convolutional encoder of a neural speech codec with Transformer speech embeddings yields a speech codec with a bitrate of $600\\,\\mathrm{bps}$ that outperforms the original neural speech codec in synthesized speech quality when trained at the same bitrate. Subjective human evaluations suggest that the quality of the resulting codec is comparable or better than that of conventional codecs operating at three to four times the rate.",
      "authors": [
        "Ali Siahkoohi",
        "Michael Chinen",
        "Tom Denton",
        "W. Bastiaan Kleijn",
        "Jan Skoglund"
      ],
      "published": "2022-07-05T18:52:11Z",
      "updated": "2022-07-05T18:52:11Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.02262v1",
      "landing_url": "https://arxiv.org/abs/2207.02262v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.02262"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper describes a quantized neural speech codec that produces low-bitrate discrete codes and reports subjective quality results, so it meets the discrete audio token focus and codec evaluation criteria, making it better to include."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a quantized neural speech codec that produces low-bitrate discrete codes and reports subjective quality results, so it meets the discrete audio token focus and codec evaluation criteria, making it better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
    "abstract": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
    "metadata": {
      "arxiv_id": "2207.03067",
      "title": "Cross-Scale Vector Quantization for Scalable Neural Speech Coding",
      "summary": "Bitrate scalability is a desirable feature for audio coding in real-time communications. Existing neural audio codecs usually enforce a specific bitrate during training, so different models need to be trained for each target bitrate, which increases the memory footprint at the sender and the receiver side and transcoding is often needed to support multiple receivers. In this paper, we introduce a cross-scale scalable vector quantization scheme (CSVQ), in which multi-scale features are encoded progressively with stepwise feature fusion and refinement. In this way, a coarse-level signal is reconstructed if only a portion of the bitstream is received, and progressively improves the quality as more bits are available. The proposed CSVQ scheme can be flexibly applied to any neural audio coding network with a mirrored auto-encoder structure to achieve bitrate scalability. Subjective results show that the proposed scheme outperforms the classical residual VQ (RVQ) with scalability. Moreover, the proposed CSVQ at 3 kbps outperforms Opus at 9 kbps and Lyra at 3kbps and it could provide a graceful quality boost with bitrate increase.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Huaying Xue",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2022-07-07T03:23:25Z",
      "updated": "2022-07-07T03:23:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03067v1",
      "landing_url": "https://arxiv.org/abs/2207.03067v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03067"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Neural codec paper introduces CSVQ to produce bitrate-scalable discrete codes with vector quantization and reports subjective comparisons, so it satisfies the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Neural codec paper introduces CSVQ to produce bitrate-scalable discrete codes with vector quantization and reports subjective comparisons, so it satisfies the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Exploring the sequence length bottleneck in the Transformer for Image Captioning",
    "abstract": "Most recent state of the art architectures rely on combinations and variations of three approaches: convolutional, recurrent and self-attentive methods. Our work attempts in laying the basis for a new research direction for sequence modeling based upon the idea of modifying the sequence length. In order to do that, we propose a new method called \"Expansion Mechanism\" which transforms either dynamically or statically the input sequence into a new one featuring a different sequence length. Furthermore, we introduce a novel architecture that exploits such method and achieves competitive performances on the MS-COCO 2014 data set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the ensemble and single model configuration respectively and 130 CIDEr-D in the official online evaluation server, despite being neither recurrent nor fully attentive. At the same time we address the efficiency aspect in our design and introduce a convenient training strategy suitable for most computational resources in contrast to the standard one. Source code is available at https://github.com/jchenghu/exploring",
    "metadata": {
      "arxiv_id": "2207.03327",
      "title": "Exploring the sequence length bottleneck in the Transformer for Image Captioning",
      "summary": "Most recent state of the art architectures rely on combinations and variations of three approaches: convolutional, recurrent and self-attentive methods. Our work attempts in laying the basis for a new research direction for sequence modeling based upon the idea of modifying the sequence length. In order to do that, we propose a new method called \"Expansion Mechanism\" which transforms either dynamically or statically the input sequence into a new one featuring a different sequence length. Furthermore, we introduce a novel architecture that exploits such method and achieves competitive performances on the MS-COCO 2014 data set, yielding 134.6 and 131.4 CIDEr-D on the Karpathy test split in the ensemble and single model configuration respectively and 130 CIDEr-D in the official online evaluation server, despite being neither recurrent nor fully attentive. At the same time we address the efficiency aspect in our design and introduce a convenient training strategy suitable for most computational resources in contrast to the standard one. Source code is available at https://github.com/jchenghu/exploring",
      "authors": [
        "Jia Cheng Hu",
        "Roberto Cavicchioli",
        "Alessandro Capotondi"
      ],
      "published": "2022-07-07T14:37:02Z",
      "updated": "2022-12-24T10:25:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03327v5",
      "landing_url": "https://arxiv.org/abs/2207.03327v5",
      "doi": "https://doi.org/10.48550/arXiv.2207.03327"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe image captioning and sequence length expansion without any discussion of discrete audio tokens, tokenizers, codecs, or quantization, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe image captioning and sequence length expansion without any discussion of discrete audio tokens, tokenizers, codecs, or quantization, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "End-to-End Binaural Speech Synthesis",
    "abstract": "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb. The network is a modified vector-quantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss. We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study. Results show that the proposed approach matches the ground truth data more closely than previous methods. In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene.",
    "metadata": {
      "arxiv_id": "2207.03697",
      "title": "End-to-End Binaural Speech Synthesis",
      "summary": "In this work, we present an end-to-end binaural speech synthesis system that combines a low-bitrate audio codec with a powerful binaural decoder that is capable of accurate speech binauralization while faithfully reconstructing environmental factors like ambient noise or reverb. The network is a modified vector-quantized variational autoencoder, trained with several carefully designed objectives, including an adversarial loss. We evaluate the proposed system on an internal binaural dataset with objective metrics and a perceptual study. Results show that the proposed approach matches the ground truth data more closely than previous methods. In particular, we demonstrate the capability of the adversarial loss in capturing environment effects needed to create an authentic auditory scene.",
      "authors": [
        "Wen Chin Huang",
        "Dejan Markovic",
        "Alexander Richard",
        "Israel Dejene Gebru",
        "Anjali Menon"
      ],
      "published": "2022-07-08T05:18:36Z",
      "updated": "2022-07-08T05:18:36Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.03697v1",
      "landing_url": "https://arxiv.org/abs/2207.03697v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.03697"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "Although the system uses a vector-quantized codec suggesting discrete latent codes, the abstract focuses on binauralization quality rather than explicitly defining or evaluating the discrete tokenization/codec design, so I’m unsure if it meets the strict discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Although the system uses a vector-quantized codec suggesting discrete latent codes, the abstract focuses on binauralization quality rather than explicitly defining or evaluating the discrete tokenization/codec design, so I’m unsure if it meets the strict discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Seasonal Encoder-Decoder Architecture for Forecasting",
    "abstract": "Deep learning (DL) in general and Recurrent neural networks (RNNs) in particular have seen high success levels in sequence based applications. This paper pertains to RNNs for time series modelling and forecasting. We propose a novel RNN architecture capturing (stochastic) seasonal correlations intelligently while capable of accurate multi-step forecasting. It is motivated from the well-known encoder-decoder (ED) architecture and multiplicative seasonal auto-regressive model. It incorporates multi-step (multi-target) learning even in the presence (or absence) of exogenous inputs. It can be employed on single or multiple sequence data. For the multiple sequence case, we also propose a novel greedy recursive procedure to build (one or more) predictive models across sequences when per-sequence data is less. We demonstrate via extensive experiments the utility of our proposed architecture both in single sequence and multiple sequence scenarios.",
    "metadata": {
      "arxiv_id": "2207.04113",
      "title": "Seasonal Encoder-Decoder Architecture for Forecasting",
      "summary": "Deep learning (DL) in general and Recurrent neural networks (RNNs) in particular have seen high success levels in sequence based applications. This paper pertains to RNNs for time series modelling and forecasting. We propose a novel RNN architecture capturing (stochastic) seasonal correlations intelligently while capable of accurate multi-step forecasting. It is motivated from the well-known encoder-decoder (ED) architecture and multiplicative seasonal auto-regressive model. It incorporates multi-step (multi-target) learning even in the presence (or absence) of exogenous inputs. It can be employed on single or multiple sequence data. For the multiple sequence case, we also propose a novel greedy recursive procedure to build (one or more) predictive models across sequences when per-sequence data is less. We demonstrate via extensive experiments the utility of our proposed architecture both in single sequence and multiple sequence scenarios.",
      "authors": [
        "Avinash Achar",
        "Soumen Pachal"
      ],
      "published": "2022-07-08T20:06:45Z",
      "updated": "2022-07-08T20:06:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.04113v1",
      "landing_url": "https://arxiv.org/abs/2207.04113v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.04113"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: The paper focuses on RNN-based time-series forecasting rather than discrete audio token or codec design, so it fails the inclusion criteria and falls under the exclusion conditions, warranting exclusion (Score: 1)."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper focuses on RNN-based time-series forecasting rather than discrete audio token or codec design, so it fails the inclusion criteria and falls under the exclusion conditions, warranting exclusion (Score: 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Subjective and Objective Quality Assessment of High-Motion Sports Videos at Low-Bitrates",
    "abstract": "Videos often have to be transmitted and stored at low bitrates due to poor network connectivity during adaptive bitrate streaming. Designing optimal bitrate ladders that would select the perceptually-optimized resolution, frame-rate, and compression level for low-bitrate videos for adaptive streaming across the internet is therefore a task of great interest. Towards that end, we conducted the first large-scale study of medium and low-bitrate videos from live sports for two codecs (Elemental AVC and HEVC) and created the Amazon Prime Video Low-Bitrate Sports (APV LBS) dataset. The study involved 94 participants and 742 videos, with more than 23,000 human opinion scores collected in total. We analyzed the data obtained and we also conducted an extensive evaluation of objective Video Quality Assessment (VQA) algorithms and benchmarked their performance, and make recommendations on bitrate ladder design. We're making the metadata and VQA features available at https://github.com/JoshuaEbenezer/lbmfr-public.",
    "metadata": {
      "arxiv_id": "2207.05798",
      "title": "Subjective and Objective Quality Assessment of High-Motion Sports Videos at Low-Bitrates",
      "summary": "Videos often have to be transmitted and stored at low bitrates due to poor network connectivity during adaptive bitrate streaming. Designing optimal bitrate ladders that would select the perceptually-optimized resolution, frame-rate, and compression level for low-bitrate videos for adaptive streaming across the internet is therefore a task of great interest. Towards that end, we conducted the first large-scale study of medium and low-bitrate videos from live sports for two codecs (Elemental AVC and HEVC) and created the Amazon Prime Video Low-Bitrate Sports (APV LBS) dataset. The study involved 94 participants and 742 videos, with more than 23,000 human opinion scores collected in total. We analyzed the data obtained and we also conducted an extensive evaluation of objective Video Quality Assessment (VQA) algorithms and benchmarked their performance, and make recommendations on bitrate ladder design. We're making the metadata and VQA features available at https://github.com/JoshuaEbenezer/lbmfr-public.",
      "authors": [
        "Joshua P. Ebenezer",
        "Yixu Chen",
        "Yongjun Wu",
        "Hai Wei",
        "Sriram Sethuraman"
      ],
      "published": "2022-07-12T19:26:53Z",
      "updated": "2022-07-12T19:26:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05798v1",
      "landing_url": "https://arxiv.org/abs/2207.05798v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.05798"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses purely on video bitrate ladders and subjective/objective video quality assessment for sports streams, with no discussion of discrete audio tokenization or codec-level token modeling, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The study focuses purely on video bitrate ladders and subjective/objective video quality assessment for sports streams, with no discussion of discrete audio tokenization or codec-level token modeling, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
    "abstract": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
    "metadata": {
      "arxiv_id": "2207.05913",
      "title": "A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System",
      "summary": "Neural-based text-to-speech (TTS) systems achieve very high-fidelity speech generation because of the rapid neural network developments. However, the huge labeled corpus and high computation cost requirements limit the possibility of developing a high-fidelity TTS system by small companies or individuals. On the other hand, a neural vocoder, which has been widely adopted for the speech generation in neural-based TTS systems, can be trained with a relatively small unlabeled corpus. Therefore, in this paper, we explore a general framework to develop a neural post-filter (NPF) for low-cost TTS systems using neural vocoders. A cyclical approach is proposed to tackle the acoustic and temporal mismatches (AM and TM) of developing an NPF. Both objective and subjective evaluations have been conducted to demonstrate the AM and TM problems and the effectiveness of the proposed framework.",
      "authors": [
        "Yi-Chiao Wu",
        "Patrick Lumban Tobing",
        "Kazuki Yasuhara",
        "Noriyuki Matsunaga",
        "Yamato Ohtani",
        "Tomoki Toda"
      ],
      "published": "2022-07-13T01:40:59Z",
      "updated": "2022-07-13T01:40:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.05913v1",
      "landing_url": "https://arxiv.org/abs/2207.05913v1",
      "doi": "https://doi.org/10.1561/116.00000020"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses neural post-filtering for low-cost TTS and mismatch refinement, without mentioning discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses neural post-filtering for low-cost TTS and mismatch refinement, without mentioning discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
    "abstract": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
    "metadata": {
      "arxiv_id": "2207.06011",
      "title": "SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate",
      "summary": "The mapping of text to speech (TTS) is non-deterministic, letters may be pronounced differently based on context, or phonemes can vary depending on various physiological and stylistic factors like gender, age, accent, emotions, etc. Neural speaker embeddings, trained to identify or verify speakers are typically used to represent and transfer such characteristics from reference speech to synthesized speech. Speech separation on the other hand is the challenging task of separating individual speakers from an overlapping mixed signal of various speakers. Speaker attractors are high-dimensional embedding vectors that pull the time-frequency bins of each speaker's speech towards themselves while repelling those belonging to other speakers. In this work, we explore the possibility of using these powerful speaker attractors for zero-shot speaker adaptation in multi-speaker TTS synthesis and propose speaker attractor text to speech (SATTS). Through various experiments, we show that SATTS can synthesize natural speech from text from an unseen target speaker's reference signal which might have less than ideal recording conditions, i.e. reverberations or mixed with other speakers.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2022-07-13T07:35:23Z",
      "updated": "2022-07-13T07:35:23Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06011v1",
      "landing_url": "https://arxiv.org/abs/2207.06011v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06011"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses speaker attractors for zero-shot TTS adaptation without any mention of discrete audio tokens, quantization, vocabularies, or relevant tokenizer/token-level evaluation, so it fails the inclusion criteria and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses speaker attractors for zero-shot TTS adaptation without any mention of discrete audio tokens, quantization, vocabularies, or relevant tokenizer/token-level evaluation, so it fails the inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
    "abstract": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
    "metadata": {
      "arxiv_id": "2207.06088",
      "title": "Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech",
      "summary": "Some recent studies have demonstrated the feasibility of single-stage neural text-to-speech, which does not need to generate mel-spectrograms but generates the raw waveforms directly from the text. Single-stage text-to-speech often faces two problems: a) the one-to-many mapping problem due to multiple speech variations and b) insufficiency of high frequency reconstruction due to the lack of supervision of ground-truth acoustic features during training. To solve the a) problem and generate more expressive speech, we propose a novel phoneme-level prosody modeling method based on a variational autoencoder with normalizing flows to model underlying prosodic information in speech. We also use the prosody predictor to support end-to-end expressive speech synthesis. Furthermore, we propose the dual parallel autoencoder to introduce supervision of the ground-truth acoustic features during training to solve the b) problem enabling our model to generate high-quality speech. We compare the synthesis quality with state-of-the-art text-to-speech systems on an internal expressive English dataset. Both qualitative and quantitative evaluations demonstrate the superiority and robustness of our method for lossless speech generation while also showing a strong capability in prosody modeling.",
      "authors": [
        "Zhengxi Liu",
        "Qiao Tian",
        "Chenxu Hu",
        "Xudong Liu",
        "Menglin Wu",
        "Yuping Wang",
        "Hang Zhao",
        "Yuxuan Wang"
      ],
      "published": "2022-07-13T09:57:06Z",
      "updated": "2022-07-13T09:57:06Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06088v1",
      "landing_url": "https://arxiv.org/abs/2207.06088v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06088"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on end-to-end waveform TTS with VAE/flows and parallel autoencoders without discussing any discrete audio tokenization or quantized vocabularies, so it fails to meet the discrete token inclusion criteria and thus should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on end-to-end waveform TTS with VAE/flows and parallel autoencoders without discussing any discrete audio tokenization or quantized vocabularies, so it fails to meet the discrete token inclusion criteria and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
    "abstract": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
    "metadata": {
      "arxiv_id": "2207.06389",
      "title": "ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech",
      "summary": "Denoising diffusion probabilistic models (DDPMs) have recently achieved leading performances in many generative tasks. However, the inherited iterative sampling process costs hinder their applications to text-to-speech deployment. Through the preliminary study on diffusion model parameterization, we find that previous gradient-based TTS models require hundreds or thousands of iterations to guarantee high sample quality, which poses a challenge for accelerating sampling. In this work, we propose ProDiff, on progressive fast diffusion model for high-quality text-to-speech. Unlike previous work estimating the gradient for data density, ProDiff parameterizes the denoising model by directly predicting clean data to avoid distinct quality degradation in accelerating sampling. To tackle the model convergence challenge with decreased diffusion iterations, ProDiff reduces the data variance in the target site via knowledge distillation. Specifically, the denoising model uses the generated mel-spectrogram from an N-step DDIM teacher as the training target and distills the behavior into a new model with N/2 steps. As such, it allows the TTS model to make sharp predictions and further reduces the sampling time by orders of magnitude. Our evaluation demonstrates that ProDiff needs only 2 iterations to synthesize high-fidelity mel-spectrograms, while it maintains sample quality and diversity competitive with state-of-the-art models using hundreds of steps. ProDiff enables a sampling speed of 24x faster than real-time on a single NVIDIA 2080Ti GPU, making diffusion models practically applicable to text-to-speech synthesis deployment for the first time. Our extensive ablation studies demonstrate that each design in ProDiff is effective, and we further show that ProDiff can be easily extended to the multi-speaker setting. Audio samples are available at \\url{https://ProDiff.github.io/.}",
      "authors": [
        "Rongjie Huang",
        "Zhou Zhao",
        "Huadai Liu",
        "Jinglin Liu",
        "Chenye Cui",
        "Yi Ren"
      ],
      "published": "2022-07-13T17:45:43Z",
      "updated": "2022-07-13T17:45:43Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.06389v1",
      "landing_url": "https://arxiv.org/abs/2207.06389v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.06389"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on diffusion-based TTS operating on continuous mel-spectrograms and does not present or evaluate any discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion criteria and matches the exclusion of continuous-feature-centric work."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on diffusion-based TTS operating on continuous mel-spectrograms and does not present or evaluate any discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion criteria and matches the exclusion of continuous-feature-centric work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
    "abstract": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
    "metadata": {
      "arxiv_id": "2207.10317",
      "title": "Ensemble Learning for Efficient VVC Bitrate Ladder Prediction",
      "summary": "Changing the encoding parameters, in particular the video resolution, is a common practice before transcoding. To this end, streaming and broadcast platforms benefit from so-called bitrate ladders to determine the optimal resolution for given bitrates. However, the task of determining the bitrate ladder can usually be challenging as, on one hand, so-called fit-for-all static ladders would waste bandwidth, and on the other hand, fully specialized ladders are often not affordable in terms of computational complexity. In this paper, we propose an ML-based scheme for predicting the bitrate ladder based on the content of the video. The baseline of our solution predicts the bitrate ladder using two constituent methods, which require no encoding passes. To further enhance the performance of the constituent methods, we integrate a conditional ensemble method to aggregate their decisions, with a negligibly limited number of encoding passes. The experiment, carried out on the optimized software encoder implementation of the VVC standard, called VVenC, shows significant performance improvement. When compared to static bitrate ladder, the proposed method can offer about 13% bitrate reduction in terms of BD-BR with a negligible additional computational overhead. Conversely, when compared to the fully specialized bitrate ladder method, the proposed method can offer about 86% to 92% complexity reduction, at cost the of only 0.8% to 0.9% coding efficiency drop in terms of BD-BR.",
      "authors": [
        "Fatemeh Nasiri",
        "Wassim Hamidouche",
        "Luce Morin",
        "Nicolas Dholland",
        "Jean-Yves Aubié"
      ],
      "published": "2022-07-21T06:19:37Z",
      "updated": "2022-07-23T08:46:39Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.10317v2",
      "landing_url": "https://arxiv.org/abs/2207.10317v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.10317"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on VVC bitrate ladder prediction for video transcoding and does not address discrete audio-token generation, quantization, or token-level modeling, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on VVC bitrate ladder prediction for video transcoding and does not address discrete audio-token generation, quantization, or token-level modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantized Image-to-Image Translation",
    "abstract": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
    "metadata": {
      "arxiv_id": "2207.13286",
      "title": "Vector Quantized Image-to-Image Translation",
      "summary": "Current image-to-image translation methods formulate the task with conditional generation models, leading to learning only the recolorization or regional changes as being constrained by the rich structural information provided by the conditional contexts. In this work, we propose introducing the vector quantization technique into the image-to-image translation framework. The vector quantized content representation can facilitate not only the translation, but also the unconditional distribution shared among different domains. Meanwhile, along with the disentangled style representation, the proposed method further enables the capability of image extension with flexibility in both intra- and inter-domains. Qualitative and quantitative experiments demonstrate that our framework achieves comparable performance to the state-of-the-art image-to-image translation and image extension methods. Compared to methods for individual tasks, the proposed method, as a unified framework, unleashes applications combining image-to-image translation, unconditional generation, and image extension altogether. For example, it provides style variability for image generation and extension, and equips image-to-image translation with further extension capabilities.",
      "authors": [
        "Yu-Jie Chen",
        "Shin-I Cheng",
        "Wei-Chen Chiu",
        "Hung-Yu Tseng",
        "Hsin-Ying Lee"
      ],
      "published": "2022-07-27T04:22:29Z",
      "updated": "2022-07-27T04:22:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13286v1",
      "landing_url": "https://arxiv.org/abs/2207.13286v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13286"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vector-quantized image-to-image translation and explicitly deals with image generation rather than discrete audio tokenization, so it fails to meet the audio-token inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector-quantized image-to-image translation and explicitly deals with image generation rather than discrete audio tokenization, so it fails to meet the audio-token inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing",
    "abstract": "Deep generative models, and particularly facial animation schemes, can be used in video conferencing applications to efficiently compress a video through a sparse set of keypoints, without the need to transmit dense motion vectors. While these schemes bring significant coding gains over conventional video codecs at low bitrates, their performance saturates quickly when the available bandwidth increases. In this paper, we propose a layered, hybrid coding scheme to overcome this limitation. Specifically, we extend a codec based on facial animation by adding an auxiliary stream consisting of a very low bitrate version of the video, obtained through a conventional video codec (e.g., HEVC). The animated and auxiliary videos are combined through a novel fusion module. Our results show consistent average BD-Rate gains in excess of -30% on a large dataset of video conferencing sequences, extending the operational range of bitrates of a facial animation codec alone",
    "metadata": {
      "arxiv_id": "2207.13530",
      "title": "A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing",
      "summary": "Deep generative models, and particularly facial animation schemes, can be used in video conferencing applications to efficiently compress a video through a sparse set of keypoints, without the need to transmit dense motion vectors. While these schemes bring significant coding gains over conventional video codecs at low bitrates, their performance saturates quickly when the available bandwidth increases. In this paper, we propose a layered, hybrid coding scheme to overcome this limitation. Specifically, we extend a codec based on facial animation by adding an auxiliary stream consisting of a very low bitrate version of the video, obtained through a conventional video codec (e.g., HEVC). The animated and auxiliary videos are combined through a novel fusion module. Our results show consistent average BD-Rate gains in excess of -30% on a large dataset of video conferencing sequences, extending the operational range of bitrates of a facial animation codec alone",
      "authors": [
        "Goluck Konuko",
        "Stéphane Lathuilière",
        "Giuseppe Valenzise"
      ],
      "published": "2022-07-27T14:03:17Z",
      "updated": "2022-07-27T14:03:17Z",
      "categories": [
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13530v1",
      "landing_url": "https://arxiv.org/abs/2207.13530v1",
      "doi": "https://doi.org/10.48550/arXiv.2207.13530"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe video compression using facial animation and HEVC, with no mention of discrete audio tokens, tokenizers, or quantized vocabularies, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe video compression using facial animation and HEVC, with no mention of discrete audio tokens, tokenizers, or quantized vocabularies, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
    "abstract": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2207.13861",
      "title": "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer",
      "summary": "Real-world image denoising is a practical image restoration problem that aims to obtain clean images from in-the-wild noisy inputs. Recently, the Vision Transformer (ViT) has exhibited a strong ability to capture long-range dependencies, and many researchers have attempted to apply the ViT to image denoising tasks. However, a real-world image is an isolated frame that makes the ViT build long-range dependencies based on the internal patches, which divides images into patches, disarranges noise patterns and damages gradient continuity. In this article, we propose to resolve this issue by using a continuous Wavelet Sliding-Transformer that builds frequency correspondences under real-world scenes, called DnSwin. Specifically, we first extract the bottom features from noisy input images by using a convolutional neural network (CNN) encoder. The key to DnSwin is to extract high-frequency and low-frequency information from the observed features and build frequency dependencies. To this end, we propose a Wavelet Sliding-Window Transformer (WSWT) that utilizes the discrete wavelet transform (DWT), self-attention and the inverse DWT (IDWT) to extract deep features. Finally, we reconstruct the deep features into denoised images using a CNN decoder. Both quantitative and qualitative evaluations conducted on real-world denoising benchmarks demonstrate that the proposed DnSwin performs favorably against the state-of-the-art methods.",
      "authors": [
        "Hao Li",
        "Zhijing Yang",
        "Xiaobin Hong",
        "Ziying Zhao",
        "Junyang Chen",
        "Yukai Shi",
        "Jinshan Pan"
      ],
      "published": "2022-07-28T02:33:57Z",
      "updated": "2022-09-13T05:14:07Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.13861v2",
      "landing_url": "https://arxiv.org/abs/2207.13861v2",
      "doi": "https://doi.org/10.1016/j.knosys.2022.109815"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on image denoising with wavelet transformers and contains no discussion of discrete audio tokenization, codec/tokenizer design, or related evaluations, so it fails every inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on image denoising with wavelet transformers and contains no discussion of discrete audio tokenization, codec/tokenizer design, or related evaluations, so it fails every inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pre-training General Trajectory Embeddings with Maximum Multi-view Entropy Coding",
    "abstract": "Spatio-temporal trajectories provide valuable information about movement and travel behavior, enabling various downstream tasks that in turn power real-world applications. Learning trajectory embeddings can improve task performance but may incur high computational costs and face limited training data availability. Pre-training learns generic embeddings by means of specially constructed pretext tasks that enable learning from unlabeled data. Existing pre-training methods face (i) difficulties in learning general embeddings due to biases towards certain downstream tasks incurred by the pretext tasks, (ii) limitations in capturing both travel semantics and spatio-temporal correlations, and (iii) the complexity of long, irregularly sampled trajectories. To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We introduce a pretext task that reduces biases in pre-trained trajectory embeddings, yielding embeddings that are useful for a wide variety of downstream tasks. We also propose an attention-based discrete encoder and a NeuralCDE-based continuous encoder that extract and represent travel behavior and continuous spatio-temporal correlations from trajectories in embeddings, respectively. Extensive experiments on two real-world datasets and three downstream tasks offer insight into the design properties of our proposal and indicate that it is capable of outperforming existing trajectory embedding methods.",
    "metadata": {
      "arxiv_id": "2207.14539",
      "title": "Pre-training General Trajectory Embeddings with Maximum Multi-view Entropy Coding",
      "summary": "Spatio-temporal trajectories provide valuable information about movement and travel behavior, enabling various downstream tasks that in turn power real-world applications. Learning trajectory embeddings can improve task performance but may incur high computational costs and face limited training data availability. Pre-training learns generic embeddings by means of specially constructed pretext tasks that enable learning from unlabeled data. Existing pre-training methods face (i) difficulties in learning general embeddings due to biases towards certain downstream tasks incurred by the pretext tasks, (ii) limitations in capturing both travel semantics and spatio-temporal correlations, and (iii) the complexity of long, irregularly sampled trajectories.\n  To tackle these challenges, we propose Maximum Multi-view Trajectory Entropy Coding (MMTEC) for learning general and comprehensive trajectory embeddings. We introduce a pretext task that reduces biases in pre-trained trajectory embeddings, yielding embeddings that are useful for a wide variety of downstream tasks. We also propose an attention-based discrete encoder and a NeuralCDE-based continuous encoder that extract and represent travel behavior and continuous spatio-temporal correlations from trajectories in embeddings, respectively. Extensive experiments on two real-world datasets and three downstream tasks offer insight into the design properties of our proposal and indicate that it is capable of outperforming existing trajectory embedding methods.",
      "authors": [
        "Yan Lin",
        "Huaiyu Wan",
        "Shengnan Guo",
        "Jilin Hu",
        "Christian S. Jensen",
        "Youfang Lin"
      ],
      "published": "2022-07-29T08:16:20Z",
      "updated": "2023-12-26T01:14:05Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.14539v2",
      "landing_url": "https://arxiv.org/abs/2207.14539v2",
      "doi": "https://doi.org/10.48550/arXiv.2207.14539"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work targets trajectory embeddings without any discrete audio tokenization/codec components or vocabulary-based token representations, so it falls outside the defined audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The work targets trajectory embeddings without any discrete audio tokenization/codec components or vocabulary-based token representations, so it falls outside the defined audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
    "abstract": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
    "metadata": {
      "arxiv_id": "2207.14686",
      "title": "Forensic License Plate Recognition with Compression-Informed Transformers",
      "summary": "Forensic license plate recognition (FLPR) remains an open challenge in legal contexts such as criminal investigations, where unreadable license plates (LPs) need to be deciphered from highly compressed and/or low resolution footage, e.g., from surveillance cameras. In this work, we propose a side-informed Transformer architecture that embeds knowledge on the input compression level to improve recognition under strong compression. We show the effectiveness of Transformers for license plate recognition (LPR) on a low-quality real-world dataset. We also provide a synthetic dataset that includes strongly degraded, illegible LP images and analyze the impact of knowledge embedding on it. The network outperforms existing FLPR methods and standard state-of-the art image recognition models while requiring less parameters. For the severest degraded images, we can improve recognition by up to 8.9 percent points.",
      "authors": [
        "Denise Moussa",
        "Anatol Maier",
        "Andreas Spruck",
        "Jürgen Seiler",
        "Christian Riess"
      ],
      "published": "2022-07-29T13:58:24Z",
      "updated": "2024-05-03T15:15:27Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2207.14686v3",
      "landing_url": "https://arxiv.org/abs/2207.14686v3",
      "doi": "https://doi.org/10.1109/ICIP46576.2022.9897178"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This forensic license plate recognition work deals with image/video compression and transformer architectures for low-quality footage, which has no connection to discrete audio token generation/quantization or their evaluation, so it fails the inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "This forensic license plate recognition work deals with image/video compression and transformer architectures for low-quality footage, which has no connection to discrete audio token generation/quantization or their evaluation, so it fails the inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
    "abstract": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
    "metadata": {
      "arxiv_id": "2208.03987",
      "title": "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model",
      "summary": "Large-scale vision foundation models have made significant progress in visual tasks on natural images, with vision transformers being the primary choice due to their good scalability and representation ability. However, large-scale models in remote sensing (RS) have not yet been sufficiently explored. In this paper, we resort to plain vision transformers with about 100 million parameters and make the first attempt to propose large vision models tailored to RS tasks and investigate how such large models perform. To handle the large sizes and objects of arbitrary orientations in RS images, we propose a new rotated varied-size window attention to replace the original full attention in transformers, which can significantly reduce the computational cost and memory footprint while learning better object representation by extracting rich context from the generated diverse windows. Experiments on detection tasks show the superiority of our model over all state-of-the-art models, achieving 81.24% mAP on the DOTA-V1.0 dataset. The results of our models on downstream classification and segmentation tasks also show competitive performance compared to existing advanced methods. Further experiments show the advantages of our models in terms of computational complexity and data efficiency in transferring.",
      "authors": [
        "Di Wang",
        "Qiming Zhang",
        "Yufei Xu",
        "Jing Zhang",
        "Bo Du",
        "Dacheng Tao",
        "Liangpei Zhang"
      ],
      "published": "2022-08-08T09:08:40Z",
      "updated": "2022-12-08T13:51:33Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.03987v4",
      "landing_url": "https://arxiv.org/abs/2208.03987v4",
      "doi": "https://doi.org/10.48550/arXiv.2208.03987"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on remote-sensing vision transformers and has no connection to discrete audio tokenization, so it fails every inclusion requirement and fits exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on remote-sensing vision transformers and has no connection to discrete audio tokenization, so it fails every inclusion requirement and fits exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DDX7: Differentiable FM Synthesis of Musical Instrument Sounds",
    "abstract": "FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",
    "metadata": {
      "arxiv_id": "2208.06169",
      "title": "DDX7: Differentiable FM Synthesis of Musical Instrument Sounds",
      "summary": "FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",
      "authors": [
        "Franco Caspe",
        "Andrew McPherson",
        "Mark Sandler"
      ],
      "published": "2022-08-12T08:39:45Z",
      "updated": "2022-08-12T08:39:45Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.06169v1",
      "landing_url": "https://arxiv.org/abs/2208.06169v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.06169"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The work centers on differentiable FM synthesis of musical instrument sounds without describing any discrete audio tokenization, finite vocabulary quantization, or codebook-based tokenizer, so it fails the inclusion requirement of focusing on discrete audio tokens and should likely be excluded."
    },
    "round-A_JuniorNano_reasoning": "The work centers on differentiable FM synthesis of musical instrument sounds without describing any discrete audio tokenization, finite vocabulary quantization, or codebook-based tokenizer, so it fails the inclusion requirement of focusing on discrete audio tokens and should likely be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition",
    "abstract": "Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.",
    "metadata": {
      "arxiv_id": "2208.07657",
      "title": "Uconv-Conformer: High Reduction of Input Sequence Length for End-to-End Speech Recognition",
      "summary": "Optimization of modern ASR architectures is among the highest priority tasks since it saves many computational resources for model training and inference. The work proposes a new Uconv-Conformer architecture based on the standard Conformer model. It consistently reduces the input sequence length by 16 times, which results in speeding up the work of the intermediate layers. To solve the convergence issue connected with such a significant reduction of the time dimension, we use upsampling blocks like in the U-Net architecture to ensure the correct CTC loss calculation and stabilize network training. The Uconv-Conformer architecture appears to be not only faster in terms of training and inference speed but also shows better WER compared to the baseline Conformer. Our best Uconv-Conformer model shows 47.8% and 23.5% inference acceleration on the CPU and GPU, respectively. Relative WER reduction is 7.3% and 9.2% on LibriSpeech test_clean and test_other respectively.",
      "authors": [
        "Andrei Andrusenko",
        "Rauf Nasretdinov",
        "Aleksei Romanenko"
      ],
      "published": "2022-08-16T10:40:15Z",
      "updated": "2023-03-11T10:00:17Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.07657v3",
      "landing_url": "https://arxiv.org/abs/2208.07657v3",
      "doi": "https://doi.org/10.48550/arXiv.2208.07657"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract only describe an ASR architecture that reduces input length via upsampling but never discusses discrete audio tokenization, quantization, or vocabulary-based tokens, so it fails the inclusion focus on discrete tokens."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract only describe an ASR architecture that reduces input length via upsampling but never discusses discrete audio tokenization, quantization, or vocabulary-based tokens, so it fails the inclusion focus on discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learned Lossless JPEG Transcoding via Joint Lossy and Residual Compression",
    "abstract": "As a commonly-used image compression format, JPEG has been broadly applied in the transmission and storage of images. To further reduce the compression cost while maintaining the quality of JPEG images, lossless transcoding technology has been proposed to recompress the compressed JPEG image in the DCT domain. Previous works, on the other hand, typically reduce the redundancy of DCT coefficients and optimize the probability prediction of entropy coding in a hand-crafted manner that lacks generalization ability and flexibility. To tackle the above challenge, we propose the learned lossless JPEG transcoding framework via Joint Lossy and Residual Compression. Instead of directly optimizing the entropy estimation, we focus on the redundancy that exists in the DCT coefficients. To the best of our knowledge, we are the first to utilize the learned end-to-end lossy transform coding to reduce the redundancy of DCT coefficients in a compact representational domain. We also introduce residual compression for lossless transcoding, which adaptively learns the distribution of residual DCT coefficients before compressing them using context-based entropy coding. Our proposed transcoding architecture shows significant superiority in the compression of JPEG images thanks to the collaboration of learned lossy transform coding and residual entropy coding. Extensive experiments on multiple datasets have demonstrated that our proposed framework can achieve about 21.49% bits saving in average based on JPEG compression, which outperforms the typical lossless transcoding framework JPEG-XL by 3.51%.",
    "metadata": {
      "arxiv_id": "2208.11673",
      "title": "Learned Lossless JPEG Transcoding via Joint Lossy and Residual Compression",
      "summary": "As a commonly-used image compression format, JPEG has been broadly applied in the transmission and storage of images. To further reduce the compression cost while maintaining the quality of JPEG images, lossless transcoding technology has been proposed to recompress the compressed JPEG image in the DCT domain. Previous works, on the other hand, typically reduce the redundancy of DCT coefficients and optimize the probability prediction of entropy coding in a hand-crafted manner that lacks generalization ability and flexibility. To tackle the above challenge, we propose the learned lossless JPEG transcoding framework via Joint Lossy and Residual Compression. Instead of directly optimizing the entropy estimation, we focus on the redundancy that exists in the DCT coefficients. To the best of our knowledge, we are the first to utilize the learned end-to-end lossy transform coding to reduce the redundancy of DCT coefficients in a compact representational domain. We also introduce residual compression for lossless transcoding, which adaptively learns the distribution of residual DCT coefficients before compressing them using context-based entropy coding. Our proposed transcoding architecture shows significant superiority in the compression of JPEG images thanks to the collaboration of learned lossy transform coding and residual entropy coding. Extensive experiments on multiple datasets have demonstrated that our proposed framework can achieve about 21.49% bits saving in average based on JPEG compression, which outperforms the typical lossless transcoding framework JPEG-XL by 3.51%.",
      "authors": [
        "Xiaoshuai Fan",
        "Xin Li",
        "Zhibo Chen"
      ],
      "published": "2022-08-24T17:12:00Z",
      "updated": "2022-08-24T17:12:00Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2208.11673v1",
      "landing_url": "https://arxiv.org/abs/2208.11673v1",
      "doi": "https://doi.org/10.48550/arXiv.2208.11673"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe JPEG image transcoding not discrete audio token learning, so it fails the inclusion focus on audio tokenization and instead fits exclusion of non-audio discrete sequences."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe JPEG image transcoding not discrete audio token learning, so it fails the inclusion focus on audio tokenization and instead fits exclusion of non-audio discrete sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning to Predict on Octree for Scalable Point Cloud Geometry Coding",
    "abstract": "Octree-based point cloud representation and compression have been adopted by the MPEG G-PCC standard. However, it only uses handcrafted methods to predict the probability that a leaf node is non-empty, which is then used for entropy coding. We propose a novel approach for predicting such probabilities for geometry coding, which applies a denoising neural network to a \"noisy\" context cube that includes both neighboring decoded voxels as well as uncoded voxels. We further propose a convolution-based model to upsample the decoded point cloud at a coarse resolution on the decoder side. Integration of the two approaches significantly improves the rate-distortion performance for geometry coding compared to the original G-PCC standard and other baseline methods for dense point clouds. The proposed octree-based entropy coding approach is naturally scalable, which is desirable for dynamic rate adaptation in point cloud streaming systems.",
    "metadata": {
      "arxiv_id": "2209.02226",
      "title": "Learning to Predict on Octree for Scalable Point Cloud Geometry Coding",
      "summary": "Octree-based point cloud representation and compression have been adopted by the MPEG G-PCC standard. However, it only uses handcrafted methods to predict the probability that a leaf node is non-empty, which is then used for entropy coding. We propose a novel approach for predicting such probabilities for geometry coding, which applies a denoising neural network to a \"noisy\" context cube that includes both neighboring decoded voxels as well as uncoded voxels. We further propose a convolution-based model to upsample the decoded point cloud at a coarse resolution on the decoder side. Integration of the two approaches significantly improves the rate-distortion performance for geometry coding compared to the original G-PCC standard and other baseline methods for dense point clouds. The proposed octree-based entropy coding approach is naturally scalable, which is desirable for dynamic rate adaptation in point cloud streaming systems.",
      "authors": [
        "Yixiang Mao",
        "Yueyu Hu",
        "Yao Wang"
      ],
      "published": "2022-09-06T05:34:35Z",
      "updated": "2022-09-06T05:34:35Z",
      "categories": [
        "eess.IV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.02226v1",
      "landing_url": "https://arxiv.org/abs/2209.02226v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.02226"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on octree point cloud geometry coding, which has nothing to do with discrete audio token generation, quantized audio tokens, or any audio-related vocabulary, so it clearly fails the inclusion domain."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on octree point cloud geometry coding, which has nothing to do with discrete audio token generation, quantized audio tokens, or any audio-related vocabulary, so it clearly fails the inclusion domain.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": {
      "reasoning": "該研究的核心主題是三維點雲的幾何壓縮，而非音訊訊號的離散化，與主題「離散音訊 token」無關。",
      "evaluation": 1
    },
    "round-A_JuniorMini_reasoning": "該研究的核心主題是三維點雲的幾何壓縮，而非音訊訊號的離散化，與主題「離散音訊 token」無關。",
    "round-A_JuniorMini_evaluation": 1,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AudioLM: a Language Modeling Approach to Audio Generation",
    "abstract": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
    "metadata": {
      "arxiv_id": "2209.03143",
      "title": "AudioLM: a Language Modeling Approach to Audio Generation",
      "summary": "We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen speakers. Furthermore, we demonstrate how our approach extends beyond speech by generating coherent piano music continuations, despite being trained without any symbolic representation of music.",
      "authors": [
        "Zalán Borsos",
        "Raphaël Marinier",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Dominik Roblek",
        "Olivier Teboul",
        "David Grangier",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2022-09-07T13:40:08Z",
      "updated": "2023-07-26T03:52:36Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.03143v2",
      "landing_url": "https://arxiv.org/abs/2209.03143v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.03143"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "AudioLM explicitly maps audio to discrete token sequences via both neural codec and self-supervised semantic tokenizers, discusses hybrid tokenization trade-offs, and evaluates generation quality, so it clearly aligns with the inclusion criteria and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "AudioLM explicitly maps audio to discrete token sequences via both neural codec and self-supervised semantic tokenizers, discusses hybrid tokenization trade-offs, and evaluates generation quality, so it clearly aligns with the inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation",
    "abstract": "Although two-stage Vector Quantized (VQ) generative models allow for synthesizing high-fidelity and high-resolution images, their quantization operator encodes similar patches within an image into the same index, resulting in a repeated artifact for similar adjacent regions using existing decoder architectures. To address this issue, we propose to incorporate the spatially conditional normalization to modulate the quantized vectors so as to insert spatially variant information to the embedded index maps, encouraging the decoder to generate more photorealistic images. Moreover, we use multichannel quantization to increase the recombination capability of the discrete codes without increasing the cost of model and codebook. Additionally, to generate discrete tokens at the second stage, we adopt a Masked Generative Image Transformer (MaskGIT) to learn an underlying prior distribution in the compressed latent space, which is much faster than the conventional autoregressive model. Experiments on two benchmark datasets demonstrate that our proposed modulated VQGAN is able to greatly improve the reconstructed image quality as well as provide high-fidelity image generation.",
    "metadata": {
      "arxiv_id": "2209.09002",
      "title": "MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation",
      "summary": "Although two-stage Vector Quantized (VQ) generative models allow for synthesizing high-fidelity and high-resolution images, their quantization operator encodes similar patches within an image into the same index, resulting in a repeated artifact for similar adjacent regions using existing decoder architectures. To address this issue, we propose to incorporate the spatially conditional normalization to modulate the quantized vectors so as to insert spatially variant information to the embedded index maps, encouraging the decoder to generate more photorealistic images. Moreover, we use multichannel quantization to increase the recombination capability of the discrete codes without increasing the cost of model and codebook. Additionally, to generate discrete tokens at the second stage, we adopt a Masked Generative Image Transformer (MaskGIT) to learn an underlying prior distribution in the compressed latent space, which is much faster than the conventional autoregressive model. Experiments on two benchmark datasets demonstrate that our proposed modulated VQGAN is able to greatly improve the reconstructed image quality as well as provide high-fidelity image generation.",
      "authors": [
        "Chuanxia Zheng",
        "Long Tung Vuong",
        "Jianfei Cai",
        "Dinh Phung"
      ],
      "published": "2022-09-19T13:26:51Z",
      "updated": "2022-09-19T13:26:51Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.09002v1",
      "landing_url": "https://arxiv.org/abs/2209.09002v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.09002"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe vector quantized image generation with no discrete audio-token focus, so it fails the inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe vector quantized image generation with no discrete audio-token focus, so it fails the inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
    "abstract": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
    "metadata": {
      "arxiv_id": "2209.10890",
      "title": "EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models",
      "summary": "Neural models are known to be over-parameterized, and recent work has shown that sparse text-to-speech (TTS) models can outperform dense models. Although a plethora of sparse methods has been proposed for other domains, such methods have rarely been applied in TTS. In this work, we seek to answer the question: what are the characteristics of selected sparse techniques on the performance and model complexity? We compare a Tacotron2 baseline and the results of applying five techniques. We then evaluate the performance via the factors of naturalness, intelligibility and prosody, while reporting model size and training time. Complementary to prior research, we find that pruning before or during training can achieve similar performance to pruning after training and can be trained much faster, while removing entire neurons degrades performance much more than removing parameters. To our best knowledge, this is the first work that compares sparsity paradigms in text-to-speech synthesis.",
      "authors": [
        "Perry Lam",
        "Huayun Zhang",
        "Nancy F. Chen",
        "Berrak Sisman"
      ],
      "published": "2022-09-22T09:47:25Z",
      "updated": "2022-09-22T09:47:25Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.10890v1",
      "landing_url": "https://arxiv.org/abs/2209.10890v1",
      "doi": "https://doi.org/10.21437/Interspeech.2022-10626"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper investigates pruning Tacotron2/Text-To-Speech models using continuous representations and lacks any discrete audio token/tokenizer work, so it fails the inclusion criteria and should be excluded; Score: 1"
    },
    "round-A_JuniorNano_reasoning": "The paper investigates pruning Tacotron2/Text-To-Speech models using continuous representations and lacks any discrete audio token/tokenizer work, so it fails the inclusion criteria and should be excluded; Score: 1",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
    "abstract": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
    "metadata": {
      "arxiv_id": "2209.11404",
      "title": "Towards Frame Rate Agnostic Multi-Object Tracking",
      "summary": "Multi-Object Tracking (MOT) is one of the most fundamental computer vision tasks that contributes to various video analysis applications. Despite the recent promising progress, current MOT research is still limited to a fixed sampling frame rate of the input stream. In fact, we empirically found that the accuracy of all recent state-of-the-art trackers drops dramatically when the input frame rate changes. For a more intelligent tracking solution, we shift the attention of our research work to the problem of Frame Rate Agnostic MOT (FraMOT), which takes frame rate insensitivity into consideration. In this paper, we propose a Frame Rate Agnostic MOT framework with a Periodic training Scheme (FAPS) to tackle the FraMOT problem for the first time. Specifically, we propose a Frame Rate Agnostic Association Module (FAAM) that infers and encodes the frame rate information to aid identity matching across multi-frame-rate inputs, improving the capability of the learned model in handling complex motion-appearance relations in FraMOT. Moreover, the association gap between training and inference is enlarged in FraMOT because those post-processing steps not included in training make a larger difference in lower frame rate scenarios. To address it, we propose Periodic Training Scheme (PTS) to reflect all post-processing steps in training via tracking pattern matching and fusion. Along with the proposed approaches, we make the first attempt to establish an evaluation method for this new task of FraMOT in two different modes, i.e., known frame rate and unknown frame rate, aiming to handle a more complex situation. The quantitative experiments on the challenging MOT17/20 dataset (FraMOT version) have clearly demonstrated that the proposed approaches can handle different frame rates better and thus improve the robustness against complicated scenarios.",
      "authors": [
        "Weitao Feng",
        "Lei Bai",
        "Yongqiang Yao",
        "Fengwei Yu",
        "Wanli Ouyang"
      ],
      "published": "2022-09-23T04:25:19Z",
      "updated": "2023-04-18T02:15:17Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11404v3",
      "landing_url": "https://arxiv.org/abs/2209.11404v3",
      "doi": "https://doi.org/10.1007/s11263-023-01943-2"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on frame-rate agnostic multi-object tracking for video, which does not involve discrete audio token generation or modeling, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on frame-rate agnostic multi-object tracking for video, which does not involve discrete audio token generation or modeling, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
    "abstract": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
    "metadata": {
      "arxiv_id": "2209.11750",
      "title": "Transformer-based Models to Deal with Heterogeneous Environments in Human Activity Recognition",
      "summary": "Human Activity Recognition (HAR) on mobile devices has been demonstrated to be possible using neural models trained on data collected from the device's inertial measurement units. These models have used Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTMs), Transformers or a combination of these to achieve state-of-the-art results with real-time performance. However, these approaches have not been extensively evaluated in real-world situations where the input data may be different from the training data. This paper highlights the issue of data heterogeneity in machine learning applications and how it can hinder their deployment in pervasive settings. To address this problem, we propose and publicly release the code of two sensor-wise Transformer architectures called HART and MobileHART for Human Activity Recognition Transformer. Our experiments on several publicly available datasets show that these HART architectures outperform previous architectures with fewer floating point operations and parameters than conventional Transformers. The results also show they are more robust to changes in mobile position or device brand and hence better suited for the heterogeneous environments encountered in real-life settings. Finally, the source code has been made publicly available.",
      "authors": [
        "Sannara EK",
        "François Portet",
        "Philippe Lalanda"
      ],
      "published": "2022-09-22T09:42:08Z",
      "updated": "2025-08-23T20:07:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11750v2",
      "landing_url": "https://arxiv.org/abs/2209.11750v2",
      "doi": "https://doi.org/10.1007/s00779-023-01776-3"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This HAR paper focuses on sensor Transformers with continuous inertial data and does not discuss encoding audio signals into discrete tokens or quantized vocabularies, so it clearly fails the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "This HAR paper focuses on sensor Transformers with continuous inertial data and does not discuss encoding audio signals into discrete tokens or quantized vocabularies, so it clearly fails the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Focused Study on Sequence Length for Dialogue Summarization",
    "abstract": "Output length is critical to dialogue summarization systems. The dialogue summary length is determined by multiple factors, including dialogue complexity, summary objective, and personal preferences. In this work, we approach dialogue summary length from three perspectives. First, we analyze the length differences between existing models' outputs and the corresponding human references and find that summarization models tend to produce more verbose summaries due to their pretraining objectives. Second, we identify salient features for summary length prediction by comparing different model settings. Third, we experiment with a length-aware summarizer and show notable improvement on existing models if summary length can be well incorporated. Analysis and experiments are conducted on popular DialogSum and SAMSum datasets to validate our findings.",
    "metadata": {
      "arxiv_id": "2209.11910",
      "title": "A Focused Study on Sequence Length for Dialogue Summarization",
      "summary": "Output length is critical to dialogue summarization systems. The dialogue summary length is determined by multiple factors, including dialogue complexity, summary objective, and personal preferences. In this work, we approach dialogue summary length from three perspectives. First, we analyze the length differences between existing models' outputs and the corresponding human references and find that summarization models tend to produce more verbose summaries due to their pretraining objectives. Second, we identify salient features for summary length prediction by comparing different model settings. Third, we experiment with a length-aware summarizer and show notable improvement on existing models if summary length can be well incorporated. Analysis and experiments are conducted on popular DialogSum and SAMSum datasets to validate our findings.",
      "authors": [
        "Bin Wang",
        "Chen Zhang",
        "Chengwei Wei",
        "Haizhou Li"
      ],
      "published": "2022-09-24T02:49:48Z",
      "updated": "2022-10-27T03:55:03Z",
      "categories": [
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.11910v2",
      "landing_url": "https://arxiv.org/abs/2209.11910v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.11910"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses dialogue summarization lengths without covering discrete audio token generation, quantization, or related evaluations, so it doesn’t meet the audio-token inclusion criteria; score: 1."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses dialogue summarization lengths without covering discrete audio token generation, quantization, or related evaluations, so it doesn’t meet the audio-token inclusion criteria; score: 1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
    "abstract": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
    "metadata": {
      "arxiv_id": "2209.12139",
      "title": "Lightweight Image Codec via Multi-Grid Multi-Block-Size Vector Quantization (MGBVQ)",
      "summary": "A multi-grid multi-block-size vector quantization (MGBVQ) method is proposed for image coding in this work. The fundamental idea of image coding is to remove correlations among pixels before quantization and entropy coding, e.g., the discrete cosine transform (DCT) and intra predictions, adopted by modern image coding standards. We present a new method to remove pixel correlations. First, by decomposing correlations into long- and short-range correlations, we represent long-range correlations in coarser grids due to their smoothness, thus leading to a multi-grid (MG) coding architecture. Second, we show that short-range correlations can be effectively coded by a suite of vector quantizers (VQs). Along this line, we argue the effectiveness of VQs of very large block sizes and present a convenient way to implement them. It is shown by experimental results that MGBVQ offers excellent rate-distortion (RD) performance, which is comparable with existing image coders, at much lower complexity. Besides, it provides a progressive coded bitstream.",
      "authors": [
        "Yifan Wang",
        "Zhanxuan Mei",
        "Ioannis Katsavounidis",
        "C. -C. Jay Kuo"
      ],
      "published": "2022-09-25T04:14:26Z",
      "updated": "2022-09-25T04:14:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.12139v1",
      "landing_url": "https://arxiv.org/abs/2209.12139v1",
      "doi": "https://doi.org/10.48550/arXiv.2209.12139"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper proposes an image codec (MGBVQ) with no connection to discrete audio tokens or audio tokenization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes an image codec (MGBVQ) with no connection to discrete audio tokens or audio tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dilated Neighborhood Attention Transformer",
    "abstract": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
    "metadata": {
      "arxiv_id": "2209.15001",
      "title": "Dilated Neighborhood Attention Transformer",
      "summary": "Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).",
      "authors": [
        "Ali Hassani",
        "Humphrey Shi"
      ],
      "published": "2022-09-29T17:57:08Z",
      "updated": "2023-01-16T18:58:58Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15001v3",
      "landing_url": "https://arxiv.org/abs/2209.15001v3",
      "doi": "https://doi.org/10.48550/arXiv.2209.15001"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Vision transformer paper on Dilated Neighborhood Attention has no mention of discrete audio tokens or tokenizer/quantization design, so it fails all inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Vision transformer paper on Dilated Neighborhood Attention has no mention of discrete audio tokens or tokenizer/quantization design, so it fails all inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
    "abstract": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
    "metadata": {
      "arxiv_id": "2209.15483",
      "title": "Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling",
      "summary": "Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.",
      "authors": [
        "Itai Gat",
        "Felix Kreuk",
        "Tu Anh Nguyen",
        "Ann Lee",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Emmanuel Dupoux",
        "Yossi Adi"
      ],
      "published": "2022-09-30T14:15:03Z",
      "updated": "2023-05-29T10:50:29Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15483v2",
      "landing_url": "https://arxiv.org/abs/2209.15483v2",
      "doi": "https://doi.org/10.48550/arXiv.2209.15483"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper clearly targets discrete speech tokens for generative spoken language modeling, proposes quantized unit robustness improvements, and evaluates downstream tasks, so it meets all inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "Paper clearly targets discrete speech tokens for generative spoken language modeling, proposes quantized unit robustness improvements, and evaluates downstream tasks, so it meets all inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Match to Win: Analysing Sequences Lengths for Efficient Self-supervised Learning in Speech and Audio",
    "abstract": "Self-supervised learning (SSL) has proven vital in speech and audio-related applications. The paradigm trains a general model on unlabeled data that can later be used to solve specific downstream tasks. This type of model is costly to train as it requires manipulating long input sequences that can only be handled by powerful centralised servers. Surprisingly, despite many attempts to increase training efficiency through model compression, the effects of truncating input sequence lengths to reduce computation have not been studied. In this paper, we provide the first empirical study of SSL pre-training for different specified sequence lengths and link this to various downstream tasks. We find that training on short sequences can dramatically reduce resource costs while retaining a satisfactory performance for all tasks. This simple one-line change would promote the migration of SSL training from data centres to user-end edge devices for more realistic and personalised applications.",
    "metadata": {
      "arxiv_id": "2209.15575",
      "title": "Match to Win: Analysing Sequences Lengths for Efficient Self-supervised Learning in Speech and Audio",
      "summary": "Self-supervised learning (SSL) has proven vital in speech and audio-related applications. The paradigm trains a general model on unlabeled data that can later be used to solve specific downstream tasks. This type of model is costly to train as it requires manipulating long input sequences that can only be handled by powerful centralised servers. Surprisingly, despite many attempts to increase training efficiency through model compression, the effects of truncating input sequence lengths to reduce computation have not been studied. In this paper, we provide the first empirical study of SSL pre-training for different specified sequence lengths and link this to various downstream tasks. We find that training on short sequences can dramatically reduce resource costs while retaining a satisfactory performance for all tasks. This simple one-line change would promote the migration of SSL training from data centres to user-end edge devices for more realistic and personalised applications.",
      "authors": [
        "Yan Gao",
        "Javier Fernandez-Marques",
        "Titouan Parcollet",
        "Pedro P. B. de Gusmao",
        "Nicholas D. Lane"
      ],
      "published": "2022-09-30T16:35:42Z",
      "updated": "2022-11-22T12:05:42Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2209.15575v4",
      "landing_url": "https://arxiv.org/abs/2209.15575v4",
      "doi": "https://doi.org/10.48550/arXiv.2209.15575"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies reducing input sequence lengths for SSL training rather than discrete audio tokenization/quantization methods, so it fails to meet the inclusion focus on discrete token production or evaluation and thus should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper studies reducing input sequence lengths for SSL training rather than discrete audio tokenization/quantization methods, so it fails to meet the inclusion focus on discrete token production or evaluation and thus should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Strength-Adaptive Adversarial Training",
    "abstract": "Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose \\emph{Strength-Adaptive Adversarial Training} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.",
    "metadata": {
      "arxiv_id": "2210.01288",
      "title": "Strength-Adaptive Adversarial Training",
      "summary": "Adversarial training (AT) is proved to reliably improve network's robustness against adversarial data. However, current AT with a pre-specified perturbation budget has limitations in learning a robust network. Firstly, applying a pre-specified perturbation budget on networks of various model capacities will yield divergent degree of robustness disparity between natural and robust accuracies, which deviates from robust network's desideratum. Secondly, the attack strength of adversarial training data constrained by the pre-specified perturbation budget fails to upgrade as the growth of network robustness, which leads to robust overfitting and further degrades the adversarial robustness. To overcome these limitations, we propose \\emph{Strength-Adaptive Adversarial Training} (SAAT). Specifically, the adversary employs an adversarial loss constraint to generate adversarial training data. Under this constraint, the perturbation budget will be adaptively adjusted according to the training state of adversarial data, which can effectively avoid robust overfitting. Besides, SAAT explicitly constrains the attack strength of training data through the adversarial loss, which manipulates model capacity scheduling during training, and thereby can flexibly control the degree of robustness disparity and adjust the tradeoff between natural accuracy and robustness. Extensive experiments show that our proposal boosts the robustness of adversarial training.",
      "authors": [
        "Chaojian Yu",
        "Dawei Zhou",
        "Li Shen",
        "Jun Yu",
        "Bo Han",
        "Mingming Gong",
        "Nannan Wang",
        "Tongliang Liu"
      ],
      "published": "2022-10-04T00:22:37Z",
      "updated": "2022-10-04T00:22:37Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.01288v1",
      "landing_url": "https://arxiv.org/abs/2210.01288v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.01288"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The title and abstract only discuss adversarial robustness with adaptive perturbation budgets and contain no mention of discrete audio tokenization, codec quantization, or related token modeling, so this study fails the inclusion criteria and deserves a score of 1."
    },
    "round-A_JuniorNano_reasoning": "The title and abstract only discuss adversarial robustness with adaptive perturbation budgets and contain no mention of discrete audio tokenization, codec quantization, or related token modeling, so this study fails the inclusion criteria and deserves a score of 1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
    "abstract": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
    "metadata": {
      "arxiv_id": "2210.01765",
      "title": "One Transformer Can Understand Both 2D & 3D Molecular Data",
      "summary": "Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. When the input data is in a particular format, the corresponding channel will be activated, and the other will be disabled. By training on 2D and 3D molecular data with properly designed supervised signals, Transformer-M automatically learns to leverage knowledge from different data modalities and correctly capture the representations. We conducted extensive experiments for Transformer-M. All empirical results show that Transformer-M can simultaneously achieve strong performance on 2D and 3D tasks, suggesting its broad applicability. The code and models will be made publicly available at https://github.com/lsj2408/Transformer-M.",
      "authors": [
        "Shengjie Luo",
        "Tianlang Chen",
        "Yixian Xu",
        "Shuxin Zheng",
        "Tie-Yan Liu",
        "Liwei Wang",
        "Di He"
      ],
      "published": "2022-10-04T17:30:31Z",
      "updated": "2023-03-28T03:01:29Z",
      "categories": [
        "cs.LG",
        "q-bio.BM",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.01765v4",
      "landing_url": "https://arxiv.org/abs/2210.01765v4",
      "doi": "https://doi.org/10.48550/arXiv.2210.01765"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on transformer for 2D/3D molecular representations rather than discrete audio token creation, quantization, or token modeling, so it plainly fails the inclusion topic (audio tokens) and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on transformer for 2D/3D molecular representations rather than discrete audio token creation, quantization, or token modeling, so it plainly fails the inclusion topic (audio tokens) and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Unified Encoder-Decoder Framework with Entity Memory",
    "abstract": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
    "metadata": {
      "arxiv_id": "2210.03273",
      "title": "A Unified Encoder-Decoder Framework with Entity Memory",
      "summary": "Entities, as important carriers of real-world knowledge, play a key role in many NLP tasks. We focus on incorporating entity knowledge into an encoder-decoder framework for informative text generation. Existing approaches tried to index, retrieve, and read external documents as evidence, but they suffered from a large computational overhead. In this work, we propose an encoder-decoder framework with an entity memory, namely EDMem. The entity knowledge is stored in the memory as latent representations, and the memory is pre-trained on Wikipedia along with encoder-decoder parameters. To precisely generate entity names, we design three decoding methods to constrain entity generation by linking entities in the memory. EDMem is a unified framework that can be used on various entity-intensive question answering and generation tasks. Extensive experimental results show that EDMem outperforms both memory-based auto-encoder models and non-memory encoder-decoder models.",
      "authors": [
        "Zhihan Zhang",
        "Wenhao Yu",
        "Chenguang Zhu",
        "Meng Jiang"
      ],
      "published": "2022-10-07T01:15:30Z",
      "updated": "2023-04-24T01:54:08Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.03273v3",
      "landing_url": "https://arxiv.org/abs/2210.03273v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.03273"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe encoder-decoder with entity memory for text/entity generation, not discrete audio tokenization or codec design, so it fails inclusion and matches exclusion (non-audio/text tokens)."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe encoder-decoder with entity memory for text/entity generation, not discrete audio tokenization or codec design, so it fails inclusion and matches exclusion (non-audio/text tokens).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
    "abstract": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
    "metadata": {
      "arxiv_id": "2210.04062",
      "title": "CoBERT: Self-Supervised Speech Representation Learning Through Code Representation Learning",
      "summary": "Speech is the surface form of a finite set of phonetic units, which can be represented by discrete codes. We propose the Code BERT (CoBERT) approach for self-supervised speech representation learning. The idea is to convert an utterance to a sequence of discrete codes, and perform code representation learning, where we predict the code representations based on a masked view of the original speech input. Unlike the prior self-distillation approaches of which the teacher and the student are of the same modality, our target model predicts representations from a different modality. CoBERT outperforms the most recent state-of-the-art performance on the ASR task and brings significant improvements on the SUPERB speech translation (ST) task. Our code and models are released at https://github.com/mct10/CoBERT.",
      "authors": [
        "Chutong Meng",
        "Junyi Ao",
        "Tom Ko",
        "Mingxuan Wang",
        "Haizhou Li"
      ],
      "published": "2022-10-08T17:15:46Z",
      "updated": "2023-07-05T16:30:48Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.04062v3",
      "landing_url": "https://arxiv.org/abs/2210.04062v3",
      "doi": "https://doi.org/10.48550/arXiv.2210.04062"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "CoBERT explicitly converts speech into discrete codes and performs code representation learning for downstream ASR/ST tasks, which addresses the discrete audio token generation/representation focus and provides evaluation of downstream benefits, so it fully meets the inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "CoBERT explicitly converts speech into discrete codes and performs code representation learning for downstream ASR/ST tasks, which addresses the discrete audio token generation/representation focus and provides evaluation of downstream benefits, so it fully meets the inclusion requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarial Speaker-Consistency Learning Using Untranscribed Speech Data for Zero-Shot Multi-Speaker Text-to-Speech",
    "abstract": "Several recently proposed text-to-speech (TTS) models achieved to generate the speech samples with the human-level quality in the single-speaker and multi-speaker TTS scenarios with a set of pre-defined speakers. However, synthesizing a new speaker's voice with a single reference audio, commonly known as zero-shot multi-speaker text-to-speech (ZSM-TTS), is still a very challenging task. The main challenge of ZSM-TTS is the speaker domain shift problem upon the speech generation of a new speaker. To mitigate this problem, we propose adversarial speaker-consistency learning (ASCL). The proposed method first generates an additional speech of a query speaker using the external untranscribed datasets at each training iteration. Then, the model learns to consistently generate the speech sample of the same speaker as the corresponding speaker embedding vector by employing an adversarial learning scheme. The experimental results show that the proposed method is effective compared to the baseline in terms of the quality and speaker similarity in ZSM-TTS.",
    "metadata": {
      "arxiv_id": "2210.05979",
      "title": "Adversarial Speaker-Consistency Learning Using Untranscribed Speech Data for Zero-Shot Multi-Speaker Text-to-Speech",
      "summary": "Several recently proposed text-to-speech (TTS) models achieved to generate the speech samples with the human-level quality in the single-speaker and multi-speaker TTS scenarios with a set of pre-defined speakers. However, synthesizing a new speaker's voice with a single reference audio, commonly known as zero-shot multi-speaker text-to-speech (ZSM-TTS), is still a very challenging task. The main challenge of ZSM-TTS is the speaker domain shift problem upon the speech generation of a new speaker. To mitigate this problem, we propose adversarial speaker-consistency learning (ASCL). The proposed method first generates an additional speech of a query speaker using the external untranscribed datasets at each training iteration. Then, the model learns to consistently generate the speech sample of the same speaker as the corresponding speaker embedding vector by employing an adversarial learning scheme. The experimental results show that the proposed method is effective compared to the baseline in terms of the quality and speaker similarity in ZSM-TTS.",
      "authors": [
        "Byoung Jin Choi",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Sung Hwan Mun",
        "Nam Soo Kim"
      ],
      "published": "2022-10-12T07:40:15Z",
      "updated": "2022-11-22T06:09:23Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.05979v2",
      "landing_url": "https://arxiv.org/abs/2210.05979v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.05979"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on adversarial speaker-consistency learning for zero-shot multi-speaker TTS using continuous speaker embeddings and does not describe any discrete audio-token quantization or tokenizer design, so it does not meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on adversarial speaker-consistency learning for zero-shot multi-speaker TTS using continuous speaker embeddings and does not describe any discrete audio-token quantization or tokenizer design, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
    "abstract": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
    "metadata": {
      "arxiv_id": "2210.06007",
      "title": "JukeDrummer: Conditional Beat-aware Audio-domain Drum Accompaniment Generation via Transformer VQ-VAE",
      "summary": "This paper proposes a model that generates a drum track in the audio domain to play along to a user-provided drum-free recording. Specifically, using paired data of drumless tracks and the corresponding human-made drum tracks, we train a Transformer model to improvise the drum part of an unseen drumless recording. We combine two approaches to encode the input audio. First, we train a vector-quantized variational autoencoder (VQ-VAE) to represent the input audio with discrete codes, which can then be readily used in a Transformer. Second, using an audio-domain beat tracking model, we compute beat-related features of the input audio and use them as embeddings in the Transformer. Instead of generating the drum track directly as waveforms, we use a separate VQ-VAE to encode the mel-spectrogram of a drum track into another set of discrete codes, and train the Transformer to predict the sequence of drum-related discrete codes. The output codes are then converted to a mel-spectrogram with a decoder, and then to the waveform with a vocoder. We report both objective and subjective evaluations of variants of the proposed model, demonstrating that the model with beat information generates drum accompaniment that is rhythmically and stylistically consistent with the input audio.",
      "authors": [
        "Yueh-Kao Wu",
        "Ching-Yu Chiu",
        "Yi-Hsuan Yang"
      ],
      "published": "2022-10-12T08:23:20Z",
      "updated": "2022-10-31T08:54:08Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.06007v2",
      "landing_url": "https://arxiv.org/abs/2210.06007v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.06007"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper trains VQ-VAE encoders to quantize audio into discrete codes and uses those tokens with a Transformer to model/generate drum accompaniment, satisfying the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Paper trains VQ-VAE encoders to quantize audio into discrete codes and uses those tokens with a Transformer to model/generate drum accompaniment, satisfying the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Frame Rate Up-Conversion Using Key Point Agnostic Frequency-Selective Mesh-to-Grid Resampling",
    "abstract": "High frame rates are desired in many fields of application. As in many cases the frame repetition rate of an already captured video has to be increased, frame rate up-conversion (FRUC) is of high interest. We conduct a motion compensated approach. From two neighboring frames, the motion is estimated and the neighboring pixels are shifted along the motion vector into the frame to be reconstructed. For displaying, these irregularly distributed mesh pixels have to be resampled onto regularly spaced grid positions. We use the model-based key point agnostic frequency-selective mesh-to-grid resampling (AFSMR) for this task and show that AFSMR works best for applications that contain irregular meshes with varying densities. AFSMR gains up to 3.2 dB in contrast to the already high performing frequency-selective mesh-to-grid resampling (FSMR). Additionally, AFSMR increases the run time by a factor of 11 relative to FSMR.",
    "metadata": {
      "arxiv_id": "2210.10444",
      "title": "Frame Rate Up-Conversion Using Key Point Agnostic Frequency-Selective Mesh-to-Grid Resampling",
      "summary": "High frame rates are desired in many fields of application. As in many cases the frame repetition rate of an already captured video has to be increased, frame rate up-conversion (FRUC) is of high interest. We conduct a motion compensated approach. From two neighboring frames, the motion is estimated and the neighboring pixels are shifted along the motion vector into the frame to be reconstructed. For displaying, these irregularly distributed mesh pixels have to be resampled onto regularly spaced grid positions. We use the model-based key point agnostic frequency-selective mesh-to-grid resampling (AFSMR) for this task and show that AFSMR works best for applications that contain irregular meshes with varying densities. AFSMR gains up to 3.2 dB in contrast to the already high performing frequency-selective mesh-to-grid resampling (FSMR). Additionally, AFSMR increases the run time by a factor of 11 relative to FSMR.",
      "authors": [
        "Viktoria Heimann",
        "Andreas Spruck",
        "André Kaup"
      ],
      "published": "2022-10-19T10:23:14Z",
      "updated": "2022-10-19T10:23:14Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.10444v1",
      "landing_url": "https://arxiv.org/abs/2210.10444v1",
      "doi": "https://doi.org/10.1109/ICASSP39728.2021.9413684"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work focuses on motion-compensated video frame rate up-conversion with mesh-to-grid resampling and has no relation to discrete audio tokens or tokenization, so it fails every inclusion and hits exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This work focuses on motion-compensated video frame rate up-conversion with mesh-to-grid resampling and has no relation to discrete audio tokens or tokenization, so it fails every inclusion and hits exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-Supervised Learning via Maximum Entropy Coding",
    "abstract": "A mainstream type of current self-supervised learning methods pursues a general-purpose representation that can be well transferred to downstream tasks, typically by optimizing on a given pretext task such as instance discrimination. In this work, we argue that existing pretext tasks inevitably introduce biases into the learned representation, which in turn leads to biased transfer performance on various downstream tasks. To cope with this issue, we propose Maximum Entropy Coding (MEC), a more principled objective that explicitly optimizes on the structure of the representation, so that the learned representation is less biased and thus generalizes better to unseen downstream tasks. Inspired by the principle of maximum entropy in information theory, we hypothesize that a generalizable representation should be the one that admits the maximum entropy among all plausible representations. To make the objective end-to-end trainable, we propose to leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for the entropy, and further derive a scalable reformulation of the objective that allows fast computation. Extensive experiments demonstrate that MEC learns a more generalizable representation than previous methods based on specific pretext tasks. It achieves state-of-the-art performance consistently on various downstream tasks, including not only ImageNet linear probe, but also semi-supervised classification, object detection, instance segmentation, and object tracking. Interestingly, we show that existing batch-wise and feature-wise self-supervised objectives could be seen equivalent to low-order approximations of MEC. Code and pre-trained models are available at https://github.com/xinliu20/MEC.",
    "metadata": {
      "arxiv_id": "2210.11464",
      "title": "Self-Supervised Learning via Maximum Entropy Coding",
      "summary": "A mainstream type of current self-supervised learning methods pursues a general-purpose representation that can be well transferred to downstream tasks, typically by optimizing on a given pretext task such as instance discrimination. In this work, we argue that existing pretext tasks inevitably introduce biases into the learned representation, which in turn leads to biased transfer performance on various downstream tasks. To cope with this issue, we propose Maximum Entropy Coding (MEC), a more principled objective that explicitly optimizes on the structure of the representation, so that the learned representation is less biased and thus generalizes better to unseen downstream tasks. Inspired by the principle of maximum entropy in information theory, we hypothesize that a generalizable representation should be the one that admits the maximum entropy among all plausible representations. To make the objective end-to-end trainable, we propose to leverage the minimal coding length in lossy data coding as a computationally tractable surrogate for the entropy, and further derive a scalable reformulation of the objective that allows fast computation. Extensive experiments demonstrate that MEC learns a more generalizable representation than previous methods based on specific pretext tasks. It achieves state-of-the-art performance consistently on various downstream tasks, including not only ImageNet linear probe, but also semi-supervised classification, object detection, instance segmentation, and object tracking. Interestingly, we show that existing batch-wise and feature-wise self-supervised objectives could be seen equivalent to low-order approximations of MEC. Code and pre-trained models are available at https://github.com/xinliu20/MEC.",
      "authors": [
        "Xin Liu",
        "Zhongdao Wang",
        "Yali Li",
        "Shengjin Wang"
      ],
      "published": "2022-10-20T17:58:30Z",
      "updated": "2022-10-20T17:58:30Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.11464v1",
      "landing_url": "https://arxiv.org/abs/2210.11464v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.11464"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describes general self-supervised learning for image representations and makes no mention of discrete audio tokens, codec quantization, or tokenization, so it does not meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describes general self-supervised learning for image representations and makes no mention of discrete audio tokens, codec quantization, or tokenization, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
    "abstract": "Encoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks. For machine translation, despite the evolution from long short-term memory networks to Transformer networks, plus the introduction and development of attention mechanism, encoder-decoder is still the de facto neural network architecture for state-of-the-art models. While the motivation for decoding information from some hidden space is straightforward, the strict separation of the encoding and decoding steps into an encoder and a decoder in the model architecture is not necessarily a must. Compared to the task of autoregressive language modeling in the target language, machine translation simply has an additional source sentence as context. Given the fact that neural language models nowadays can already handle rather long contexts in the target language, it is natural to ask whether simply concatenating the source and target sentences and training a language model to do translation would work. In this work, we investigate the aforementioned concept for machine translation. Specifically, we experiment with bilingual translation, translation with additional target monolingual data, and multilingual translation. In all cases, this alternative approach performs on par with the baseline encoder-decoder Transformer, suggesting that an encoder-decoder architecture might be redundant for neural machine translation.",
    "metadata": {
      "arxiv_id": "2210.11807",
      "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?",
      "summary": "Encoder-decoder architecture is widely adopted for sequence-to-sequence modeling tasks. For machine translation, despite the evolution from long short-term memory networks to Transformer networks, plus the introduction and development of attention mechanism, encoder-decoder is still the de facto neural network architecture for state-of-the-art models. While the motivation for decoding information from some hidden space is straightforward, the strict separation of the encoding and decoding steps into an encoder and a decoder in the model architecture is not necessarily a must. Compared to the task of autoregressive language modeling in the target language, machine translation simply has an additional source sentence as context. Given the fact that neural language models nowadays can already handle rather long contexts in the target language, it is natural to ask whether simply concatenating the source and target sentences and training a language model to do translation would work. In this work, we investigate the aforementioned concept for machine translation. Specifically, we experiment with bilingual translation, translation with additional target monolingual data, and multilingual translation. In all cases, this alternative approach performs on par with the baseline encoder-decoder Transformer, suggesting that an encoder-decoder architecture might be redundant for neural machine translation.",
      "authors": [
        "Yingbo Gao",
        "Christian Herold",
        "Zijian Yang",
        "Hermann Ney"
      ],
      "published": "2022-10-21T08:33:55Z",
      "updated": "2022-10-21T08:33:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.11807v1",
      "landing_url": "https://arxiv.org/abs/2210.11807v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.11807"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract discuss encoder-decoder vs language-model paradigms for machine translation and mention neither audio signals nor discrete audio tokenization, so it fails the topic’s core inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract discuss encoder-decoder vs language-model paradigms for machine translation and mention neither audio signals nor discrete audio tokenization, so it fails the topic’s core inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Permutation Invariant Training for Universal Sound Separation",
    "abstract": "Universal sound separation consists of separating mixes with arbitrary sounds of different types, and permutation invariant training (PIT) is used to train source agnostic models that do so. In this work, we complement PIT with adversarial losses but find it challenging with the standard formulation used in speech source separation. We overcome this challenge with a novel I-replacement context-based adversarial loss, and by training with multiple discriminators. Our experiments show that by simply improving the loss (keeping the same model and dataset) we obtain a non-negligible improvement of 1.4 dB SI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be effective at reducing spectral holes, ubiquitous in mask-based separation models, which highlights the potential relevance of adversarial losses for source separation.",
    "metadata": {
      "arxiv_id": "2210.12108",
      "title": "Adversarial Permutation Invariant Training for Universal Sound Separation",
      "summary": "Universal sound separation consists of separating mixes with arbitrary sounds of different types, and permutation invariant training (PIT) is used to train source agnostic models that do so. In this work, we complement PIT with adversarial losses but find it challenging with the standard formulation used in speech source separation. We overcome this challenge with a novel I-replacement context-based adversarial loss, and by training with multiple discriminators. Our experiments show that by simply improving the loss (keeping the same model and dataset) we obtain a non-negligible improvement of 1.4 dB SI-SNRi in the reverberant FUSS dataset. We also find adversarial PIT to be effective at reducing spectral holes, ubiquitous in mask-based separation models, which highlights the potential relevance of adversarial losses for source separation.",
      "authors": [
        "Emilian Postolache",
        "Jordi Pons",
        "Santiago Pascual",
        "Joan Serrà"
      ],
      "published": "2022-10-21T17:04:17Z",
      "updated": "2023-03-06T13:59:42Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12108v2",
      "landing_url": "https://arxiv.org/abs/2210.12108v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.12108"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on improving adversarial permutation invariant training for sound separation without discussing discrete audio tokens/quantization or any tokenizer/token-level modeling, so it violates the inclusion requirements and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on improving adversarial permutation invariant training for sound separation without discussing discrete audio tokens/quantization or any tokenizer/token-level modeling, so it violates the inclusion requirements and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices",
    "abstract": "Real-time object pose estimation and tracking is challenging but essential for emerging augmented reality (AR) applications. In general, state-of-the-art methods address this problem using deep neural networks which indeed yield satisfactory results. Nevertheless, the high computational cost of these methods makes them unsuitable for mobile devices where real-world applications usually take place. In addition, head-mounted displays such as AR glasses require at least 90~FPS to avoid motion sickness, which further complicates the problem. We propose a flexible-frame-rate object pose estimation and tracking system for mobile devices. It is a monocular visual-inertial-based system with a client-server architecture. Inertial measurement unit (IMU) pose propagation is performed on the client side for high speed tracking, and RGB image-based 3D pose estimation is performed on the server side to obtain accurate poses, after which the pose is sent to the client side for visual-inertial fusion, where we propose a bias self-correction mechanism to reduce drift. We also propose a pose inspection algorithm to detect tracking failures and incorrect pose estimation. Connected by high-speed networking, our system supports flexible frame rates up to 120 FPS and guarantees high precision and real-time tracking on low-end devices. Both simulations and real world experiments show that our method achieves accurate and robust object tracking.",
    "metadata": {
      "arxiv_id": "2210.12476",
      "title": "A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for Mobile Devices",
      "summary": "Real-time object pose estimation and tracking is challenging but essential for emerging augmented reality (AR) applications. In general, state-of-the-art methods address this problem using deep neural networks which indeed yield satisfactory results. Nevertheless, the high computational cost of these methods makes them unsuitable for mobile devices where real-world applications usually take place. In addition, head-mounted displays such as AR glasses require at least 90~FPS to avoid motion sickness, which further complicates the problem. We propose a flexible-frame-rate object pose estimation and tracking system for mobile devices. It is a monocular visual-inertial-based system with a client-server architecture. Inertial measurement unit (IMU) pose propagation is performed on the client side for high speed tracking, and RGB image-based 3D pose estimation is performed on the server side to obtain accurate poses, after which the pose is sent to the client side for visual-inertial fusion, where we propose a bias self-correction mechanism to reduce drift. We also propose a pose inspection algorithm to detect tracking failures and incorrect pose estimation. Connected by high-speed networking, our system supports flexible frame rates up to 120 FPS and guarantees high precision and real-time tracking on low-end devices. Both simulations and real world experiments show that our method achieves accurate and robust object tracking.",
      "authors": [
        "Yo-Chung Lau",
        "Kuan-Wei Tseng",
        "I-Ju Hsieh",
        "Hsiao-Ching Tseng",
        "Yi-Ping Hung"
      ],
      "published": "2022-10-22T15:26:50Z",
      "updated": "2022-10-22T15:26:50Z",
      "categories": [
        "cs.CV",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.12476v1",
      "landing_url": "https://arxiv.org/abs/2210.12476v1",
      "doi": "https://doi.org/10.1109/JISPIN.2023.3340987"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on visual-inertial object tracking for AR devices with no discrete audio token generation, quantization, or tokenizer discussion, so it fails to meet any inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on visual-inertial object tracking for AR devices with no discrete audio token generation, quantization, or tokenizer discussion, so it fails to meet any inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-rate adaptive transform coding for video compression",
    "abstract": "Contemporary lossy image and video coding standards rely on transform coding, the process through which pixels are mapped to an alternative representation to facilitate efficient data compression. Despite impressive performance of end-to-end optimized compression with deep neural networks, the high computational and space demands of these models has prevented them from superseding the relatively simple transform coding found in conventional video codecs. In this study, we propose learned transforms and entropy coding that may either serve as (non)linear drop-in replacements, or enhancements for linear transforms in existing codecs. These transforms can be multi-rate, allowing a single model to operate along the entire rate-distortion curve. To demonstrate the utility of our framework, we augmented the DCT with learned quantization matrices and adaptive entropy coding to compress intra-frame AV1 block prediction residuals. We report substantial BD-rate and perceptual quality improvements over more complex nonlinear transforms at a fraction of the computational cost.",
    "metadata": {
      "arxiv_id": "2210.14308",
      "title": "Multi-rate adaptive transform coding for video compression",
      "summary": "Contemporary lossy image and video coding standards rely on transform coding, the process through which pixels are mapped to an alternative representation to facilitate efficient data compression. Despite impressive performance of end-to-end optimized compression with deep neural networks, the high computational and space demands of these models has prevented them from superseding the relatively simple transform coding found in conventional video codecs. In this study, we propose learned transforms and entropy coding that may either serve as (non)linear drop-in replacements, or enhancements for linear transforms in existing codecs. These transforms can be multi-rate, allowing a single model to operate along the entire rate-distortion curve. To demonstrate the utility of our framework, we augmented the DCT with learned quantization matrices and adaptive entropy coding to compress intra-frame AV1 block prediction residuals. We report substantial BD-rate and perceptual quality improvements over more complex nonlinear transforms at a fraction of the computational cost.",
      "authors": [
        "Lyndon R. Duong",
        "Bohan Li",
        "Cheng Chen",
        "Jingning Han"
      ],
      "published": "2022-10-25T20:11:42Z",
      "updated": "2023-02-18T00:10:04Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.14308v2",
      "landing_url": "https://arxiv.org/abs/2210.14308v2",
      "doi": "https://doi.org/10.48550/arXiv.2210.14308"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about learned video transform coding for AV1 residuals, so it neither focuses on discrete audio tokens nor describes codec/token quantization for audio, hence it fails all inclusion requirements and matches the exclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper is about learned video transform coding for AV1 residuals, so it neither focuses on discrete audio tokens nor describes codec/token quantization for audio, hence it fails all inclusion requirements and matches the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Trade-off between reconstruction loss and feature alignment for domain generalization",
    "abstract": "Domain generalization (DG) is a branch of transfer learning that aims to train the learning models on several seen domains and subsequently apply these pre-trained models to other unseen (unknown but related) domains. To deal with challenging settings in DG where both data and label of the unseen domain are not available at training time, the most common approach is to design the classifiers based on the domain-invariant representation features, i.e., the latent representations that are unchanged and transferable between domains. Contrary to popular belief, we show that designing classifiers based on invariant representation features alone is necessary but insufficient in DG. Our analysis indicates the necessity of imposing a constraint on the reconstruction loss induced by representation functions to preserve most of the relevant information about the label in the latent space. More importantly, we point out the trade-off between minimizing the reconstruction loss and achieving domain alignment in DG. Our theoretical results motivate a new DG framework that jointly optimizes the reconstruction loss and the domain discrepancy. Both theoretical and numerical results are provided to justify our approach.",
    "metadata": {
      "arxiv_id": "2210.15000",
      "title": "Trade-off between reconstruction loss and feature alignment for domain generalization",
      "summary": "Domain generalization (DG) is a branch of transfer learning that aims to train the learning models on several seen domains and subsequently apply these pre-trained models to other unseen (unknown but related) domains. To deal with challenging settings in DG where both data and label of the unseen domain are not available at training time, the most common approach is to design the classifiers based on the domain-invariant representation features, i.e., the latent representations that are unchanged and transferable between domains. Contrary to popular belief, we show that designing classifiers based on invariant representation features alone is necessary but insufficient in DG. Our analysis indicates the necessity of imposing a constraint on the reconstruction loss induced by representation functions to preserve most of the relevant information about the label in the latent space. More importantly, we point out the trade-off between minimizing the reconstruction loss and achieving domain alignment in DG. Our theoretical results motivate a new DG framework that jointly optimizes the reconstruction loss and the domain discrepancy. Both theoretical and numerical results are provided to justify our approach.",
      "authors": [
        "Thuan Nguyen",
        "Boyang Lyu",
        "Prakash Ishwar",
        "Matthias Scheutz",
        "Shuchin Aeron"
      ],
      "published": "2022-10-26T19:40:25Z",
      "updated": "2022-10-26T19:40:25Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15000v1",
      "landing_url": "https://arxiv.org/abs/2210.15000v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.15000"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on domain generalization and invariant representations rather than discrete audio tokenization, so it fails the inclusion criteria and should be excluded (1)."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on domain generalization and invariant representations rather than discrete audio tokenization, so it fails the inclusion criteria and should be excluded (1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
    "abstract": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
    "metadata": {
      "arxiv_id": "2210.15759",
      "title": "Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge",
      "summary": "Recent progress in self-supervised or unsupervised machine learning has opened the possibility of building a full speech processing system from raw audio without using any textual representations or expert labels such as phonemes, dictionaries or parse trees. The contribution of the Zero Resource Speech Challenge series since 2015 has been to break down this long-term objective into four well-defined tasks -- Acoustic Unit Discovery, Spoken Term Discovery, Discrete Resynthesis, and Spoken Language Modeling -- and introduce associated metrics and benchmarks enabling model comparison and cumulative progress. We present an overview of the six editions of this challenge series since 2015, discuss the lessons learned, and outline the areas which need more work or give puzzling results.",
      "authors": [
        "Ewan Dunbar",
        "Nicolas Hamilakis",
        "Emmanuel Dupoux"
      ],
      "published": "2022-10-27T20:32:41Z",
      "updated": "2022-10-27T20:32:41Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.15759v1",
      "landing_url": "https://arxiv.org/abs/2210.15759v1",
      "doi": "https://doi.org/10.1109/JSTSP.2022.3206084"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Overview of the Zero Resource Speech Challenge chronicles multiple editions focused on discovering and benchmarking discrete acoustic units from raw audio—which aligns with the discrete audio token learning and evaluation criteria—so it is better to include."
    },
    "round-A_JuniorNano_reasoning": "Overview of the Zero Resource Speech Challenge chronicles multiple editions focused on discovering and benchmarking discrete acoustic units from raw audio—which aligns with the discrete audio token learning and evaluation criteria—so it is better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
    "abstract": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
    "metadata": {
      "arxiv_id": "2210.16755",
      "title": "token2vec: A Joint Self-Supervised Pre-training Framework Using Unpaired Speech and Text",
      "summary": "Self-supervised pre-training has been successful in both text and speech processing. Speech and text offer different but complementary information. The question is whether we are able to perform a speech-text joint pre-training on unpaired speech and text. In this paper, we take the idea of self-supervised pre-training one step further and propose token2vec, a novel joint pre-training framework for unpaired speech and text based on discrete representations of speech. Firstly, due to the distinct characteristics between speech and text modalities, where speech is continuous while text is discrete, we first discretize speech into a sequence of discrete speech tokens to solve the modality mismatch problem. Secondly, to solve the length mismatch problem, where the speech sequence is usually much longer than text sequence, we convert the words of text into phoneme sequences and randomly repeat each phoneme in the sequences. Finally, we feed the discrete speech and text tokens into a modality-agnostic Transformer encoder and pre-train with token-level masking language modeling (tMLM). Experiments show that token2vec is significantly superior to various speech-only pre-training baselines, with up to 17.7% relative WER reduction. Token2vec model is also validated on a non-ASR task, i.e., spoken intent classification, and shows good transferability.",
      "authors": [
        "Xianghu Yue",
        "Junyi Ao",
        "Xiaoxue Gao",
        "Haizhou Li"
      ],
      "published": "2022-10-30T06:38:19Z",
      "updated": "2022-10-30T06:38:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2210.16755v1",
      "landing_url": "https://arxiv.org/abs/2210.16755v1",
      "doi": "https://doi.org/10.48550/arXiv.2210.16755"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "token2vec explicitly discretizes speech into finite speech tokens and aligns them with phoneme-based text tokens for joint self-supervised pretraining, and it evaluates token-level modeling (WER reduction and intent classification), so it fits the discrete audio-token inclusion criteria without violating any exclusions."
    },
    "round-A_JuniorNano_reasoning": "token2vec explicitly discretizes speech into finite speech tokens and aligns them with phoneme-based text tokens for joint self-supervised pretraining, and it evaluates token-level modeling (WER reduction and intent classification), so it fits the discrete audio-token inclusion criteria without violating any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio Language Modeling using Perceptually-Guided Discrete Representations",
    "abstract": "In this work, we study the task of Audio Language Modeling, in which we aim at learning probabilistic models for audio that can be used for generation and completion. We use a state-of-the-art perceptually-guided audio compression model, to encode audio to discrete representations. Next, we train a transformer-based causal language model using these representations. At inference time, we perform audio auto-completion by encoding an audio prompt as a discrete sequence, feeding it to the audio language model, sampling from the model, and synthesizing the corresponding time-domain signal. We evaluate the quality of samples generated by our method on Audioset, the largest dataset for general audio to date, and show that it is superior to the evaluated baseline audio encoders. We additionally provide an extensive analysis to better understand the trade-off between audio-quality and language-modeling capabilities. Samples:link.",
    "metadata": {
      "arxiv_id": "2211.01223",
      "title": "Audio Language Modeling using Perceptually-Guided Discrete Representations",
      "summary": "In this work, we study the task of Audio Language Modeling, in which we aim at learning probabilistic models for audio that can be used for generation and completion. We use a state-of-the-art perceptually-guided audio compression model, to encode audio to discrete representations. Next, we train a transformer-based causal language model using these representations. At inference time, we perform audio auto-completion by encoding an audio prompt as a discrete sequence, feeding it to the audio language model, sampling from the model, and synthesizing the corresponding time-domain signal. We evaluate the quality of samples generated by our method on Audioset, the largest dataset for general audio to date, and show that it is superior to the evaluated baseline audio encoders. We additionally provide an extensive analysis to better understand the trade-off between audio-quality and language-modeling capabilities. Samples:link.",
      "authors": [
        "Felix Kreuk",
        "Yaniv Taigman",
        "Adam Polyak",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Alexandre Défossez",
        "Yossi Adi"
      ],
      "published": "2022-11-02T16:02:45Z",
      "updated": "2022-11-04T10:50:00Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.01223v2",
      "landing_url": "https://arxiv.org/abs/2211.01223v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.01223"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper centers on perceptually guided audio compression to produce discrete tokens that are modeled by a transformer for generation/completion, directly matching the discrete audio token inclusion criteria, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on perceptually guided audio compression to produce discrete tokens that are modeled by a transformer for generation/completion, directly matching the discrete audio token inclusion criteria, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Channel-Aware Pretraining of Joint Encoder-Decoder Self-Supervised Model for Telephonic-Speech ASR",
    "abstract": "This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of ~4% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.",
    "metadata": {
      "arxiv_id": "2211.01669",
      "title": "Channel-Aware Pretraining of Joint Encoder-Decoder Self-Supervised Model for Telephonic-Speech ASR",
      "summary": "This paper proposes a novel technique to obtain better downstream ASR performance from a joint encoder-decoder self-supervised model when trained with speech pooled from two different channels (narrow and wide band). The joint encoder-decoder self-supervised model extends the HuBERT model with a Transformer decoder. HuBERT performs clustering of features and predicts the class of every input frame. In simple pooling, which is our baseline, there is no way to identify the channel information. To incorporate channel information, we have proposed non-overlapping cluster IDs for speech from different channels. Our method gives a relative improvement of ~4% over the joint encoder-decoder self-supervised model built with simple pooling of data, which serves as our baseline.",
      "authors": [
        "Vrunda N. Sukhadia",
        "A. Arunkumar",
        "S. Umesh"
      ],
      "published": "2022-11-03T09:27:16Z",
      "updated": "2023-06-03T19:58:03Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.01669v2",
      "landing_url": "https://arxiv.org/abs/2211.01669v2",
      "doi": "https://doi.org/10.48550/arXiv.2211.01669"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The study focuses on channel-aware pretraining of a self-supervised ASR model and does not propose or analyze a discrete audio tokenization/quantization scheme with explicit token vocabularies, so it better be excluded."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on channel-aware pretraining of a self-supervised ASR model and does not propose or analyze a discrete audio tokenization/quantization scheme with explicit token vocabularies, so it better be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
    "abstract": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
    "metadata": {
      "arxiv_id": "2211.11386",
      "title": "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism",
      "summary": "Existing deep calibrated photometric stereo networks basically aggregate observations under different lights based on the pre-defined operations such as linear projection and max pooling. While they are effective with the dense capture, simple first-order operations often fail to capture the high-order interactions among observations under small number of different lights. To tackle this issue, this paper presents a deep sparse calibrated photometric stereo network named {\\it PS-Transformer} which leverages the learnable self-attention mechanism to properly capture the complex inter-image interactions. PS-Transformer builds upon the dual-branch design to explore both pixel-wise and image-wise features and individual feature is trained with the intermediate surface normal supervision to maximize geometric feasibility. A new synthetic dataset named CyclesPS+ is also presented with the comprehensive analysis to successfully train the photometric stereo networks. Extensive results on the publicly available benchmark datasets demonstrate that the surface normal prediction accuracy of the proposed method significantly outperforms other state-of-the-art algorithms with the same number of input images and is even comparable to that of dense algorithms which input 10$\\times$ larger number of images.",
      "authors": [
        "Satoshi Ikehata"
      ],
      "published": "2022-11-21T11:58:25Z",
      "updated": "2022-11-21T11:58:25Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.11386v1",
      "landing_url": "https://arxiv.org/abs/2211.11386v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.11386"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on photometric stereo vision without any discrete audio token/codec content, so it fails all inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on photometric stereo vision without any discrete audio token/codec content, so it fails all inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SRTGAN: Triplet Loss based Generative Adversarial Network for Real-World Super-Resolution",
    "abstract": "Many applications such as forensics, surveillance, satellite imaging, medical imaging, etc., demand High-Resolution (HR) images. However, obtaining an HR image is not always possible due to the limitations of optical sensors and their costs. An alternative solution called Single Image Super-Resolution (SISR) is a software-driven approach that aims to take a Low-Resolution (LR) image and obtain the HR image. Most supervised SISR solutions use ground truth HR image as a target and do not include the information provided in the LR image, which could be valuable. In this work, we introduce Triplet Loss-based Generative Adversarial Network hereafter referred as SRTGAN for Image Super-Resolution problem on real-world degradation. We introduce a new triplet-based adversarial loss function that exploits the information provided in the LR image by using it as a negative sample. Allowing the patch-based discriminator with access to both HR and LR images optimizes to better differentiate between HR and LR images; hence, improving the adversary. Further, we propose to fuse the adversarial loss, content loss, perceptual loss, and quality loss to obtain Super-Resolution (SR) image with high perceptual fidelity. We validate the superior performance of the proposed method over the other existing methods on the RealSR dataset in terms of quantitative and qualitative metrics.",
    "metadata": {
      "arxiv_id": "2211.12180",
      "title": "SRTGAN: Triplet Loss based Generative Adversarial Network for Real-World Super-Resolution",
      "summary": "Many applications such as forensics, surveillance, satellite imaging, medical imaging, etc., demand High-Resolution (HR) images. However, obtaining an HR image is not always possible due to the limitations of optical sensors and their costs. An alternative solution called Single Image Super-Resolution (SISR) is a software-driven approach that aims to take a Low-Resolution (LR) image and obtain the HR image. Most supervised SISR solutions use ground truth HR image as a target and do not include the information provided in the LR image, which could be valuable. In this work, we introduce Triplet Loss-based Generative Adversarial Network hereafter referred as SRTGAN for Image Super-Resolution problem on real-world degradation. We introduce a new triplet-based adversarial loss function that exploits the information provided in the LR image by using it as a negative sample. Allowing the patch-based discriminator with access to both HR and LR images optimizes to better differentiate between HR and LR images; hence, improving the adversary. Further, we propose to fuse the adversarial loss, content loss, perceptual loss, and quality loss to obtain Super-Resolution (SR) image with high perceptual fidelity. We validate the superior performance of the proposed method over the other existing methods on the RealSR dataset in terms of quantitative and qualitative metrics.",
      "authors": [
        "Dhruv Patel",
        "Abhinav Jain",
        "Simran Bawkar",
        "Manav Khorasiya",
        "Kalpesh Prajapati",
        "Kishor Upla",
        "Kiran Raja",
        "Raghavendra Ramachandra",
        "Christoph Busch"
      ],
      "published": "2022-11-22T11:17:07Z",
      "updated": "2022-11-22T11:17:07Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12180v1",
      "landing_url": "https://arxiv.org/abs/2211.12180v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.12180"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about image super-resolution GANs, which neither discusses nor relies on discrete audio tokens or vocabularies, so it fails all topic inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper is about image super-resolution GANs, which neither discusses nor relies on discrete audio tokens or vocabularies, so it fails all topic inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time",
    "abstract": "This work focuses on low bitrate video streaming scenarios (e.g. 50 - 200Kbps) where the video quality is severely compromised. We present a family of novel deep generative models for enhancing perceptual video quality of such streams by performing super-resolution while also removing compression artifacts. Our model, which we call SuperTran, consumes as input a single high-quality, high-resolution reference images in addition to the low-quality, low-resolution video stream. The model thus learns how to borrow or copy visual elements like textures from the reference image and fill in the remaining details from the low resolution stream in order to produce perceptually enhanced output video. The reference frame can be sent once at the start of the video session or be retrieved from a gallery. Importantly, the resulting output has substantially better detail than what has been otherwise possible with methods that only use a low resolution input such as the SuperVEGAN method. SuperTran works in real-time (up to 30 frames/sec) on the cloud alongside standard pipelines.",
    "metadata": {
      "arxiv_id": "2211.12604",
      "title": "SuperTran: Reference Based Video Transformer for Enhancing Low Bitrate Streams in Real Time",
      "summary": "This work focuses on low bitrate video streaming scenarios (e.g. 50 - 200Kbps) where the video quality is severely compromised. We present a family of novel deep generative models for enhancing perceptual video quality of such streams by performing super-resolution while also removing compression artifacts. Our model, which we call SuperTran, consumes as input a single high-quality, high-resolution reference images in addition to the low-quality, low-resolution video stream. The model thus learns how to borrow or copy visual elements like textures from the reference image and fill in the remaining details from the low resolution stream in order to produce perceptually enhanced output video. The reference frame can be sent once at the start of the video session or be retrieved from a gallery. Importantly, the resulting output has substantially better detail than what has been otherwise possible with methods that only use a low resolution input such as the SuperVEGAN method. SuperTran works in real-time (up to 30 frames/sec) on the cloud alongside standard pipelines.",
      "authors": [
        "Tejas Khot",
        "Nataliya Shapovalova",
        "Silviu Andrei",
        "Walterio Mayol-Cuevas"
      ],
      "published": "2022-11-22T22:03:11Z",
      "updated": "2022-11-22T22:03:11Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.12604v1",
      "landing_url": "https://arxiv.org/abs/2211.12604v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.12604"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on reference-based video super-resolution at low bitrates with no mention of discrete audio-tokenization, so it fails to meet any inclusion criteria centered on audio discrete tokens."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on reference-based video super-resolution at low bitrates with no mention of discrete audio-tokenization, so it fails to meet any inclusion criteria centered on audio discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Homology-constrained vector quantization entropy regularizer",
    "abstract": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
    "metadata": {
      "arxiv_id": "2211.14363",
      "title": "Homology-constrained vector quantization entropy regularizer",
      "summary": "This paper describes an entropy regularization term for vector quantization (VQ) based on the analysis of persistent homology of the VQ embeddings. Higher embedding entropy positively correlates with higher codebook utilization, mitigating overfit towards the identity and codebook collapse in VQ-based autoencoders [1]. We show that homology-constrained regularization is an effective way to increase entropy of the VQ process (approximated to input entropy) while preserving the approximated topology in the quantized latent space, averaged over mini batches. This work further explores some patterns of persistent homology diagrams of latents formed by vector quantization. We implement and test the proposed algorithm as a module integrated into a sample VQ-VAE. Linked code repository provides a functioning implementation of the proposed architecture, referred to as homology-constrained vector quantization (HC-VQ) further in this work.",
      "authors": [
        "Ivan Volkov"
      ],
      "published": "2022-11-25T20:09:22Z",
      "updated": "2022-11-25T20:09:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14363v1",
      "landing_url": "https://arxiv.org/abs/2211.14363v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14363"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on a homology-constrained entropy regularizer for vector quantization embeddings in autoencoders without addressing discrete audio token generation, tokenizer/codebook specs, or audio-specific evaluation, so it fails the inclusion criteria and warrants exclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a homology-constrained entropy regularizer for vector quantization embeddings in autoencoders without addressing discrete audio token generation, tokenizer/codebook specs, or audio-specific evaluation, so it fails the inclusion criteria and warrants exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Contextual Expressive Text-to-Speech",
    "abstract": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
    "metadata": {
      "arxiv_id": "2211.14548",
      "title": "Contextual Expressive Text-to-Speech",
      "summary": "The goal of expressive Text-to-speech (TTS) is to synthesize natural speech with desired content, prosody, emotion, or timbre, in high expressiveness. Most of previous studies attempt to generate speech from given labels of styles and emotions, which over-simplifies the problem by classifying styles and emotions into a fixed number of pre-defined categories. In this paper, we introduce a new task setting, Contextual TTS (CTTS). The main idea of CTTS is that how a person speaks depends on the particular context she is in, where the context can typically be represented as text. Thus, in the CTTS task, we propose to utilize such context to guide the speech synthesis process instead of relying on explicit labels of styles and emotions. To achieve this task, we construct a synthetic dataset and develop an effective framework. Experiments show that our framework can generate high-quality expressive speech based on the given context both in synthetic datasets and real-world scenarios.",
      "authors": [
        "Jianhong Tu",
        "Zeyu Cui",
        "Xiaohuan Zhou",
        "Siqi Zheng",
        "Kai Hu",
        "Ju Fan",
        "Chang Zhou"
      ],
      "published": "2022-11-26T12:06:21Z",
      "updated": "2022-11-26T12:06:21Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14548v1",
      "landing_url": "https://arxiv.org/abs/2211.14548v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14548"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discussion of discrete audio tokenization, quantization, or vocabularies—focus is on context-guided expressive TTS, so it fails to meet inclusion about token generation."
    },
    "round-A_JuniorNano_reasoning": "No discussion of discrete audio tokenization, quantization, or vocabularies—focus is on context-guided expressive TTS, so it fails to meet inclusion about token generation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Rademacher Complexity of Deep Neural Networks",
    "abstract": "Deep neural networks are vulnerable to adversarial attacks. Ideally, a robust model shall perform well on both the perturbed training data and the unseen perturbed test data. It is found empirically that fitting perturbed training data is not hard, but generalizing to perturbed test data is quite difficult. To better understand adversarial generalization, it is of great interest to study the adversarial Rademacher complexity (ARC) of deep neural networks. However, how to bound ARC in multi-layers cases is largely unclear due to the difficulty of analyzing adversarial loss in the definition of ARC. There have been two types of attempts of ARC. One is to provide the upper bound of ARC in linear and one-hidden layer cases. However, these approaches seem hard to extend to multi-layer cases. Another is to modify the adversarial loss and provide upper bounds of Rademacher complexity on such surrogate loss in multi-layer cases. However, such variants of Rademacher complexity are not guaranteed to be bounds for meaningful robust generalization gaps (RGG). In this paper, we provide a solution to this unsolved problem. Specifically, we provide the first bound of adversarial Rademacher complexity of deep neural networks. Our approach is based on covering numbers. We provide a method to handle the robustify function classes of DNNs such that we can calculate the covering numbers. Finally, we provide experiments to study the empirical implication of our bounds and provide an analysis of poor adversarial generalization.",
    "metadata": {
      "arxiv_id": "2211.14966",
      "title": "Adversarial Rademacher Complexity of Deep Neural Networks",
      "summary": "Deep neural networks are vulnerable to adversarial attacks. Ideally, a robust model shall perform well on both the perturbed training data and the unseen perturbed test data. It is found empirically that fitting perturbed training data is not hard, but generalizing to perturbed test data is quite difficult. To better understand adversarial generalization, it is of great interest to study the adversarial Rademacher complexity (ARC) of deep neural networks. However, how to bound ARC in multi-layers cases is largely unclear due to the difficulty of analyzing adversarial loss in the definition of ARC. There have been two types of attempts of ARC. One is to provide the upper bound of ARC in linear and one-hidden layer cases. However, these approaches seem hard to extend to multi-layer cases. Another is to modify the adversarial loss and provide upper bounds of Rademacher complexity on such surrogate loss in multi-layer cases. However, such variants of Rademacher complexity are not guaranteed to be bounds for meaningful robust generalization gaps (RGG). In this paper, we provide a solution to this unsolved problem. Specifically, we provide the first bound of adversarial Rademacher complexity of deep neural networks. Our approach is based on covering numbers. We provide a method to handle the robustify function classes of DNNs such that we can calculate the covering numbers. Finally, we provide experiments to study the empirical implication of our bounds and provide an analysis of poor adversarial generalization.",
      "authors": [
        "Jiancong Xiao",
        "Yanbo Fan",
        "Ruoyu Sun",
        "Zhi-Quan Luo"
      ],
      "published": "2022-11-27T23:24:37Z",
      "updated": "2022-11-27T23:24:37Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.14966v1",
      "landing_url": "https://arxiv.org/abs/2211.14966v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.14966"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper studies adversarial Rademacher complexity for deep neural networks, which is unrelated to discrete audio token quantization/tokenizer research, so it fails to meet any inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "This paper studies adversarial Rademacher complexity for deep neural networks, which is unrelated to discrete audio token quantization/tokenizer research, so it fails to meet any inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DBA: Efficient Transformer with Dynamic Bilinear Low-Rank Attention",
    "abstract": "Many studies have been conducted to improve the efficiency of Transformer from quadric to linear. Among them, the low-rank-based methods aim to learn the projection matrices to compress the sequence length. However, the projection matrices are fixed once they have been learned, which compress sequence length with dedicated coefficients for tokens in the same position. Adopting such input-invariant projections ignores the fact that the most informative part of a sequence varies from sequence to sequence, thus failing to preserve the most useful information that lies in varied positions. In addition, previous efficient Transformers only focus on the influence of sequence length while neglecting the effect of hidden state dimension. To address the aforementioned problems, we present an efficient yet effective attention mechanism, namely the Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequence length and hidden state dimension while maintaining state-of-the-art performance. Specifically, we first theoretically demonstrate that the sequence length can be compressed non-destructively from a novel perspective of information theory, with compression matrices dynamically determined by the input sequence. Furthermore, we show that the hidden state dimension can be approximated by extending the Johnson-Lindenstrauss lemma, optimizing the attention in bilinear form. Theoretical analysis shows that DBA is proficient in capturing high-order relations in cross-attention problems. Experiments over tasks with diverse sequence length conditions show that DBA achieves state-of-the-art performance compared with various strong baselines while maintaining less memory consumption with higher speed.",
    "metadata": {
      "arxiv_id": "2211.16368",
      "title": "DBA: Efficient Transformer with Dynamic Bilinear Low-Rank Attention",
      "summary": "Many studies have been conducted to improve the efficiency of Transformer from quadric to linear. Among them, the low-rank-based methods aim to learn the projection matrices to compress the sequence length. However, the projection matrices are fixed once they have been learned, which compress sequence length with dedicated coefficients for tokens in the same position. Adopting such input-invariant projections ignores the fact that the most informative part of a sequence varies from sequence to sequence, thus failing to preserve the most useful information that lies in varied positions. In addition, previous efficient Transformers only focus on the influence of sequence length while neglecting the effect of hidden state dimension. To address the aforementioned problems, we present an efficient yet effective attention mechanism, namely the Dynamic Bilinear Low-Rank Attention (DBA), which compresses the sequence length by input-sensitive dynamic projection matrices and achieves linear time and space complexity by jointly optimizing the sequence length and hidden state dimension while maintaining state-of-the-art performance. Specifically, we first theoretically demonstrate that the sequence length can be compressed non-destructively from a novel perspective of information theory, with compression matrices dynamically determined by the input sequence. Furthermore, we show that the hidden state dimension can be approximated by extending the Johnson-Lindenstrauss lemma, optimizing the attention in bilinear form. Theoretical analysis shows that DBA is proficient in capturing high-order relations in cross-attention problems. Experiments over tasks with diverse sequence length conditions show that DBA achieves state-of-the-art performance compared with various strong baselines while maintaining less memory consumption with higher speed.",
      "authors": [
        "Bosheng Qin",
        "Juncheng Li",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2022-11-24T03:06:36Z",
      "updated": "2022-11-24T03:06:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2211.16368v1",
      "landing_url": "https://arxiv.org/abs/2211.16368v1",
      "doi": "https://doi.org/10.48550/arXiv.2211.16368"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Transformer efficiency work unrelated to discrete audio token generation/projecting; it doesn’t target audio codec tokens or quantized vocabularies, so exclude."
    },
    "round-A_JuniorNano_reasoning": "Transformer efficiency work unrelated to discrete audio token generation/projecting; it doesn’t target audio codec tokens or quantized vocabularies, so exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechLMScore: Evaluating speech generation using speech language model",
    "abstract": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
    "metadata": {
      "arxiv_id": "2212.04559",
      "title": "SpeechLMScore: Evaluating speech generation using speech language model",
      "summary": "While human evaluation is the most reliable metric for evaluating speech generation systems, it is generally costly and time-consuming. Previous studies on automatic speech quality assessment address the problem by predicting human evaluation scores with machine learning models. However, they rely on supervised learning and thus suffer from high annotation costs and domain-shift problems. We propose SpeechLMScore, an unsupervised metric to evaluate generated speech using a speech-language model. SpeechLMScore computes the average log-probability of a speech signal by mapping it into discrete tokens and measures the average probability of generating the sequence of tokens. Therefore, it does not require human annotation and is a highly scalable framework. Evaluation results demonstrate that the proposed metric shows a promising correlation with human evaluation scores on different speech generation tasks including voice conversion, text-to-speech, and speech enhancement.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Takaaki Saeki",
        "Shinji Watanabe"
      ],
      "published": "2022-12-08T21:00:15Z",
      "updated": "2022-12-08T21:00:15Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.04559v1",
      "landing_url": "https://arxiv.org/abs/2212.04559v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.04559"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "This paper proposes an unsupervised metric that scores speech via a language model over discretized tokens, but it does not describe or evaluate how those discrete audio tokens are produced, quantified, or parametrized, so it misses the required focus on token generation/design even though it uses them."
    },
    "round-A_JuniorNano_reasoning": "This paper proposes an unsupervised metric that scores speech via a language model over discretized tokens, but it does not describe or evaluate how those discrete audio tokens are produced, quantified, or parametrized, so it misses the required focus on token generation/design even though it uses them.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Preventing RNN from Using Sequence Length as a Feature",
    "abstract": "Recurrent neural networks are deep learning topologies that can be trained to classify long documents. However, in our recent work, we found a critical problem with these cells: they can use the length differences between texts of different classes as a prominent classification feature. This has the effect of producing models that are brittle and fragile to concept drift, can provide misleading performances and are trivially explainable regardless of text content. This paper illustrates the problem using synthetic and real-world data and provides a simple solution using weight decay regularization.",
    "metadata": {
      "arxiv_id": "2212.08276",
      "title": "Preventing RNN from Using Sequence Length as a Feature",
      "summary": "Recurrent neural networks are deep learning topologies that can be trained to classify long documents. However, in our recent work, we found a critical problem with these cells: they can use the length differences between texts of different classes as a prominent classification feature. This has the effect of producing models that are brittle and fragile to concept drift, can provide misleading performances and are trivially explainable regardless of text content. This paper illustrates the problem using synthetic and real-world data and provides a simple solution using weight decay regularization.",
      "authors": [
        "Jean-Thomas Baillargeon",
        "Hélène Cossette",
        "Luc Lamontagne"
      ],
      "published": "2022-12-16T04:23:36Z",
      "updated": "2022-12-16T04:23:36Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08276v1",
      "landing_url": "https://arxiv.org/abs/2212.08276v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08276"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper about RNN classifying documents and combating length bias has no discrete audio tokenization focus so it fails all inclusion criteria, so exclude it."
    },
    "round-A_JuniorNano_reasoning": "Paper about RNN classifying documents and combating length bias has no discrete audio tokenization focus so it fails all inclusion criteria, so exclude it.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
    "abstract": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
    "metadata": {
      "arxiv_id": "2212.08329",
      "title": "Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder",
      "summary": "Text-to-speech synthesis (TTS) is a task to convert texts into speech. Two of the factors that have been driving TTS are the advancements of probabilistic models and latent representation learning. We propose a TTS method based on latent variable conversion using a diffusion probabilistic model and the variational autoencoder (VAE). In our TTS method, we use a waveform model based on VAE, a diffusion model that predicts the distribution of latent variables in the waveform model from texts, and an alignment model that learns alignments between the text and speech latent sequences. Our method integrates diffusion with VAE by modeling both mean and variance parameters with diffusion, where the target distribution is determined by approximation from VAE. This latent variable conversion framework potentially enables us to flexibly incorporate various latent feature extractors. Our experiments show that our method is robust to linguistic labels with poor orthography and alignment errors.",
      "authors": [
        "Yusuke Yasuda",
        "Tomoki Toda"
      ],
      "published": "2022-12-16T08:14:04Z",
      "updated": "2022-12-16T08:14:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08329v1",
      "landing_url": "https://arxiv.org/abs/2212.08329v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.08329"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on continuous latent-variable modeling for TTS with a diffusion+VAE pipeline and never describes quantized/finite-vocabulary discrete audio tokens or tokenizer details, so it fails the discrete-token inclusion and meets the exclusion based on continuous representations."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on continuous latent-variable modeling for TTS with a diffusion+VAE pipeline and never describes quantized/finite-vocabulary discrete audio tokens or tokenizer details, so it fails the discrete-token inclusion and meets the exclusion based on continuous representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models",
    "abstract": "Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.",
    "metadata": {
      "arxiv_id": "2212.08399",
      "title": "Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models",
      "summary": "Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.",
      "authors": [
        "Jean-Thomas Baillargeon",
        "Luc Lamontagne"
      ],
      "published": "2022-12-16T10:46:20Z",
      "updated": "2024-03-14T16:49:24Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.08399v2",
      "landing_url": "https://arxiv.org/abs/2212.08399v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.08399"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study addresses sequence length bias in text Transformer classifiers and does not mention any discrete audio-token generation, quantization, or language-modeling details, so it fails to meet the audio-token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Study addresses sequence length bias in text Transformer classifiers and does not mention any discrete audio-token generation, quantization, or language-modeling details, so it fails to meet the audio-token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
    "abstract": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
    "metadata": {
      "arxiv_id": "2212.09058",
      "title": "BEATs: Audio Pre-Training with Acoustic Tokenizers",
      "summary": "The massive growth of self-supervised learning (SSL) has been witnessed in language, vision, speech, and audio domains over the past few years. While discrete label prediction is widely adopted for other modalities, the state-of-the-art audio SSL models still employ reconstruction loss for pre-training. Compared with reconstruction loss, semantic-rich discrete label prediction encourages the SSL model to abstract the high-level audio semantics and discard the redundant details as in human perception. However, a semantic-rich acoustic tokenizer for general audio pre-training is usually not straightforward to obtain, due to the continuous property of audio and unavailable phoneme sequences like speech. To tackle this challenge, we propose BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations. In the first iteration, we use random projection as the acoustic tokenizer to train an audio SSL model in a mask and label prediction manner. Then, we train an acoustic tokenizer for the next iteration by distilling the semantic knowledge from the pre-trained or fine-tuned audio SSL model. The iteration is repeated with the hope of mutual promotion of the acoustic tokenizer and audio SSL model. The experimental results demonstrate our acoustic tokenizers can generate discrete labels with rich audio semantics and our audio SSL models achieve state-of-the-art results across various audio classification benchmarks, even outperforming previous models that use more training data and model parameters significantly. Specifically, we set a new state-of-the-art mAP 50.6% on AudioSet-2M for audio-only models without using any external data, and 98.1% accuracy on ESC-50. The code and pre-trained models are available at https://aka.ms/beats.",
      "authors": [
        "Sanyuan Chen",
        "Yu Wu",
        "Chengyi Wang",
        "Shujie Liu",
        "Daniel Tompkins",
        "Zhuo Chen",
        "Furu Wei"
      ],
      "published": "2022-12-18T10:41:55Z",
      "updated": "2022-12-18T10:41:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.09058v1",
      "landing_url": "https://arxiv.org/abs/2212.09058v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.09058"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper explicitly learns semantic-rich discrete acoustic tokens via an iterative tokenizer–SSL co-training (matching the discrete audio token generation focus) and evaluates on downstream benchmarks with token details, so it meets all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly learns semantic-rich discrete acoustic tokens via an iterative tokenizer–SSL co-training (matching the discrete audio token generation focus) and evaluates on downstream benchmarks with token details, so it meets all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Emotion Selectable End-to-End Text-based Speech Editing",
    "abstract": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
    "metadata": {
      "arxiv_id": "2212.10191",
      "title": "Emotion Selectable End-to-End Text-based Speech Editing",
      "summary": "Text-based speech editing allows users to edit speech by intuitively cutting, copying, and pasting text to speed up the process of editing speech. In the previous work, CampNet (context-aware mask prediction network) is proposed to realize text-based speech editing, significantly improving the quality of edited speech. This paper aims at a new task: adding emotional effect to the editing speech during the text-based speech editing to make the generated speech more expressive. To achieve this task, we propose Emo-CampNet (emotion CampNet), which can provide the option of emotional attributes for the generated speech in text-based speech editing and has the one-shot ability to edit unseen speakers' speech. Firstly, we propose an end-to-end emotion-selectable text-based speech editing model. The key idea of the model is to control the emotion of generated speech by introducing additional emotion attributes based on the context-aware mask prediction network. Secondly, to prevent the emotion of the generated speech from being interfered by the emotional components in the original speech, a neutral content generator is proposed to remove the emotion from the original speech, which is optimized by the generative adversarial framework. Thirdly, two data augmentation methods are proposed to enrich the emotional and pronunciation information in the training set, which can enable the model to edit the unseen speaker's speech. The experimental results that 1) Emo-CampNet can effectively control the emotion of the generated speech in the process of text-based speech editing; And can edit unseen speakers' speech. 2) Detailed ablation experiments further prove the effectiveness of emotional selectivity and data augmentation methods. The demo page is available at https://hairuo55.github.io/Emo-CampNet/",
      "authors": [
        "Tao Wang",
        "Jiangyan Yi",
        "Ruibo Fu",
        "Jianhua Tao",
        "Zhengqi Wen",
        "Chu Yuan Zhang"
      ],
      "published": "2022-12-20T12:02:40Z",
      "updated": "2022-12-20T12:02:40Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.10191v1",
      "landing_url": "https://arxiv.org/abs/2212.10191v1",
      "doi": "https://doi.org/10.48550/arXiv.2212.10191"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on emotion-selectable speech editing without any discussion of discrete tokenization/quantization or token-level modeling, so it fails to meet the discrete audio token criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on emotion-selectable speech editing without any discussion of discrete tokenization/quantization or token-level modeling, so it fails to meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
    "abstract": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
    "metadata": {
      "arxiv_id": "2212.14538",
      "title": "Transformer in Transformer as Backbone for Deep Reinforcement Learning",
      "summary": "Designing better deep networks and better reinforcement learning (RL) algorithms are both important for deep RL. This work focuses on the former. Previous methods build the network with several modules like CNN, LSTM and Attention. Recent methods combine the Transformer with these modules for better performance. However, it requires tedious optimization skills to train a network composed of mixed modules, making these methods inconvenient to be used in practice. In this paper, we propose to design \\emph{pure Transformer-based networks} for deep RL, aiming at providing off-the-shelf backbones for both the online and offline settings. Specifically, the Transformer in Transformer (TIT) backbone is proposed, which cascades two Transformers in a very natural way: the inner one is used to process a single observation, while the outer one is responsible for processing the observation history; combining both is expected to extract spatial-temporal representations for good decision-making. Experiments show that TIT can achieve satisfactory performance in different settings consistently.",
      "authors": [
        "Hangyu Mao",
        "Rui Zhao",
        "Hao Chen",
        "Jianye Hao",
        "Yiqun Chen",
        "Dong Li",
        "Junge Zhang",
        "Zhen Xiao"
      ],
      "published": "2022-12-30T03:50:38Z",
      "updated": "2023-01-03T06:51:22Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2212.14538v2",
      "landing_url": "https://arxiv.org/abs/2212.14538v2",
      "doi": "https://doi.org/10.48550/arXiv.2212.14538"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on Transformer backbones for deep reinforcement learning rather than discrete audio tokenization/quantization, so it fails to meet any inclusion criteria and violates the exclusion condition.; Score=1"
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on Transformer backbones for deep reinforcement learning rather than discrete audio tokenization/quantization, so it fails to meet any inclusion criteria and violates the exclusion condition.; Score=1",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "eVAE: Evolutionary Variational Autoencoder",
    "abstract": "The surrogate loss of variational autoencoders (VAEs) poses various challenges to their training, inducing the imbalance between task fitting and representation inference. To avert this, the existing strategies for VAEs focus on adjusting the tradeoff by introducing hyperparameters, deriving a tighter bound under some mild assumptions, or decomposing the loss components per certain neural settings. VAEs still suffer from uncertain tradeoff learning.We propose a novel evolutionary variational autoencoder (eVAE) building on the variational information bottleneck (VIB) theory and integrative evolutionary neural learning. eVAE integrates a variational genetic algorithm into VAE with variational evolutionary operators including variational mutation, crossover, and evolution. Its inner-outer-joint training mechanism synergistically and dynamically generates and updates the uncertain tradeoff learning in the evidence lower bound (ELBO) without additional constraints. Apart from learning a lossy compression and representation of data under the VIB assumption, eVAE presents an evolutionary paradigm to tune critical factors of VAEs and deep neural networks and addresses the premature convergence and random search problem by integrating evolutionary optimization into deep learning. Experiments show that eVAE addresses the KL-vanishing problem for text generation with low reconstruction loss, generates all disentangled factors with sharp images, and improves the image generation quality,respectively. eVAE achieves better reconstruction loss, disentanglement, and generation-inference balance than its competitors.",
    "metadata": {
      "arxiv_id": "2301.00011",
      "title": "eVAE: Evolutionary Variational Autoencoder",
      "summary": "The surrogate loss of variational autoencoders (VAEs) poses various challenges to their training, inducing the imbalance between task fitting and representation inference. To avert this, the existing strategies for VAEs focus on adjusting the tradeoff by introducing hyperparameters, deriving a tighter bound under some mild assumptions, or decomposing the loss components per certain neural settings. VAEs still suffer from uncertain tradeoff learning.We propose a novel evolutionary variational autoencoder (eVAE) building on the variational information bottleneck (VIB) theory and integrative evolutionary neural learning. eVAE integrates a variational genetic algorithm into VAE with variational evolutionary operators including variational mutation, crossover, and evolution. Its inner-outer-joint training mechanism synergistically and dynamically generates and updates the uncertain tradeoff learning in the evidence lower bound (ELBO) without additional constraints. Apart from learning a lossy compression and representation of data under the VIB assumption, eVAE presents an evolutionary paradigm to tune critical factors of VAEs and deep neural networks and addresses the premature convergence and random search problem by integrating evolutionary optimization into deep learning. Experiments show that eVAE addresses the KL-vanishing problem for text generation with low reconstruction loss, generates all disentangled factors with sharp images, and improves the image generation quality,respectively. eVAE achieves better reconstruction loss, disentanglement, and generation-inference balance than its competitors.",
      "authors": [
        "Zhangkai Wu",
        "Longbing Cao",
        "Lei Qi"
      ],
      "published": "2023-01-01T23:54:35Z",
      "updated": "2023-01-01T23:54:35Z",
      "categories": [
        "cs.NE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00011v1",
      "landing_url": "https://arxiv.org/abs/2301.00011v1",
      "doi": "https://doi.org/10.1109/TNNLS.2024.3359275"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes a variational autoencoder evolution method for general representation learning without any discrete audio token/codec focus, so it fails every inclusion criterion and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a variational autoencoder evolution method for general representation learning without any discrete audio token/codec focus, so it fails every inclusion criterion and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
    "abstract": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
    "metadata": {
      "arxiv_id": "2301.00591",
      "title": "Analysing Discrete Self Supervised Speech Representation for Spoken Language Modeling",
      "summary": "This work profoundly analyzes discrete self-supervised speech representations (units) through the eyes of Generative Spoken Language Modeling (GSLM). Following the findings of such an analysis, we propose practical improvements to the discrete unit for the GSLM. First, we start comprehending these units by analyzing them in three axes: interpretation, visualization, and resynthesis. Our analysis finds a high correlation between the speech units to phonemes and phoneme families, while their correlation with speaker or gender is weaker. Additionally, we found redundancies in the extracted units and claim that one reason may be the units' context. Following this analysis, we propose a new, unsupervised metric to measure unit redundancies. Finally, we use this metric to develop new methods that improve the robustness of units' clustering and show significant improvement considering zero-resource speech metrics such as ABX. Code and analysis tools are available under the following link: https://github.com/slp-rl/SLM-Discrete-Representations",
      "authors": [
        "Amitay Sicherman",
        "Yossi Adi"
      ],
      "published": "2023-01-02T10:36:40Z",
      "updated": "2023-03-01T09:59:54Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.00591v3",
      "landing_url": "https://arxiv.org/abs/2301.00591v3",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10097097"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract focus on discrete self-supervised speech units (quantized tokens) analyzed and improved for spoken language modeling with evaluations (ABX) and tooling, so it clearly matches the discrete audio token inclusion criteria without any exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on discrete self-supervised speech units (quantized tokens) analyzed and improved for spoken language modeling with evaluations (ABX) and tooling, so it clearly matches the discrete audio token inclusion criteria without any exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
    "abstract": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
    "metadata": {
      "arxiv_id": "2301.02111",
      "title": "Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers",
      "summary": "We introduce a language modeling approach for text to speech synthesis (TTS). Specifically, we train a neural codec language model (called Vall-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. During the pre-training stage, we scale up the TTS training data to 60K hours of English speech which is hundreds of times larger than existing systems. Vall-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as an acoustic prompt. Experiment results show that Vall-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, we find Vall-E could preserve the speaker's emotion and acoustic environment of the acoustic prompt in synthesis. See https://aka.ms/valle for demos of our work.",
      "authors": [
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Ziqiang Zhang",
        "Long Zhou",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-01-05T15:37:15Z",
      "updated": "2023-01-05T15:37:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.02111v1",
      "landing_url": "https://arxiv.org/abs/2301.02111v1",
      "doi": "https://doi.org/10.48550/arXiv.2301.02111"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper trains neural codec language model Vall-E on discrete codec codes (quantized tokens) for zero-shot TTS with pretraining and subjective evaluations, matching the discrete audio token focus and including codec design and quality assessment."
    },
    "round-A_JuniorNano_reasoning": "The paper trains neural codec language model Vall-E on discrete codec codes (quantized tokens) for zero-shot TTS with pretraining and subjective evaluations, matching the discrete audio token focus and including codec design and quality assessment.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Optimized learned entropy coding parameters for practical neural-based image and video compression",
    "abstract": "Neural-based image and video codecs are significantly more power-efficient when weights and activations are quantized to low-precision integers. While there are general-purpose techniques for reducing quantization effects, large losses can occur when specific entropy coding properties are not considered. This work analyzes how entropy coding is affected by parameter quantizations, and provides a method to minimize losses. It is shown that, by using a certain type of coding parameters to be learned, uniform quantization becomes practically optimal, also simplifying the minimization of code memory requirements. The mathematical properties of the new representation are presented, and its effectiveness is demonstrated by coding experiments, showing that good results can be obtained with precision as low as 4~bits per network output, and practically no loss with 8~bits.",
    "metadata": {
      "arxiv_id": "2301.08752",
      "title": "Optimized learned entropy coding parameters for practical neural-based image and video compression",
      "summary": "Neural-based image and video codecs are significantly more power-efficient when weights and activations are quantized to low-precision integers. While there are general-purpose techniques for reducing quantization effects, large losses can occur when specific entropy coding properties are not considered. This work analyzes how entropy coding is affected by parameter quantizations, and provides a method to minimize losses. It is shown that, by using a certain type of coding parameters to be learned, uniform quantization becomes practically optimal, also simplifying the minimization of code memory requirements. The mathematical properties of the new representation are presented, and its effectiveness is demonstrated by coding experiments, showing that good results can be obtained with precision as low as 4~bits per network output, and practically no loss with 8~bits.",
      "authors": [
        "Amir Said",
        "Reza Pourreza",
        "Hoang Le"
      ],
      "published": "2023-01-20T18:01:31Z",
      "updated": "2023-01-20T18:01:31Z",
      "categories": [
        "eess.IV",
        "cs.IT",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.08752v1",
      "landing_url": "https://arxiv.org/abs/2301.08752v1",
      "doi": "https://doi.org/10.1109/ICIP46576.2022.9897505"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe neural image/video compression focusing on entropy coding for low-precision quantization, which never discusses discrete audio tokens or their quantizers, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe neural image/video compression focusing on entropy coding for low-precision quantization, which never discusses discrete audio tokens or their quantizers, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Image Super-Resolution using Efficient Striped Window Transformer",
    "abstract": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
    "metadata": {
      "arxiv_id": "2301.09869",
      "title": "Image Super-Resolution using Efficient Striped Window Transformer",
      "summary": "Transformers have achieved remarkable results in single-image super-resolution (SR). However, the challenge of balancing model performance and complexity has hindered their application in lightweight SR (LSR). To tackle this challenge, we propose an efficient striped window transformer (ESWT). We revisit the normalization layer in the transformer and design a concise and efficient transformer structure to build the ESWT. Furthermore, we introduce a striped window mechanism to model long-term dependencies more efficiently. To fully exploit the potential of the ESWT, we propose a novel flexible window training strategy that can improve the performance of the ESWT without additional cost. Extensive experiments show that ESWT outperforms state-of-the-art LSR transformers, and achieves a better trade-off between model performance and complexity. The ESWT requires fewer parameters, incurs faster inference, smaller FLOPs, and less memory consumption, making it a promising solution for LSR.",
      "authors": [
        "Jinpeng Shi",
        "Hui Li",
        "Tianle Liu",
        "Yulong Liu",
        "Mingjian Zhang",
        "Jinchen Zhu",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2023-01-24T09:09:35Z",
      "updated": "2023-03-14T07:03:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.09869v2",
      "landing_url": "https://arxiv.org/abs/2301.09869v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.09869"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on image super-resolution with transformer windows and never mentions discrete audio tokens, codecs, quantization, or token-level modeling, so it does not meet the audio-token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on image super-resolution with transformer windows and never mentions discrete audio tokens, codecs, quantization, or token-level modeling, so it does not meet the audio-token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
    "abstract": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
    "metadata": {
      "arxiv_id": "2301.10048",
      "title": "Exploiting Optical Flow Guidance for Transformer-Based Video Inpainting",
      "summary": "Transformers have been widely used for video processing owing to the multi-head self attention (MHSA) mechanism. However, the MHSA mechanism encounters an intrinsic difficulty for video inpainting, since the features associated with the corrupted regions are degraded and incur inaccurate self attention. This problem, termed query degradation, may be mitigated by first completing optical flows and then using the flows to guide the self attention, which was verified in our previous work - flow-guided transformer (FGT). We further exploit the flow guidance and propose FGT++ to pursue more effective and efficient video inpainting. First, we design a lightweight flow completion network by using local aggregation and edge loss. Second, to address the query degradation, we propose a flow guidance feature integration module, which uses the motion discrepancy to enhance the features, together with a flow-guided feature propagation module that warps the features according to the flows. Third, we decouple the transformer along the temporal and spatial dimensions, where flows are used to select the tokens through a temporally deformable MHSA mechanism, and global tokens are combined with the inner-window local tokens through a dual perspective MHSA mechanism. FGT++ is experimentally evaluated to be outperforming the existing video inpainting networks qualitatively and quantitatively.",
      "authors": [
        "Kaidong Zhang",
        "Jialun Peng",
        "Jingjing Fu",
        "Dong Liu"
      ],
      "published": "2023-01-24T14:44:44Z",
      "updated": "2024-03-19T04:02:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.10048v2",
      "landing_url": "https://arxiv.org/abs/2301.10048v2",
      "doi": "https://doi.org/10.1109/TPAMI.2024.3361010"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on transformer-based video inpainting with optical flow guidance, which is unrelated to discrete audio tokenization, so it fails the inclusion topics and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on transformer-based video inpainting with optical flow guidance, which is unrelated to discrete audio tokenization, so it fails the inclusion topics and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
    "abstract": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
    "metadata": {
      "arxiv_id": "2301.13662",
      "title": "InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt",
      "summary": "Expressive text-to-speech (TTS) aims to synthesize different speaking style speech according to human's demands. Nowadays, there are two common ways to control speaking styles: (1) Pre-defining a group of speaking style and using categorical index to denote different speaking style. However, there are limitations in the diversity of expressiveness, as these models can only generate the pre-defined styles. (2) Using reference speech as style input, which results in a problem that the extracted style information is not intuitive or interpretable. In this study, we attempt to use natural language as style prompt to control the styles in the synthetic speech, e.g., \"Sigh tone in full of sad mood with some helpless feeling\". Considering that there is no existing TTS corpus which is proper to benchmark this novel task, we first construct a speech corpus, whose speech samples are annotated with not only content transcriptions but also style descriptions in natural language. Then we propose an expressive TTS model, named as InstructTTS, which is novel in the sense of following aspects: (1) We fully take the advantage of self-supervised learning and cross-modal metric learning, and propose a novel three-stage training procedure to obtain a robust sentence embedding model, which can effectively capture semantic information from the style prompts and control the speaking style in the generated speech. (2) We propose to model acoustic features in discrete latent space and train a novel discrete diffusion probabilistic model to generate vector-quantized (VQ) acoustic tokens rather than the commonly-used mel spectrogram. (3) We jointly apply mutual information (MI) estimation and minimization during acoustic model training to minimize style-speaker and style-content MI, avoiding possible content and speaker information leakage from the style prompt.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Chao Weng",
        "Helen Meng"
      ],
      "published": "2023-01-31T14:26:52Z",
      "updated": "2023-06-25T11:42:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13662v2",
      "landing_url": "https://arxiv.org/abs/2301.13662v2",
      "doi": "https://doi.org/10.48550/arXiv.2301.13662"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "InstructTTS explicitly quantizes acoustic features into vector-quantized tokens and trains a discrete diffusion model on those tokens to control style, so it matches the discrete audio-token focus and exceeds the inclusion thresholds."
    },
    "round-A_JuniorNano_reasoning": "InstructTTS explicitly quantizes acoustic features into vector-quantized tokens and trains a discrete diffusion model on those tokens to control style, so it matches the discrete audio-token focus and exceeds the inclusion thresholds.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control",
    "abstract": "The creation of unique control methods for a hand prosthesis is still a problem that has to be addressed. The best choice of a human-machine interface (HMI) that should be used to enable natural control is still a challenge. Surface electromyography (sEMG), the most popular option, has a variety of difficult-to-fix issues (electrode displacement, sweat, fatigue). The ultrasound imaging-based methodology offers a means of recognising complex muscle activity and configuration with a greater SNR and less hardware requirements as compared to sEMG. In this study, a prototype system for high frame rate ultrasound imaging for prosthetic arm control is proposed. Using the proposed framework, a virtual robotic hand simulation is developed that can mimic a human hand as illustrated in the link [10]. The proposed classification model simulating four hand gestures has a classification accuracy of more than 90%.",
    "metadata": {
      "arxiv_id": "2301.13809",
      "title": "A Prototype System for High Frame Rate Ultrasound Imaging based Prosthetic Arm Control",
      "summary": "The creation of unique control methods for a hand prosthesis is still a problem that has to be addressed. The best choice of a human-machine interface (HMI) that should be used to enable natural control is still a challenge. Surface electromyography (sEMG), the most popular option, has a variety of difficult-to-fix issues (electrode displacement, sweat, fatigue). The ultrasound imaging-based methodology offers a means of recognising complex muscle activity and configuration with a greater SNR and less hardware requirements as compared to sEMG. In this study, a prototype system for high frame rate ultrasound imaging for prosthetic arm control is proposed. Using the proposed framework, a virtual robotic hand simulation is developed that can mimic a human hand as illustrated in the link [10]. The proposed classification model simulating four hand gestures has a classification accuracy of more than 90%.",
      "authors": [
        "Ayush Singh",
        "Pisharody Harikrishnan Gopalkrishnan",
        "Mahesh Raveendranatha Panicker"
      ],
      "published": "2023-01-31T17:53:16Z",
      "updated": "2023-04-18T10:46:10Z",
      "categories": [
        "cs.RO",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2301.13809v3",
      "landing_url": "https://arxiv.org/abs/2301.13809v3",
      "doi": "https://doi.org/10.48550/arXiv.2301.13809"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Not about discrete audio tokens at all but rather ultrasound-based prosthetic control, so it fails to meet the topic requirements."
    },
    "round-A_JuniorNano_reasoning": "Not about discrete audio tokens at all but rather ultrasound-based prosthetic control, so it fails to meet the topic requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts",
    "abstract": "Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary's capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings BR-DRO objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.",
    "metadata": {
      "arxiv_id": "2302.02931",
      "title": "Bitrate-Constrained DRO: Beyond Worst Case Robustness To Unknown Group Shifts",
      "summary": "Training machine learning models robust to distribution shifts is critical for real-world applications. Some robust training algorithms (e.g., Group DRO) specialize to group shifts and require group information on all training points. Other methods (e.g., CVaR DRO) that do not need group annotations can be overly conservative, since they naively upweight high loss points which may form a contrived set that does not correspond to any meaningful group in the real world (e.g., when the high loss points are randomly mislabeled training points). In this work, we address limitations in prior approaches by assuming a more nuanced form of group shift: conditioned on the label, we assume that the true group function (indicator over group) is simple. For example, we may expect that group shifts occur along low bitrate features (e.g., image background, lighting). Thus, we aim to learn a model that maintains high accuracy on simple group functions realized by these low bitrate features, that need not spend valuable model capacity achieving high accuracy on contrived groups of examples. Based on this, we consider the two-player game formulation of DRO where the adversary's capacity is bitrate-constrained. Our resulting practical algorithm, Bitrate-Constrained DRO (BR-DRO), does not require group information on training samples yet matches the performance of Group DRO on datasets that have training group annotations and that of CVaR DRO on long-tailed distributions. Our theoretical analysis reveals that in some settings BR-DRO objective can provably yield statistically efficient and less conservative solutions than unconstrained CVaR DRO.",
      "authors": [
        "Amrith Setlur",
        "Don Dennis",
        "Benjamin Eysenbach",
        "Aditi Raghunathan",
        "Chelsea Finn",
        "Virginia Smith",
        "Sergey Levine"
      ],
      "published": "2023-02-06T17:07:16Z",
      "updated": "2023-10-12T03:47:00Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.02931v2",
      "landing_url": "https://arxiv.org/abs/2302.02931v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.02931"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about bitrate-constrained DRO for group shifts in ML rather than any work on discrete audio token generation, quantization, or token-level evaluation, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper is about bitrate-constrained DRO for group shifts in ML rather than any work on discrete audio token generation, quantization, or token-level evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
    "abstract": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
    "metadata": {
      "arxiv_id": "2302.03540",
      "title": "Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision",
      "summary": "We introduce SPEAR-TTS, a multi-speaker text-to-speech (TTS) system that can be trained with minimal supervision. By combining two types of discrete speech representations, we cast TTS as a composition of two sequence-to-sequence tasks: from text to high-level semantic tokens (akin to \"reading\") and from semantic tokens to low-level acoustic tokens (\"speaking\"). Decoupling these two tasks enables training of the \"speaking\" module using abundant audio-only data, and unlocks the highly efficient combination of pretraining and backtranslation to reduce the need for parallel data when training the \"reading\" component. To control the speaker identity, we adopt example prompting, which allows SPEAR-TTS to generalize to unseen speakers using only a short sample of 3 seconds, without any explicit speaker representation or speaker-id labels. Our experiments demonstrate that SPEAR-TTS achieves a character error rate that is competitive with state-of-the-art methods using only 15 minutes of parallel data, while matching ground-truth speech in terms of naturalness and acoustic quality, as measured in subjective tests.",
      "authors": [
        "Eugene Kharitonov",
        "Damien Vincent",
        "Zalán Borsos",
        "Raphaël Marinier",
        "Sertan Girgin",
        "Olivier Pietquin",
        "Matt Sharifi",
        "Marco Tagliasacchi",
        "Neil Zeghidour"
      ],
      "published": "2023-02-07T15:48:31Z",
      "updated": "2023-02-07T15:48:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.03540v1",
      "landing_url": "https://arxiv.org/abs/2302.03540v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.03540"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The system explicitly relies on discrete semantic and acoustic token representations learned via two sequence-to-sequence stages and evaluates them in a TTS setting, so it clearly centers on discrete audio token generation and modeling and meets all inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "The system explicitly relies on discrete semantic and acoustic token representations learned via two sequence-to-sequence stages and evaluates them in a TTS setting, so it clearly centers on discrete audio token generation and modeling and meets all inclusion requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
    "abstract": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
    "metadata": {
      "arxiv_id": "2302.04215",
      "title": "A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech",
      "summary": "Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
      "authors": [
        "Li-Wei Chen",
        "Shinji Watanabe",
        "Alexander Rudnicky"
      ],
      "published": "2023-02-08T17:34:32Z",
      "updated": "2023-02-08T17:34:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.04215v1",
      "landing_url": "https://arxiv.org/abs/2302.04215v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.04215"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract describe a vector-quantized TTS system that learns discrete codes (multiple code groups) to handle real-world speech, directly matching the discrete audio token criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a vector-quantized TTS system that learns discrete codes (multiple code groups) to handle real-world speech, directly matching the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks",
    "abstract": "We propose a novel gradient-based attack against transformer-based language models that searches for an adversarial example in a continuous space of token probabilities. Our algorithm mitigates the gap between adversarial loss for continuous and discrete text representations by performing multi-step quantization in a quantization-compensation loop. Experiments show that our method significantly outperforms other approaches on various natural language processing (NLP) tasks.",
    "metadata": {
      "arxiv_id": "2302.05120",
      "title": "Step by Step Loss Goes Very Far: Multi-Step Quantization for Adversarial Text Attacks",
      "summary": "We propose a novel gradient-based attack against transformer-based language models that searches for an adversarial example in a continuous space of token probabilities. Our algorithm mitigates the gap between adversarial loss for continuous and discrete text representations by performing multi-step quantization in a quantization-compensation loop. Experiments show that our method significantly outperforms other approaches on various natural language processing (NLP) tasks.",
      "authors": [
        "Piotr Gaiński",
        "Klaudia Bałazy"
      ],
      "published": "2023-02-10T08:50:51Z",
      "updated": "2023-02-10T08:50:51Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05120v1",
      "landing_url": "https://arxiv.org/abs/2302.05120v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05120"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on adversarial word-token attacks for NLP transformers, not on discrete audio-token quantization or codec design, so it fails to meet the audio discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on adversarial word-token attacks for NLP transformers, not on discrete audio-token quantization or codec design, so it fails to meet the audio discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
    "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
    "metadata": {
      "arxiv_id": "2302.05406",
      "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
      "summary": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call \"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model \"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results \"still make sense\" for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
      "authors": [
        "Pedro Colon-Hernandez",
        "Henry Lieberman",
        "Yida Xin",
        "Claire Yin",
        "Cynthia Breazeal",
        "Peter Chin"
      ],
      "published": "2023-02-10T18:21:13Z",
      "updated": "2023-02-10T18:21:13Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.05406v1",
      "landing_url": "https://arxiv.org/abs/2302.05406v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.05406"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on textual commonsense inference with transformers and GANs, not on discrete audio tokenization or codec research, so it conflicts with the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on textual commonsense inference with transformers and GANs, not on discrete audio tokenization or codec research, so it conflicts with the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction",
    "abstract": "Multi-scale features are essential for dense prediction tasks, such as object detection, instance segmentation, and semantic segmentation. The prevailing methods usually utilize a classification backbone to extract multi-scale features and then fuse these features using a lightweight module (e.g., the fusion module in FPN and BiFPN, two typical object detection methods). However, as these methods allocate most computational resources to the classification backbone, the multi-scale feature fusion in these methods is delayed, which may lead to inadequate feature fusion. While some methods perform feature fusion from early stages, they either fail to fully leverage high-level features to guide low-level feature learning or have complex structures, resulting in sub-optimal performance. We propose a streamlined cascade encoder-decoder network, dubbed CEDNet, tailored for dense \\mbox{prediction} tasks. All stages in CEDNet share the same encoder-decoder structure and perform multi-scale feature fusion within the decoder. A hallmark of CEDNet is its ability to incorporate high-level features from early stages to guide low-level feature learning in subsequent stages, thereby enhancing the effectiveness of multi-scale feature fusion. We explored three well-known encoder-decoder structures: Hourglass, UNet, and FPN. When integrated into CEDNet, they performed much better than traditional methods that use a pre-designed classification backbone combined with a lightweight fusion module. Extensive experiments on object detection, instance segmentation, and semantic segmentation demonstrated the effectiveness of our method. The code is available at https://github.com/zhanggang001/CEDNet.",
    "metadata": {
      "arxiv_id": "2302.06052",
      "title": "CEDNet: A Cascade Encoder-Decoder Network for Dense Prediction",
      "summary": "Multi-scale features are essential for dense prediction tasks, such as object detection, instance segmentation, and semantic segmentation. The prevailing methods usually utilize a classification backbone to extract multi-scale features and then fuse these features using a lightweight module (e.g., the fusion module in FPN and BiFPN, two typical object detection methods). However, as these methods allocate most computational resources to the classification backbone, the multi-scale feature fusion in these methods is delayed, which may lead to inadequate feature fusion. While some methods perform feature fusion from early stages, they either fail to fully leverage high-level features to guide low-level feature learning or have complex structures, resulting in sub-optimal performance. We propose a streamlined cascade encoder-decoder network, dubbed CEDNet, tailored for dense \\mbox{prediction} tasks. All stages in CEDNet share the same encoder-decoder structure and perform multi-scale feature fusion within the decoder. A hallmark of CEDNet is its ability to incorporate high-level features from early stages to guide low-level feature learning in subsequent stages, thereby enhancing the effectiveness of multi-scale feature fusion. We explored three well-known encoder-decoder structures: Hourglass, UNet, and FPN. When integrated into CEDNet, they performed much better than traditional methods that use a pre-designed classification backbone combined with a lightweight fusion module. Extensive experiments on object detection, instance segmentation, and semantic segmentation demonstrated the effectiveness of our method. The code is available at https://github.com/zhanggang001/CEDNet.",
      "authors": [
        "Gang Zhang",
        "Ziyi Li",
        "Chufeng Tang",
        "Jianmin Li",
        "Xiaolin Hu"
      ],
      "published": "2023-02-13T02:03:55Z",
      "updated": "2023-10-31T07:30:17Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.06052v2",
      "landing_url": "https://arxiv.org/abs/2302.06052v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.06052"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on computer-vision encoder-decoder networks for dense prediction, not on discrete audio tokenization or codecs, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on computer-vision encoder-decoder networks for dense prediction, not on discrete audio tokenization or codecs, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Enhancement with Multi-granularity Vector Quantization",
    "abstract": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
    "metadata": {
      "arxiv_id": "2302.08342",
      "title": "Speech Enhancement with Multi-granularity Vector Quantization",
      "summary": "With advances in deep learning, neural network based speech enhancement (SE) has developed rapidly in the last decade. Meanwhile, the self-supervised pre-trained model and vector quantization (VQ) have achieved excellent performance on many speech-related tasks, while they are less explored on SE. As it was shown in our previous work that utilizing a VQ module to discretize noisy speech representations is beneficial for speech denoising, in this work we therefore study the impact of using VQ at different layers with different number of codebooks. Different VQ modules indeed enable to extract multiple-granularity speech features. Following an attention mechanism, the contextual features extracted by a pre-trained model are fused with the local features extracted by the encoder, such that both global and local information are preserved to reconstruct the enhanced speech. Experimental results on the Valentini dataset show that the proposed model can improve the SE performance, where the impact of choosing pre-trained models is also revealed.",
      "authors": [
        "Xiao-Ying Zhao",
        "Qiu-Shi Zhu",
        "Jie Zhang"
      ],
      "published": "2023-02-16T14:53:41Z",
      "updated": "2023-02-16T14:53:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.08342v1",
      "landing_url": "https://arxiv.org/abs/2302.08342v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.08342"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper studies vector quantization codebooks at multiple layers to discretize noisy speech representations, aligning with discrete audio tokens and providing evaluations on reconstruction/SE performance, so it meets the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper studies vector quantization codebooks at multiple layers to discretize noisy speech representations, aligning with discrete audio tokens and providing evaluations on reconstruction/SE performance, so it meets the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
    "abstract": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
    "metadata": {
      "arxiv_id": "2302.10287",
      "title": "CertViT: Certified Robustness of Pre-Trained Vision Transformers",
      "summary": "Lipschitz bounded neural networks are certifiably robust and have a good trade-off between clean and certified accuracy. Existing Lipschitz bounding methods train from scratch and are limited to moderately sized networks (< 6M parameters). They require a fair amount of hyper-parameter tuning and are computationally prohibitive for large networks like Vision Transformers (5M to 660M parameters). Obtaining certified robustness of transformers is not feasible due to the non-scalability and inflexibility of the current methods. This work presents CertViT, a two-step proximal-projection method to achieve certified robustness from pre-trained weights. The proximal step tries to lower the Lipschitz bound and the projection step tries to maintain the clean accuracy of pre-trained weights. We show that CertViT networks have better certified accuracy than state-of-the-art Lipschitz trained networks. We apply CertViT on several variants of pre-trained vision transformers and show adversarial robustness using standard attacks. Code : https://github.com/sagarverma/transformer-lipschitz",
      "authors": [
        "Kavya Gupta",
        "Sagar Verma"
      ],
      "published": "2023-02-01T06:09:19Z",
      "updated": "2023-02-01T06:09:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10287v1",
      "landing_url": "https://arxiv.org/abs/2302.10287v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.10287"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Fails to meet any discrete audio token criteria—vision-transformer robustness unrelated to quantized audio tokens—so score 1."
    },
    "round-A_JuniorNano_reasoning": "Fails to meet any discrete audio token criteria—vision-transformer robustness unrelated to quantized audio tokens—so score 1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
    "abstract": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
    "metadata": {
      "arxiv_id": "2302.10866",
      "title": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
      "summary": "Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.",
      "authors": [
        "Michael Poli",
        "Stefano Massaroli",
        "Eric Nguyen",
        "Daniel Y. Fu",
        "Tri Dao",
        "Stephen Baccus",
        "Yoshua Bengio",
        "Stefano Ermon",
        "Christopher Ré"
      ],
      "published": "2023-02-21T18:29:25Z",
      "updated": "2023-04-19T20:08:39Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.10866v3",
      "landing_url": "https://arxiv.org/abs/2302.10866v3",
      "doi": "https://doi.org/10.48550/arXiv.2302.10866"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe a subquadratic convolutional transformer replacement for text language modeling, not the construction or evaluation of discrete audio tokenizers/codecs, so it fails every inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a subquadratic convolutional transformer replacement for text language modeling, not the construction or evaluation of discrete audio tokenizers/codecs, so it fails every inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
    "abstract": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
    "metadata": {
      "arxiv_id": "2302.13458",
      "title": "Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow",
      "summary": "There are two types of methods for non-autoregressive text-to-speech models to learn the one-to-many relationship between text and speech effectively. The first one is to use an advanced generative framework such as normalizing flow (NF). The second one is to use variance information such as pitch or energy together when generating speech. For the second type, it is also possible to control the variance factors by adjusting the variance values provided to a model. In this paper, we propose a novel model called VarianceFlow combining the advantages of the two types. By modeling the variance with NF, VarianceFlow predicts the variance information more precisely with improved speech quality. Also, the objective function of NF makes the model use the variance information and the text in a disentangled manner resulting in more precise variance control. In experiments, VarianceFlow shows superior performance over other state-of-the-art TTS models both in terms of speech quality and controllability.",
      "authors": [
        "Yoonhyung Lee",
        "Jinhyeok Yang",
        "Kyomin Jung"
      ],
      "published": "2023-02-27T01:12:19Z",
      "updated": "2023-02-27T01:12:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.13458v1",
      "landing_url": "https://arxiv.org/abs/2302.13458v1",
      "doi": "https://doi.org/10.1109/ICASSP43922.2022.9747050"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This TTS paper focuses on variance modeling via normalizing flows using continuous speech features rather than proposing or evaluating discrete audio tokenization or quantization schemes, so it clearly falls outside the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This TTS paper focuses on variance modeling via normalizing flows using continuous speech features rather than proposing or evaluating discrete audio tokenization or quantization schemes, so it clearly falls outside the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Full Stack Optimization of Transformer Inference: a Survey",
    "abstract": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
    "metadata": {
      "arxiv_id": "2302.14017",
      "title": "Full Stack Optimization of Transformer Inference: a Survey",
      "summary": "Recent advances in state-of-the-art DNN architecture design have been moving toward Transformer models. These models achieve superior accuracy across a wide range of applications. This trend has been consistent over the past several years since Transformer models were originally introduced. However, the amount of compute and bandwidth required for inference of recent Transformer models is growing at a significant rate, and this has made their deployment in latency-sensitive applications challenging. As such, there has been an increased focus on making Transformer models more efficient, with methods that range from changing the architecture design, all the way to developing dedicated domain-specific accelerators. In this work, we survey different approaches for efficient Transformer inference, including: (i) analysis and profiling of the bottlenecks in existing Transformer architectures and their similarities and differences with previous convolutional models; (ii) implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU, as well as linear operations, on hardware design; (iii) approaches for optimizing a fixed Transformer architecture; (iv) challenges in finding the right mapping and scheduling of operations for Transformer models; and (v) approaches for optimizing Transformer models by adapting the architecture using neural architecture search. Finally, we perform a case study by applying the surveyed optimizations on Gemmini, the open-source, full-stack DNN accelerator generator, and we show how each of these approaches can yield improvements, compared to previous benchmark results on Gemmini. Among other things, we find that a full-stack co-design approach with the aforementioned methods can result in up to 88.7x speedup with a minimal performance degradation for Transformer inference.",
      "authors": [
        "Sehoon Kim",
        "Coleman Hooper",
        "Thanakul Wattanawong",
        "Minwoo Kang",
        "Ruohan Yan",
        "Hasan Genc",
        "Grace Dinh",
        "Qijing Huang",
        "Kurt Keutzer",
        "Michael W. Mahoney",
        "Yakun Sophia Shao",
        "Amir Gholami"
      ],
      "published": "2023-02-27T18:18:13Z",
      "updated": "2023-02-27T18:18:13Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14017v1",
      "landing_url": "https://arxiv.org/abs/2302.14017v1",
      "doi": "https://doi.org/10.48550/arXiv.2302.14017"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper surveys transformer inference optimization and hardware efficiency, which has no focus on discrete audio token generation, quantization, or evaluation, so it clearly fails to meet the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper surveys transformer inference optimization and hardware efficiency, which has no focus on discrete audio token generation, quantization, or evaluation, so it clearly fails to meet the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reconstruction-based Out-of-Distribution Detection for Short-Range FMCW Radar",
    "abstract": "Out-of-distribution (OOD) detection recently has drawn attention due to its critical role in the safe deployment of modern neural network architectures in real-world applications. The OOD detectors aim to distinguish samples that lie outside the training distribution in order to avoid the overconfident predictions of machine learning models on OOD data. Existing detectors, which mainly rely on the logit, intermediate feature space, softmax score, or reconstruction loss, manage to produce promising results. However, most of these methods are developed for the image domain. In this study, we propose a novel reconstruction-based OOD detector to operate on the radar domain. Our method exploits an autoencoder (AE) and its latent representation to detect the OOD samples. We propose two scores incorporating the patch-based reconstruction loss and the energy value calculated from the latent representations of each patch. We achieve an AUROC of 90.72% on our dataset collected by using 60 GHz short-range FMCW Radar. The experiments demonstrate that, in terms of AUROC and AUPR, our method outperforms the baseline (AE) and the other state-of-the-art methods. Also, thanks to its model size of 641 kB, our detector is suitable for embedded usage.",
    "metadata": {
      "arxiv_id": "2302.14192",
      "title": "Reconstruction-based Out-of-Distribution Detection for Short-Range FMCW Radar",
      "summary": "Out-of-distribution (OOD) detection recently has drawn attention due to its critical role in the safe deployment of modern neural network architectures in real-world applications. The OOD detectors aim to distinguish samples that lie outside the training distribution in order to avoid the overconfident predictions of machine learning models on OOD data. Existing detectors, which mainly rely on the logit, intermediate feature space, softmax score, or reconstruction loss, manage to produce promising results. However, most of these methods are developed for the image domain. In this study, we propose a novel reconstruction-based OOD detector to operate on the radar domain. Our method exploits an autoencoder (AE) and its latent representation to detect the OOD samples. We propose two scores incorporating the patch-based reconstruction loss and the energy value calculated from the latent representations of each patch. We achieve an AUROC of 90.72% on our dataset collected by using 60 GHz short-range FMCW Radar. The experiments demonstrate that, in terms of AUROC and AUPR, our method outperforms the baseline (AE) and the other state-of-the-art methods. Also, thanks to its model size of 641 kB, our detector is suitable for embedded usage.",
      "authors": [
        "Sabri Mustafa Kahya",
        "Muhammet Sami Yavuz",
        "Eckehard Steinbach"
      ],
      "published": "2023-02-27T23:03:51Z",
      "updated": "2023-06-15T09:26:42Z",
      "categories": [
        "eess.SP",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14192v2",
      "landing_url": "https://arxiv.org/abs/2302.14192v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14192"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on radar-based OOD detection with reconstruction loss and autoencoders rather than any research on discrete audio tokens, tokenization, quantization, or vocabulary design, so it fails the inclusion criteria and matches exclusion of non-audio-token work."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on radar-based OOD detection with reconstruction loss and autoencoders rather than any research on discrete audio tokens, tokenization, quantization, or vocabulary design, so it fails the inclusion criteria and matches exclusion of non-audio-token work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
    "abstract": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
    "metadata": {
      "arxiv_id": "2302.14337",
      "title": "UniFLG: Unified Facial Landmark Generator from Text or Speech",
      "summary": "Talking face generation has been extensively investigated owing to its wide applicability. The two primary frameworks used for talking face generation comprise a text-driven framework, which generates synchronized speech and talking faces from text, and a speech-driven framework, which generates talking faces from speech. To integrate these frameworks, this paper proposes a unified facial landmark generator (UniFLG). The proposed system exploits end-to-end text-to-speech not only for synthesizing speech but also for extracting a series of latent representations that are common to text and speech, and feeds it to a landmark decoder to generate facial landmarks. We demonstrate that our system achieves higher naturalness in both speech synthesis and facial landmark generation compared to the state-of-the-art text-driven method. We further demonstrate that our system can generate facial landmarks from speech of speakers without facial video data or even speech data.",
      "authors": [
        "Kentaro Mitsui",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2023-02-28T06:05:43Z",
      "updated": "2023-05-19T02:43:32Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.SD",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2302.14337v2",
      "landing_url": "https://arxiv.org/abs/2302.14337v2",
      "doi": "https://doi.org/10.48550/arXiv.2302.14337"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on text/speech-driven talking face generation via latent representations and landmark decoders, without proposing or evaluating discrete audio-codec/tokenization mechanisms, so it fails to meet the discrete audio token criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on text/speech-driven talking face generation via latent representations and landmark decoders, without proposing or evaluating discrete audio-codec/tokenization mechanisms, so it fails to meet the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
    "abstract": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
    "metadata": {
      "arxiv_id": "2303.00957",
      "title": "Preference Transformer: Modeling Human Preferences using Transformers for RL",
      "summary": "Preference-based reinforcement learning (RL) provides a framework to train agents using human preferences between two behaviors. However, preference-based RL has been challenging to scale since it requires a large amount of human feedback to learn a reward function aligned with human intent. In this paper, we present Preference Transformer, a neural architecture that models human preferences using transformers. Unlike prior approaches assuming human judgment is based on the Markovian rewards which contribute to the decision equally, we introduce a new preference model based on the weighted sum of non-Markovian rewards. We then design the proposed preference model using a transformer architecture that stacks causal and bidirectional self-attention layers. We demonstrate that Preference Transformer can solve a variety of control tasks using real human preferences, while prior approaches fail to work. We also show that Preference Transformer can induce a well-specified reward and attend to critical events in the trajectory by automatically capturing the temporal dependencies in human decision-making. Code is available on the project website: https://sites.google.com/view/preference-transformer.",
      "authors": [
        "Changyeon Kim",
        "Jongjin Park",
        "Jinwoo Shin",
        "Honglak Lee",
        "Pieter Abbeel",
        "Kimin Lee"
      ],
      "published": "2023-03-02T04:24:29Z",
      "updated": "2023-03-02T04:24:29Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.00957v1",
      "landing_url": "https://arxiv.org/abs/2303.00957v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.00957"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Preference Transformer focuses on preference-based RL for human behavior modeling and doesn’t involve discrete audio token generation/codec or quantization, so it fails the inclusion scope entirely."
    },
    "round-A_JuniorNano_reasoning": "Preference Transformer focuses on preference-based RL for human behavior modeling and doesn’t involve discrete audio token generation/codec or quantization, so it fails the inclusion scope entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
    "abstract": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
    "metadata": {
      "arxiv_id": "2303.02939",
      "title": "FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model",
      "summary": "Neural text-to-speech (TTS) generally consists of cascaded architecture with separately optimized acoustic model and vocoder, or end-to-end architecture with continuous mel-spectrograms or self-extracted speech frames as the intermediate representations to bridge acoustic model and vocoder, which suffers from two limitations: 1) the continuous acoustic frames are hard to predict with phoneme only, and acoustic information like duration or pitch is also needed to solve the one-to-many problem, which is not easy to scale on large scale and noise datasets; 2) to achieve diverse speech output based on continuous speech features, complex VAE or flow-based models are usually required. In this paper, we propose FoundationTTS, a new speech synthesis system with a neural audio codec for discrete speech token extraction and waveform reconstruction and a large language model for discrete token generation from linguistic (phoneme) tokens. Specifically, 1) we propose a hierarchical codec network based on vector-quantized auto-encoders with adversarial training (VQ-GAN), which first extracts continuous frame-level speech representations with fine-grained codec, and extracts a discrete token from each continuous speech frame with coarse-grained codec; 2) we jointly optimize speech token, linguistic tokens, speaker token together with a large language model and predict the discrete speech tokens autoregressively. Experiments show that FoundationTTS achieves a MOS gain of +0.14 compared to the baseline system. In ASR customization tasks, our method achieves 7.09\\% and 10.35\\% WERR respectively over two strong customized ASR baselines.",
      "authors": [
        "Ruiqing Xue",
        "Yanqing Liu",
        "Lei He",
        "Xu Tan",
        "Linquan Liu",
        "Edward Lin",
        "Sheng Zhao"
      ],
      "published": "2023-03-06T07:17:15Z",
      "updated": "2023-03-08T03:06:47Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.02939v3",
      "landing_url": "https://arxiv.org/abs/2303.02939v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.02939"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "FoundationTTS explicitly builds discrete neural audio codec tokens and jointly models them with a large language model for reconstruction and ASR customization evaluation, satisfying the inclusion requirements without relying on purely continuous representations."
    },
    "round-A_JuniorNano_reasoning": "FoundationTTS explicitly builds discrete neural audio codec tokens and jointly models them with a large language model for reconstruction and ASR customization evaluation, satisfying the inclusion requirements without relying on purely continuous representations.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
    "abstract": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
    "metadata": {
      "arxiv_id": "2303.03600",
      "title": "Adaptive Knowledge Distillation between Text and Speech Pre-trained Models",
      "summary": "Learning on a massive amount of speech corpus leads to the recent success of many self-supervised speech models. With knowledge distillation, these models may also benefit from the knowledge encoded by language models that are pre-trained on rich sources of texts. The distillation process, however, is challenging due to the modal disparity between textual and speech embedding spaces. This paper studies metric-based distillation to align the embedding space of text and speech with only a small amount of data without modifying the model structure. Since the semantic and granularity gap between text and speech has been omitted in literature, which impairs the distillation, we propose the Prior-informed Adaptive knowledge Distillation (PAD) that adaptively leverages text/speech units of variable granularity and prior distributions to achieve better global and local alignments between text and speech pre-trained models. We evaluate on three spoken language understanding benchmarks to show that PAD is more effective in transferring linguistic knowledge than other metric-based distillation approaches.",
      "authors": [
        "Jinjie Ni",
        "Yukun Ma",
        "Wen Wang",
        "Qian Chen",
        "Dianwen Ng",
        "Han Lei",
        "Trung Hieu Nguyen",
        "Chong Zhang",
        "Bin Ma",
        "Erik Cambria"
      ],
      "published": "2023-03-07T02:31:57Z",
      "updated": "2023-03-07T02:31:57Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03600v1",
      "landing_url": "https://arxiv.org/abs/2303.03600v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03600"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on metric-based knowledge distillation between text and speech pretrained models without proposing or evaluating discrete audio tokens, so it fails to meet the discrete token inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on metric-based knowledge distillation between text and speech pretrained models without proposing or evaluating discrete audio tokens, so it fails to meet the discrete token inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
    "abstract": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
    "metadata": {
      "arxiv_id": "2303.03926",
      "title": "Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling",
      "summary": "We propose a cross-lingual neural codec language model, VALL-E X, for cross-lingual speech synthesis. Specifically, we extend VALL-E and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong in-context learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker's voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at \\url{https://aka.ms/vallex}.",
      "authors": [
        "Ziqiang Zhang",
        "Long Zhou",
        "Chengyi Wang",
        "Sanyuan Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Yanqing Liu",
        "Huaming Wang",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Furu Wei"
      ],
      "published": "2023-03-07T14:31:55Z",
      "updated": "2023-03-07T14:31:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.03926v1",
      "landing_url": "https://arxiv.org/abs/2303.03926v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.03926"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Focuses on a neural codec language model that predicts discrete acoustic token sequences for cross-lingual speech synthesis, satisfying inclusion criteria without touching the exclusion cases, so I rate it a 5."
    },
    "round-A_JuniorNano_reasoning": "Focuses on a neural codec language model that predicts discrete acoustic token sequences for cross-lingual speech synthesis, satisfying inclusion criteria without touching the exclusion cases, so I rate it a 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Entropy Coding Improvement for Low-complexity Compressive Auto-encoders",
    "abstract": "End-to-end image and video compression using auto-encoders (AE) offers new appealing perspectives in terms of rate-distortion gains and applications. While most complex models are on par with the latest compression standard like VVC/H.266 on objective metrics, practical implementation and complexity remain strong issues for real-world applications. In this paper, we propose a practical implementation suitable for realistic applications, leading to a low-complexity model. We demonstrate that some gains can be achieved on top of a state-of-the-art low-complexity AE, even when using simpler implementation. Improvements include off-training entropy coding improvement and encoder side Rate Distortion Optimized Quantization. Results show a 19% improvement in BDrate on basic implementation of fully-factorized model, and 15.3% improvement compared to the original implementation. The proposed implementation also allows a direct integration of such approaches on a variety of platforms.",
    "metadata": {
      "arxiv_id": "2303.05962",
      "title": "Entropy Coding Improvement for Low-complexity Compressive Auto-encoders",
      "summary": "End-to-end image and video compression using auto-encoders (AE) offers new appealing perspectives in terms of rate-distortion gains and applications. While most complex models are on par with the latest compression standard like VVC/H.266 on objective metrics, practical implementation and complexity remain strong issues for real-world applications. In this paper, we propose a practical implementation suitable for realistic applications, leading to a low-complexity model. We demonstrate that some gains can be achieved on top of a state-of-the-art low-complexity AE, even when using simpler implementation. Improvements include off-training entropy coding improvement and encoder side Rate Distortion Optimized Quantization. Results show a 19% improvement in BDrate on basic implementation of fully-factorized model, and 15.3% improvement compared to the original implementation. The proposed implementation also allows a direct integration of such approaches on a variety of platforms.",
      "authors": [
        "Franck Galpin",
        "Muhammet Balcilar",
        "Frédéric Lefebvre",
        "Fabien Racapé",
        "Pierre Hellier"
      ],
      "published": "2023-03-10T14:50:18Z",
      "updated": "2023-10-04T09:06:08Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.05962v2",
      "landing_url": "https://arxiv.org/abs/2303.05962v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.05962"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on auto-encoder image/video compression without describing any discrete audio tokenization, so it fails the topic requirement and should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on auto-encoder image/video compression without describing any discrete audio tokenization, so it fails the topic requirement and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
    "abstract": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
    "metadata": {
      "arxiv_id": "2303.06424",
      "title": "Regularized Vector Quantization for Tokenized Image Synthesis",
      "summary": "Quantizing images into discrete representations has been a fundamental problem in unified generative modeling. Predominant approaches learn the discrete representation either in a deterministic manner by selecting the best-matching token or in a stochastic manner by sampling from a predicted distribution. However, deterministic quantization suffers from severe codebook collapse and misalignment with inference stage while stochastic quantization suffers from low codebook utilization and perturbed reconstruction objective. This paper presents a regularized vector quantization framework that allows to mitigate above issues effectively by applying regularization from two perspectives. The first is a prior distribution regularization which measures the discrepancy between a prior token distribution and the predicted token distribution to avoid codebook collapse and low codebook utilization. The second is a stochastic mask regularization that introduces stochasticity during quantization to strike a good balance between inference stage misalignment and unperturbed reconstruction objective. In addition, we design a probabilistic contrastive loss which serves as a calibrated metric to further mitigate the perturbed reconstruction objective. Extensive experiments show that the proposed quantization framework outperforms prevailing vector quantization methods consistently across different generative models including auto-regressive models and diffusion models.",
      "authors": [
        "Jiahui Zhang",
        "Fangneng Zhan",
        "Christian Theobalt",
        "Shijian Lu"
      ],
      "published": "2023-03-11T15:20:54Z",
      "updated": "2023-10-14T06:17:12Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06424v2",
      "landing_url": "https://arxiv.org/abs/2303.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.06424"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vector quantization for image tokens rather than discrete audio tokens, so it fails to meet the audio-centric inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization for image tokens rather than discrete audio tokens, so it fails to meet the audio-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
    "abstract": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
    "metadata": {
      "arxiv_id": "2303.06705",
      "title": "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement",
      "summary": "When enhancing low-light images, many deep learning algorithms are based on the Retinex theory. However, the Retinex model does not consider the corruptions hidden in the dark or introduced by the light-up process. Besides, these methods usually require a tedious multi-stage training pipeline and rely on convolutional neural networks, showing limitations in capturing long-range dependencies. In this paper, we formulate a simple yet principled One-stage Retinex-based Framework (ORF). ORF first estimates the illumination information to light up the low-light image and then restores the corruption to produce the enhanced image. We design an Illumination-Guided Transformer (IGT) that utilizes illumination representations to direct the modeling of non-local interactions of regions with different lighting conditions. By plugging IGT into ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitative and qualitative experiments demonstrate that our Retinexformer significantly outperforms state-of-the-art methods on thirteen benchmarks. The user study and application on low-light object detection also reveal the latent practical values of our method. Code, models, and results are available at https://github.com/caiyuanhao1998/Retinexformer",
      "authors": [
        "Yuanhao Cai",
        "Hao Bian",
        "Jing Lin",
        "Haoqian Wang",
        "Radu Timofte",
        "Yulun Zhang"
      ],
      "published": "2023-03-12T16:54:08Z",
      "updated": "2023-10-26T22:19:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.06705v3",
      "landing_url": "https://arxiv.org/abs/2303.06705v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.06705"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe image enhancement via Retinex Transformer without any discrete audio tokenization or codec design, so it clearly violates inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe image enhancement via Retinex Transformer without any discrete audio tokenization or codec design, so it clearly violates inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Vision Transformers Efficient from A Token Sparsification View",
    "abstract": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
    "metadata": {
      "arxiv_id": "2303.08685",
      "title": "Making Vision Transformers Efficient from A Token Sparsification View",
      "summary": "The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can achieve the same accuracy with more than 100% inference speed improvement and nearly 60% FLOPs reduction; on Swin-(Tiny,Small,Base), we can employ 16 semantic tokens in each window to further speed it up by around 20% with slight accuracy increase. Besides great success in image classification, we also extend our method to video recognition. In addition, we design a STViT-R(ecover) network to restore the detailed spatial information based on the STViT, making it work for downstream tasks, which is powerless for previous token sparsification methods. Experiments demonstrate that our method can achieve competitive results compared to the original networks in object detection and instance segmentation, with over 30% FLOPs reduction for backbone. Code is available at http://github.com/changsn/STViT-R",
      "authors": [
        "Shuning Chang",
        "Pichao Wang",
        "Ming Lin",
        "Fan Wang",
        "David Junhao Zhang",
        "Rong Jin",
        "Mike Zheng Shou"
      ],
      "published": "2023-03-15T15:12:36Z",
      "updated": "2023-03-30T11:56:29Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.08685v2",
      "landing_url": "https://arxiv.org/abs/2303.08685v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.08685"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on token sparsification in vision transformers for image/video tasks and does not involve discrete audio tokenization, codec/quantization, or evaluation, so it fails every inclusion criterion and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on token sparsification in vision transformers for image/video tasks and does not involve discrete audio tokenization, codec/quantization, or evaluation, so it fails every inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LMCodec: A Low Bitrate Speech Codec With Causal Transformer Models",
    "abstract": "We introduce LMCodec, a causal neural speech codec that provides high quality audio at very low bitrates. The backbone of the system is a causal convolutional codec that encodes audio into a hierarchy of coarse-to-fine tokens using residual vector quantization. LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes. A second Transformer predicts the uncertainty of the next codes given the past transmitted codes, and is used to perform conditional entropy coding. A MUSHRA subjective test was conducted and shows that the quality is comparable to reference codecs at higher bitrates. Example audio is available at https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec.",
    "metadata": {
      "arxiv_id": "2303.12984",
      "title": "LMCodec: A Low Bitrate Speech Codec With Causal Transformer Models",
      "summary": "We introduce LMCodec, a causal neural speech codec that provides high quality audio at very low bitrates. The backbone of the system is a causal convolutional codec that encodes audio into a hierarchy of coarse-to-fine tokens using residual vector quantization. LMCodec trains a Transformer language model to predict the fine tokens from the coarse ones in a generative fashion, allowing for the transmission of fewer codes. A second Transformer predicts the uncertainty of the next codes given the past transmitted codes, and is used to perform conditional entropy coding. A MUSHRA subjective test was conducted and shows that the quality is comparable to reference codecs at higher bitrates. Example audio is available at https://mjenrungrot.github.io/chrome-media-audio-papers/publications/lmcodec.",
      "authors": [
        "Teerapat Jenrungrot",
        "Michael Chinen",
        "W. Bastiaan Kleijn",
        "Jan Skoglund",
        "Zalán Borsos",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-03-23T01:27:38Z",
      "updated": "2023-03-23T01:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.12984v1",
      "landing_url": "https://arxiv.org/abs/2303.12984v1",
      "doi": "https://doi.org/10.48550/arXiv.2303.12984"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "LMCodec uses discrete codec tokens via hierarchical residual vector quantization and transformer token modeling, with subjective quality evaluation, so it clearly satisfies inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "LMCodec uses discrete codec tokens via hierarchical residual vector quantization and transformer token modeling, with subjective quality evaluation, so it clearly satisfies inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
    "abstract": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
    "metadata": {
      "arxiv_id": "2303.13336",
      "title": "A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI",
      "summary": "Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered in this survey.",
      "authors": [
        "Chenshuang Zhang",
        "Chaoning Zhang",
        "Sheng Zheng",
        "Mengchun Zhang",
        "Maryam Qamar",
        "Sung-Ho Bae",
        "In So Kweon"
      ],
      "published": "2023-03-23T15:17:15Z",
      "updated": "2023-04-02T09:27:20Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.13336v2",
      "landing_url": "https://arxiv.org/abs/2303.13336v2",
      "doi": "https://doi.org/10.48550/arXiv.2303.13336"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Survey focuses on diffusion-model-based speech synthesis/enhancement and does not cover discrete audio tokenization or codec/quantization schemes, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Survey focuses on diffusion-model-based speech synthesis/enhancement and does not cover discrete audio tokenization or codec/quantization schemes, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CF-Font: Content Fusion for Few-shot Font Generation",
    "abstract": "Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style defined with a few reference images in a target domain. However, the content feature extracted using a representative font might not be optimal. In light of this, we propose a content fusion module (CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also allows to optimize the style representation vector of reference images through a lightweight iterative style-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruction loss (namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at https://github.com/wangchi95/CF-Font.",
    "metadata": {
      "arxiv_id": "2303.14017",
      "title": "CF-Font: Content Fusion for Few-shot Font Generation",
      "summary": "Content and style disentanglement is an effective way to achieve few-shot font generation. It allows to transfer the style of the font image in a source domain to the style defined with a few reference images in a target domain. However, the content feature extracted using a representative font might not be optimal. In light of this, we propose a content fusion module (CFM) to project the content feature into a linear space defined by the content features of basis fonts, which can take the variation of content features caused by different fonts into consideration. Our method also allows to optimize the style representation vector of reference images through a lightweight iterative style-vector refinement (ISR) strategy. Moreover, we treat the 1D projection of a character image as a probability distribution and leverage the distance between two distributions as the reconstruction loss (namely projected character loss, PCL). Compared to L2 or L1 reconstruction loss, the distribution distance pays more attention to the global shape of characters. We have evaluated our method on a dataset of 300 fonts with 6.5k characters each. Experimental results verify that our method outperforms existing state-of-the-art few-shot font generation methods by a large margin. The source code can be found at https://github.com/wangchi95/CF-Font.",
      "authors": [
        "Chi Wang",
        "Min Zhou",
        "Tiezheng Ge",
        "Yuning Jiang",
        "Hujun Bao",
        "Weiwei Xu"
      ],
      "published": "2023-03-24T14:18:40Z",
      "updated": "2024-04-15T08:22:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14017v3",
      "landing_url": "https://arxiv.org/abs/2303.14017v3",
      "doi": "https://doi.org/10.48550/arXiv.2303.14017"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on font generation with content/style disentanglement and not on generating or analyzing discrete audio tokens, so it does not meet any inclusion criteria and clearly violates the topic scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on font generation with content/style disentanglement and not on generating or analyzing discrete audio tokens, so it does not meet any inclusion criteria and clearly violates the topic scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Prediction of the morphological evolution of a splashing drop using an encoder-decoder",
    "abstract": "The impact of a drop on a solid surface is an important phenomenon that has various implications and applications. However, the multiphase nature of this phenomenon causes complications in the prediction of its morphological evolution, especially when the drop splashes. While most machine-learning-based drop-impact studies have centred around physical parameters, this study used a computer-vision strategy by training an encoder-decoder to predict the drop morphologies using image data. Herein, we show that this trained encoder-decoder is able to successfully generate videos that show the morphologies of splashing and non-splashing drops. Remarkably, in each frame of these generated videos, the spreading diameter of the drop was found to be in good agreement with that of the actual videos. Moreover, there was also a high accuracy in splashing/non-splashing prediction. These findings demonstrate the ability of the trained encoder-decoder to generate videos that can accurately represent the drop morphologies. This approach provides a faster and cheaper alternative to experimental and numerical studies.",
    "metadata": {
      "arxiv_id": "2303.14109",
      "title": "Prediction of the morphological evolution of a splashing drop using an encoder-decoder",
      "summary": "The impact of a drop on a solid surface is an important phenomenon that has various implications and applications. However, the multiphase nature of this phenomenon causes complications in the prediction of its morphological evolution, especially when the drop splashes. While most machine-learning-based drop-impact studies have centred around physical parameters, this study used a computer-vision strategy by training an encoder-decoder to predict the drop morphologies using image data. Herein, we show that this trained encoder-decoder is able to successfully generate videos that show the morphologies of splashing and non-splashing drops. Remarkably, in each frame of these generated videos, the spreading diameter of the drop was found to be in good agreement with that of the actual videos. Moreover, there was also a high accuracy in splashing/non-splashing prediction. These findings demonstrate the ability of the trained encoder-decoder to generate videos that can accurately represent the drop morphologies. This approach provides a faster and cheaper alternative to experimental and numerical studies.",
      "authors": [
        "Jingzu Yee",
        "Daichi Igarashi",
        "Shun Miyatake",
        "Yoshiyuki Tagawa"
      ],
      "published": "2023-03-24T16:18:22Z",
      "updated": "2023-03-24T16:18:22Z",
      "categories": [
        "physics.flu-dyn",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2303.14109v1",
      "landing_url": "https://arxiv.org/abs/2303.14109v1",
      "doi": "https://doi.org/10.1088/2632-2153/acc727"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about predicting splashing drop morphology using image-based encoder-decoder, unrelated to discrete audio token generation or evaluation, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper is about predicting splashing drop morphology using image-based encoder-decoder, unrelated to discrete audio token generation or evaluation, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Online variable-length source coding for minimum bitrate LQG control",
    "abstract": "We propose an adaptive coding approach to achieve linear-quadratic-Gaussian (LQG) control with near-minimum bitrate prefix-free feedback. Our approach combines a recent analysis of a quantizer design for minimum rate LQG control with work on universal lossless source coding for sources on countable alphabets. In the aforementioned quantizer design, it was established that the quantizer outputs are an asymptotically stationary, ergodic process. To enable LQG control with provably near-minimum bitrate, the quantizer outputs must be encoded into binary codewords efficiently. This is possible given knowledge of the probability distributions of the quantizer outputs, or of their limiting distribution. Obtaining such knowledge is challenging; the distributions do not readily admit closed form descriptions. This motivates the application of universal source coding. Our main theoretical contribution in this work is a proof that (after an invertible transformation), the quantizer outputs are random variables that fall within an exponential or power-law envelope class (depending on the plant dimension). Using ideas from universal coding on envelope classes, we develop a practical, zero-delay version of these algorithms that operates with fixed precision arithmetic. We evaluate the performance of this algorithm numerically, and demonstrate competitive results with respect to fundamental tradeoffs between bitrate and LQG control performance.",
    "metadata": {
      "arxiv_id": "2304.00593",
      "title": "Online variable-length source coding for minimum bitrate LQG control",
      "summary": "We propose an adaptive coding approach to achieve linear-quadratic-Gaussian (LQG) control with near-minimum bitrate prefix-free feedback. Our approach combines a recent analysis of a quantizer design for minimum rate LQG control with work on universal lossless source coding for sources on countable alphabets. In the aforementioned quantizer design, it was established that the quantizer outputs are an asymptotically stationary, ergodic process. To enable LQG control with provably near-minimum bitrate, the quantizer outputs must be encoded into binary codewords efficiently. This is possible given knowledge of the probability distributions of the quantizer outputs, or of their limiting distribution. Obtaining such knowledge is challenging; the distributions do not readily admit closed form descriptions. This motivates the application of universal source coding. Our main theoretical contribution in this work is a proof that (after an invertible transformation), the quantizer outputs are random variables that fall within an exponential or power-law envelope class (depending on the plant dimension). Using ideas from universal coding on envelope classes, we develop a practical, zero-delay version of these algorithms that operates with fixed precision arithmetic. We evaluate the performance of this algorithm numerically, and demonstrate competitive results with respect to fundamental tradeoffs between bitrate and LQG control performance.",
      "authors": [
        "Travis C. Cuvelier",
        "Takashi Tanaka",
        "Robert W. Heath"
      ],
      "published": "2023-04-02T18:35:27Z",
      "updated": "2023-04-02T18:35:27Z",
      "categories": [
        "cs.IT",
        "eess.SP",
        "eess.SY",
        "math.OC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.00593v1",
      "landing_url": "https://arxiv.org/abs/2304.00593v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.00593"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on LQG control feedback coding, not on discrete audio-token generation/tokenization or codec evaluations, so it violates the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on LQG control feedback coding, not on discrete audio-token generation/tokenization or codec evaluations, so it violates the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding",
    "abstract": "Learning-based video compression has been extensively studied over the past years, but it still has limitations in adapting to various motion patterns and entropy models. In this paper, we propose multi-mode video compression (MMVC), a block wise mode ensemble deep video compression framework that selects the optimal mode for feature domain prediction adapting to different motion patterns. Proposed multi-modes include ConvLSTM-based feature domain prediction, optical flow conditioned feature domain prediction, and feature propagation to address a wide range of cases from static scenes without apparent motions to dynamic scenes with a moving camera. We partition the feature space into blocks for temporal prediction in spatial block-based representations. For entropy coding, we consider both dense and sparse post-quantization residual blocks, and apply optional run-length coding to sparse residuals to improve the compression rate. In this sense, our method uses a dual-mode entropy coding scheme guided by a binary density map, which offers significant rate reduction surpassing the extra cost of transmitting the binary selection map. We validate our scheme with some of the most popular benchmarking datasets. Compared with state-of-the-art video compression schemes and standard codecs, our method yields better or competitive results measured with PSNR and MS-SSIM.",
    "metadata": {
      "arxiv_id": "2304.02273",
      "title": "MMVC: Learned Multi-Mode Video Compression with Block-based Prediction Mode Selection and Density-Adaptive Entropy Coding",
      "summary": "Learning-based video compression has been extensively studied over the past years, but it still has limitations in adapting to various motion patterns and entropy models. In this paper, we propose multi-mode video compression (MMVC), a block wise mode ensemble deep video compression framework that selects the optimal mode for feature domain prediction adapting to different motion patterns. Proposed multi-modes include ConvLSTM-based feature domain prediction, optical flow conditioned feature domain prediction, and feature propagation to address a wide range of cases from static scenes without apparent motions to dynamic scenes with a moving camera. We partition the feature space into blocks for temporal prediction in spatial block-based representations. For entropy coding, we consider both dense and sparse post-quantization residual blocks, and apply optional run-length coding to sparse residuals to improve the compression rate. In this sense, our method uses a dual-mode entropy coding scheme guided by a binary density map, which offers significant rate reduction surpassing the extra cost of transmitting the binary selection map. We validate our scheme with some of the most popular benchmarking datasets. Compared with state-of-the-art video compression schemes and standard codecs, our method yields better or competitive results measured with PSNR and MS-SSIM.",
      "authors": [
        "Bowen Liu",
        "Yu Chen",
        "Rakesh Chowdary Machineni",
        "Shiyu Liu",
        "Hun-Seok Kim"
      ],
      "published": "2023-04-05T07:37:48Z",
      "updated": "2023-04-05T07:37:48Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02273v1",
      "landing_url": "https://arxiv.org/abs/2304.02273v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02273"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on video compression and does not deal with generating or evaluating discrete audio tokens or their associated vocabularies, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on video compression and does not deal with generating or evaluating discrete audio tokens or their associated vocabularies, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Shape complexity estimation using VAE",
    "abstract": "In this paper, we compare methods for estimating the complexity of two-dimensional shapes and introduce a method that exploits reconstruction loss of Variational Autoencoders with different sizes of latent vectors. Although complexity of a shape is not a well defined attribute, different aspects of it can be estimated. We demonstrate that our methods captures some aspects of shape complexity. Code and training details will be publicly available.",
    "metadata": {
      "arxiv_id": "2304.02766",
      "title": "Shape complexity estimation using VAE",
      "summary": "In this paper, we compare methods for estimating the complexity of two-dimensional shapes and introduce a method that exploits reconstruction loss of Variational Autoencoders with different sizes of latent vectors. Although complexity of a shape is not a well defined attribute, different aspects of it can be estimated. We demonstrate that our methods captures some aspects of shape complexity. Code and training details will be publicly available.",
      "authors": [
        "Markus Rothgaenger",
        "Andrew Melnik",
        "Helge Ritter"
      ],
      "published": "2023-04-05T22:14:53Z",
      "updated": "2023-04-05T22:14:53Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.02766v1",
      "landing_url": "https://arxiv.org/abs/2304.02766v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.02766"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper studies VAE-based shape complexity estimation in 2D shapes with no audio, discrete audio tokens, quantization, or codec discussion, so it violates the inclusion scope and fails the topic criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper studies VAE-based shape complexity estimation in 2D shapes with no audio, discrete audio tokens, quantization, or codec discussion, so it violates the inclusion scope and fails the topic criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
    "abstract": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
    "metadata": {
      "arxiv_id": "2304.03635",
      "title": "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image",
      "summary": "3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.",
      "authors": [
        "Changlong Jiang",
        "Yang Xiao",
        "Cunlin Wu",
        "Mingyang Zhang",
        "Jinghong Zheng",
        "Zhiguo Cao",
        "Joey Tianyi Zhou"
      ],
      "published": "2023-04-07T13:30:36Z",
      "updated": "2023-04-07T13:30:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03635v1",
      "landing_url": "https://arxiv.org/abs/2304.03635v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03635"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe 3D interacting hand pose estimation from RGB and not discrete audio token work, so it fails all inclusion criteria and matches exclusion of non-audio domain."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe 3D interacting hand pose estimation from RGB and not discrete audio token work, so it fails all inclusion criteria and matches exclusion of non-audio domain.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
    "abstract": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
    "metadata": {
      "arxiv_id": "2304.03940",
      "title": "Unsupervised Speech Representation Pooling Using Vector Quantization",
      "summary": "With the advent of general-purpose speech representations from large-scale self-supervised models, applying a single model to multiple downstream tasks is becoming a de-facto approach. However, the pooling problem remains; the length of speech representations is inherently variable. The naive average pooling is often used, even though it ignores the characteristics of speech, such as differently lengthed phonemes. Hence, we design a novel pooling method to squash acoustically similar representations via vector quantization, which does not require additional training, unlike attention-based pooling. Further, we evaluate various unsupervised pooling methods on various self-supervised models. We gather diverse methods scattered around speech and text to evaluate on various tasks: keyword spotting, speaker identification, intent classification, and emotion recognition. Finally, we quantitatively and qualitatively analyze our method, comparing it with supervised pooling methods.",
      "authors": [
        "Jeongkyun Park",
        "Kwanghee Choi",
        "Hyunjun Heo",
        "Hyung-Min Park"
      ],
      "published": "2023-04-08T07:03:01Z",
      "updated": "2023-04-08T07:03:01Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.03940v1",
      "landing_url": "https://arxiv.org/abs/2304.03940v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.03940"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The abstract focuses on a vector-quantized pooling method for speech representations without describing any discrete-token/vocabulary definition or token-level modeling, so it doesn’t match the discrete audio token inclusion focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on a vector-quantized pooling method for speech representations without describing any discrete-token/vocabulary definition or token-level modeling, so it doesn’t match the discrete audio token inclusion focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
    "abstract": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
    "metadata": {
      "arxiv_id": "2304.04052",
      "title": "Decoder-Only or Encoder-Decoder? Interpreting Language Model as a Regularized Encoder-Decoder",
      "summary": "The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.",
      "authors": [
        "Zihao Fu",
        "Wai Lam",
        "Qian Yu",
        "Anthony Man-Cho So",
        "Shengding Hu",
        "Zhiyuan Liu",
        "Nigel Collier"
      ],
      "published": "2023-04-08T15:44:29Z",
      "updated": "2023-04-08T15:44:29Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.04052v1",
      "landing_url": "https://arxiv.org/abs/2304.04052v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.04052"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe NLP seq2seq architectures and attention analysis, not any discrete audio token quantization or codec work, so it fails inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe NLP seq2seq architectures and attention analysis, not any discrete audio token quantization or codec work, so it fails inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hard Patches Mining for Masked Image Modeling",
    "abstract": "Masked image modeling (MIM) has attracted much research attention due to its promising potential for learning scalable visual representations. In typical approaches, models usually focus on predicting specific contents of masked patches, and their performances are highly related to pre-defined mask strategies. Intuitively, this procedure can be considered as training a student (the model) on solving given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems, but also stand in the shoes of a teacher to produce a more challenging problem by itself. To this end, we propose Hard Patches Mining (HPM), a brand-new framework for MIM pre-training. We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next. It adopts a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Experiments under various settings demonstrate the effectiveness of HPM in constructing masked images. Furthermore, we empirically find that solely introducing the loss prediction objective leads to powerful representations, verifying the efficacy of the ability to be aware of where is hard to reconstruct.",
    "metadata": {
      "arxiv_id": "2304.05919",
      "title": "Hard Patches Mining for Masked Image Modeling",
      "summary": "Masked image modeling (MIM) has attracted much research attention due to its promising potential for learning scalable visual representations. In typical approaches, models usually focus on predicting specific contents of masked patches, and their performances are highly related to pre-defined mask strategies. Intuitively, this procedure can be considered as training a student (the model) on solving given problems (predict masked patches). However, we argue that the model should not only focus on solving given problems, but also stand in the shoes of a teacher to produce a more challenging problem by itself. To this end, we propose Hard Patches Mining (HPM), a brand-new framework for MIM pre-training. We observe that the reconstruction loss can naturally be the metric of the difficulty of the pre-training task. Therefore, we introduce an auxiliary loss predictor, predicting patch-wise losses first and deciding where to mask next. It adopts a relative relationship learning strategy to prevent overfitting to exact reconstruction loss values. Experiments under various settings demonstrate the effectiveness of HPM in constructing masked images. Furthermore, we empirically find that solely introducing the loss prediction objective leads to powerful representations, verifying the efficacy of the ability to be aware of where is hard to reconstruct.",
      "authors": [
        "Haochen Wang",
        "Kaiyou Song",
        "Junsong Fan",
        "Yuxi Wang",
        "Jin Xie",
        "Zhaoxiang Zhang"
      ],
      "published": "2023-04-12T15:38:23Z",
      "updated": "2023-04-12T15:38:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.05919v1",
      "landing_url": "https://arxiv.org/abs/2304.05919v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.05919"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract focus on masked image modeling, with no discrete audio tokenization, codec, or token-level modeling content, so it fails the inclusion scope and hits the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract focus on masked image modeling, with no discrete audio tokenization, codec, or token-level modeling content, so it fails the inclusion scope and hits the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
    "abstract": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
    "metadata": {
      "arxiv_id": "2304.09116",
      "title": "NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers",
      "summary": "Scaling text-to-speech (TTS) to large-scale, multi-speaker, and in-the-wild datasets is important to capture the diversity in human speech such as speaker identities, prosodies, and styles (e.g., singing). Current large TTS systems usually quantize speech into discrete tokens and use language models to generate these tokens one by one, which suffer from unstable prosody, word skipping/repeating issue, and poor voice quality. In this paper, we develop NaturalSpeech 2, a TTS system that leverages a neural audio codec with residual vector quantizers to get the quantized latent vectors and uses a diffusion model to generate these latent vectors conditioned on text input. To enhance the zero-shot capability that is important to achieve diverse speech synthesis, we design a speech prompting mechanism to facilitate in-context learning in the diffusion model and the duration/pitch predictor. We scale NaturalSpeech 2 to large-scale datasets with 44K hours of speech and singing data and evaluate its voice quality on unseen speakers. NaturalSpeech 2 outperforms previous TTS systems by a large margin in terms of prosody/timbre similarity, robustness, and voice quality in a zero-shot setting, and performs novel zero-shot singing synthesis with only a speech prompt. Audio samples are available at https://speechresearch.github.io/naturalspeech2.",
      "authors": [
        "Kai Shen",
        "Zeqian Ju",
        "Xu Tan",
        "Yanqing Liu",
        "Yichong Leng",
        "Lei He",
        "Tao Qin",
        "Sheng Zhao",
        "Jiang Bian"
      ],
      "published": "2023-04-18T16:31:59Z",
      "updated": "2023-05-30T16:09:10Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09116v3",
      "landing_url": "https://arxiv.org/abs/2304.09116v3",
      "doi": "https://doi.org/10.48550/arXiv.2304.09116"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Uses a neural audio codec with residual vector quantizers to produce discrete latent tokens that fuel the diffusion-based TTS system, so it clearly studies discrete audio token generation and modeling and meets all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Uses a neural audio codec with residual vector quantizers to produce discrete latent tokens that fuel the diffusion-based TTS system, so it clearly studies discrete audio token generation and modeling and meets all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transformer-Based Visual Segmentation: A Survey",
    "abstract": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
    "metadata": {
      "arxiv_id": "2304.09854",
      "title": "Transformer-Based Visual Segmentation: A Survey",
      "summary": "Visual segmentation seeks to partition images, video frames, or point clouds into multiple segments or groups. This technique has numerous real-world applications, such as autonomous driving, image editing, robot sensing, and medical analysis. Over the past decade, deep learning-based methods have made remarkable strides in this area. Recently, transformers, a type of neural network based on self-attention originally designed for natural language processing, have considerably surpassed previous convolutional or recurrent approaches in various vision processing tasks. Specifically, vision transformers offer robust, unified, and even simpler solutions for various segmentation tasks. This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements. We first review the background, encompassing problem definitions, datasets, and prior convolutional methods. Next, we summarize a meta-architecture that unifies all recent transformer-based approaches. Based on this meta-architecture, we examine various method designs, including modifications to the meta-architecture and associated applications. We also present several closely related settings, including 3D point cloud segmentation, foundation model tuning, domain-aware segmentation, efficient segmentation, and medical segmentation. Additionally, we compile and re-evaluate the reviewed methods on several well-established datasets. Finally, we identify open challenges in this field and propose directions for future research. The project page can be found at https://github.com/lxtGH/Awesome-Segmentation-With-Transformer. We will also continually monitor developments in this rapidly evolving field.",
      "authors": [
        "Xiangtai Li",
        "Henghui Ding",
        "Haobo Yuan",
        "Wenwei Zhang",
        "Jiangmiao Pang",
        "Guangliang Cheng",
        "Kai Chen",
        "Ziwei Liu",
        "Chen Change Loy"
      ],
      "published": "2023-04-19T17:59:02Z",
      "updated": "2024-08-04T04:30:45Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.09854v4",
      "landing_url": "https://arxiv.org/abs/2304.09854v4",
      "doi": "https://doi.org/10.48550/arXiv.2304.09854"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Topic is transformer-based visual segmentation (vision) not about discrete audio tokens or tokenization, so it fails the inclusion criteria entirely and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Topic is transformer-based visual segmentation (vision) not about discrete audio tokens or tokenization, so it fails the inclusion criteria entirely and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
    "abstract": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
    "metadata": {
      "arxiv_id": "2304.11750",
      "title": "DiffVoice: Text-to-Speech with Latent Diffusion",
      "summary": "In this work, we present DiffVoice, a novel text-to-speech model based on latent diffusion. We propose to first encode speech signals into a phoneme-rate latent representation with a variational autoencoder enhanced by adversarial training, and then jointly model the duration and the latent representation with a diffusion model. Subjective evaluations on LJSpeech and LibriTTS datasets demonstrate that our method beats the best publicly available systems in naturalness. By adopting recent generative inverse problem solving algorithms for diffusion models, DiffVoice achieves the state-of-the-art performance in text-based speech editing, and zero-shot adaptation.",
      "authors": [
        "Zhijun Liu",
        "Yiwei Guo",
        "Kai Yu"
      ],
      "published": "2023-04-23T21:05:33Z",
      "updated": "2023-04-23T21:05:33Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.11750v1",
      "landing_url": "https://arxiv.org/abs/2304.11750v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.11750"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "DiffVoice focuses on continuous latent representations and diffusion-based modeling without presenting any discrete audio tokenization/quantization framework, so it fails the core inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "DiffVoice focuses on continuous latent representations and diffusion-based modeling without presenting any discrete audio tokenization/quantization framework, so it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
    "abstract": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
    "metadata": {
      "arxiv_id": "2304.12404",
      "title": "Semantic Tokenizer for Enhanced Natural Language Processing",
      "summary": "Traditionally, NLP performance improvement has been focused on improving models and increasing the number of model parameters. NLP vocabulary construction has remained focused on maximizing the number of words represented through subword regularization. We present a novel tokenizer that uses semantics to drive vocabulary construction. The tokenizer includes a trainer that uses stemming to enhance subword formation. Further optimizations and adaptations are implemented to minimize the number of words that cannot be encoded. The encoder is updated to integrate with the trainer. The tokenizer is implemented as a drop-in replacement for the SentencePiece tokenizer. The new tokenizer more than doubles the number of wordforms represented in the vocabulary. The enhanced vocabulary significantly improves NLP model convergence, and improves quality of word and sentence embeddings. Our experimental results show top performance on two Glue tasks using BERT-base, improving on models more than 50X in size.",
      "authors": [
        "Sandeep Mehta",
        "Darpan Shah",
        "Ravindra Kulkarni",
        "Cornelia Caragea"
      ],
      "published": "2023-04-24T19:33:41Z",
      "updated": "2023-04-24T19:33:41Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2304.12404v1",
      "landing_url": "https://arxiv.org/abs/2304.12404v1",
      "doi": "https://doi.org/10.48550/arXiv.2304.12404"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on semantic tokenizer for NLP text/embeddings without any mention of audio signals, codecs, quantization, or discrete audio tokens, so it doesn’t satisfy the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on semantic tokenizer for NLP text/embeddings without any mention of audio signals, codecs, quantization, or discrete audio tokens, so it doesn’t satisfy the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
    "abstract": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
    "metadata": {
      "arxiv_id": "2305.00225",
      "title": "Just Noticeable Difference-aware Per-Scene Bitrate-laddering for Adaptive Video Streaming",
      "summary": "In video streaming applications, a fixed set of bitrate-resolution pairs (known as a bitrate ladder) is typically used during the entire streaming session. However, an optimized bitrate ladder per scene may result in (i) decreased storage or delivery costs or/and (ii) increased Quality of Experience. This paper introduces a Just Noticeable Difference (JND)-aware per-scene bitrate ladder prediction scheme (JASLA) for adaptive video-on-demand streaming applications. JASLA predicts jointly optimized resolutions and corresponding constant rate factors (CRFs) using spatial and temporal complexity features for a given set of target bitrates for every scene, which yields an efficient constrained Variable Bitrate encoding. Moreover, bitrate-resolution pairs that yield distortion lower than one JND are eliminated. Experimental results show that, on average, JASLA yields bitrate savings of 34.42% and 42.67% to maintain the same PSNR and VMAF, respectively, compared to the reference HTTP Live Streaming (HLS) bitrate ladder Constant Bitrate encoding using x265 HEVC encoder, where the maximum resolution of streaming is Full HD (1080p). Moreover, a 54.34% average cumulative decrease in storage space is observed.",
      "authors": [
        "Vignesh V Menon",
        "Jingwen Zhu",
        "Prajit T Rajendran",
        "Hadi Amirpour",
        "Patrick Le Callet",
        "Christian Timmerer"
      ],
      "published": "2023-04-29T10:32:44Z",
      "updated": "2023-04-29T10:32:44Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.00225v1",
      "landing_url": "https://arxiv.org/abs/2305.00225v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.00225"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe video bitrate ladder optimization and JND-aware scene encoding, with no indication of discrete audio token generation, quantization, or token-level modeling, so it violates the inclusion focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe video bitrate ladder optimization and JND-aware scene encoding, with no indication of discrete audio token generation, quantization, or token-level modeling, so it violates the inclusion focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec",
    "abstract": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}",
    "metadata": {
      "arxiv_id": "2305.02765",
      "title": "HiFi-Codec: Group-residual Vector quantization for High Fidelity Audio Codec",
      "summary": "Audio codec models are widely used in audio communication as a crucial technique for compressing audio into discrete representations. Nowadays, audio codec models are increasingly utilized in generation fields as intermediate representations. For instance, AudioLM is an audio generation model that uses the discrete representation of SoundStream as a training target, while VALL-E employs the Encodec model as an intermediate feature to aid TTS tasks. Despite their usefulness, two challenges persist: (1) training these audio codec models can be difficult due to the lack of publicly available training processes and the need for large-scale data and GPUs; (2) achieving good reconstruction performance requires many codebooks, which increases the burden on generation models. In this study, we propose a group-residual vector quantization (GRVQ) technique and use it to develop a novel \\textbf{Hi}gh \\textbf{Fi}delity Audio Codec model, HiFi-Codec, which only requires 4 codebooks. We train all the models using publicly available TTS data such as LibriTTS, VCTK, AISHELL, and more, with a total duration of over 1000 hours, using 8 GPUs. Our experimental results show that HiFi-Codec outperforms Encodec in terms of reconstruction performance despite requiring only 4 codebooks. To facilitate research in audio codec and generation, we introduce AcademiCodec, the first open-source audio codec toolkit that offers training codes and pre-trained models for Encodec, SoundStream, and HiFi-Codec. Code and pre-trained model can be found on: \\href{https://github.com/yangdongchao/AcademiCodec}{https://github.com/yangdongchao/AcademiCodec}",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Rongjie Huang",
        "Jinchuan Tian",
        "Chao Weng",
        "Yuexian Zou"
      ],
      "published": "2023-05-04T12:11:13Z",
      "updated": "2023-05-07T09:22:04Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.02765v2",
      "landing_url": "https://arxiv.org/abs/2305.02765v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.02765"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper introduces a discrete neural audio codec (HiFi-Codec) using group-residual quantization with explicit codebook/token design, reconstruction evaluation, and open-source assets, fully matching the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a discrete neural audio codec (HiFi-Codec) using group-residual quantization with explicit codebook/token design, reconstruction evaluation, and open-source assets, fully matching the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN",
    "abstract": "Deep convolutional neural network has made great achievements in sea-land clutter classification for over-the-horizon-radar (OTHR). The premise is that a large number of labeled training samples must be provided for a sea-land clutter classifier. In practical engineering applications, it is relatively easy to obtain label-free sea-land clutter samples. However, the labeling process is extremely cumbersome and requires expertise in the field of OTHR. To solve this problem, we propose an improved generative adversarial network, namely weighted loss semi-supervised generative adversarial network (WL-SSGAN). Specifically, we propose a joint feature matching loss by weighting the middle layer features of the discriminator of semi-supervised generative adversarial network. Furthermore, we propose the weighted loss of WL-SSGAN by linearly weighting standard adversarial loss and joint feature matching loss. The semi-supervised classification performance of WL-SSGAN is evaluated on a sea-land clutter dataset. The experimental results show that WL-SSGAN can improve the performance of the fully supervised classifier with only a small number of labeled samples by utilizing a large number of unlabeled sea-land clutter samples. Further, the proposed weighted loss is superior to both the adversarial loss and the feature matching loss. Additionally, we compare WL-SSGAN with conventional semi-supervised classification methods and demonstrate that WL-SSGAN achieves the highest classification accuracy.",
    "metadata": {
      "arxiv_id": "2305.04021",
      "title": "A Sea-Land Clutter Classification Framework for Over-the-Horizon-Radar Based on Weighted Loss Semi-supervised GAN",
      "summary": "Deep convolutional neural network has made great achievements in sea-land clutter classification for over-the-horizon-radar (OTHR). The premise is that a large number of labeled training samples must be provided for a sea-land clutter classifier. In practical engineering applications, it is relatively easy to obtain label-free sea-land clutter samples. However, the labeling process is extremely cumbersome and requires expertise in the field of OTHR. To solve this problem, we propose an improved generative adversarial network, namely weighted loss semi-supervised generative adversarial network (WL-SSGAN). Specifically, we propose a joint feature matching loss by weighting the middle layer features of the discriminator of semi-supervised generative adversarial network. Furthermore, we propose the weighted loss of WL-SSGAN by linearly weighting standard adversarial loss and joint feature matching loss. The semi-supervised classification performance of WL-SSGAN is evaluated on a sea-land clutter dataset. The experimental results show that WL-SSGAN can improve the performance of the fully supervised classifier with only a small number of labeled samples by utilizing a large number of unlabeled sea-land clutter samples. Further, the proposed weighted loss is superior to both the adversarial loss and the feature matching loss. Additionally, we compare WL-SSGAN with conventional semi-supervised classification methods and demonstrate that WL-SSGAN achieves the highest classification accuracy.",
      "authors": [
        "Xiaoxuan Zhang",
        "Zengfu Wang",
        "Kun Lu",
        "Quan Pan",
        "Yang Li"
      ],
      "published": "2023-05-06T11:45:45Z",
      "updated": "2023-05-06T11:45:45Z",
      "categories": [
        "cs.CV",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.04021v1",
      "landing_url": "https://arxiv.org/abs/2305.04021v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.04021"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper deals with radar sea-land clutter classification using semi-supervised GANs but does not discuss discrete audio tokens, tokenizers, or any quantized vocabularies, so it fails all inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper deals with radar sea-land clutter classification using semi-supervised GANs but does not discuss discrete audio tokens, tokenizers, or any quantized vocabularies, so it fails all inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SR+Codec: a Benchmark of Super-Resolution for Video Compression Bitrate Reduction",
    "abstract": "In recent years, there has been significant interest in Super-Resolution (SR), which focuses on generating a high-resolution image from a low-resolution input. Deep learning-based methods for super-resolution have been particularly popular and have shown impressive results on various benchmarks. However, research indicates that these methods may not perform as well on strongly compressed videos. We developed a super-resolution benchmark to analyze SR's capacity to upscale compressed videos. Our dataset employed video codecs based on five widely-used compression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 19 popular SR models using our benchmark and evaluated their ability to restore details and their susceptibility to compression artifacts. To get an accurate perceptual ranking of SR models, we conducted a crowd-sourced side-by-side comparison of their outputs. We found that some SR models, combined with compression, allow us to reduce the video bitrate without significant loss of quality. We also compared a range of image and video quality metrics with subjective scores to evaluate their accuracy on super-resolved compressed videos. The benchmark is publicly available at https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html",
    "metadata": {
      "arxiv_id": "2305.04844",
      "title": "SR+Codec: a Benchmark of Super-Resolution for Video Compression Bitrate Reduction",
      "summary": "In recent years, there has been significant interest in Super-Resolution (SR), which focuses on generating a high-resolution image from a low-resolution input. Deep learning-based methods for super-resolution have been particularly popular and have shown impressive results on various benchmarks. However, research indicates that these methods may not perform as well on strongly compressed videos. We developed a super-resolution benchmark to analyze SR's capacity to upscale compressed videos. Our dataset employed video codecs based on five widely-used compression standards: H.264, H.265, H.266, AV1, and AVS3. We assessed 19 popular SR models using our benchmark and evaluated their ability to restore details and their susceptibility to compression artifacts. To get an accurate perceptual ranking of SR models, we conducted a crowd-sourced side-by-side comparison of their outputs. We found that some SR models, combined with compression, allow us to reduce the video bitrate without significant loss of quality. We also compared a range of image and video quality metrics with subjective scores to evaluate their accuracy on super-resolved compressed videos. The benchmark is publicly available at https://videoprocessing.ai/benchmarks/super-resolution-for-video-compression.html",
      "authors": [
        "Evgeney Bogatyrev",
        "Ivan Molodetskikh",
        "Dmitriy Vatolin"
      ],
      "published": "2023-05-08T16:42:55Z",
      "updated": "2024-12-04T17:34:13Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.04844v3",
      "landing_url": "https://arxiv.org/abs/2305.04844v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.04844"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies video super-resolution and compression artifacts without any discussion of discrete audio tokenizers, quantized vocabularies, or audio-centric token modeling, so it fails the inclusion and hits the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper studies video super-resolution and compression artifacts without any discussion of discrete audio tokenizers, quantized vocabularies, or audio-centric token modeling, so it fails the inclusion and hits the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
    "abstract": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets -- two in the legal domain and two in the biomedical domain, each with two levels of label granularity -- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.",
    "metadata": {
      "arxiv_id": "2305.05627",
      "title": "An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text",
      "summary": "Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets -- two in the legal domain and two in the biomedical domain, each with two levels of label granularity -- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.",
      "authors": [
        "Yova Kementchedjhieva",
        "Ilias Chalkidis"
      ],
      "published": "2023-05-09T17:13:53Z",
      "updated": "2023-05-09T17:13:53Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.05627v1",
      "landing_url": "https://arxiv.org/abs/2305.05627v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.05627"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Topic is encoder-decoder multi-label classification for legal/biomedical text and does not address discrete audio tokens or codec/token evaluations, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Topic is encoder-decoder multi-label classification for legal/biomedical text and does not address discrete audio tokens or codec/token evaluations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
    "abstract": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
    "metadata": {
      "arxiv_id": "2305.06788",
      "title": "Vector Quantization with Error Uniformly Distributed over an Arbitrary Set",
      "summary": "For uniform scalar quantization, the error distribution is approximately a uniform distribution over an interval (which is also a 1-dimensional ball). Nevertheless, for lattice vector quantization, the error distribution is uniform not over a ball, but over the basic cell of the quantization lattice. In this paper, we construct vector quantizers with periodic properties, where the error is uniformly distributed over the n-ball, or any other prescribed set. We then prove upper and lower bounds on the entropy of the quantized signals. We also discuss how our construction can be applied to give a randomized quantization scheme with a nonuniform error distribution.",
      "authors": [
        "Chih Wei Ling",
        "Cheuk Ting Li"
      ],
      "published": "2023-05-11T13:23:42Z",
      "updated": "2024-01-24T13:44:44Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.06788v4",
      "landing_url": "https://arxiv.org/abs/2305.06788v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.06788"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on generic vector quantization with error distributions over arbitrary sets and does not target discrete audio tokens, codec/token construction, or audio-language-modeling use cases specified, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on generic vector quantization with error distributions over arbitrary sets and does not target discrete audio tokens, codec/token construction, or audio-language-modeling use cases specified, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SoundStorm: Efficient Parallel Audio Generation",
    "abstract": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
    "metadata": {
      "arxiv_id": "2305.09636",
      "title": "SoundStorm: Efficient Parallel Audio Generation",
      "summary": "We present SoundStorm, a model for efficient, non-autoregressive audio generation. SoundStorm receives as input the semantic tokens of AudioLM, and relies on bidirectional attention and confidence-based parallel decoding to generate the tokens of a neural audio codec. Compared to the autoregressive generation approach of AudioLM, our model produces audio of the same quality and with higher consistency in voice and acoustic conditions, while being two orders of magnitude faster. SoundStorm generates 30 seconds of audio in 0.5 seconds on a TPU-v4. We demonstrate the ability of our model to scale audio generation to longer sequences by synthesizing high-quality, natural dialogue segments, given a transcript annotated with speaker turns and a short prompt with the speakers' voices.",
      "authors": [
        "Zalán Borsos",
        "Matt Sharifi",
        "Damien Vincent",
        "Eugene Kharitonov",
        "Neil Zeghidour",
        "Marco Tagliasacchi"
      ],
      "published": "2023-05-16T17:41:25Z",
      "updated": "2023-05-16T17:41:25Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.09636v1",
      "landing_url": "https://arxiv.org/abs/2305.09636v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.09636"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "SoundStorm clearly generates discrete codec tokens from semantic inputs while evaluating generation quality and speed, matching inclusion requirements and no exclusion triggers, so include it (score: 5)."
    },
    "round-A_JuniorNano_reasoning": "SoundStorm clearly generates discrete codec tokens from semantic inputs while evaluating generation quality and speed, matching inclusion requirements and no exclusion triggers, so include it (score: 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A unified front-end framework for English text-to-speech synthesis",
    "abstract": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
    "metadata": {
      "arxiv_id": "2305.10666",
      "title": "A unified front-end framework for English text-to-speech synthesis",
      "summary": "The front-end is a critical component of English text-to-speech (TTS) systems, responsible for extracting linguistic features that are essential for a text-to-speech model to synthesize speech, such as prosodies and phonemes. The English TTS front-end typically consists of a text normalization (TN) module, a prosody word prosody phrase (PWPP) module, and a grapheme-to-phoneme (G2P) module. However, current research on the English TTS front-end focuses solely on individual modules, neglecting the interdependence between them and resulting in sub-optimal performance for each module. Therefore, this paper proposes a unified front-end framework that captures the dependencies among the English TTS front-end modules. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art (SOTA) performance in all modules.",
      "authors": [
        "Zelin Ying",
        "Chen Li",
        "Yu Dong",
        "Qiuqiang Kong",
        "Qiao Tian",
        "Yuanyuan Huo",
        "Yuxuan Wang"
      ],
      "published": "2023-05-18T02:57:54Z",
      "updated": "2024-03-25T10:59:04Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.10666v3",
      "landing_url": "https://arxiv.org/abs/2305.10666v3",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447144"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper describes an English TTS front-end framework and focuses on interconnected linguistic modules (TN, PWPP, G2P) with no mention of discrete audio tokens, quantization, or vocabulary-based tokenization, so it fails the inclusion scope targeting discrete audio-token research."
    },
    "round-A_JuniorNano_reasoning": "This paper describes an English TTS front-end framework and focuses on interconnected linguistic modules (TN, PWPP, G2P) with no mention of discrete audio tokens, quantization, or vocabulary-based tokenization, so it fails the inclusion scope targeting discrete audio-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
    "abstract": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
    "metadata": {
      "arxiv_id": "2305.11403",
      "title": "Efficient Mixed Transformer for Single Image Super-Resolution",
      "summary": "Recently, Transformer-based methods have achieved impressive results in single image super-resolution (SISR). However, the lack of locality mechanism and high complexity limit their application in the field of super-resolution (SR). To solve these problems, we propose a new method, Efficient Mixed Transformer (EMT) in this study. Specifically, we propose the Mixed Transformer Block (MTB), consisting of multiple consecutive transformer layers, in some of which the Pixel Mixer (PM) is used to replace the Self-Attention (SA). PM can enhance the local knowledge aggregation with pixel shifting operations. At the same time, no additional complexity is introduced as PM has no parameters and floating-point operations. Moreover, we employ striped window for SA (SWSA) to gain an efficient global dependency modelling by utilizing image anisotropy. Experimental results show that EMT outperforms the existing methods on benchmark dataset and achieved state-of-the-art performance. The Code is available at https://github.com/Fried-Rice-Lab/FriedRiceLab.",
      "authors": [
        "Ling Zheng",
        "Jinchen Zhu",
        "Jinpeng Shi",
        "Shizhuang Weng"
      ],
      "published": "2023-05-19T03:19:38Z",
      "updated": "2023-06-19T06:56:23Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11403v5",
      "landing_url": "https://arxiv.org/abs/2305.11403v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.11403"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on mixed transformer design for image super-resolution and mentions no discrete audio tokens, tokenization, or codec quantization related to audio, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on mixed transformer design for image super-resolution and mentions no discrete audio tokens, tokenization, or codec quantization related to audio, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization",
    "abstract": "Existing vector quantization (VQ) based autoregressive models follow a two-stage generation paradigm that first learns a codebook to encode images as discrete codes, and then completes generation based on the learned codebook. However, they encode fixed-size image regions into fixed-length codes and ignore their naturally different information densities, which results in insufficiency in important regions and redundancy in unimportant ones, and finally degrades the generation quality and speed. Moreover, the fixed-length coding leads to an unnatural raster-scan autoregressive generation. To address the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization VAE (DQ-VAE) which encodes image regions into variable-length codes based on their information densities for an accurate and compact code representation. (2) DQ-Transformer which thereby generates images autoregressively from coarse-grained (smooth regions with fewer codes) to fine-grained (details regions with more codes) by modeling the position and content of codes in each granularity alternately, through a novel stacked-transformer architecture and shared-content, non-shared position input layers designs. Comprehensive experiments on various generation tasks validate our superiorities in both effectiveness and efficiency. Code will be released at https://github.com/CrossmodalGroup/DynamicVectorQuantization.",
    "metadata": {
      "arxiv_id": "2305.11718",
      "title": "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization",
      "summary": "Existing vector quantization (VQ) based autoregressive models follow a two-stage generation paradigm that first learns a codebook to encode images as discrete codes, and then completes generation based on the learned codebook. However, they encode fixed-size image regions into fixed-length codes and ignore their naturally different information densities, which results in insufficiency in important regions and redundancy in unimportant ones, and finally degrades the generation quality and speed. Moreover, the fixed-length coding leads to an unnatural raster-scan autoregressive generation. To address the problem, we propose a novel two-stage framework: (1) Dynamic-Quantization VAE (DQ-VAE) which encodes image regions into variable-length codes based on their information densities for an accurate and compact code representation. (2) DQ-Transformer which thereby generates images autoregressively from coarse-grained (smooth regions with fewer codes) to fine-grained (details regions with more codes) by modeling the position and content of codes in each granularity alternately, through a novel stacked-transformer architecture and shared-content, non-shared position input layers designs. Comprehensive experiments on various generation tasks validate our superiorities in both effectiveness and efficiency. Code will be released at https://github.com/CrossmodalGroup/DynamicVectorQuantization.",
      "authors": [
        "Mengqi Huang",
        "Zhendong Mao",
        "Zhuowei Chen",
        "Yongdong Zhang"
      ],
      "published": "2023-05-19T14:56:05Z",
      "updated": "2023-05-19T14:56:05Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11718v1",
      "landing_url": "https://arxiv.org/abs/2305.11718v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.11718"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on variable-length vector quantization for image autoregressive generation rather than discrete audio tokens/codecs, so it fails the audio-centric inclusion criteria and violates exclusion rules."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on variable-length vector quantization for image autoregressive generation rather than discrete audio tokens/codecs, so it fails the audio-centric inclusion criteria and violates exclusion rules.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pengi: An Audio Language Model for Audio Tasks",
    "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question & Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
    "metadata": {
      "arxiv_id": "2305.11834",
      "title": "Pengi: An Audio Language Model for Audio Tasks",
      "summary": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question & Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
      "authors": [
        "Soham Deshmukh",
        "Benjamin Elizalde",
        "Rita Singh",
        "Huaming Wang"
      ],
      "published": "2023-05-19T17:20:56Z",
      "updated": "2024-01-19T01:42:32Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11834v2",
      "landing_url": "https://arxiv.org/abs/2305.11834v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.11834"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Pengi frames audio tasks as text generation with continuous embeddings feeding a language model and does not describe any discrete tokenization, quantization, or codebook design for audio tokens, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Pengi frames audio tasks as text generation with continuous embeddings feeding a language model and does not describe any discrete tokenization, quantization, or codebook design for audio tokens, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks.",
    "metadata": {
      "arxiv_id": "2305.11862",
      "title": "Reducing Sequence Length by Predicting Edit Operations with Large Language Models",
      "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, the models that generate all target tokens in such tasks have a tendency to simply copy the input text as is, without making needed changes, because the difference between input and output texts is minimal in the training data. This is also inefficient because the computational cost grows quadratically with the target sequence length with Transformer. This paper proposes predicting edit spans for the source text for local sequence transduction tasks. Representing an edit span with a position of the source text and corrected tokens, we can reduce the length of the target sequence and the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit spans. Experiments show that the proposed method achieves comparable performance to the baseline in four tasks, paraphrasing, formality style transfer, GEC, and text simplification, despite reducing the length of the target text by as small as 21%. Furthermore, we report that the task-specific fine-tuning with the proposed method achieved state-of-the-art performance in the four tasks.",
      "authors": [
        "Masahiro Kaneko",
        "Naoaki Okazaki"
      ],
      "published": "2023-05-19T17:51:05Z",
      "updated": "2023-10-21T00:57:02Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.11862v2",
      "landing_url": "https://arxiv.org/abs/2305.11862v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.11862"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Topic focuses on textual edit-span prediction without any discrete audio token generation, quantization, or codec work, so it fails all inclusion and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Topic focuses on textual edit-span prediction without any discrete audio token generation, quantization, or codec work, so it fails all inclusion and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "abstract": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
    "metadata": {
      "arxiv_id": "2305.12200",
      "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
      "summary": "Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/",
      "authors": [
        "Yuyue Wang",
        "Huan Xiao",
        "Yihan Wu",
        "Ruihua Song"
      ],
      "published": "2023-05-20T14:24:45Z",
      "updated": "2023-05-20T14:24:45Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12200v1",
      "landing_url": "https://arxiv.org/abs/2305.12200v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12200"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on low-resource expressive TTS for stand-up comedians without any mention of discrete audio tokenization, quantization, or finite vocabulary representations, so it fails the inclusion criteria (and matches exclusion for continuous features), hence exclude."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on low-resource expressive TTS for stand-up comedians without any mention of discrete audio tokenization, quantization, or finite vocabulary representations, so it fails the inclusion criteria (and matches exclusion for continuous features), hence exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
    "abstract": "Finetuning pretrained language models (LMs) have enabled appealing performance on a diverse array of tasks. The intriguing task-agnostic property has driven a shifted focus from task-specific to task-agnostic distillation of LMs. While task-agnostic, compute-efficient, performance-preserved LMs can be yielded by task-agnostic distillation, previous studies mainly sit in distillation of either encoder-only LMs (e.g., BERT) or decoder-only ones (e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g., T5) can posit very distinguished behaviors. Frustratingly, we discover that existing task-agnostic distillation methods can fail to handle the distillation of encoder-decoder LMs. To the demand, we explore a few paths and uncover a path named as MiniEnD that successfully tackles the distillation of encoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language understanding and abstractive summarization. The results showcase that MiniEnD is generally effective and is competitive compared to other alternatives. We further scale MiniEnD up to distillation of 3B encoder-decoder language models with interpolated distillation. The results imply the opportunities and challenges in distilling large language models (e.g., LLaMA).",
    "metadata": {
      "arxiv_id": "2305.12330",
      "title": "Task-agnostic Distillation of Encoder-Decoder Language Models",
      "summary": "Finetuning pretrained language models (LMs) have enabled appealing performance on a diverse array of tasks. The intriguing task-agnostic property has driven a shifted focus from task-specific to task-agnostic distillation of LMs. While task-agnostic, compute-efficient, performance-preserved LMs can be yielded by task-agnostic distillation, previous studies mainly sit in distillation of either encoder-only LMs (e.g., BERT) or decoder-only ones (e.g., GPT) yet largely neglect that distillation of encoder-decoder LMs (e.g., T5) can posit very distinguished behaviors. Frustratingly, we discover that existing task-agnostic distillation methods can fail to handle the distillation of encoder-decoder LMs. To the demand, we explore a few paths and uncover a path named as MiniEnD that successfully tackles the distillation of encoder-decoder LMs in a task-agnostic fashion. We examine MiniEnD on language understanding and abstractive summarization. The results showcase that MiniEnD is generally effective and is competitive compared to other alternatives. We further scale MiniEnD up to distillation of 3B encoder-decoder language models with interpolated distillation. The results imply the opportunities and challenges in distilling large language models (e.g., LLaMA).",
      "authors": [
        "Chen Zhang",
        "Yang Yang",
        "Jingang Wang",
        "Dawei Song"
      ],
      "published": "2023-05-21T03:35:45Z",
      "updated": "2023-05-21T03:35:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.12330v1",
      "landing_url": "https://arxiv.org/abs/2305.12330v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.12330"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract cover task-agnostic distillation of encoder-decoder language models, which has nothing to do with discrete audio token generation, quantization, or codec evaluation, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract cover task-agnostic distillation of encoder-decoder language models, which has nothing to do with discrete audio token generation, quantization, or codec evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Defenses via Vector Quantization",
    "abstract": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
    "metadata": {
      "arxiv_id": "2305.13651",
      "title": "Adversarial Defenses via Vector Quantization",
      "summary": "Adversarial attacks pose significant challenges to the robustness of modern deep neural networks in computer vision, and defending these networks against adversarial attacks has attracted intense research efforts. Among various defense strategies, preprocessing-based defenses are practically appealing since there is no need to train the network under protection. However, such approaches typically do not achieve comparable robustness as other methods such as adversarial training. In this paper, we propose a novel framework for preprocessing-based defenses, where a vector quantizer is used as a preprocessor. This framework, inspired by and extended from Randomized Discretization (RandDisc), is theoretically principled by rate-distortion theory: indeed, RandDisc may be viewed as a scalar quantizer, and rate-distortion theory suggests that such quantization schemes are inferior to vector quantization. In our framework, the preprocessing vector quantizer treats the input image as a collection of patches and finds a set of representative patches based on the patch distributions; each original patch is then modified according to the representative patches close to it. We present two lightweight defenses in this framework, referred to as patched RandDisc (pRD) and sliding-window RandDisc (swRD), where the patches are disjoint in the former and overlapping in the latter. We show that vector-quantization-based defenses have certifiable robust accuracy and that pRD and swRD demonstrate state-of-the-art performances, surpassing RandDisc by a large margin. Notably, the proposed defenses possess the obfuscated gradients property. Our experiments however show that pRD and swRD remain effective under the STE and EOT attacks, which are designed specifically for defenses with gradient obfuscation. ...",
      "authors": [
        "Zhiyi Dong",
        "Yongyi Mao"
      ],
      "published": "2023-05-23T03:49:41Z",
      "updated": "2025-07-09T23:51:43Z",
      "categories": [
        "cs.LG",
        "cs.CR",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13651v2",
      "landing_url": "https://arxiv.org/abs/2305.13651v2",
      "doi": "https://doi.org/10.1016/j.neucom.2025.130703"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on image-based adversarial defenses via vector quantization with no mention of discrete audio tokenization, so it fails the audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on image-based adversarial defenses via vector quantization with no mention of discrete audio tokenization, so it fails the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EfficientSpeech: An On-Device Text to Speech Model",
    "abstract": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
    "metadata": {
      "arxiv_id": "2305.13905",
      "title": "EfficientSpeech: An On-Device Text to Speech Model",
      "summary": "State of the art (SOTA) neural text to speech (TTS) models can generate natural-sounding synthetic voices. These models are characterized by large memory footprints and substantial number of operations due to the long-standing focus on speech quality with cloud inference in mind. Neural TTS models are generally not designed to perform standalone speech syntheses on resource-constrained and no Internet access edge devices. In this work, an efficient neural TTS called EfficientSpeech that synthesizes speech on an ARM CPU in real-time is proposed. EfficientSpeech uses a shallow non-autoregressive pyramid-structure transformer forming a U-Network. EfficientSpeech has 266k parameters and consumes 90 MFLOPS only or about 1% of the size and amount of computation in modern compact models such as Mixer-TTS. EfficientSpeech achieves an average mel generation real-time factor of 104.3 on an RPi4. Human evaluation shows only a slight degradation in audio quality as compared to FastSpeech2.",
      "authors": [
        "Rowel Atienza"
      ],
      "published": "2023-05-23T10:28:41Z",
      "updated": "2023-05-23T10:28:41Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.13905v1",
      "landing_url": "https://arxiv.org/abs/2305.13905v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.13905"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "EfficientSpeech focuses on designing an efficient TTS model with continuous mel-generation and deployment metrics, without describing any discrete audio tokenization/quantization pipeline or vocabulary, so it fails the inclusion criteria centered on discrete tokens."
    },
    "round-A_JuniorNano_reasoning": "EfficientSpeech focuses on designing an efficient TTS model with continuous mel-generation and deployment metrics, without describing any discrete audio tokenization/quantization pipeline or vocabulary, so it fails the inclusion criteria centered on discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras",
    "abstract": "Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estimate depth values with an average RMS error of 0.076, a 44.5% improvement compared to an optical flow-based comparison method. Our method can also improve depth map quality by estimating depth values for missing and invalidated pixels. The proposed method can be easily applied to existing depth cameras and facilitates the use of depth cameras in applications that require high-speed depth estimation. We also showcase the effectiveness of the framework in upsampling different sparse datasets e.g. video object segmentation. As a demonstration of our method, we integrated our framework into existing body tracking systems and demonstrated the robustness of the proposed method in such applications.",
    "metadata": {
      "arxiv_id": "2305.14731",
      "title": "AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras",
      "summary": "Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estimate depth values with an average RMS error of 0.076, a 44.5% improvement compared to an optical flow-based comparison method. Our method can also improve depth map quality by estimating depth values for missing and invalidated pixels. The proposed method can be easily applied to existing depth cameras and facilitates the use of depth cameras in applications that require high-speed depth estimation. We also showcase the effectiveness of the framework in upsampling different sparse datasets e.g. video object segmentation. As a demonstration of our method, we integrated our framework into existing body tracking systems and demonstrated the robustness of the proposed method in such applications.",
      "authors": [
        "Peyman Gholami",
        "Robert Xiao"
      ],
      "published": "2023-05-24T05:09:43Z",
      "updated": "2023-05-24T05:09:43Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14731v1",
      "landing_url": "https://arxiv.org/abs/2305.14731v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.14731"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on depth map reconstruction for video cameras and lacks any discrete audio tokenization, so it fails to meet inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on depth map reconstruction for video cameras and lacks any discrete audio tokenization, so it fails to meet inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
    "abstract": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
    "metadata": {
      "arxiv_id": "2305.14858",
      "title": "Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers",
      "summary": "Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.",
      "authors": [
        "Zixuan Jiang",
        "Jiaqi Gu",
        "Hanqing Zhu",
        "David Z. Pan"
      ],
      "published": "2023-05-24T08:08:26Z",
      "updated": "2023-10-26T04:32:49Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.14858v2",
      "landing_url": "https://arxiv.org/abs/2305.14858v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.14858"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on Transformer normalization equivalences rather than discrete audio tokenization/token learning, so it fails all inclusion criteria and matches the exclusion for non-audio-token research."
    },
    "round-A_JuniorNano_reasoning": "Focuses on Transformer normalization equivalences rather than discrete audio tokenization/token learning, so it fails all inclusion criteria and matches the exclusion for non-audio-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
    "abstract": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
    "metadata": {
      "arxiv_id": "2305.15255",
      "title": "Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM",
      "summary": "We present Spectron, a novel approach to adapting pre-trained large language models (LLMs) to perform spoken question answering (QA) and speech continuation. By endowing the LLM with a pre-trained speech encoder, our model becomes able to take speech inputs and generate speech outputs. The entire system is trained end-to-end and operates directly on spectrograms, simplifying our architecture. Key to our approach is a training objective that jointly supervises speech recognition, text continuation, and speech synthesis using only paired speech-text pairs, enabling a `cross-modal' chain-of-thought within a single decoding pass. Our method surpasses existing spoken language models in speaker preservation and semantic coherence. Furthermore, the proposed model improves upon direct initialization in retaining the knowledge of the original LLM as demonstrated through spoken QA datasets. We release our audio samples (https://michelleramanovich.github.io/spectron/spectron) and spoken QA dataset (https://github.com/google-research-datasets/LLAMA1-Test-Set).",
      "authors": [
        "Eliya Nachmani",
        "Alon Levkovitch",
        "Roy Hirsch",
        "Julian Salazar",
        "Chulayuth Asawaroengchai",
        "Soroosh Mariooryad",
        "Ehud Rivlin",
        "RJ Skerry-Ryan",
        "Michelle Tadmor Ramanovich"
      ],
      "published": "2023-05-24T15:39:43Z",
      "updated": "2024-05-31T01:29:27Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15255v4",
      "landing_url": "https://arxiv.org/abs/2305.15255v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.15255"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on directly operating on spectrograms and speech encoders/decoders without defining or evaluating a discrete audio token vocabulary or quantization scheme, so it fails the inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on directly operating on spectrograms and speech encoders/decoders without defining or evaluating a discrete audio token vocabulary or quantization scheme, so it fails the inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
    "abstract": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
    "metadata": {
      "arxiv_id": "2305.15688",
      "title": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking",
      "summary": "Most existing RGB-based trackers target low frame rate benchmarks of around 30 frames per second. This setting restricts the tracker's functionality in the real world, especially for fast motion. Event-based cameras as bioinspired sensors provide considerable potential for high frame rate tracking due to their high temporal resolution. However, event-based cameras cannot offer fine-grained texture information like conventional cameras. This unique complementarity motivates us to combine conventional frames and events for high frame rate object tracking under various challenging conditions. Inthispaper, we propose an end-to-end network consisting of multi-modality alignment and fusion modules to effectively combine meaningful information from both modalities at different measurement rates. The alignment module is responsible for cross-style and cross-frame-rate alignment between frame and event modalities under the guidance of the moving cues furnished by events. While the fusion module is accountable for emphasizing valuable features and suppressing noise information by the mutual complement between the two modalities. Extensive experiments show that the proposed approach outperforms state-of-the-art trackers by a significant margin in high frame rate tracking. With the FE240hz dataset, our approach achieves high frame rate tracking up to 240Hz.",
      "authors": [
        "Jiqing Zhang",
        "Yuanchen Wang",
        "Wenxi Liu",
        "Meng Li",
        "Jinpeng Bai",
        "Baocai Yin",
        "Xin Yang"
      ],
      "published": "2023-05-25T03:34:24Z",
      "updated": "2023-05-25T03:34:24Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15688v1",
      "landing_url": "https://arxiv.org/abs/2305.15688v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15688"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses entirely on high frame rate RGB+event tracking, without any reference to discrete audio tokenization, vocabularies, codecs, or token-level evaluation, so it clearly violates the inclusion criteria and meets the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses entirely on high frame rate RGB+event tracking, without any reference to discrete audio tokenization, vocabularies, codecs, or token-level evaluation, so it clearly violates the inclusion criteria and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Neural Music Generation",
    "abstract": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation. Our samples are available at https://Efficient-MeLoDy.github.io/.",
    "metadata": {
      "arxiv_id": "2305.15719",
      "title": "Efficient Neural Music Generation",
      "summary": "Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.",
      "authors": [
        "Max W. Y. Lam",
        "Qiao Tian",
        "Tang Li",
        "Zongyu Yin",
        "Siyuan Feng",
        "Ming Tu",
        "Yuliang Ji",
        "Rui Xia",
        "Mingbo Ma",
        "Xuchen Song",
        "Jitong Chen",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2023-05-25T05:02:35Z",
      "updated": "2023-05-25T05:02:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.15719v1",
      "landing_url": "https://arxiv.org/abs/2305.15719v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.15719"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper builds on MusicLM semantic discrete tokens and discusses using LM-guided diffusion to efficiently decode them for high-quality music generation, meeting the inclusion focus on discrete audio tokens while providing concrete decoder/token usage even if continuous diffusion is involved."
    },
    "round-A_JuniorNano_reasoning": "The paper builds on MusicLM semantic discrete tokens and discusses using LM-guided diffusion to efficiently decode them for high-quality music generation, meeting the inclusion focus on discrete audio tokens while providing concrete decoder/token usage even if continuous diffusion is involved.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
    "abstract": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
    "metadata": {
      "arxiv_id": "2305.16608",
      "title": "AudioDec: An Open-source Streaming High-fidelity Neural Audio Codec",
      "summary": "A good audio codec for live applications such as telecommunication is characterized by three key properties: (1) compression, i.e.\\ the bitrate that is required to transmit the signal should be as low as possible; (2) latency, i.e.\\ encoding and decoding the signal needs to be fast enough to enable communication without or with only minimal noticeable delay; and (3) reconstruction quality of the signal. In this work, we propose an open-source, streamable, and real-time neural audio codec that achieves strong performance along all three axes: it can reconstruct highly natural sounding 48~kHz speech signals while operating at only 12~kbps and running with less than 6~ms (GPU)/10~ms (CPU) latency. An efficient training paradigm is also demonstrated for developing such neural audio codecs for real-world scenarios. Both objective and subjective evaluations using the VCTK corpus are provided. To sum up, AudioDec is a well-developed plug-and-play benchmark for audio codec applications.",
      "authors": [
        "Yi-Chiao Wu",
        "Israel D. Gebru",
        "Dejan Marković",
        "Alexander Richard"
      ],
      "published": "2023-05-26T04:01:16Z",
      "updated": "2023-05-26T04:01:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.16608v1",
      "landing_url": "https://arxiv.org/abs/2305.16608v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10096509"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "AudioDec presents an open-source neural codec with discrete codes for streaming high-fidelity audio plus quality evaluations, satisfying the token/codec focus and performance reporting requirements, so it should be included definitively."
    },
    "round-A_JuniorNano_reasoning": "AudioDec presents an open-source neural codec with discrete codes for streaming high-fidelity audio plus quality evaluations, satisfying the token/codec focus and performance reporting requirements, so it should be included definitively.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions",
    "abstract": "Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\\widetilde{O}(\\sqrt{T} + C^{\\textsf{P}})$ regret where $C^{\\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that further refinements of the algorithm not only maintains the same regret bound, but also simultaneously adapts to easier environments (where losses are generated in a certain stochastically constrained manner as in Jin et al. [2021]) and achieves $\\widetilde{O}(U + \\sqrt{UC^{\\textsf{L}}} + C^{\\textsf{P}})$ regret, where $U$ is some standard gap-dependent coefficient and $C^{\\textsf{L}}$ is the amount of corruption on losses.",
    "metadata": {
      "arxiv_id": "2305.17380",
      "title": "No-Regret Online Reinforcement Learning with Adversarial Losses and Transitions",
      "summary": "Existing online learning algorithms for adversarial Markov Decision Processes achieve ${O}(\\sqrt{T})$ regret after $T$ rounds of interactions even if the loss functions are chosen arbitrarily by an adversary, with the caveat that the transition function has to be fixed. This is because it has been shown that adversarial transition functions make no-regret learning impossible. Despite such impossibility results, in this work, we develop algorithms that can handle both adversarial losses and adversarial transitions, with regret increasing smoothly in the degree of maliciousness of the adversary. More concretely, we first propose an algorithm that enjoys $\\widetilde{O}(\\sqrt{T} + C^{\\textsf{P}})$ regret where $C^{\\textsf{P}}$ measures how adversarial the transition functions are and can be at most ${O}(T)$. While this algorithm itself requires knowledge of $C^{\\textsf{P}}$, we further develop a black-box reduction approach that removes this requirement. Moreover, we also show that further refinements of the algorithm not only maintains the same regret bound, but also simultaneously adapts to easier environments (where losses are generated in a certain stochastically constrained manner as in Jin et al. [2021]) and achieves $\\widetilde{O}(U + \\sqrt{UC^{\\textsf{L}}} + C^{\\textsf{P}})$ regret, where $U$ is some standard gap-dependent coefficient and $C^{\\textsf{L}}$ is the amount of corruption on losses.",
      "authors": [
        "Tiancheng Jin",
        "Junyan Liu",
        "Chloé Rouyer",
        "William Chang",
        "Chen-Yu Wei",
        "Haipeng Luo"
      ],
      "published": "2023-05-27T06:10:17Z",
      "updated": "2023-10-26T17:12:30Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.17380v3",
      "landing_url": "https://arxiv.org/abs/2305.17380v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.17380"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper addresses adversarial reinforcement learning with regret bounds, unrelated to discrete audio tokens/tokenizers so it fails every inclusion criterion and matches the exclusion domain."
    },
    "round-A_JuniorNano_reasoning": "Paper addresses adversarial reinforcement learning with regret bounds, unrelated to discrete audio tokens/tokenizers so it fails every inclusion criterion and matches the exclusion domain.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Geometric Algebra Transformer",
    "abstract": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
    "metadata": {
      "arxiv_id": "2305.18415",
      "title": "Geometric Algebra Transformer",
      "summary": "Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient 16-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from n-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.",
      "authors": [
        "Johann Brehmer",
        "Pim de Haan",
        "Sönke Behrends",
        "Taco Cohen"
      ],
      "published": "2023-05-28T18:48:50Z",
      "updated": "2023-11-20T08:31:51Z",
      "categories": [
        "cs.LG",
        "cs.RO",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18415v3",
      "landing_url": "https://arxiv.org/abs/2305.18415v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.18415"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on a general Geometric Algebra Transformer for 3D geometric data, which does not address discrete audio tokenization, codec/quantization, or token-level evaluation, so it fails all inclusion criteria and matches exclusion factors."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a general Geometric Algebra Transformer for 3D geometric data, which does not address discrete audio tokenization, codec/quantization, or token-level evaluation, so it fails all inclusion criteria and matches exclusion factors.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Wave to Syntax: Probing spoken language models for syntax",
    "abstract": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
    "metadata": {
      "arxiv_id": "2305.18957",
      "title": "Wave to Syntax: Probing spoken language models for syntax",
      "summary": "Understanding which information is encoded in deep models of spoken and written language has been the focus of much research in recent years, as it is crucial for debugging and improving these architectures. Most previous work has focused on probing for speaker characteristics, acoustic and phonological information in models of spoken language, and for syntactic information in models of written language. Here we focus on the encoding of syntax in several self-supervised and visually grounded models of spoken language. We employ two complementary probing methods, combined with baselines and reference representations to quantify the degree to which syntactic structure is encoded in the activations of the target models. We show that syntax is captured most prominently in the middle layers of the networks, and more explicitly within models with more parameters.",
      "authors": [
        "Gaofei Shen",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2023-05-30T11:43:18Z",
      "updated": "2023-05-30T11:43:18Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.18957v1",
      "landing_url": "https://arxiv.org/abs/2305.18957v1",
      "doi": "https://doi.org/10.21437/Interspeech.2023-679"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on syntax probing in spoken language models without any mention of discrete audio tokenization or quantized codebooks, so it does not meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on syntax probing in spoken language models without any mention of discrete audio tokenization or quantized codebooks, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Compression with Bayesian Implicit Neural Representations",
    "abstract": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $β$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $β$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity.",
    "metadata": {
      "arxiv_id": "2305.19185",
      "title": "Compression with Bayesian Implicit Neural Representations",
      "summary": "Many common types of data can be represented as functions that map coordinates to signal values, such as pixel locations to RGB values in the case of an image. Based on this view, data can be compressed by overfitting a compact neural network to its functional representation and then encoding the network weights. However, most current solutions for this are inefficient, as quantization to low-bit precision substantially degrades the reconstruction quality. To address this issue, we propose overfitting variational Bayesian neural networks to the data and compressing an approximate posterior weight sample using relative entropy coding instead of quantizing and entropy coding it. This strategy enables direct optimization of the rate-distortion performance by minimizing the $β$-ELBO, and target different rate-distortion trade-offs for a given network architecture by adjusting $β$. Moreover, we introduce an iterative algorithm for learning prior weight distributions and employ a progressive refinement process for the variational posterior that significantly enhances performance. Experiments show that our method achieves strong performance on image and audio compression while retaining simplicity.",
      "authors": [
        "Zongyu Guo",
        "Gergely Flamich",
        "Jiajun He",
        "Zhibo Chen",
        "José Miguel Hernández-Lobato"
      ],
      "published": "2023-05-30T16:29:52Z",
      "updated": "2023-10-29T09:38:17Z",
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19185v5",
      "landing_url": "https://arxiv.org/abs/2305.19185v5",
      "doi": "https://doi.org/10.48550/arXiv.2305.19185"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on compressing continuous neural representations with Bayesian overfitting and entropy coding, without discussing finite-vocabulary discrete audio tokens or quantized tokenizers, so it fails the inclusion criteria and meets exclusion conditions for continuous representations."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on compressing continuous neural representations with Bayesian overfitting and entropy coding, without discussing finite-vocabulary discrete audio tokens or quantized tokenizers, so it fails the inclusion criteria and meets exclusion conditions for continuous representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
    "abstract": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
    "metadata": {
      "arxiv_id": "2305.19269",
      "title": "Make-A-Voice: Unified Voice Synthesis With Discrete Representation",
      "summary": "Various applications of voice synthesis have been developed independently despite the fact that they generate \"voice\" as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a \"coarse-to-fine\" approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io",
      "authors": [
        "Rongjie Huang",
        "Chunlei Zhang",
        "Yongqi Wang",
        "Dongchao Yang",
        "Luping Liu",
        "Zhenhui Ye",
        "Ziyue Jiang",
        "Chao Weng",
        "Zhou Zhao",
        "Dong Yu"
      ],
      "published": "2023-05-30T17:59:26Z",
      "updated": "2023-05-30T17:59:26Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19269v1",
      "landing_url": "https://arxiv.org/abs/2305.19269v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19269"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Make-A-Voice clearly focuses on discrete semantic and acoustic tokens with quantized stages and codec-like modeling for scalable voice synthesis, satisfying all inclusion criteria without hitting any exclusion rules."
    },
    "round-A_JuniorNano_reasoning": "Make-A-Voice clearly focuses on discrete semantic and acoustic tokens with quantized stages and codec-like modeling for scalable voice synthesis, satisfying all inclusion criteria without hitting any exclusion rules.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
    "abstract": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
    "metadata": {
      "arxiv_id": "2305.19533",
      "title": "Lightening-Transformer: A Dynamically-operated Optically-interconnected Photonic Transformer Accelerator",
      "summary": "The wide adoption and significant computing resource of attention-based transformers, e.g., Vision Transformers and large language models (LLM), have driven the demand for efficient hardware accelerators. There is a growing interest in exploring photonics as an alternative technology to digital electronics due to its high energy efficiency and ultra-fast processing speed. Photonic accelerators have shown promising results for CNNs, which mainly rely on weight-static linear operations. However, they encounter issues when efficiently supporting Transformer architectures, questioning the applicability of photonics to advanced ML tasks. The primary hurdle lies in their inefficiency in handling unique workloads in Transformers, i.e., dynamic and full-range tensor multiplication. In this work, we propose Lightening-Transformer, the first light-empowered, high-performance, and energy-efficient photonic Transformer accelerator. To overcome prior designs' fundamental limitations, we introduce a novel dynamically-operated photonic tensor core, DPTC, a crossbar array of interference-based optical vector dot-product engines supporting highly parallel, dynamic, and full-range matrix multiplication. Furthermore, we design a dedicated accelerator that integrates our novel photonic computing cores with photonic interconnects for inter-core data broadcast, fully unleashing the power of optics. Comprehensive evaluations show that ours achieves >2.6x energy and >12x latency reductions compared to prior photonic accelerators and delivers the lowest energy cost and 2 to 3 orders of magnitude lower energy-delay product compared to electronic Transformer accelerators, all while maintaining digital-comparable accuracy. Our work highlights the immense potential of photonics for advanced ML workloads, such as Transformer-backboned LLM. Our work is available at https://github.com/zhuhanqing/Lightening-Transformer.",
      "authors": [
        "Hanqing Zhu",
        "Jiaqi Gu",
        "Hanrui Wang",
        "Zixuan Jiang",
        "Zhekai Zhang",
        "Rongxing Tang",
        "Chenghao Feng",
        "Song Han",
        "Ray T. Chen",
        "David Z. Pan"
      ],
      "published": "2023-05-31T03:37:11Z",
      "updated": "2023-12-31T23:22:41Z",
      "categories": [
        "cs.ET",
        "cs.AR",
        "physics.optics"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19533v3",
      "landing_url": "https://arxiv.org/abs/2305.19533v3",
      "doi": "https://doi.org/10.48550/arXiv.2305.19533"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on photonic hardware accelerators for Transformers, unrelated to discrete audio tokens or tokenization methods, so it fails to meet any inclusion criteria and matches exclusion of non-audio token research."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on photonic hardware accelerators for Transformers, unrelated to discrete audio tokens or tokenization methods, so it fails to meet any inclusion criteria and matches exclusion of non-audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
    "abstract": "Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.",
    "metadata": {
      "arxiv_id": "2305.19567",
      "title": "DC CoMix TTS: An End-to-End Expressive TTS with Discrete Code Collaborated with Mixer",
      "summary": "Despite the huge successes made in neutral TTS, content-leakage remains a challenge. In this paper, we propose a new input representation and simple architecture to achieve improved prosody modeling. Inspired by the recent success in the use of discrete code in TTS, we introduce discrete code to the input of the reference encoder. Specifically, we leverage the vector quantizer from the audio compression model to exploit the diverse acoustic information it has already been trained on. In addition, we apply the modified MLP-Mixer to the reference encoder, making the architecture lighter. As a result, we train the prosody transfer TTS in an end-to-end manner. We prove the effectiveness of our method through both subjective and objective evaluations. We demonstrate that the reference encoder learns better speaker-independent prosody when discrete code is utilized as input in the experiments. In addition, we obtain comparable results even when fewer parameters are inputted.",
      "authors": [
        "Yerin Choi",
        "Myoung-Wan Koo"
      ],
      "published": "2023-05-31T05:36:22Z",
      "updated": "2023-06-28T11:42:15Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19567v4",
      "landing_url": "https://arxiv.org/abs/2305.19567v4",
      "doi": "https://doi.org/10.48550/arXiv.2305.19567"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Reasoning: Uses discrete codes from a neural audio codec for prosody transfer and reports evaluations, so it matches the discrete-token inclusion focus, suggesting inclusion is preferable (score 4)."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: Uses discrete codes from a neural audio codec for prosody transfer and reports evaluations, so it matches the discrete-token inclusion focus, suggesting inclusion is preferable (score 4).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
    "abstract": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
    "metadata": {
      "arxiv_id": "2305.19750",
      "title": "Text-to-Speech Pipeline for Swiss German -- A comparison",
      "summary": "In this work, we studied the synthesis of Swiss German speech using different Text-to-Speech (TTS) models. We evaluated the TTS models on three corpora, and we found, that VITS models performed best, hence, using them for further testing. We also introduce a new method to evaluate TTS models by letting the discriminator of a trained vocoder GAN model predict whether a given waveform is human or synthesized. In summary, our best model delivers speech synthesis for different Swiss German dialects with previously unachieved quality.",
      "authors": [
        "Tobias Bollinger",
        "Jan Deriu",
        "Manfred Vogel"
      ],
      "published": "2023-05-31T11:33:18Z",
      "updated": "2023-05-31T11:33:18Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19750v1",
      "landing_url": "https://arxiv.org/abs/2305.19750v1",
      "doi": "https://doi.org/10.48550/arXiv.2305.19750"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on TTS model comparison for Swiss German without describing any discrete tokenization/quantization or vocab-level modeling, so it does not meet the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on TTS model comparison for Swiss German without describing any discrete tokenization/quantization or vocab-level modeling, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
    "abstract": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
    "metadata": {
      "arxiv_id": "2305.19957",
      "title": "DeepSolo++: Let Transformer Decoder with Explicit Points Solo for Multilingual Text Spotting",
      "summary": "End-to-end text spotting aims to integrate scene text detection and recognition into a unified framework. Dealing with the relationship between the two sub-tasks plays a pivotal role in designing effective spotters. Although Transformer-based methods eliminate the heuristic post-processing, they still suffer from the synergy issue between the sub-tasks and low training efficiency. Besides, they overlook the exploring on multilingual text spotting which requires an extra script identification task. In this paper, we present DeepSolo++, a simple DETR-like baseline that lets a single decoder with explicit points solo for text detection, recognition, and script identification simultaneously. Technically, for each text instance, we represent the character sequence as ordered points and model them with learnable explicit point queries. After passing a single decoder, the point queries have encoded requisite text semantics and locations, thus can be further decoded to the center line, boundary, script, and confidence of text via very simple prediction heads in parallel. Furthermore, we show the surprisingly good extensibility of our method, in terms of character class, language type, and task. On the one hand, our method not only performs well in English scenes but also masters the transcription with complex font structure and a thousand-level character classes, such as Chinese. On the other hand, our DeepSolo++ achieves better performance on the additionally introduced script identification task with a simpler training pipeline compared with previous methods. In addition, our models are also compatible with line annotations, which require much less annotation cost than polygons. The code is available at \\url{https://github.com/ViTAE-Transformer/DeepSolo}.",
      "authors": [
        "Maoyuan Ye",
        "Jing Zhang",
        "Shanshan Zhao",
        "Juhua Liu",
        "Tongliang Liu",
        "Bo Du",
        "Dacheng Tao"
      ],
      "published": "2023-05-31T15:44:00Z",
      "updated": "2024-03-18T13:30:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2305.19957v2",
      "landing_url": "https://arxiv.org/abs/2305.19957v2",
      "doi": "https://doi.org/10.48550/arXiv.2305.19957"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on visual text spotting with Transformer decoder and multilingual script identification, which does not address discrete audio token generation or modeling, so it fails the inclusion scope entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on visual text spotting with Transformer decoder and multilingual script identification, which does not address discrete audio token generation or modeling, so it fails the inclusion scope entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
    "abstract": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
    "metadata": {
      "arxiv_id": "2306.00697",
      "title": "How Generative Spoken Language Modeling Encodes Noisy Speech: Investigation from Phonetics to Syntactics",
      "summary": "We examine the speech modeling potential of generative spoken language modeling (GSLM), which involves using learned symbols derived from data rather than phonemes for speech analysis and synthesis. Since GSLM facilitates textless spoken language processing, exploring its effectiveness is critical for paving the way for novel paradigms in spoken-language processing. This paper presents the findings of GSLM's encoding and decoding effectiveness at the spoken-language and speech levels. Through speech resynthesis experiments, we revealed that resynthesis errors occur at the levels ranging from phonology to syntactics and GSLM frequently resynthesizes natural but content-altered speech.",
      "authors": [
        "Joonyong Park",
        "Shinnosuke Takamichi",
        "Tomohiko Nakamura",
        "Kentaro Seki",
        "Detai Xin",
        "Hiroshi Saruwatari"
      ],
      "published": "2023-06-01T14:07:19Z",
      "updated": "2023-06-01T14:07:19Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.00697v1",
      "landing_url": "https://arxiv.org/abs/2306.00697v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.00697"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The study evaluates generative spoken language modeling via learned symbols that replace phonemes and analyzes encoding/decoding across resynthesis, matching the discrete audio token focus, so it seems better to include despite limited detail on token specs."
    },
    "round-A_JuniorNano_reasoning": "The study evaluates generative spoken language modeling via learned symbols that replace phonemes and analyzes encoding/decoding across resynthesis, matching the discrete audio token focus, so it seems better to include despite limited detail on token specs.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hierarchical Attention Encoder Decoder",
    "abstract": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
    "metadata": {
      "arxiv_id": "2306.01070",
      "title": "Hierarchical Attention Encoder Decoder",
      "summary": "Recent advances in large language models have shown that autoregressive modeling can generate complex and novel sequences that have many real-world applications. However, these models must generate outputs autoregressively, which becomes time-consuming when dealing with long sequences. Hierarchical autoregressive approaches that compress data have been proposed as a solution, but these methods still generate outputs at the original data frequency, resulting in slow and memory-intensive models. In this paper, we propose a model based on the Hierarchical Recurrent Encoder Decoder (HRED) architecture. This model independently encodes input sub-sequences without global context, processes these sequences using a lower-frequency model, and decodes outputs at the original data frequency. By interpreting the encoder as an implicitly defined embedding matrix and using sampled softmax estimation, we develop a training algorithm that can train the entire model without a high-frequency decoder, which is the most memory and compute-intensive part of hierarchical approaches. In a final, brief phase, we train the decoder to generate data at the original granularity. Our algorithm significantly reduces memory requirements for training autoregressive models and it also improves the total training wall-clock time.",
      "authors": [
        "Asier Mujika"
      ],
      "published": "2023-06-01T18:17:23Z",
      "updated": "2023-06-01T18:17:23Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01070v1",
      "landing_url": "https://arxiv.org/abs/2306.01070v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.01070"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes a hierarchical autoregressive text modeling architecture without any mention of discrete audio tokens, quantization, or codec/tokenizer design, so it clearly fails the inclusion focus on discrete audio token generation and instead falls under the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a hierarchical autoregressive text modeling architecture without any mention of discrete audio tokens, quantization, or codec/tokenizer design, so it clearly fails the inclusion focus on discrete audio token generation and instead falls under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
    "abstract": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
    "metadata": {
      "arxiv_id": "2306.01506",
      "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
      "summary": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
      "authors": [
        "Marvin Lavechin",
        "Yaya Sy",
        "Hadrien Titeux",
        "María Andrea Cruz Blandón",
        "Okko Räsänen",
        "Hervé Bredin",
        "Emmanuel Dupoux",
        "Alejandrina Cristia"
      ],
      "published": "2023-06-02T12:54:38Z",
      "updated": "2023-06-08T12:22:30Z",
      "categories": [
        "cs.CL",
        "eess.AS",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.01506v2",
      "landing_url": "https://arxiv.org/abs/2306.01506v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-978"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper only presents a spoken-language-model benchmark without discussing discrete audio tokenizers/codecs or quantized vocabularies, so it fails to meet the inclusion criteria focused on discrete token design or evaluation."
    },
    "round-A_JuniorNano_reasoning": "The paper only presents a spoken-language-model benchmark without discussing discrete audio tokenizers/codecs or quantized vocabularies, so it fails to meet the inclusion criteria focused on discrete token design or evaluation.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Constant Sequence Extension for Fast Search Using Weighted Hamming Distance",
    "abstract": "Representing visual data using compact binary codes is attracting increasing attention as binary codes are used as direct indices into hash table(s) for fast non-exhaustive search. Recent methods show that ranking binary codes using weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating query-adaptive weights for each bit can better retrieve query-related items. However, search using WHD is slower than that using HD. One main challenge is that the complexity of extending a monotone increasing sequence using WHD to probe buckets in hash table(s) for existing methods is at least proportional to the square of the sequence length, while that using HD is proportional to the sequence length. To overcome this challenge, we propose a novel fast non-exhaustive search method using WHD. The key idea is to design a constant sequence extension algorithm to perform each sequence extension in constant computational complexity and the total complexity is proportional to the sequence length, which is justified by theoretical analysis. Experimental results show that our method is faster than other WHD-based search methods. Also, compared with the HD-based non-exhaustive search method, our method has comparable efficiency but retrieves more query-related items for the dataset of up to one billion items.",
    "metadata": {
      "arxiv_id": "2306.03612",
      "title": "Constant Sequence Extension for Fast Search Using Weighted Hamming Distance",
      "summary": "Representing visual data using compact binary codes is attracting increasing attention as binary codes are used as direct indices into hash table(s) for fast non-exhaustive search. Recent methods show that ranking binary codes using weighted Hamming distance (WHD) rather than Hamming distance (HD) by generating query-adaptive weights for each bit can better retrieve query-related items. However, search using WHD is slower than that using HD. One main challenge is that the complexity of extending a monotone increasing sequence using WHD to probe buckets in hash table(s) for existing methods is at least proportional to the square of the sequence length, while that using HD is proportional to the sequence length. To overcome this challenge, we propose a novel fast non-exhaustive search method using WHD. The key idea is to design a constant sequence extension algorithm to perform each sequence extension in constant computational complexity and the total complexity is proportional to the sequence length, which is justified by theoretical analysis. Experimental results show that our method is faster than other WHD-based search methods. Also, compared with the HD-based non-exhaustive search method, our method has comparable efficiency but retrieves more query-related items for the dataset of up to one billion items.",
      "authors": [
        "Zhenyu Weng",
        "Huiping Zhuang",
        "Haizhou Li",
        "Zhiping Lin"
      ],
      "published": "2023-06-06T12:00:53Z",
      "updated": "2023-06-06T12:00:53Z",
      "categories": [
        "cs.DS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.03612v1",
      "landing_url": "https://arxiv.org/abs/2306.03612v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.03612"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on fast weighted Hamming distance search over binary codes for general visual data retrieval and has no connection to discrete audio tokenization, so it does not meet the specified inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on fast weighted Hamming distance search over binary codes for general visual data retrieval and has no connection to discrete audio tokenization, so it does not meet the specified inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Point Cloud Video Anomaly Detection Based on Point Spatio-Temporal Auto-Encoder",
    "abstract": "Video anomaly detection has great potential in enhancing safety in the production and monitoring of crucial areas. Currently, most video anomaly detection methods are based on RGB modality, but its redundant semantic information may breach the privacy of residents or patients. The 3D data obtained by depth camera and LiDAR can accurately locate anomalous events in 3D space while preserving human posture and motion information. Identifying individuals through the point cloud is difficult due to its sparsity, which protects personal privacy. In this study, we propose Point Spatio-Temporal Auto-Encoder (PSTAE), an autoencoder framework that uses point cloud videos as input to detect anomalies in point cloud videos. We introduce PSTOp and PSTTransOp to maintain spatial geometric and temporal motion information in point cloud videos. To measure the reconstruction loss of the proposed autoencoder framework, we propose a reconstruction loss measurement strategy based on a shallow feature extractor. Experimental results on the TIMo dataset show that our method outperforms currently representative depth modality-based methods in terms of AUROC and has superior performance in detecting Medical Issue anomalies. These results suggest the potential of point cloud modality in video anomaly detection. Our method sets a new state-of-the-art (SOTA) on the TIMo dataset.",
    "metadata": {
      "arxiv_id": "2306.04466",
      "title": "Point Cloud Video Anomaly Detection Based on Point Spatio-Temporal Auto-Encoder",
      "summary": "Video anomaly detection has great potential in enhancing safety in the production and monitoring of crucial areas. Currently, most video anomaly detection methods are based on RGB modality, but its redundant semantic information may breach the privacy of residents or patients. The 3D data obtained by depth camera and LiDAR can accurately locate anomalous events in 3D space while preserving human posture and motion information. Identifying individuals through the point cloud is difficult due to its sparsity, which protects personal privacy. In this study, we propose Point Spatio-Temporal Auto-Encoder (PSTAE), an autoencoder framework that uses point cloud videos as input to detect anomalies in point cloud videos. We introduce PSTOp and PSTTransOp to maintain spatial geometric and temporal motion information in point cloud videos. To measure the reconstruction loss of the proposed autoencoder framework, we propose a reconstruction loss measurement strategy based on a shallow feature extractor. Experimental results on the TIMo dataset show that our method outperforms currently representative depth modality-based methods in terms of AUROC and has superior performance in detecting Medical Issue anomalies. These results suggest the potential of point cloud modality in video anomaly detection. Our method sets a new state-of-the-art (SOTA) on the TIMo dataset.",
      "authors": [
        "Tengjiao He",
        "Wenguang Wang"
      ],
      "published": "2023-06-04T10:30:28Z",
      "updated": "2023-06-04T10:30:28Z",
      "categories": [
        "cs.CV",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.04466v1",
      "landing_url": "https://arxiv.org/abs/2306.04466v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.04466"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses point cloud video anomaly detection with depth/LiDAR data, none of which involves discrete audio tokens or tokenization of audio signals, so it fails the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses point cloud video anomaly detection with depth/LiDAR data, none of which involves discrete audio tokens or tokenization of audio signals, so it fails the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput",
    "abstract": "Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\\times$ throughput over those systems that assume the worst case for the output sequence length.",
    "metadata": {
      "arxiv_id": "2306.06000",
      "title": "S$^{3}$: Increasing GPU Utilization during Generative Inference for Higher Throughput",
      "summary": "Generating texts with a large language model (LLM) consumes massive amounts of memory. Apart from the already-large model parameters, the key/value (KV) cache that holds information about previous tokens in a sequence can grow to be even larger than the model itself. This problem is exacerbated in one of the current LLM serving frameworks which reserves the maximum sequence length of memory for the KV cache to guarantee generating a complete sequence as they do not know the output sequence length. This restricts us to use a smaller batch size leading to lower GPU utilization and above all, lower throughput. We argue that designing a system with a priori knowledge of the output sequence can mitigate this problem. To this end, we propose S$^{3}$, which predicts the output sequence length, schedules generation queries based on the prediction to increase device resource utilization and throughput, and handle mispredictions. Our proposed method achieves 6.49$\\times$ throughput over those systems that assume the worst case for the output sequence length.",
      "authors": [
        "Yunho Jin",
        "Chun-Feng Wu",
        "David Brooks",
        "Gu-Yeon Wei"
      ],
      "published": "2023-06-09T16:13:43Z",
      "updated": "2023-06-09T16:13:43Z",
      "categories": [
        "cs.AR",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06000v1",
      "landing_url": "https://arxiv.org/abs/2306.06000v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06000"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on GPU scheduling for LLM text inference without any mention of discrete audio token generation, tokenizers, or audio codecs, so it clearly fails the inclusion and meets the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on GPU scheduling for LLM text inference without any mention of discrete audio token generation, tokenizers, or audio codecs, so it clearly fails the inclusion and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
    "abstract": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
    "metadata": {
      "arxiv_id": "2306.06814",
      "title": "HiddenSinger: High-Quality Singing Voice Synthesis via Neural Audio Codec and Latent Diffusion Models",
      "summary": "Recently, denoising diffusion models have demonstrated remarkable performance among generative models in various domains. However, in the speech domain, the application of diffusion models for synthesizing time-varying audio faces limitations in terms of complexity and controllability, as speech synthesis requires very high-dimensional samples with long-term acoustic features. To alleviate the challenges posed by model complexity in singing voice synthesis, we propose HiddenSinger, a high-quality singing voice synthesis system using a neural audio codec and latent diffusion models. To ensure high-fidelity audio, we introduce an audio autoencoder that can encode audio into an audio codec as a compressed representation and reconstruct the high-fidelity audio from the low-dimensional compressed latent vector. Subsequently, we use the latent diffusion models to sample a latent representation from a musical score. In addition, our proposed model is extended to an unsupervised singing voice learning framework, HiddenSinger-U, to train the model using an unlabeled singing voice dataset. Experimental results demonstrate that our model outperforms previous models in terms of audio quality. Furthermore, the HiddenSinger-U can synthesize high-quality singing voices of speakers trained solely on unlabeled data.",
      "authors": [
        "Ji-Sang Hwang",
        "Sang-Hoon Lee",
        "Seong-Whan Lee"
      ],
      "published": "2023-06-12T01:21:41Z",
      "updated": "2023-06-12T01:21:41Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.06814v1",
      "landing_url": "https://arxiv.org/abs/2306.06814v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.06814"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on high-fidelity singing voice synthesis via a neural audio codec and latent diffusion but does not describe transforming audio into discrete token sequences with vocabularies, quantization specs, or token-level evaluations as required, so it is better to exclude."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on high-fidelity singing voice synthesis via a neural audio codec and latent diffusion but does not describe transforming audio into discrete token sequences with vocabularies, quantization specs, or token-level evaluations as required, so it is better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
    "abstract": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
    "metadata": {
      "arxiv_id": "2306.07547",
      "title": "UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding",
      "summary": "The utilization of discrete speech tokens, divided into semantic tokens and acoustic tokens, has been proven superior to traditional acoustic feature mel-spectrograms in terms of naturalness and robustness for text-to-speech (TTS) synthesis. Recent popular models, such as VALL-E and SPEAR-TTS, allow zero-shot speaker adaptation through auto-regressive (AR) continuation of acoustic tokens extracted from a short speech prompt. However, these AR models are restricted to generate speech only in a left-to-right direction, making them unsuitable for speech editing where both preceding and following contexts are provided. Furthermore, these models rely on acoustic tokens, which have audio quality limitations imposed by the performance of audio codec models. In this study, we propose a unified context-aware TTS framework called UniCATS, which is capable of both speech continuation and editing. UniCATS comprises two components, an acoustic model CTX-txt2vec and a vocoder CTX-vec2wav. CTX-txt2vec employs contextual VQ-diffusion to predict semantic tokens from the input text, enabling it to incorporate the semantic context and maintain seamless concatenation with the surrounding context. Following that, CTX-vec2wav utilizes contextual vocoding to convert these semantic tokens into waveforms, taking into consideration the acoustic context. Our experimental results demonstrate that CTX-vec2wav outperforms HifiGAN and AudioLM in terms of speech resynthesis from semantic tokens. Moreover, we show that UniCATS achieves state-of-the-art performance in both speech continuation and editing.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Feiyu Shen",
        "Zhijun Liu",
        "Zheng Liang",
        "Xie Chen",
        "Shuai Wang",
        "Hui Zhang",
        "Kai Yu"
      ],
      "published": "2023-06-13T05:38:34Z",
      "updated": "2024-03-28T13:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.07547v6",
      "landing_url": "https://arxiv.org/abs/2306.07547v6",
      "doi": "https://doi.org/10.1609/aaai.v38i16.29747"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "UniCATS focuses on discrete semantic and acoustic speech tokens for TTS generation/editing and provides vocoding evaluations, so it satisfies the inclusion criteria and earns the top score (5)."
    },
    "round-A_JuniorNano_reasoning": "UniCATS focuses on discrete semantic and acoustic speech tokens for TTS generation/editing and provides vocoding evaluations, so it satisfies the inclusion criteria and earns the top score (5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
    "abstract": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
    "metadata": {
      "arxiv_id": "2306.10521",
      "title": "LM-VC: Zero-shot Voice Conversion via Speech Generation based on Language Models",
      "summary": "Language model (LM) based audio generation frameworks, e.g., AudioLM, have recently achieved new state-of-the-art performance in zero-shot audio generation. In this paper, we explore the feasibility of LMs for zero-shot voice conversion. An intuitive approach is to follow AudioLM - Tokenizing speech into semantic and acoustic tokens respectively by HuBERT and SoundStream, and converting source semantic tokens to target acoustic tokens conditioned on acoustic tokens of the target speaker. However, such an approach encounters several issues: 1) the linguistic content contained in semantic tokens may get dispersed during multi-layer modeling while the lengthy speech input in the voice conversion task makes contextual learning even harder; 2) the semantic tokens still contain speaker-related information, which may be leaked to the target speech, lowering the target speaker similarity; 3) the generation diversity in the sampling of the LM can lead to unexpected outcomes during inference, leading to unnatural pronunciation and speech quality degradation. To mitigate these problems, we propose LM-VC, a two-stage language modeling approach that generates coarse acoustic tokens for recovering the source linguistic content and target speaker's timbre, and then reconstructs the fine for acoustic details as converted speech. Specifically, to enhance content preservation and facilitates better disentanglement, a masked prefix LM with a mask prediction strategy is used for coarse acoustic modeling. This model is encouraged to recover the masked content from the surrounding context and generate target speech based on the target speaker's utterance and corrupted semantic tokens. Besides, to further alleviate the sampling error in the generation, an external LM, which employs window attention to capture the local acoustic relations, is introduced to participate in the coarse acoustic modeling.",
      "authors": [
        "Zhichao Wang",
        "Yuanzhe Chen",
        "Lei Xie",
        "Qiao Tian",
        "Yuping Wang"
      ],
      "published": "2023-06-18T10:59:06Z",
      "updated": "2023-08-21T02:21:06Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10521v2",
      "landing_url": "https://arxiv.org/abs/2306.10521v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10521"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "LM-VC explicitly builds on discrete semantic tokens (HuBERT) and codec tokens (SoundStream) in a two-stage language modeling pipeline for zero-shot voice conversion, clearly aligning with the discrete audio-token focus and presenting token-level modeling/adjustments rather than purely continuous representations."
    },
    "round-A_JuniorNano_reasoning": "LM-VC explicitly builds on discrete semantic tokens (HuBERT) and codec tokens (SoundStream) in a two-stage language modeling pipeline for zero-shot voice conversion, clearly aligning with the discrete audio-token focus and presenting token-level modeling/adjustments rather than purely continuous representations.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators",
    "abstract": "Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.",
    "metadata": {
      "arxiv_id": "2306.10900",
      "title": "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators",
      "summary": "Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.",
      "authors": [
        "Yaqi Zhang",
        "Di Huang",
        "Bin Liu",
        "Shixiang Tang",
        "Yan Lu",
        "Lu Chen",
        "Lei Bai",
        "Qi Chu",
        "Nenghai Yu",
        "Wanli Ouyang"
      ],
      "published": "2023-06-19T12:58:17Z",
      "updated": "2024-03-18T04:14:50Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.10900v2",
      "landing_url": "https://arxiv.org/abs/2306.10900v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.10900"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on multimodal human motion generation using LLMs and poses, with no discrete audio tokenization, codec, or token evaluation, so it fails to meet the discrete audio token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multimodal human motion generation using LLMs and poses, with no discrete audio tokenization, codec, or token evaluation, so it fails to meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Visual-Aware Text-to-Speech",
    "abstract": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
    "metadata": {
      "arxiv_id": "2306.12020",
      "title": "Visual-Aware Text-to-Speech",
      "summary": "Dynamically synthesizing talking speech that actively responds to a listening head is critical during the face-to-face interaction. For example, the speaker could take advantage of the listener's facial expression to adjust the tones, stressed syllables, or pauses. In this work, we present a new visual-aware text-to-speech (VA-TTS) task to synthesize speech conditioned on both textual inputs and sequential visual feedback (e.g., nod, smile) of the listener in face-to-face communication. Different from traditional text-to-speech, VA-TTS highlights the impact of visual modality. On this newly-minted task, we devise a baseline model to fuse phoneme linguistic information and listener visual signals for speech synthesis. Extensive experiments on multimodal conversation dataset ViCo-X verify our proposal for generating more natural audio with scenario-appropriate rhythm and prosody.",
      "authors": [
        "Mohan Zhou",
        "Yalong Bai",
        "Wei Zhang",
        "Ting Yao",
        "Tiejun Zhao",
        "Tao Mei"
      ],
      "published": "2023-06-21T05:11:39Z",
      "updated": "2023-06-21T05:11:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.12020v1",
      "landing_url": "https://arxiv.org/abs/2306.12020v1",
      "doi": "https://doi.org/10.1109/ICASSP49357.2023.10095084"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on cross-modal TTS conditioned on visual feedback without introducing or evaluating discrete audio tokens, tokenizer/codebook designs, or quantized vocabularies, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on cross-modal TTS conditioned on visual feedback without introducing or evaluating discrete audio tokens, tokenizer/codebook designs, or quantized vocabularies, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "You Can Mask More For Extremely Low-Bitrate Image Compression",
    "abstract": "Learned image compression (LIC) methods have experienced significant progress during recent years. However, these methods are primarily dedicated to optimizing the rate-distortion (R-D) performance at medium and high bitrates (> 0.1 bits per pixel (bpp)), while research on extremely low bitrates is limited. Besides, existing methods fail to explicitly explore the image structure and texture components crucial for image compression, treating them equally alongside uninformative components in networks. This can cause severe perceptual quality degradation, especially under low-bitrate scenarios. In this work, inspired by the success of pre-trained masked autoencoders (MAE) in many downstream tasks, we propose to rethink its mask sampling strategy from structure and texture perspectives for high redundancy reduction and discriminative feature representation, further unleashing the potential of LIC methods. Therefore, we present a dual-adaptive masking approach (DA-Mask) that samples visible patches based on the structure and texture distributions of original images. We combine DA-Mask and pre-trained MAE in masked image modeling (MIM) as an initial compressor that abstracts informative semantic context and texture representations. Such a pipeline can well cooperate with LIC networks to achieve further secondary compression while preserving promising reconstruction quality. Consequently, we propose a simple yet effective masked compression model (MCM), the first framework that unifies MIM and LIC end-to-end for extremely low-bitrate image compression. Extensive experiments have demonstrated that our approach outperforms recent state-of-the-art methods in R-D performance, visual quality, and downstream applications, at very low bitrates. Our code is available at https://github.com/lianqi1008/MCM.git.",
    "metadata": {
      "arxiv_id": "2306.15561",
      "title": "You Can Mask More For Extremely Low-Bitrate Image Compression",
      "summary": "Learned image compression (LIC) methods have experienced significant progress during recent years. However, these methods are primarily dedicated to optimizing the rate-distortion (R-D) performance at medium and high bitrates (> 0.1 bits per pixel (bpp)), while research on extremely low bitrates is limited. Besides, existing methods fail to explicitly explore the image structure and texture components crucial for image compression, treating them equally alongside uninformative components in networks. This can cause severe perceptual quality degradation, especially under low-bitrate scenarios. In this work, inspired by the success of pre-trained masked autoencoders (MAE) in many downstream tasks, we propose to rethink its mask sampling strategy from structure and texture perspectives for high redundancy reduction and discriminative feature representation, further unleashing the potential of LIC methods. Therefore, we present a dual-adaptive masking approach (DA-Mask) that samples visible patches based on the structure and texture distributions of original images. We combine DA-Mask and pre-trained MAE in masked image modeling (MIM) as an initial compressor that abstracts informative semantic context and texture representations. Such a pipeline can well cooperate with LIC networks to achieve further secondary compression while preserving promising reconstruction quality. Consequently, we propose a simple yet effective masked compression model (MCM), the first framework that unifies MIM and LIC end-to-end for extremely low-bitrate image compression. Extensive experiments have demonstrated that our approach outperforms recent state-of-the-art methods in R-D performance, visual quality, and downstream applications, at very low bitrates. Our code is available at https://github.com/lianqi1008/MCM.git.",
      "authors": [
        "Anqi Li",
        "Feng Li",
        "Jiaxin Han",
        "Huihui Bai",
        "Runmin Cong",
        "Chunjie Zhang",
        "Meng Wang",
        "Weisi Lin",
        "Yao Zhao"
      ],
      "published": "2023-06-27T15:36:22Z",
      "updated": "2023-06-27T15:36:22Z",
      "categories": [
        "cs.CV",
        "cs.MM",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15561v1",
      "landing_url": "https://arxiv.org/abs/2306.15561v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.15561"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on masked image compression and introduces a computer-vision-centric MAE+LIC model without any discrete audio token, tokenizer, or codec content, so it clearly fails the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on masked image compression and introduces a computer-vision-centric MAE+LIC model without any discrete audio token, tokenizer, or codec content, so it clearly fails the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
    "abstract": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
    "metadata": {
      "arxiv_id": "2306.15687",
      "title": "Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale",
      "summary": "Large-scale generative models such as GPT and DALL-E have revolutionized the research community. These models not only generate high fidelity outputs, but are also generalists which can solve tasks not explicitly taught. In contrast, speech generative models are still primitive in terms of scale and task generalization. In this paper, we present Voicebox, the most versatile text-guided generative model for speech at scale. Voicebox is a non-autoregressive flow-matching model trained to infill speech, given audio context and text, trained on over 50K hours of speech that are not filtered or enhanced. Similar to GPT, Voicebox can perform many different tasks through in-context learning, but is more flexible as it can also condition on future context. Voicebox can be used for mono or cross-lingual zero-shot text-to-speech synthesis, noise removal, content editing, style conversion, and diverse sample generation. In particular, Voicebox outperforms the state-of-the-art zero-shot TTS model VALL-E on both intelligibility (5.9% vs 1.9% word error rates) and audio similarity (0.580 vs 0.681) while being up to 20 times faster. Audio samples can be found in \\url{https://voicebox.metademolab.com}.",
      "authors": [
        "Matthew Le",
        "Apoorv Vyas",
        "Bowen Shi",
        "Brian Karrer",
        "Leda Sari",
        "Rashel Moritz",
        "Mary Williamson",
        "Vimal Manohar",
        "Yossi Adi",
        "Jay Mahadeokar",
        "Wei-Ning Hsu"
      ],
      "published": "2023-06-23T16:23:24Z",
      "updated": "2023-10-19T13:23:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15687v2",
      "landing_url": "https://arxiv.org/abs/2306.15687v2",
      "doi": "https://doi.org/10.48550/arXiv.2306.15687"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe a large-scale speech generative model with no mention of discrete audio tokens, tokenizers, quantization, or vocab specifications that meet the discrete-token inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a large-scale speech generative model with no mention of discrete audio tokens, tokenizers, quantization, or vocab specifications that meet the discrete-token inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Federated Deep Reinforcement Learning-based Bitrate Adaptation for Dynamic Adaptive Streaming over HTTP",
    "abstract": "In video streaming over HTTP, the bitrate adaptation selects the quality of video chunks depending on the current network condition. Some previous works have applied deep reinforcement learning (DRL) algorithms to determine the chunk's bitrate from the observed states to maximize the quality-of-experience (QoE). However, to build an intelligent model that can predict in various environments, such as 3G, 4G, Wifi, \\textit{etc.}, the states observed from these environments must be sent to a server for training centrally. In this work, we integrate federated learning (FL) to DRL-based rate adaptation to train a model appropriate for different environments. The clients in the proposed framework train their model locally and only update the weights to the server. The simulations show that our federated DRL-based rate adaptations, called FDRLABR with different DRL algorithms, such as deep Q-learning, advantage actor-critic, and proximal policy optimization, yield better performance than the traditional bitrate adaptation methods in various environments.",
    "metadata": {
      "arxiv_id": "2306.15860",
      "title": "Federated Deep Reinforcement Learning-based Bitrate Adaptation for Dynamic Adaptive Streaming over HTTP",
      "summary": "In video streaming over HTTP, the bitrate adaptation selects the quality of video chunks depending on the current network condition. Some previous works have applied deep reinforcement learning (DRL) algorithms to determine the chunk's bitrate from the observed states to maximize the quality-of-experience (QoE). However, to build an intelligent model that can predict in various environments, such as 3G, 4G, Wifi, \\textit{etc.}, the states observed from these environments must be sent to a server for training centrally. In this work, we integrate federated learning (FL) to DRL-based rate adaptation to train a model appropriate for different environments. The clients in the proposed framework train their model locally and only update the weights to the server. The simulations show that our federated DRL-based rate adaptations, called FDRLABR with different DRL algorithms, such as deep Q-learning, advantage actor-critic, and proximal policy optimization, yield better performance than the traditional bitrate adaptation methods in various environments.",
      "authors": [
        "Phuong L. Vo",
        "Nghia T. Nguyen",
        "Long Luu",
        "Canh T. Dinh",
        "Nguyen H. Tran",
        "Tuan-Anh Le"
      ],
      "published": "2023-06-28T01:23:05Z",
      "updated": "2023-06-28T01:23:05Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2306.15860v1",
      "landing_url": "https://arxiv.org/abs/2306.15860v1",
      "doi": "https://doi.org/10.48550/arXiv.2306.15860"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on federated deep RL for bitrate adaptation in video streaming and does not discuss transforming audio signals into discrete token sequences, so it fails all inclusion criteria and falls under the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on federated deep RL for bitrate adaptation in video streaming and does not discuss transforming audio signals into discrete token sequences, so it fails all inclusion criteria and falls under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
    "abstract": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
    "metadata": {
      "arxiv_id": "2307.00024",
      "title": "EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech",
      "summary": "State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.",
      "authors": [
        "Daria Diatlova",
        "Vitaly Shutov"
      ],
      "published": "2023-06-28T19:34:16Z",
      "updated": "2023-06-28T19:34:16Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00024v1",
      "landing_url": "https://arxiv.org/abs/2307.00024v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00024"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on emotion-enhanced FastSpeech2 speech synthesis without any mention of discrete audio tokenization, quantization, or vocabulary design, so it fails the inclusion criteria and hits the exclusion for relying purely on continuous representations."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on emotion-enhanced FastSpeech2 speech synthesis without any mention of discrete audio tokenization, quantization, or vocabulary design, so it fails the inclusion criteria and hits the exclusion for relying purely on continuous representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning",
    "abstract": "Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.",
    "metadata": {
      "arxiv_id": "2307.00498",
      "title": "Data-Free Quantization via Mixed-Precision Compensation without Fine-Tuning",
      "summary": "Neural network quantization is a very promising solution in the field of model compression, but its resulting accuracy highly depends on a training/fine-tuning process and requires the original data. This not only brings heavy computation and time costs but also is not conducive to privacy and sensitive information protection. Therefore, a few recent works are starting to focus on data-free quantization. However, data-free quantization does not perform well while dealing with ultra-low precision quantization. Although researchers utilize generative methods of synthetic data to address this problem partially, data synthesis needs to take a lot of computation and time. In this paper, we propose a data-free mixed-precision compensation (DF-MPC) method to recover the performance of an ultra-low precision quantized model without any data and fine-tuning process. By assuming the quantized error caused by a low-precision quantized layer can be restored via the reconstruction of a high-precision quantized layer, we mathematically formulate the reconstruction loss between the pre-trained full-precision model and its layer-wise mixed-precision quantized model. Based on our formulation, we theoretically deduce the closed-form solution by minimizing the reconstruction loss of the feature maps. Since DF-MPC does not require any original/synthetic data, it is a more efficient method to approximate the full-precision model. Experimentally, our DF-MPC is able to achieve higher accuracy for an ultra-low precision quantized model compared to the recent methods without any data and fine-tuning process.",
      "authors": [
        "Jun Chen",
        "Shipeng Bai",
        "Tianxin Huang",
        "Mengmeng Wang",
        "Guanzhong Tian",
        "Yong Liu"
      ],
      "published": "2023-07-02T07:16:29Z",
      "updated": "2025-02-15T02:00:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00498v2",
      "landing_url": "https://arxiv.org/abs/2307.00498v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.00498"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on data-free quantization for neural networks rather than discrete audio tokens or tokenization/quantization of audio representations, so it fails to meet the topic requirements."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on data-free quantization for neural networks rather than discrete audio tokens or tokenization/quantization of audio representations, so it fails to meet the topic requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
    "abstract": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
    "metadata": {
      "arxiv_id": "2307.00729",
      "title": "An End-to-End Multi-Module Audio Deepfake Generation System for ADD Challenge 2023",
      "summary": "The task of synthetic speech generation is to generate language content from a given text, then simulating fake human voice.The key factors that determine the effect of synthetic speech generation mainly include speed of generation, accuracy of word segmentation, naturalness of synthesized speech, etc. This paper builds an end-to-end multi-module synthetic speech generation model, including speaker encoder, synthesizer based on Tacotron2, and vocoder based on WaveRNN. In addition, we perform a lot of comparative experiments on different datasets and various model structures. Finally, we won the first place in the ADD 2023 challenge Track 1.1 with the weighted deception success rate (WDSR) of 44.97%.",
      "authors": [
        "Sheng Zhao",
        "Qilong Yuan",
        "Yibo Duan",
        "Zhuoyue Chen"
      ],
      "published": "2023-07-03T03:21:23Z",
      "updated": "2023-07-03T03:21:23Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00729v1",
      "landing_url": "https://arxiv.org/abs/2307.00729v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.00729"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes multi-module TTS using Tacotron2/WaveRNN for deepfake audio, but it never discusses discrete audio tokens, quantization, tokenizers, or token-level evaluation, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper describes multi-module TTS using Tacotron2/WaveRNN for deepfake audio, but it never discusses discrete audio tokens, quantization, tokenizers, or token-level evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
    "abstract": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
    "metadata": {
      "arxiv_id": "2307.00782",
      "title": "ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading",
      "summary": "While state-of-the-art Text-to-Speech systems can generate natural speech of very high quality at sentence level, they still meet great challenges in speech generation for paragraph / long-form reading. Such deficiencies are due to i) ignorance of cross-sentence contextual information, and ii) high computation and memory cost for long-form synthesis. To address these issues, this work develops a lightweight yet effective TTS system, ContextSpeech. Specifically, we first design a memory-cached recurrence mechanism to incorporate global text and speech context into sentence encoding. Then we construct hierarchically-structured textual semantics to broaden the scope for global context enhancement. Additionally, we integrate linearized self-attention to improve model efficiency. Experiments show that ContextSpeech significantly improves the voice quality and prosody expressiveness in paragraph reading with competitive model efficiency. Audio samples are available at: https://contextspeech.github.io/demo/",
      "authors": [
        "Yujia Xiao",
        "Shaofei Zhang",
        "Xi Wang",
        "Xu Tan",
        "Lei He",
        "Sheng Zhao",
        "Frank K. Soong",
        "Tan Lee"
      ],
      "published": "2023-07-03T06:55:03Z",
      "updated": "2023-10-07T08:32:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.00782v2",
      "landing_url": "https://arxiv.org/abs/2307.00782v2",
      "doi": "https://doi.org/10.21437/Interspeech.2023-122"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "ContextSpeech focuses on improving paragraph-level TTS quality via contextual encoding and efficient attention but does not involve quantized discrete audio tokens or tokenizer/codebook design, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "ContextSpeech focuses on improving paragraph-level TTS quality via contextual encoding and efficient attention but does not involve quantized discrete audio tokens or tokenizer/codebook design, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
    "abstract": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
    "metadata": {
      "arxiv_id": "2307.02486",
      "title": "LongNet: Scaling Transformers to 1,000,000,000 Tokens",
      "summary": "Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. To address this issue, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between any two tokens in a sequence; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.",
      "authors": [
        "Jiayu Ding",
        "Shuming Ma",
        "Li Dong",
        "Xingxing Zhang",
        "Shaohan Huang",
        "Wenhui Wang",
        "Nanning Zheng",
        "Furu Wei"
      ],
      "published": "2023-07-05T17:59:38Z",
      "updated": "2023-07-19T12:25:35Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.02486v2",
      "landing_url": "https://arxiv.org/abs/2307.02486v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.02486"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Does not address discrete audio token generation/quantization—only general long-range Transformers—so it clearly fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Does not address discrete audio token generation/quantization—only general long-range Transformers—so it clearly fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
    "abstract": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
    "metadata": {
      "arxiv_id": "2307.03436",
      "title": "Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded Video",
      "summary": "Content providers increasingly replace traditional constant bitrate with variable bitrate (VBR) encoding in real-time video communication systems for better video quality. However, VBR encoding often leads to large and frequent bitrate fluctuation, inevitably deteriorating the efficiency of existing adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to consider the network dynamics and VBR-encoding-induced video bitrate fluctuations jointly for deploying the best ABR policy. With this aim, Anableps uses sender-side information from the past to predict the video bitrate range of upcoming frames. Such bitrate range is then combined with the receiver-side observations to set the proper bitrate target for video encoding using a reinforcement-learning-based ABR model. As revealed by extensive experiments on a real-world trace-driven testbed, our Anableps outperforms the GCC with significant improvement of quality of experience, e.g., 1.88x video quality, 57% less bitrate consumption, 85% less stalling, and 74% shorter interaction delay.",
      "authors": [
        "Zicheng Zhang",
        "Hao Chen",
        "Xun Cao",
        "Zhan Ma"
      ],
      "published": "2023-07-07T07:47:45Z",
      "updated": "2023-07-07T07:47:45Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03436v1",
      "landing_url": "https://arxiv.org/abs/2307.03436v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.03436"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract discuss adaptive bitrate for VBR video streaming; they do not cover discrete audio tokenization or quantized vocabularies, so the paper fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract discuss adaptive bitrate for VBR video streaming; they do not cover discrete audio tokenization or quantized vocabularies, so the paper fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On decoder-only architecture for speech-to-text and large language model integration",
    "abstract": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
    "metadata": {
      "arxiv_id": "2307.03917",
      "title": "On decoder-only architecture for speech-to-text and large language model integration",
      "summary": "Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion.",
      "authors": [
        "Jian Wu",
        "Yashesh Gaur",
        "Zhuo Chen",
        "Long Zhou",
        "Yimeng Zhu",
        "Tianrui Wang",
        "Jinyu Li",
        "Shujie Liu",
        "Bo Ren",
        "Linquan Liu",
        "Yu Wu"
      ],
      "published": "2023-07-08T06:47:58Z",
      "updated": "2023-10-02T06:57:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.03917v3",
      "landing_url": "https://arxiv.org/abs/2307.03917v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.03917"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Does not present discrete audio token/tokenizer design—just integrates continuous acoustic features with decoder-only LLMs—so it fails the inclusion focus on discrete audio tokens."
    },
    "round-A_JuniorNano_reasoning": "Does not present discrete audio token/tokenizer design—just integrates continuous acoustic features with decoder-only LLMs—so it fails the inclusion focus on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
    "abstract": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
    "metadata": {
      "arxiv_id": "2307.04686",
      "title": "VampNet: Music Generation via Masked Acoustic Token Modeling",
      "summary": "We introduce VampNet, a masked acoustic token modeling approach to music synthesis, compression, inpainting, and variation. We use a variable masking schedule during training which allows us to sample coherent music from the model by applying a variety of masking approaches (called prompts) during inference. VampNet is non-autoregressive, leveraging a bidirectional transformer architecture that attends to all tokens in a forward pass. With just 36 sampling passes, VampNet can generate coherent high-fidelity musical waveforms. We show that by prompting VampNet in various ways, we can apply it to tasks like music compression, inpainting, outpainting, continuation, and looping with variation (vamping). Appropriately prompted, VampNet is capable of maintaining style, genre, instrumentation, and other high-level aspects of the music. This flexible prompting capability makes VampNet a powerful music co-creation tool. Code and audio samples are available online.",
      "authors": [
        "Hugo Flores Garcia",
        "Prem Seetharaman",
        "Rithesh Kumar",
        "Bryan Pardo"
      ],
      "published": "2023-07-10T16:42:03Z",
      "updated": "2023-07-12T17:06:41Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.04686v2",
      "landing_url": "https://arxiv.org/abs/2307.04686v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.04686"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "The abstract mentions masked acoustic token modeling for music synthesis but does not describe the discrete tokenizer/quantization setup or evaluations, so it is unclear whether the work squarely targets the discrete audio token design focus required for inclusion."
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions masked acoustic token modeling for music synthesis but does not describe the discrete tokenizer/quantization setup or evaluations, so it is unclear whether the work squarely targets the discrete audio token design focus required for inclusion.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Image Reconstruction using Enhanced Vision Transformer",
    "abstract": "Removing noise from images is a challenging and fundamental problem in the field of computer vision. Images captured by modern cameras are inevitably degraded by noise which limits the accuracy of any quantitative measurements on those images. In this project, we propose a novel image reconstruction framework which can be used for tasks such as image denoising, deblurring or inpainting. The model proposed in this project is based on Vision Transformer (ViT) that takes 2D images as input and outputs embeddings which can be used for reconstructing denoised images. We incorporate four additional optimization techniques in the framework to improve the model reconstruction capability, namely Locality Sensitive Attention (LSA), Shifted Patch Tokenization (SPT), Rotary Position Embeddings (RoPE) and adversarial loss function inspired from Generative Adversarial Networks (GANs). LSA, SPT and RoPE enable the transformer to learn from the dataset more efficiently, while the adversarial loss function enhances the resolution of the reconstructed images. Based on our experiments, the proposed architecture outperforms the benchmark U-Net model by more than 3.5\\% structural similarity (SSIM) for the reconstruction tasks of image denoising and inpainting. The proposed enhancements further show an improvement of \\textasciitilde5\\% SSIM over the benchmark for both tasks.",
    "metadata": {
      "arxiv_id": "2307.05616",
      "title": "Image Reconstruction using Enhanced Vision Transformer",
      "summary": "Removing noise from images is a challenging and fundamental problem in the field of computer vision. Images captured by modern cameras are inevitably degraded by noise which limits the accuracy of any quantitative measurements on those images. In this project, we propose a novel image reconstruction framework which can be used for tasks such as image denoising, deblurring or inpainting. The model proposed in this project is based on Vision Transformer (ViT) that takes 2D images as input and outputs embeddings which can be used for reconstructing denoised images. We incorporate four additional optimization techniques in the framework to improve the model reconstruction capability, namely Locality Sensitive Attention (LSA), Shifted Patch Tokenization (SPT), Rotary Position Embeddings (RoPE) and adversarial loss function inspired from Generative Adversarial Networks (GANs). LSA, SPT and RoPE enable the transformer to learn from the dataset more efficiently, while the adversarial loss function enhances the resolution of the reconstructed images. Based on our experiments, the proposed architecture outperforms the benchmark U-Net model by more than 3.5\\% structural similarity (SSIM) for the reconstruction tasks of image denoising and inpainting. The proposed enhancements further show an improvement of \\textasciitilde5\\% SSIM over the benchmark for both tasks.",
      "authors": [
        "Nikhil Verma",
        "Deepkamal Kaur",
        "Lydia Chau"
      ],
      "published": "2023-07-11T02:14:18Z",
      "updated": "2023-07-11T02:14:18Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.05616v1",
      "landing_url": "https://arxiv.org/abs/2307.05616v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.05616"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on image reconstruction using vision transformers and not on discrete audio tokens, so it fails to meet any inclusion criteria and matches the exclusion for non-audio token studies."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on image reconstruction using vision transformers and not on discrete audio tokens, so it fails to meet any inclusion criteria and matches the exclusion for non-audio token studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Controllable Emphasis with zero data for text-to-speech",
    "abstract": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
    "metadata": {
      "arxiv_id": "2307.07062",
      "title": "Controllable Emphasis with zero data for text-to-speech",
      "summary": "We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\\%$ and correct testers' identification of the emphasized word in a sentence by $40\\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.",
      "authors": [
        "Arnaud Joly",
        "Marco Nicolis",
        "Ekaterina Peterova",
        "Alessandro Lombardi",
        "Ammar Abbas",
        "Arent van Korlaar",
        "Aman Hussain",
        "Parul Sharma",
        "Alexis Moinet",
        "Mateusz Lajszczak",
        "Penny Karanasou",
        "Antonio Bonafonte",
        "Thomas Drugman",
        "Elena Sokolova"
      ],
      "published": "2023-07-13T21:06:23Z",
      "updated": "2023-07-13T21:06:23Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.07062v1",
      "landing_url": "https://arxiv.org/abs/2307.07062v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.07062"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper only describes duration-modification emphasis for TTS without any discrete audio token quantization or token level modeling/analysis, so it does not meet the discrete token inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper only describes duration-modification emphasis for TTS without any discrete audio token quantization or token level modeling/analysis, so it does not meet the discrete token inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Extreme Image Compression using Fine-tuned VQGANs",
    "abstract": "Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. However, their efficacy and applicability to achieve extreme compression ratios ($<0.05$ bpp) remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)--based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields a strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We propose clustering a pre-trained large-scale codebook into smaller codebooks through the K-means algorithm, yielding variable bitrates and different levels of reconstruction quality within the coding framework. Furthermore, we introduce a transformer to predict lost indices and restore images in unstable environments. Extensive qualitative and quantitative experiments on various benchmark datasets demonstrate that the proposed framework outperforms state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception at extremely low bitrates ($\\le 0.04$ bpp). Remarkably, even with the loss of up to $20\\%$ of indices, the images can be effectively restored with minimal perceptual loss.",
    "metadata": {
      "arxiv_id": "2307.08265",
      "title": "Extreme Image Compression using Fine-tuned VQGANs",
      "summary": "Recent advances in generative compression methods have demonstrated remarkable progress in enhancing the perceptual quality of compressed data, especially in scenarios with low bitrates. However, their efficacy and applicability to achieve extreme compression ratios ($<0.05$ bpp) remain constrained. In this work, we propose a simple yet effective coding framework by introducing vector quantization (VQ)--based generative models into the image compression domain. The main insight is that the codebook learned by the VQGAN model yields a strong expressive capacity, facilitating efficient compression of continuous information in the latent space while maintaining reconstruction quality. Specifically, an image can be represented as VQ-indices by finding the nearest codeword, which can be encoded using lossless compression methods into bitstreams. We propose clustering a pre-trained large-scale codebook into smaller codebooks through the K-means algorithm, yielding variable bitrates and different levels of reconstruction quality within the coding framework. Furthermore, we introduce a transformer to predict lost indices and restore images in unstable environments. Extensive qualitative and quantitative experiments on various benchmark datasets demonstrate that the proposed framework outperforms state-of-the-art codecs in terms of perceptual quality-oriented metrics and human perception at extremely low bitrates ($\\le 0.04$ bpp). Remarkably, even with the loss of up to $20\\%$ of indices, the images can be effectively restored with minimal perceptual loss.",
      "authors": [
        "Qi Mao",
        "Tinghan Yang",
        "Yinuo Zhang",
        "Zijian Wang",
        "Meng Wang",
        "Shiqi Wang",
        "Siwei Ma"
      ],
      "published": "2023-07-17T06:14:19Z",
      "updated": "2023-12-15T14:39:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.08265v3",
      "landing_url": "https://arxiv.org/abs/2307.08265v3",
      "doi": "https://doi.org/10.48550/arXiv.2307.08265"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper deals with image compression using VQGAN, not discrete audio tokens or audio-related codec/token research, so it fails the inclusion focus entirely and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper deals with image compression using VQGAN, not discrete audio tokens or audio-related codec/token research, so it fails the inclusion focus entirely and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks",
    "abstract": "For problems in image processing and many other fields, a large class of effective neural networks has encoder-decoder-based architectures. Although these networks have made impressive performances, mathematical explanations of their architectures are still underdeveloped. In this paper, we study the encoder-decoder-based network architecture from the algorithmic perspective and provide a mathematical explanation. We use the two-phase Potts model for image segmentation as an example for our explanations. We associate the segmentation problem with a control problem in the continuous setting. Then, multigrid method and operator splitting scheme, the PottsMGNet, are used to discretize the continuous control model. We show that the resulting discrete PottsMGNet is equivalent to an encoder-decoder-based network. With minor modifications, it is shown that a number of the popular encoder-decoder-based neural networks are just instances of the proposed PottsMGNet. By incorporating the Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet has shown to be robust with the network parameters such as network width and depth and achieved remarkable performance on datasets with very large noise. In nearly all our experiments, the new network always performs better or as good on accuracy and dice score than existing networks for image segmentation.",
    "metadata": {
      "arxiv_id": "2307.09039",
      "title": "PottsMGNet: A Mathematical Explanation of Encoder-Decoder Based Neural Networks",
      "summary": "For problems in image processing and many other fields, a large class of effective neural networks has encoder-decoder-based architectures. Although these networks have made impressive performances, mathematical explanations of their architectures are still underdeveloped. In this paper, we study the encoder-decoder-based network architecture from the algorithmic perspective and provide a mathematical explanation. We use the two-phase Potts model for image segmentation as an example for our explanations. We associate the segmentation problem with a control problem in the continuous setting. Then, multigrid method and operator splitting scheme, the PottsMGNet, are used to discretize the continuous control model. We show that the resulting discrete PottsMGNet is equivalent to an encoder-decoder-based network. With minor modifications, it is shown that a number of the popular encoder-decoder-based neural networks are just instances of the proposed PottsMGNet. By incorporating the Soft-Threshold-Dynamics into the PottsMGNet as a regularizer, the PottsMGNet has shown to be robust with the network parameters such as network width and depth and achieved remarkable performance on datasets with very large noise. In nearly all our experiments, the new network always performs better or as good on accuracy and dice score than existing networks for image segmentation.",
      "authors": [
        "Xue-Cheng Tai",
        "Hao Liu",
        "Raymond Chan"
      ],
      "published": "2023-07-18T07:48:48Z",
      "updated": "2023-09-15T13:53:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.09039v2",
      "landing_url": "https://arxiv.org/abs/2307.09039v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.09039"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies encoder-decoder architectures for image segmentation using Potts models and says nothing about discrete audio tokens, tokenizers, quantization, or audio-language modeling, so it fails the inclusion criteria and meets the exclusion grounds."
    },
    "round-A_JuniorNano_reasoning": "The paper studies encoder-decoder architectures for image segmentation using Potts models and says nothing about discrete audio tokens, tokenizers, quantization, or audio-language modeling, so it fails the inclusion criteria and meets the exclusion grounds.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
    "abstract": "Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead.",
    "metadata": {
      "arxiv_id": "2307.13337",
      "title": "Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks",
      "summary": "Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead.",
      "authors": [
        "Cheeun Hong",
        "Kyoung Mu Lee"
      ],
      "published": "2023-07-25T08:50:01Z",
      "updated": "2024-07-18T08:37:08Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.13337v2",
      "landing_url": "https://arxiv.org/abs/2307.13337v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.13337"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about quantizing image super-resolution networks and has no connection to discrete audio tokens or tokenizer design, so it fails the inclusion domain entirely."
    },
    "round-A_JuniorNano_reasoning": "The paper is about quantizing image super-resolution networks and has no connection to discrete audio tokens or tokenizer design, so it fails the inclusion domain entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application",
    "abstract": "This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\\bigO(kN^2\\sqrt{T\\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.",
    "metadata": {
      "arxiv_id": "2307.14549",
      "title": "Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application",
      "summary": "This paper presents an efficient algorithm to solve the sleeping bandit with multiple plays problem in the context of an online recommendation system. The problem involves bounded, adversarial loss and unknown i.i.d. distributions for arm availability. The proposed algorithm extends the sleeping bandit algorithm for single arm selection and is guaranteed to achieve theoretical performance with regret upper bounded by $\\bigO(kN^2\\sqrt{T\\log T})$, where $k$ is the number of arms selected per time step, $N$ is the total number of arms, and $T$ is the time horizon.",
      "authors": [
        "Jianjun Yuan",
        "Wei Lee Woon",
        "Ludovik Coba"
      ],
      "published": "2023-07-27T00:11:59Z",
      "updated": "2023-07-27T00:11:59Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14549v1",
      "landing_url": "https://arxiv.org/abs/2307.14549v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.14549"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Adversarial sleeping bandit work has nothing to do with discrete audio tokenization or quantized audio modeling, so it should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "Adversarial sleeping bandit work has nothing to do with discrete audio tokenization or quantized audio modeling, so it should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes",
    "abstract": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
    "metadata": {
      "arxiv_id": "2307.14713",
      "title": "GaitMorph: Transforming Gait by Optimally Transporting Discrete Codes",
      "summary": "Gait, the manner of walking, has been proven to be a reliable biometric with uses in surveillance, marketing and security. A promising new direction for the field is training gait recognition systems without explicit human annotations, through self-supervised learning approaches. Such methods are heavily reliant on strong augmentations for the same walking sequence to induce more data variability and to simulate additional walking variations. Current data augmentation schemes are heuristic and cannot provide the necessary data variation as they are only able to provide simple temporal and spatial distortions. In this work, we propose GaitMorph, a novel method to modify the walking variation for an input gait sequence. Our method entails the training of a high-compression model for gait skeleton sequences that leverages unlabelled data to construct a discrete and interpretable latent space, which preserves identity-related features. Furthermore, we propose a method based on optimal transport theory to learn latent transport maps on the discrete codebook that morph gait sequences between variations. We perform extensive experiments and show that our method is suitable to synthesize additional views for an input sequence.",
      "authors": [
        "Adrian Cosma",
        "Emilian Radoi"
      ],
      "published": "2023-07-27T09:09:28Z",
      "updated": "2023-07-27T09:09:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.14713v1",
      "landing_url": "https://arxiv.org/abs/2307.14713v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.14713"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper targets gait sequence morphing without any discussion of audio codecs or discrete audio tokens, so it fails the inclusion criteria focused on discrete audio tokenization."
    },
    "round-A_JuniorNano_reasoning": "The paper targets gait sequence morphing without any discussion of audio codecs or discrete audio tokens, so it fails the inclusion criteria focused on discrete audio tokenization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Entropy Coding Based on Binary Encoding for Mixed-Radix Digits",
    "abstract": "The necessity of radix conversion of numeric data is an indispensable component in any complete analysis of digital computation. In this paper, we propose a binary encoding for mixed-radix digits. Second, a variant of rANS coding based on this conversion is given, which supports parallel decoding. The simulations show that the proposed coding in serial mode has a higher throughput than the baseline (with the speed-up factor about 2X) without loss of compression ratio, and it outperforms the existing 2-way interleaving implementation.",
    "metadata": {
      "arxiv_id": "2307.15864",
      "title": "An Entropy Coding Based on Binary Encoding for Mixed-Radix Digits",
      "summary": "The necessity of radix conversion of numeric data is an indispensable component in any complete analysis of digital computation. In this paper, we propose a binary encoding for mixed-radix digits. Second, a variant of rANS coding based on this conversion is given, which supports parallel decoding. The simulations show that the proposed coding in serial mode has a higher throughput than the baseline (with the speed-up factor about 2X) without loss of compression ratio, and it outperforms the existing 2-way interleaving implementation.",
      "authors": [
        "Na Wang",
        "Wei Yan",
        "Sian-Jheng Lin",
        "Yuliang Huang"
      ],
      "published": "2023-07-29T02:00:21Z",
      "updated": "2023-09-12T11:54:26Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.15864v2",
      "landing_url": "https://arxiv.org/abs/2307.15864v2",
      "doi": "https://doi.org/10.48550/arXiv.2307.15864"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on mixed-radix entropy coding for numeric data rather than discrete audio-token generation, quantization, or token-level modeling, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on mixed-radix entropy coding for numeric data rather than discrete audio-token generation, quantization, or token-level modeling, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
    "abstract": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
    "metadata": {
      "arxiv_id": "2307.16679",
      "title": "Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech",
      "summary": "Neural text-to-speech systems are often optimized on L1/L2 losses, which make strong assumptions about the distributions of the target data space. Aiming to improve those assumptions, Normalizing Flows and Diffusion Probabilistic Models were recently proposed as alternatives. In this paper, we compare traditional L1/L2-based approaches to diffusion and flow-based approaches for the tasks of prosody and mel-spectrogram prediction for text-to-speech synthesis. We use a prosody model to generate log-f0 and duration features, which are used to condition an acoustic model that generates mel-spectrograms. Experimental results demonstrate that the flow-based model achieves the best performance for spectrogram prediction, improving over equivalent diffusion and L1 models. Meanwhile, both diffusion and flow-based prosody predictors result in significant improvements over a typical L2-trained prosody models.",
      "authors": [
        "Guangyan Zhang",
        "Thomas Merritt",
        "Manuel Sam Ribeiro",
        "Biel Tura-Vecino",
        "Kayoko Yanagisawa",
        "Kamil Pokora",
        "Abdelhamid Ezzerg",
        "Sebastian Cygert",
        "Ammar Abbas",
        "Piotr Bilinski",
        "Roberto Barra-Chicote",
        "Daniel Korzekwa",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2023-07-31T13:57:04Z",
      "updated": "2023-07-31T13:57:04Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2307.16679v1",
      "landing_url": "https://arxiv.org/abs/2307.16679v1",
      "doi": "https://doi.org/10.48550/arXiv.2307.16679"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on flow/diffusion prosody/acoustic modeling for continuous spectrograms without any discrete token/quantization discussion, so it fails to meet the discrete-audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Focuses on flow/diffusion prosody/acoustic modeling for continuous spectrograms without any discrete token/quantization discussion, so it fails to meet the discrete-audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
    "abstract": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
    "metadata": {
      "arxiv_id": "2308.00531",
      "title": "Adaptive Bitrate Video Semantic Communication over Wireless Networks",
      "summary": "This paper investigates the adaptive bitrate (ABR) video semantic communication over wireless networks. In the considered model, video sensing devices must transmit video semantic information to an edge server, to facilitate ubiquitous video sensing services such as road environment monitoring at the edge server in autonomous driving scenario. However, due to the varying wireless network conditions, it is challenging to guarantee both low transmission delay and high semantic accuracy at the same time if devices continuously transmit a fixed bitrate video semantic information. To address this challenge, we develop an adaptive bitrate video semantic communication (ABRVSC) system, in which devices adaptively adjust the bitrate of video semantic information according to network conditions. Specifically, we first define the quality of experience (QoE) for video semantic communication. Subsequently, a swin transformer-based semantic codec is proposed to extract semantic information with considering the influence of QoE. Then, we propose an Actor-Critic based ABR algorithm for the semantic codec to enhance the robustness of the proposed ABRVSC scheme against network variations. Simulation results demonstrate that at low bitrates, the mean intersection over union (MIoU) of the proposed ABRVSC scheme is nearly twice that of the traditional scheme. Moreover, the proposed ABRVSC scheme, which increases the QoE in video semantic communication by 36.57%, exhibits more robustness against network variations compared to both the fixed bitrate schemes and traditional ABR schemes.",
      "authors": [
        "Wentao Gong",
        "Haonan Tong",
        "Sihua Wang",
        "Zhaohui Yang",
        "Xinxin He",
        "Changchuan Yin"
      ],
      "published": "2023-08-01T13:25:10Z",
      "updated": "2023-08-01T13:25:10Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.00531v1",
      "landing_url": "https://arxiv.org/abs/2308.00531v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.00531"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe adaptive video semantic communication over wireless networks, which does not target discrete audio token representations or tokenizer design, so it fails to match inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe adaptive video semantic communication over wireless networks, which does not target discrete audio token representations or tokenizer design, so it fails to match inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
    "abstract": "While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2.",
    "metadata": {
      "arxiv_id": "2308.01018",
      "title": "SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis",
      "summary": "While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2.",
      "authors": [
        "Ramanan Sivaguru",
        "Vasista Sai Lodagala",
        "S Umesh"
      ],
      "published": "2023-08-02T08:59:52Z",
      "updated": "2023-08-02T08:59:52Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.01018v1",
      "landing_url": "https://arxiv.org/abs/2308.01018v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.01018"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper augments FastSpeech2 with SSL features and reconstruction losses without defining or evaluating any discrete token/tokenizer pipeline, so it fails the discrete audio token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper augments FastSpeech2 with SSL features and reconstruction losses without defining or evaluating any discrete token/tokenizer pipeline, so it fails the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs",
    "abstract": "GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.",
    "metadata": {
      "arxiv_id": "2308.02117",
      "title": "VQGraph: Rethinking Graph Representation Space for Bridging GNNs and MLPs",
      "summary": "GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively. Code: https://github.com/YangLing0818/VQGraph.",
      "authors": [
        "Ling Yang",
        "Ye Tian",
        "Minkai Xu",
        "Zhongyi Liu",
        "Shenda Hong",
        "Wei Qu",
        "Wentao Zhang",
        "Bin Cui",
        "Muhan Zhang",
        "Jure Leskovec"
      ],
      "published": "2023-08-04T02:58:08Z",
      "updated": "2024-03-06T15:06:27Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.02117v3",
      "landing_url": "https://arxiv.org/abs/2308.02117v3",
      "doi": "https://doi.org/10.48550/arXiv.2308.02117"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on graph representation distillation between GNNs and MLPs rather than discrete audio tokenization/quantization, so it fails the inclusion topic entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on graph representation distillation between GNNs and MLPs rather than discrete audio tokenization/quantization, so it fails the inclusion topic entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gloss Alignment Using Word Embeddings",
    "abstract": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
    "metadata": {
      "arxiv_id": "2308.04248",
      "title": "Gloss Alignment Using Word Embeddings",
      "summary": "Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \\acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effectiveness of our method on the \\acf{mdgs} and \\acf{bobsl} datasets, recovering up to a 33.22 BLEU-1 score in word alignment.",
      "authors": [
        "Harry Walsh",
        "Ozge Mercanoglu Sincan",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2023-08-08T13:26:53Z",
      "updated": "2023-08-08T13:26:53Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.04248v1",
      "landing_url": "https://arxiv.org/abs/2308.04248v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.04248"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on aligning sign language sign spotting with subtitles using language models rather than learning discrete audio tokens or quantized vocabularies, so it fails the inclusion criteria and meets the exclusion criteria for non-audio-token research."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on aligning sign language sign spotting with subtitles using language models rather than learning discrete audio tokens or quantized vocabularies, so it fails the inclusion criteria and meets the exclusion criteria for non-audio-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
    "abstract": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
    "metadata": {
      "arxiv_id": "2308.05502",
      "title": "Bringing order into the realm of Transformer-based language models for artificial intelligence and law",
      "summary": "Transformer-based language models (TLMs) have widely been recognized to be a cutting-edge technology for the successful development of deep-learning-based solutions to problems and applications that require natural language processing and understanding. Like for other textual domains, TLMs have indeed pushed the state-of-the-art of AI approaches for many tasks of interest in the legal domain. Despite the first Transformer model being proposed about six years ago, there has been a rapid progress of this technology at an unprecedented rate, whereby BERT and related models represent a major reference, also in the legal domain. This article provides the first systematic overview of TLM-based methods for AI-driven problems and tasks in the legal sphere. A major goal is to highlight research advances in this field so as to understand, on the one hand, how the Transformers have contributed to the success of AI in supporting legal processes, and on the other hand, what are the current limitations and opportunities for further research development.",
      "authors": [
        "Candida M. Greco",
        "Andrea Tagarelli"
      ],
      "published": "2023-08-10T11:14:22Z",
      "updated": "2024-02-03T09:54:51Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.IR",
        "cs.NE",
        "physics.soc-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05502v2",
      "landing_url": "https://arxiv.org/abs/2308.05502v2",
      "doi": "https://doi.org/10.1007/s10506-023-09374-7"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on Transformer models for legal text, not on discrete audio-token generation, quantization, or evaluation, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on Transformer models for legal text, not on discrete audio-token generation, quantization, or evaluation, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking",
    "abstract": "Multi-object tracking (MOT) at low frame rates can reduce computational, storage and power overhead to better meet the constraints of edge devices. Many existing MOT methods suffer from significant performance degradation in low-frame-rate videos due to significant location and appearance changes between adjacent frames. To this end, we propose to explore collaborative tracking learning (ColTrack) for frame-rate-insensitive MOT in a query-based end-to-end manner. Multiple historical queries of the same target jointly track it with richer temporal descriptions. Meanwhile, we insert an information refinement module between every two temporal blocking decoders to better fuse temporal clues and refine features. Moreover, a tracking object consistency loss is proposed to guide the interaction between historical queries. Extensive experimental results demonstrate that in high-frame-rate videos, ColTrack obtains higher performance than state-of-the-art methods on large-scale datasets Dancetrack and BDD100K, and outperforms the existing end-to-end methods on MOT17. More importantly, ColTrack has a significant advantage over state-of-the-art methods in low-frame-rate videos, which allows it to obtain faster processing speeds by reducing frame-rate requirements while maintaining higher performance. Code will be released at https://github.com/yolomax/ColTrack",
    "metadata": {
      "arxiv_id": "2308.05911",
      "title": "Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking",
      "summary": "Multi-object tracking (MOT) at low frame rates can reduce computational, storage and power overhead to better meet the constraints of edge devices. Many existing MOT methods suffer from significant performance degradation in low-frame-rate videos due to significant location and appearance changes between adjacent frames. To this end, we propose to explore collaborative tracking learning (ColTrack) for frame-rate-insensitive MOT in a query-based end-to-end manner. Multiple historical queries of the same target jointly track it with richer temporal descriptions. Meanwhile, we insert an information refinement module between every two temporal blocking decoders to better fuse temporal clues and refine features. Moreover, a tracking object consistency loss is proposed to guide the interaction between historical queries. Extensive experimental results demonstrate that in high-frame-rate videos, ColTrack obtains higher performance than state-of-the-art methods on large-scale datasets Dancetrack and BDD100K, and outperforms the existing end-to-end methods on MOT17. More importantly, ColTrack has a significant advantage over state-of-the-art methods in low-frame-rate videos, which allows it to obtain faster processing speeds by reducing frame-rate requirements while maintaining higher performance. Code will be released at https://github.com/yolomax/ColTrack",
      "authors": [
        "Yiheng Liu",
        "Junta Wu",
        "Yi Fu"
      ],
      "published": "2023-08-11T02:25:58Z",
      "updated": "2023-09-12T14:01:07Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.05911v2",
      "landing_url": "https://arxiv.org/abs/2308.05911v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.05911"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on frame-rate-insensitive multi-object tracking in video rather than any discrete audio token generation/tokenizer research, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on frame-rate-insensitive multi-object tracking in video rather than any discrete audio token generation/tokenizer research, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
    "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
    "metadata": {
      "arxiv_id": "2308.06873",
      "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
      "summary": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
      "authors": [
        "Xiaofei Wang",
        "Manthan Thakker",
        "Zhuo Chen",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Sanyuan Chen",
        "Min Tang",
        "Shujie Liu",
        "Jinyu Li",
        "Takuya Yoshioka"
      ],
      "published": "2023-08-14T01:01:19Z",
      "updated": "2024-06-25T18:38:28Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.06873v2",
      "landing_url": "https://arxiv.org/abs/2308.06873v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.06873"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "SpeechX explicitly centers on a neural codec language model that presumably uses discrete codes for multi-task speech generation, fulfilling the discrete-token focus and codec evaluation needs, so better to include; 4"
    },
    "round-A_JuniorNano_reasoning": "SpeechX explicitly centers on a neural codec language model that presumably uses discrete codes for multi-task speech generation, fulfilling the discrete-token focus and codec evaluation needs, so better to include; 4",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models",
    "abstract": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.",
    "metadata": {
      "arxiv_id": "2308.07922",
      "title": "RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder Language Models",
      "summary": "In this paper, we investigate the in-context learning ability of retrieval-augmented encoder-decoder language models. We first conduct a comprehensive analysis of existing models and identify their limitations in in-context learning, primarily due to a mismatch between pretraining and inference, as well as a restricted context length. To address these issues, we propose RAVEN, a model that combines retrieval-augmented masked language modeling and prefix language modeling. We further introduce Fusion-in-Context Learning to enhance the few-shot performance by enabling the model to leverage more in-context examples without requiring additional training. Through extensive experiments, we demonstrate that our simple yet effective design significantly improves performance, achieving results comparable to the most advanced language models in certain scenarios, despite having substantially fewer parameters. Our work underscores the potential of retrieval-augmented encoder-decoder language models for in-context learning and encourages further research in this direction.",
      "authors": [
        "Jie Huang",
        "Wei Ping",
        "Peng Xu",
        "Mohammad Shoeybi",
        "Kevin Chen-Chuan Chang",
        "Bryan Catanzaro"
      ],
      "published": "2023-08-15T17:59:18Z",
      "updated": "2024-08-19T05:46:56Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.07922v3",
      "landing_url": "https://arxiv.org/abs/2308.07922v3",
      "doi": "https://doi.org/10.48550/arXiv.2308.07922"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on retrieval-augmented encoder-decoder language models for in-context learning with textual data and makes no mention of discrete audio tokens, tokenizers, quantization, or codec evaluations, so it fails all inclusion criteria and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on retrieval-augmented encoder-decoder language models for in-context learning with textual data and makes no mention of discrete audio tokens, tokenizers, quantization, or codec evaluations, so it fails all inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Network Training Strategy to Enhance Anomaly Detection Performance: A Perspective on Reconstruction Loss Amplification",
    "abstract": "Unsupervised anomaly detection (UAD) is a widely adopted approach in industry due to rare anomaly occurrences and data imbalance. A desirable characteristic of an UAD model is contained generalization ability which excels in the reconstruction of seen normal patterns but struggles with unseen anomalies. Recent studies have pursued to contain the generalization capability of their UAD models in reconstruction from different perspectives, such as design of neural network (NN) structure and training strategy. In contrast, we note that containing of generalization ability in reconstruction can also be obtained simply from steep-shaped loss landscape. Motivated by this, we propose a loss landscape sharpening method by amplifying the reconstruction loss, dubbed Loss AMPlification (LAMP). LAMP deforms the loss landscape into a steep shape so the reconstruction error on unseen anomalies becomes greater. Accordingly, the anomaly detection performance is improved without any change of the NN architecture. Our findings suggest that LAMP can be easily applied to any reconstruction error metrics in UAD settings where the reconstruction model is trained with anomaly-free samples only.",
    "metadata": {
      "arxiv_id": "2308.14595",
      "title": "Neural Network Training Strategy to Enhance Anomaly Detection Performance: A Perspective on Reconstruction Loss Amplification",
      "summary": "Unsupervised anomaly detection (UAD) is a widely adopted approach in industry due to rare anomaly occurrences and data imbalance. A desirable characteristic of an UAD model is contained generalization ability which excels in the reconstruction of seen normal patterns but struggles with unseen anomalies. Recent studies have pursued to contain the generalization capability of their UAD models in reconstruction from different perspectives, such as design of neural network (NN) structure and training strategy. In contrast, we note that containing of generalization ability in reconstruction can also be obtained simply from steep-shaped loss landscape. Motivated by this, we propose a loss landscape sharpening method by amplifying the reconstruction loss, dubbed Loss AMPlification (LAMP). LAMP deforms the loss landscape into a steep shape so the reconstruction error on unseen anomalies becomes greater. Accordingly, the anomaly detection performance is improved without any change of the NN architecture. Our findings suggest that LAMP can be easily applied to any reconstruction error metrics in UAD settings where the reconstruction model is trained with anomaly-free samples only.",
      "authors": [
        "YeongHyeon Park",
        "Sungho Kang",
        "Myung Jin Kim",
        "Hyeonho Jeong",
        "Hyunkyu Park",
        "Hyeong Seok Kim",
        "Juneho Yi"
      ],
      "published": "2023-08-28T14:06:36Z",
      "updated": "2023-08-28T14:06:36Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.14595v1",
      "landing_url": "https://arxiv.org/abs/2308.14595v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.14595"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on sharpening loss landscapes for unsupervised anomaly detection reconstruction without touching discrete audio tokens, so it fails to meet the discrete token generation/focus requirement."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on sharpening loss landscapes for unsupervised anomaly detection reconstruction without touching discrete audio tokens, so it fails to meet the discrete token generation/focus requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DTrOCR: Decoder-only Transformer for Optical Character Recognition",
    "abstract": "Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.",
    "metadata": {
      "arxiv_id": "2308.15996",
      "title": "DTrOCR: Decoder-only Transformer for Optical Character Recognition",
      "summary": "Typical text recognition methods rely on an encoder-decoder structure, in which the encoder extracts features from an image, and the decoder produces recognized text from these features. In this study, we propose a simpler and more effective method for text recognition, known as the Decoder-only Transformer for Optical Character Recognition (DTrOCR). This method uses a decoder-only Transformer to take advantage of a generative language model that is pre-trained on a large corpus. We examined whether a generative language model that has been successful in natural language processing can also be effective for text recognition in computer vision. Our experiments demonstrated that DTrOCR outperforms current state-of-the-art methods by a large margin in the recognition of printed, handwritten, and scene text in both English and Chinese.",
      "authors": [
        "Masato Fujitake"
      ],
      "published": "2023-08-30T12:37:03Z",
      "updated": "2023-08-30T12:37:03Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.15996v1",
      "landing_url": "https://arxiv.org/abs/2308.15996v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.15996"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on decoder-only OCR for text, not on discrete audio token generation/analysis, so it clearly fails to meet the audio discrete token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on decoder-only OCR for text, not on discrete audio token generation/analysis, so it clearly fails to meet the audio discrete token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
    "abstract": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
    "metadata": {
      "arxiv_id": "2308.16552",
      "title": "Prompt-enhanced Hierarchical Transformer Elevating Cardiopulmonary Resuscitation Instruction via Temporal Action Segmentation",
      "summary": "The vast majority of people who suffer unexpected cardiac arrest are performed cardiopulmonary resuscitation (CPR) by passersby in a desperate attempt to restore life, but endeavors turn out to be fruitless on account of disqualification. Fortunately, many pieces of research manifest that disciplined training will help to elevate the success rate of resuscitation, which constantly desires a seamless combination of novel techniques to yield further advancement. To this end, we collect a custom CPR video dataset in which trainees make efforts to behave resuscitation on mannequins independently in adherence to approved guidelines, thereby devising an auxiliary toolbox to assist supervision and rectification of intermediate potential issues via modern deep learning methodologies. Our research empirically views this problem as a temporal action segmentation (TAS) task in computer vision, which aims to segment an untrimmed video at a frame-wise level. Here, we propose a Prompt-enhanced hierarchical Transformer (PhiTrans) that integrates three indispensable modules, including a textual prompt-based Video Features Extractor (VFE), a transformer-based Action Segmentation Executor (ASE), and a regression-based Prediction Refinement Calibrator (PRC). The backbone of the model preferentially derives from applications in three approved public datasets (GTEA, 50Salads, and Breakfast) collected for TAS tasks, which accounts for the excavation of the segmentation pipeline on the CPR dataset. In general, we unprecedentedly probe into a feasible pipeline that genuinely elevates the CPR instruction qualification via action segmentation in conjunction with cutting-edge deep learning techniques. Associated experiments advocate our implementation with multiple metrics surpassing 91.0%.",
      "authors": [
        "Yang Liu",
        "Xiaoyun Zhong",
        "Shiyao Zhai",
        "Zhicheng Du",
        "Zhenyuan Gao",
        "Qiming Huang",
        "Canyang Zhang",
        "Bin Jiang",
        "Vijay Kumar Pandey",
        "Sanyang Han",
        "Runming Wang",
        "Yuxing Han",
        "Peiwu Qin"
      ],
      "published": "2023-08-31T08:43:52Z",
      "updated": "2023-08-31T08:43:52Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16552v1",
      "landing_url": "https://arxiv.org/abs/2308.16552v1",
      "doi": "https://doi.org/10.48550/arXiv.2308.16552"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe CPR video action segmentation with transformers, which has no relation to discrete audio tokens or any tokenizer/quantization work, so it clearly fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe CPR video action segmentation with transformers, which has no relation to discrete audio tokens or any tokenizer/quantization work, so it clearly fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fast Compressed Segmentation Volumes for Scientific Visualization",
    "abstract": "Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.",
    "metadata": {
      "arxiv_id": "2308.16619",
      "title": "Fast Compressed Segmentation Volumes for Scientific Visualization",
      "summary": "Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.",
      "authors": [
        "Max Piochowiak",
        "Carsten Dachsbacher"
      ],
      "published": "2023-08-31T10:25:54Z",
      "updated": "2023-11-16T10:47:51Z",
      "categories": [
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16619v2",
      "landing_url": "https://arxiv.org/abs/2308.16619v2",
      "doi": "https://doi.org/10.1109/TVCG.2023.3326573"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on GPU-friendly compression of voxel segmentation volumes rather than any form of discrete audio tokenization, so it fails the inclusion criteria and matches exclusion (non-audio domain) requirements."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on GPU-friendly compression of voxel segmentation volumes rather than any form of discrete audio tokenization, so it fails the inclusion criteria and matches exclusion (non-audio domain) requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
    "abstract": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
    "metadata": {
      "arxiv_id": "2308.16692",
      "title": "SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models",
      "summary": "Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers. Furthermore, We construct a Unified Speech Language Model (USLM) leveraging SpeechTokenizer. Experiments show that SpeechTokenizer performs comparably to EnCodec in speech reconstruction and demonstrates strong performance on the SLMTokBench benchmark. Also, USLM outperforms VALL-E in zero-shot Text-to-Speech tasks. Code and models are available at https://github.com/ZhangXInFD/SpeechTokenizer/.",
      "authors": [
        "Xin Zhang",
        "Dong Zhang",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2023-08-31T12:53:09Z",
      "updated": "2024-01-23T01:56:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2308.16692v2",
      "landing_url": "https://arxiv.org/abs/2308.16692v2",
      "doi": "https://doi.org/10.48550/arXiv.2308.16692"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract describe a discrete-token tokenizer combining semantic and codec tokens with RVQ, benchmarking reconstruction and downstream LM tasks so it clearly fits the inclusion criteria and avoids exclusions."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a discrete-token tokenizer combining semantic and codec tokens with RVQ, benchmarking reconstruction and downstream LM tasks so it clearly fits the inclusion criteria and avoids exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
    "abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
    "metadata": {
      "arxiv_id": "2309.00169",
      "title": "RepCodec: A Speech Representation Codec for Speech Tokenization",
      "summary": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec. We believe our method can facilitate large language modeling research on speech processing.",
      "authors": [
        "Zhichao Huang",
        "Chutong Meng",
        "Tom Ko"
      ],
      "published": "2023-08-31T23:26:10Z",
      "updated": "2024-07-22T09:53:44Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.00169v3",
      "landing_url": "https://arxiv.org/abs/2309.00169v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.00169"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "RepCodec explicitly learns quantized speech tokens from SSL representations, discusses codec/tokenization pipeline, and evaluates downstream tasks so it satisfies inclusion criteria fully with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "RepCodec explicitly learns quantized speech tokens from SSL representations, discusses codec/tokenization pipeline, and evaluates downstream tasks so it satisfies inclusion criteria fully with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
    "abstract": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
    "metadata": {
      "arxiv_id": "2309.01692",
      "title": "Mask-Attention-Free Transformer for 3D Instance Segmentation",
      "summary": "Recently, transformer-based methods have dominated 3D instance segmentation, where mask attention is commonly involved. Specifically, object queries are guided by the initial instance masks in the first cross-attention, and then iteratively refine themselves in a similar manner. However, we observe that the mask-attention pipeline usually leads to slow convergence due to low-recall initial instance masks. Therefore, we abandon the mask attention design and resort to an auxiliary center regression task instead. Through center regression, we effectively overcome the low-recall issue and perform cross-attention by imposing positional prior. To reach this goal, we develop a series of position-aware designs. First, we learn a spatial distribution of 3D locations as the initial position queries. They spread over the 3D space densely, and thus can easily capture the objects in a scene with a high recall. Moreover, we present relative position encoding for the cross-attention and iterative refinement for more accurate position queries. Experiments show that our approach converges 4x faster than existing work, sets a new state of the art on ScanNetv2 3D instance segmentation benchmark, and also demonstrates superior performance across various datasets. Code and models are available at https://github.com/dvlab-research/Mask-Attention-Free-Transformer.",
      "authors": [
        "Xin Lai",
        "Yuhui Yuan",
        "Ruihang Chu",
        "Yukang Chen",
        "Han Hu",
        "Jiaya Jia"
      ],
      "published": "2023-09-04T16:09:28Z",
      "updated": "2023-09-04T16:09:28Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.01692v1",
      "landing_url": "https://arxiv.org/abs/2309.01692v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.01692"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe a 3D vision transformer for instance segmentation with positional queries, not discrete audio tokenization or related codec/quantization work, so it fails to meet any of the inclusion criteria and triggers exclusion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a 3D vision transformer for instance segmentation with positional queries, not discrete audio tokenization or related codec/quantization work, so it fails to meet any of the inclusion criteria and triggers exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BOLA360: Near-optimal View and Bitrate Adaptation for 360-degree Video Streaming",
    "abstract": "Recent advances in omnidirectional cameras and AR/VR headsets have spurred the adoption of 360-degree videos that are widely believed to be the future of online video streaming. 360-degree videos allow users to wear a head-mounted display (HMD) and experience the video as if they are physically present in the scene. Streaming high-quality 360-degree videos at scale is an unsolved problem that is more challenging than traditional (2D) video delivery. The data rate required to stream 360-degree videos is an order of magnitude more than traditional videos. Further, the penalty for rebuffering events where the video freezes or displays a blank screen is more severe as it may cause cybersickness. We propose an online adaptive bitrate (ABR) algorithm for 360-degree videos called BOLA360 that runs inside the client's video player and orchestrates the download of video segments from the server so as to maximize the quality-of-experience (QoE) of the user. BOLA360 conserves bandwidth by downloading only those video segments that are likely to fall within the field-of-view (FOV) of the user. In addition, BOLA360 continually adapts the bitrate of the downloaded video segments so as to enable a smooth playback without rebuffering. We prove that BOLA360 is near-optimal with respect to an optimal offline algorithm that maximizes QoE. Further, we evaluate BOLA360 on a wide range of network and user head movement profiles and show that it provides $13.6\\%$ to $372.5\\%$ more QoE than state-of-the-art algorithms. While ABR algorithms for traditional (2D) videos have been well-studied over the last decade, our work is the first ABR algorithm for 360-degree videos with both theoretical and empirical guarantees on its performance.",
    "metadata": {
      "arxiv_id": "2309.04023",
      "title": "BOLA360: Near-optimal View and Bitrate Adaptation for 360-degree Video Streaming",
      "summary": "Recent advances in omnidirectional cameras and AR/VR headsets have spurred the adoption of 360-degree videos that are widely believed to be the future of online video streaming. 360-degree videos allow users to wear a head-mounted display (HMD) and experience the video as if they are physically present in the scene. Streaming high-quality 360-degree videos at scale is an unsolved problem that is more challenging than traditional (2D) video delivery. The data rate required to stream 360-degree videos is an order of magnitude more than traditional videos. Further, the penalty for rebuffering events where the video freezes or displays a blank screen is more severe as it may cause cybersickness. We propose an online adaptive bitrate (ABR) algorithm for 360-degree videos called BOLA360 that runs inside the client's video player and orchestrates the download of video segments from the server so as to maximize the quality-of-experience (QoE) of the user. BOLA360 conserves bandwidth by downloading only those video segments that are likely to fall within the field-of-view (FOV) of the user. In addition, BOLA360 continually adapts the bitrate of the downloaded video segments so as to enable a smooth playback without rebuffering. We prove that BOLA360 is near-optimal with respect to an optimal offline algorithm that maximizes QoE. Further, we evaluate BOLA360 on a wide range of network and user head movement profiles and show that it provides $13.6\\%$ to $372.5\\%$ more QoE than state-of-the-art algorithms. While ABR algorithms for traditional (2D) videos have been well-studied over the last decade, our work is the first ABR algorithm for 360-degree videos with both theoretical and empirical guarantees on its performance.",
      "authors": [
        "Ali Zeynali",
        "Mahsa Sahebdel",
        "Mohammad Hajiesmaili",
        "Ramesh K. Sitaraman"
      ],
      "published": "2023-09-07T21:30:57Z",
      "updated": "2024-10-01T04:17:05Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04023v2",
      "landing_url": "https://arxiv.org/abs/2309.04023v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.04023"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on 360-degree video streaming adaptation rather than discrete audio token generation, quantization, or evaluation, so it fails to meet the topic’s inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Focuses on 360-degree video streaming adaptation rather than discrete audio token generation, quantization, or evaluation, so it fails to meet the topic’s inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Cross-Utterance Conditioned VAE for Speech Generation",
    "abstract": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
    "metadata": {
      "arxiv_id": "2309.04156",
      "title": "Cross-Utterance Conditioned VAE for Speech Generation",
      "summary": "Speech synthesis systems powered by neural networks hold promise for multimedia production, but frequently face issues with producing expressive speech and seamless editing. In response, we present the Cross-Utterance Conditioned Variational Autoencoder speech synthesis (CUC-VAE S2) framework to enhance prosody and ensure natural speech generation. This framework leverages the powerful representational capabilities of pre-trained language models and the re-expression abilities of variational autoencoders (VAEs). The core component of the CUC-VAE S2 framework is the cross-utterance CVAE, which extracts acoustic, speaker, and textual features from surrounding sentences to generate context-sensitive prosodic features, more accurately emulating human prosody generation. We further propose two practical algorithms tailored for distinct speech synthesis applications: CUC-VAE TTS for text-to-speech and CUC-VAE SE for speech editing. The CUC-VAE TTS is a direct application of the framework, designed to generate audio with contextual prosody derived from surrounding texts. On the other hand, the CUC-VAE SE algorithm leverages real mel spectrogram sampling conditioned on contextual information, producing audio that closely mirrors real sound and thereby facilitating flexible speech editing based on text such as deletion, insertion, and replacement. Experimental results on the LibriTTS datasets demonstrate that our proposed models significantly enhance speech synthesis and editing, producing more natural and expressive speech.",
      "authors": [
        "Yang Li",
        "Cheng Yu",
        "Guangzhi Sun",
        "Weiqin Zu",
        "Zheng Tian",
        "Ying Wen",
        "Wei Pan",
        "Chao Zhang",
        "Jun Wang",
        "Yang Yang",
        "Fanglei Sun"
      ],
      "published": "2023-09-08T06:48:41Z",
      "updated": "2024-09-19T13:41:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.04156v2",
      "landing_url": "https://arxiv.org/abs/2309.04156v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.04156"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes a CVAE-based speech synthesis/editing method operating on continuous prosodic features and mel spectrograms without introducing or analyzing discrete audio tokens/quantized vocabularies, so it fails the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a CVAE-based speech synthesis/editing method operating on continuous prosodic features and mel spectrograms without introducing or analyzing discrete audio tokens/quantized vocabularies, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
    "abstract": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
    "metadata": {
      "arxiv_id": "2309.05027",
      "title": "VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching",
      "summary": "Although diffusion models in text-to-speech have become a popular choice due to their strong generative ability, the intrinsic complexity of sampling from diffusion models harms their efficiency. Alternatively, we propose VoiceFlow, an acoustic model that utilizes a rectified flow matching algorithm to achieve high synthesis quality with a limited number of sampling steps. VoiceFlow formulates the process of generating mel-spectrograms into an ordinary differential equation conditional on text inputs, whose vector field is then estimated. The rectified flow technique then effectively straightens its sampling trajectory for efficient synthesis. Subjective and objective evaluations on both single and multi-speaker corpora showed the superior synthesis quality of VoiceFlow compared to the diffusion counterpart. Ablation studies further verified the validity of the rectified flow technique in VoiceFlow.",
      "authors": [
        "Yiwei Guo",
        "Chenpeng Du",
        "Ziyang Ma",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-09-10T13:47:39Z",
      "updated": "2024-09-01T14:57:31Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.HC",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05027v3",
      "landing_url": "https://arxiv.org/abs/2309.05027v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.05027"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "VoiceFlow focuses on continuous mel-spectrogram generation with flow matching and never describes any discrete token/vocabulary or quantization, so it fails the inclusion criteria and matches the exclusion of continuous-feature-centric work."
    },
    "round-A_JuniorNano_reasoning": "VoiceFlow focuses on continuous mel-spectrogram generation with flow matching and never describes any discrete token/vocabulary or quantization, so it fails the inclusion criteria and matches the exclusion of continuous-feature-centric work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
    "abstract": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
    "metadata": {
      "arxiv_id": "2309.05224",
      "title": "SparseSwin: Swin Transformer with Sparse Transformer Block",
      "summary": "Advancements in computer vision research have put transformer architecture as the state of the art in computer vision tasks. One of the known drawbacks of the transformer architecture is the high number of parameters, this can lead to a more complex and inefficient algorithm. This paper aims to reduce the number of parameters and in turn, made the transformer more efficient. We present Sparse Transformer (SparTa) Block, a modified transformer block with an addition of a sparse token converter that reduces the number of tokens used. We use the SparTa Block inside the Swin T architecture (SparseSwin) to leverage Swin capability to downsample its input and reduce the number of initial tokens to be calculated. The proposed SparseSwin model outperforms other state of the art models in image classification with an accuracy of 86.96%, 97.43%, and 85.35% on the ImageNet100, CIFAR10, and CIFAR100 datasets respectively. Despite its fewer parameters, the result highlights the potential of a transformer architecture using a sparse token converter with a limited number of tokens to optimize the use of the transformer and improve its performance.",
      "authors": [
        "Krisna Pinasthika",
        "Blessius Sheldo Putra Laksono",
        "Riyandi Banovbi Putera Irsal",
        "Syifa Hukma Shabiyya",
        "Novanto Yudistira"
      ],
      "published": "2023-09-11T04:03:43Z",
      "updated": "2023-09-11T04:03:43Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.05224v1",
      "landing_url": "https://arxiv.org/abs/2309.05224v1",
      "doi": "https://doi.org/10.1016/j.neucom.2024.127433"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on sparse transformers for computer vision/image classification and does not involve discrete audio tokens, tokenizers/codecs, or related evaluations, so it clearly fails the inclusion topic and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on sparse transformers for computer vision/image classification and does not involve discrete audio tokens, tokenizers/codecs, or related evaluations, so it clearly fails the inclusion topic and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Minimum Bitrate Neuromorphic Encoding for Continuous-Time Gauss-Markov Processes",
    "abstract": "In this work, we study minimum data rate tracking of a dynamical system under a neuromorphic event-based sensing paradigm. We begin by bridging the gap between continuous-time (CT) system dynamics and information theory's causal rate distortion theory. We motivate the use of non-singular source codes to quantify bitrates in event-based sampling schemes. This permits an analysis of minimum bitrate event-based tracking using tools already established in the control and information theory literature. We derive novel, nontrivial lower bounds to event-based sensing, and compare the lower bound with the performance of well-known schemes in the established literature.",
    "metadata": {
      "arxiv_id": "2309.06504",
      "title": "Minimum Bitrate Neuromorphic Encoding for Continuous-Time Gauss-Markov Processes",
      "summary": "In this work, we study minimum data rate tracking of a dynamical system under a neuromorphic event-based sensing paradigm. We begin by bridging the gap between continuous-time (CT) system dynamics and information theory's causal rate distortion theory. We motivate the use of non-singular source codes to quantify bitrates in event-based sampling schemes. This permits an analysis of minimum bitrate event-based tracking using tools already established in the control and information theory literature. We derive novel, nontrivial lower bounds to event-based sensing, and compare the lower bound with the performance of well-known schemes in the established literature.",
      "authors": [
        "Travis Cuvelier",
        "Ronald Ogden",
        "Takashi Tanaka"
      ],
      "published": "2023-09-12T18:21:36Z",
      "updated": "2023-09-12T18:21:36Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06504v1",
      "landing_url": "https://arxiv.org/abs/2309.06504v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06504"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on neuromorphic event-based tracking for Gauss-Markov processes and minimum bitrate control, which doesn’t involve discrete audio tokens, codec quantization, or any token-level modeling as required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on neuromorphic event-based tracking for Gauss-Markov processes and minimum bitrate control, which doesn’t involve discrete audio tokens, codec quantization, or any token-level modeling as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
    "abstract": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
    "metadata": {
      "arxiv_id": "2309.06787",
      "title": "DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation",
      "summary": "In the Text-to-speech(TTS) task, the latent diffusion model has excellent fidelity and generalization, but its expensive resource consumption and slow inference speed have always been a challenging. This paper proposes Discrete Diffusion Model with Contrastive Learning for Text-to-Speech Generation(DCTTS). The following contributions are made by DCTTS: 1) The TTS diffusion model based on discrete space significantly lowers the computational consumption of the diffusion model and improves sampling speed; 2) The contrastive learning method based on discrete space is used to enhance the alignment connection between speech and text and improve sampling quality; and 3) It uses an efficient text encoder to simplify the model's parameters and increase computational efficiency. The experimental results demonstrate that the approach proposed in this paper has outstanding speech synthesis quality and sampling speed while significantly reducing the resource consumption of diffusion model. The synthesized samples are available at https://github.com/lawtherWu/DCTTS.",
      "authors": [
        "Zhichao Wu",
        "Qiulin Li",
        "Sixing Liu",
        "Qun Yang"
      ],
      "published": "2023-09-13T08:22:38Z",
      "updated": "2023-09-13T08:22:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.06787v1",
      "landing_url": "https://arxiv.org/abs/2309.06787v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.06787"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Abstract describes a diffusion-based TTS model with discrete latent space but provides no information about codec/tokenizer design, quantization, or discrete audio-token evaluation, so it fails to meet the discrete-token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a diffusion-based TTS model with discrete latent space but provides no information about codec/tokenizer design, quantization, or discrete audio-token evaluation, so it fails to meet the discrete-token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation",
    "abstract": "The attention-based encoder-decoder (AED) speech recognition model has been widely successful in recent years. However, the joint optimization of acoustic model and language model in end-to-end manner has created challenges for text adaptation. In particular, effective, quick and inexpensive adaptation with text input has become a primary concern for deploying AED systems in the industry. To address this issue, we propose a novel model, the hybrid attention-based encoder-decoder (HAED) speech recognition model that preserves the modularity of conventional hybrid automatic speech recognition systems. Our HAED model separates the acoustic and language models, allowing for the use of conventional text-based language model adaptation techniques. We demonstrate that the proposed HAED model yields 23% relative Word Error Rate (WER) improvements when out-of-domain text data is used for language model adaptation, with only a minor degradation in WER on a general test set compared with the conventional AED model.",
    "metadata": {
      "arxiv_id": "2309.07369",
      "title": "Hybrid Attention-based Encoder-decoder Model for Efficient Language Model Adaptation",
      "summary": "The attention-based encoder-decoder (AED) speech recognition model has been widely successful in recent years. However, the joint optimization of acoustic model and language model in end-to-end manner has created challenges for text adaptation. In particular, effective, quick and inexpensive adaptation with text input has become a primary concern for deploying AED systems in the industry. To address this issue, we propose a novel model, the hybrid attention-based encoder-decoder (HAED) speech recognition model that preserves the modularity of conventional hybrid automatic speech recognition systems. Our HAED model separates the acoustic and language models, allowing for the use of conventional text-based language model adaptation techniques. We demonstrate that the proposed HAED model yields 23% relative Word Error Rate (WER) improvements when out-of-domain text data is used for language model adaptation, with only a minor degradation in WER on a general test set compared with the conventional AED model.",
      "authors": [
        "Shaoshi Ling",
        "Guoli Ye",
        "Rui Zhao",
        "Yifan Gong"
      ],
      "published": "2023-09-14T01:07:36Z",
      "updated": "2024-09-14T22:31:37Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07369v2",
      "landing_url": "https://arxiv.org/abs/2309.07369v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.07369"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on hybrid AED architecture for speech recognition language model adaptation and does not cover discrete audio token generation, quantization, or tokenizer details, so it fails the inclusion topic and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on hybrid AED architecture for speech recognition language model adaptation and does not cover discrete audio token generation, quantization, or tokenizer details, so it fails the inclusion topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Direct Text to Speech Translation System using Acoustic Units",
    "abstract": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
    "metadata": {
      "arxiv_id": "2309.07478",
      "title": "Direct Text to Speech Translation System using Acoustic Units",
      "summary": "This paper proposes a direct text to speech translation system using discrete acoustic units. This framework employs text in different source languages as input to generate speech in the target language without the need for text transcriptions in this language. Motivated by the success of acoustic units in previous works for direct speech to speech translation systems, we use the same pipeline to extract the acoustic units using a speech encoder combined with a clustering algorithm. Once units are obtained, an encoder-decoder architecture is trained to predict them. Then a vocoder generates speech from units. Our approach for direct text to speech translation was tested on the new CVSS corpus with two different text mBART models employed as initialisation. The systems presented report competitive performance for most of the language pairs evaluated. Besides, results show a remarkable improvement when initialising our proposed architecture with a model pre-trained with more languages.",
      "authors": [
        "Victoria Mingote",
        "Pablo Gimeno",
        "Luis Vicente",
        "Sameer Khurana",
        "Antoine Laurent",
        "Jarod Duret"
      ],
      "published": "2023-09-14T07:35:14Z",
      "updated": "2023-09-14T07:35:14Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07478v1",
      "landing_url": "https://arxiv.org/abs/2309.07478v1",
      "doi": "https://doi.org/10.1109/LSP.2023.3313513"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper describes direct TTS translation via discrete acoustic units derived through encoder+clustering and used for downstream vocoder generation, so it clearly focuses on discrete audio tokens with evaluation, matching the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper describes direct TTS translation via discrete acoustic units derived through encoder+clustering and used for downstream vocoder generation, so it clearly focuses on discrete audio tokens with evaluation, matching the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
    "abstract": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
    "metadata": {
      "arxiv_id": "2309.07937",
      "title": "Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks",
      "summary": "We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.",
      "authors": [
        "Soumi Maiti",
        "Yifan Peng",
        "Shukjae Choi",
        "Jee-weon Jung",
        "Xuankai Chang",
        "Shinji Watanabe"
      ],
      "published": "2023-09-14T03:13:18Z",
      "updated": "2024-01-24T15:36:31Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.07937v3",
      "landing_url": "https://arxiv.org/abs/2309.07937v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.07937"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper clearly targets discrete speech tokens derived from SSL features for multiple speech tasks, describes token integration and evaluation improvements, and thus satisfies the inclusion criteria and no exclusion ones (Score: 5)."
    },
    "round-A_JuniorNano_reasoning": "The paper clearly targets discrete speech tokens derived from SSL features for multiple speech tasks, describes token integration and evaluation improvements, and thus satisfies the inclusion criteria and no exclusion ones (Score: 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
    "abstract": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
    "metadata": {
      "arxiv_id": "2309.08531",
      "title": "Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens",
      "summary": "In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.",
      "authors": [
        "Minsu Kim",
        "Jeongsoo Choi",
        "Soumi Maiti",
        "Jeong Hun Yeo",
        "Shinji Watanabe",
        "Yong Man Ro"
      ],
      "published": "2023-09-15T16:48:34Z",
      "updated": "2023-09-15T16:48:34Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "eess.AS",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08531v1",
      "landing_url": "https://arxiv.org/abs/2309.08531v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.08531"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The work centers on quantized speech units derived from a self-supervised model and shows downstream gains on Im2Sp benchmarks, so it satisfies discrete audio token inclusion requirements and merits inclusion."
    },
    "round-A_JuniorNano_reasoning": "The work centers on quantized speech units derived from a self-supervised model and shows downstream gains on Im2Sp benchmarks, so it satisfies discrete audio token inclusion requirements and merits inclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation",
    "abstract": "Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.",
    "metadata": {
      "arxiv_id": "2309.08876",
      "title": "Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation",
      "summary": "Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.",
      "authors": [
        "Emiru Tsunoo",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2023-09-16T04:57:37Z",
      "updated": "2024-01-09T09:02:23Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.08876v2",
      "landing_url": "https://arxiv.org/abs/2309.08876v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.08876"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on decoder-only ASR architecture using CTC prompts and text augmentation without defining or evaluating discrete audio tokens or tokenizers, so it does not meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on decoder-only ASR architecture using CTC prompts and text augmentation without defining or evaluating discrete audio tokens or tokenizers, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Adversarial Training Cost with Gradient Approximation",
    "abstract": "Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments demonstrate that this efficiency improvement can be achieved without any or with very little loss in accuracy on natural and adversarial examples, which show that our proposed method saves up to 60\\% of the training time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100 datasets.",
    "metadata": {
      "arxiv_id": "2309.09464",
      "title": "Reducing Adversarial Training Cost with Gradient Approximation",
      "summary": "Deep learning models have achieved state-of-the-art performances in various domains, while they are vulnerable to the inputs with well-crafted but small perturbations, which are named after adversarial examples (AEs). Among many strategies to improve the model robustness against AEs, Projected Gradient Descent (PGD) based adversarial training is one of the most effective methods. Unfortunately, the prohibitive computational overhead of generating strong enough AEs, due to the maximization of the loss function, sometimes makes the regular PGD adversarial training impractical when using larger and more complicated models. In this paper, we propose that the adversarial loss can be approximated by the partial sum of Taylor series. Furthermore, we approximate the gradient of adversarial loss and propose a new and efficient adversarial training method, adversarial training with gradient approximation (GAAT), to reduce the cost of building up robust models. Additionally, extensive experiments demonstrate that this efficiency improvement can be achieved without any or with very little loss in accuracy on natural and adversarial examples, which show that our proposed method saves up to 60\\% of the training time with comparable model test accuracy on MNIST, CIFAR-10 and CIFAR-100 datasets.",
      "authors": [
        "Huihui Gong"
      ],
      "published": "2023-09-18T03:55:41Z",
      "updated": "2023-10-10T05:59:43Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.09464v3",
      "landing_url": "https://arxiv.org/abs/2309.09464v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.09464"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on reducing adversarial training cost for image classifiers via gradient approximations, so it neither targets discrete audio tokens nor discusses any tokenizer/codec quantization design, and thus fails every inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on reducing adversarial training cost for image classifiers via gradient approximations, so it neither targets discrete audio tokens nor discusses any tokenizer/codec quantization design, and thus fails every inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spatial-Assistant Encoder-Decoder Network for Real Time Semantic Segmentation",
    "abstract": "Semantic segmentation is an essential technology for self-driving cars to comprehend their surroundings. Currently, real-time semantic segmentation networks commonly employ either encoder-decoder architecture or two-pathway architecture. Generally speaking, encoder-decoder models tend to be quicker,whereas two-pathway models exhibit higher accuracy. To leverage both strengths, we present the Spatial-Assistant Encoder-Decoder Network (SANet) to fuse the two architectures. In the overall architecture, we uphold the encoder-decoder design while maintaining the feature maps in the middle section of the encoder and utilizing atrous convolution branches for same-resolution feature extraction. Toward the end of the encoder, we integrate the asymmetric pooling pyramid pooling module (APPPM) to optimize the semantic extraction of the feature maps. This module incorporates asymmetric pooling layers that extract features at multiple resolutions. In the decoder, we present a hybrid attention module, SAD, that integrates horizontal and vertical attention to facilitate the combination of various branches. To ascertain the effectiveness of our approach, our SANet model achieved competitive results on the real-time CamVid and cityscape datasets. By employing a single 2080Ti GPU, SANet achieved a 78.4 % mIOU at 65.1 FPS on the Cityscape test dataset and 78.8 % mIOU at 147 FPS on the CamVid test dataset. The training code and model for SANet are available at https://github.com/CuZaoo/SANet-main",
    "metadata": {
      "arxiv_id": "2309.10519",
      "title": "Spatial-Assistant Encoder-Decoder Network for Real Time Semantic Segmentation",
      "summary": "Semantic segmentation is an essential technology for self-driving cars to comprehend their surroundings. Currently, real-time semantic segmentation networks commonly employ either encoder-decoder architecture or two-pathway architecture. Generally speaking, encoder-decoder models tend to be quicker,whereas two-pathway models exhibit higher accuracy. To leverage both strengths, we present the Spatial-Assistant Encoder-Decoder Network (SANet) to fuse the two architectures. In the overall architecture, we uphold the encoder-decoder design while maintaining the feature maps in the middle section of the encoder and utilizing atrous convolution branches for same-resolution feature extraction. Toward the end of the encoder, we integrate the asymmetric pooling pyramid pooling module (APPPM) to optimize the semantic extraction of the feature maps. This module incorporates asymmetric pooling layers that extract features at multiple resolutions. In the decoder, we present a hybrid attention module, SAD, that integrates horizontal and vertical attention to facilitate the combination of various branches. To ascertain the effectiveness of our approach, our SANet model achieved competitive results on the real-time CamVid and cityscape datasets. By employing a single 2080Ti GPU, SANet achieved a 78.4 % mIOU at 65.1 FPS on the Cityscape test dataset and 78.8 % mIOU at 147 FPS on the CamVid test dataset. The training code and model for SANet are available at https://github.com/CuZaoo/SANet-main",
      "authors": [
        "Yalun Wang",
        "Shidong Chen",
        "Huicong Bian",
        "Weixiao Li",
        "Qin Lu"
      ],
      "published": "2023-09-19T10:59:42Z",
      "updated": "2023-09-19T10:59:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10519v1",
      "landing_url": "https://arxiv.org/abs/2309.10519v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10519"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on real-time semantic segmentation for vision and does not discuss discrete audio tokens or codec/tokenizer design, so it fails every inclusion criterion and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on real-time semantic segmentation for vision and does not discuss discrete audio tokens or codec/tokenizer design, so it fails every inclusion criterion and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
    "abstract": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
    "metadata": {
      "arxiv_id": "2309.10922",
      "title": "Discrete Audio Representation as an Alternative to Mel-Spectrograms for Speaker and Speech Recognition",
      "summary": "Discrete audio representation, aka audio tokenization, has seen renewed interest driven by its potential to facilitate the application of text language modeling approaches in audio domain. To this end, various compression and representation-learning based tokenization schemes have been proposed. However, there is limited investigation into the performance of compression-based audio tokens compared to well-established mel-spectrogram features across various speaker and speech related tasks. In this paper, we evaluate compression based audio tokens on three tasks: Speaker Verification, Diarization and (Multi-lingual) Speech Recognition. Our findings indicate that (i) the models trained on audio tokens perform competitively, on average within $1\\%$ of mel-spectrogram features for all the tasks considered, and do not surpass them yet. (ii) these models exhibit robustness for out-of-domain narrowband data, particularly in speaker tasks. (iii) audio tokens allow for compression to 20x compared to mel-spectrogram features with minimal loss of performance in speech and speaker related tasks, which is crucial for low bit-rate applications, and (iv) the examined Residual Vector Quantization (RVQ) based audio tokenizer exhibits a low-pass frequency response characteristic, offering a plausible explanation for the observed results, and providing insight for future tokenizer designs.",
      "authors": [
        "Krishna C. Puvvada",
        "Nithin Rao Koluguri",
        "Kunal Dhawan",
        "Jagadeesh Balam",
        "Boris Ginsburg"
      ],
      "published": "2023-09-19T20:49:05Z",
      "updated": "2023-09-19T20:49:05Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.10922v1",
      "landing_url": "https://arxiv.org/abs/2309.10922v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.10922"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "This paper evaluates compression-based discrete audio tokens (e.g., RVQ) across speaker/speech tasks with quantization and tokenizer analysis, clearly matching the discrete-token inclusion criteria without violating exclusions."
    },
    "round-A_JuniorNano_reasoning": "This paper evaluates compression-based discrete audio tokens (e.g., RVQ) across speaker/speech tasks with quantization and tokenizer analysis, clearly matching the discrete-token inclusion criteria without violating exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
    "abstract": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
    "metadata": {
      "arxiv_id": "2309.11977",
      "title": "Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts",
      "summary": "Zero-shot text-to-speech (TTS) synthesis aims to clone any unseen speaker's voice without adaptation parameters. By quantizing speech waveform into discrete acoustic tokens and modeling these tokens with the language model, recent language model-based TTS models show zero-shot speaker adaptation capabilities with only a 3-second acoustic prompt of an unseen speaker. However, they are limited by the length of the acoustic prompt, which makes it difficult to clone personal speaking style. In this paper, we propose a novel zero-shot TTS model with the multi-scale acoustic prompts based on a neural codec language model VALL-E. A speaker-aware text encoder is proposed to learn the personal speaking style at the phoneme-level from the style prompt consisting of multiple sentences. Following that, a VALL-E based acoustic decoder is utilized to model the timbre from the timbre prompt at the frame-level and generate speech. The experimental results show that our proposed method outperforms baselines in terms of naturalness and speaker similarity, and can achieve better performance by scaling out to a longer style prompt.",
      "authors": [
        "Shun Lei",
        "Yixuan Zhou",
        "Liyang Chen",
        "Dan Luo",
        "Zhiyong Wu",
        "Xixin Wu",
        "Shiyin Kang",
        "Tao Jiang",
        "Yahui Zhou",
        "Yuxing Han",
        "Helen Meng"
      ],
      "published": "2023-09-21T11:22:22Z",
      "updated": "2024-04-09T08:39:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.11977v3",
      "landing_url": "https://arxiv.org/abs/2309.11977v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.11977"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Model leverages quantized discrete acoustic tokens via a codec-style language model (VALL-E) for zero-shot voice cloning, matching the defined discrete audio token focus and evaluations, so it clearly belongs in the set."
    },
    "round-A_JuniorNano_reasoning": "Model leverages quantized discrete acoustic tokens via a codec-style language model (VALL-E) for zero-shot voice cloning, matching the defined discrete audio token focus and evaluations, so it clearly belongs in the set.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Parallelizing non-linear sequential models over the sequence length",
    "abstract": "Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work serves as the first step to unlock the potential of non-linear sequential models for long sequence problems.",
    "metadata": {
      "arxiv_id": "2309.12252",
      "title": "Parallelizing non-linear sequential models over the sequence length",
      "summary": "Sequential models, such as Recurrent Neural Networks and Neural Ordinary Differential Equations, have long suffered from slow training due to their inherent sequential nature. For many years this bottleneck has persisted, as many thought sequential models could not be parallelized. We challenge this long-held belief with our parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The algorithm does not need any special structure in the sequential models' architecture, making it applicable to a wide range of architectures. Using our method, training sequential models can be more than 10 times faster than the common sequential method without any meaningful difference in the training results. Leveraging this accelerated training, we discovered the efficacy of the Gated Recurrent Unit in a long time series classification problem with 17k time samples. By overcoming the training bottleneck, our work serves as the first step to unlock the potential of non-linear sequential models for long sequence problems.",
      "authors": [
        "Yi Heng Lim",
        "Qi Zhu",
        "Joshua Selfridge",
        "Muhammad Firmansyah Kasim"
      ],
      "published": "2023-09-21T16:52:34Z",
      "updated": "2024-01-16T16:56:11Z",
      "categories": [
        "cs.LG",
        "cs.DC",
        "physics.comp-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.12252v3",
      "landing_url": "https://arxiv.org/abs/2309.12252v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.12252"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper targets general sequential model training speedups rather than discrete audio token generation or analysis, so it fails the inclusion scope and receives a 1."
    },
    "round-A_JuniorNano_reasoning": "Paper targets general sequential model training speedups rather than discrete audio token generation or analysis, so it fails the inclusion scope and receives a 1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speaker anonymization using neural audio codec language models",
    "abstract": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
    "metadata": {
      "arxiv_id": "2309.14129",
      "title": "Speaker anonymization using neural audio codec language models",
      "summary": "The vast majority of approaches to speaker anonymization involve the extraction of fundamental frequency estimates, linguistic features and a speaker embedding which is perturbed to obfuscate the speaker identity before an anonymized speech waveform is resynthesized using a vocoder. Recent work has shown that x-vector transformations are difficult to control consistently: other sources of speaker information contained within fundamental frequency and linguistic features are re-entangled upon vocoding, meaning that anonymized speech signals still contain speaker information. We propose an approach based upon neural audio codecs (NACs), which are known to generate high-quality synthetic speech when combined with language models. NACs use quantized codes, which are known to effectively bottleneck speaker-related information: we demonstrate the potential of speaker anonymization systems based on NAC language modeling by applying the evaluation framework of the Voice Privacy Challenge 2022.",
      "authors": [
        "Michele Panariello",
        "Francesco Nespoli",
        "Massimiliano Todisco",
        "Nicholas Evans"
      ],
      "published": "2023-09-25T13:32:09Z",
      "updated": "2024-01-12T15:05:34Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14129v3",
      "landing_url": "https://arxiv.org/abs/2309.14129v3",
      "doi": "https://doi.org/10.48550/arXiv.2309.14129"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The study centers on neural audio codecs that quantize speech into discrete codes and models those codes with a language model for speaker anonymization while evaluating using the Voice Privacy Challenge, so it satisfies the discrete-token focus and evaluation requirements."
    },
    "round-A_JuniorNano_reasoning": "The study centers on neural audio codecs that quantize speech into discrete codes and models those codes with a language model for speaker anonymization while evaluating using the Voice Privacy Challenge, so it satisfies the discrete-token focus and evaluation requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
    "abstract": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
    "metadata": {
      "arxiv_id": "2309.14324",
      "title": "Towards General-Purpose Text-Instruction-Guided Voice Conversion",
      "summary": "This paper introduces a novel voice conversion (VC) model, guided by text instructions such as \"articulate slowly with a deep tone\" or \"speak in a cheerful boyish voice\". Unlike traditional methods that rely on reference utterances to determine the attributes of the converted speech, our model adds versatility and specificity to voice conversion. The proposed VC model is a neural codec language model which processes a sequence of discrete codes, resulting in the code sequence of converted speech. It utilizes text instructions as style prompts to modify the prosody and emotional information of the given speech. In contrast to previous approaches, which often rely on employing separate encoders like prosody and content encoders to handle different aspects of the source speech, our model handles various information of speech in an end-to-end manner. Experiments have demonstrated the impressive capabilities of our model in comprehending instructions and delivering reasonable results.",
      "authors": [
        "Chun-Yi Kuan",
        "Chen An Li",
        "Tsu-Yuan Hsu",
        "Tse-Yang Lin",
        "Ho-Lam Chung",
        "Kai-Wei Chang",
        "Shuo-yiin Chang",
        "Hung-yi Lee"
      ],
      "published": "2023-09-25T17:52:09Z",
      "updated": "2024-01-16T13:53:56Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14324v2",
      "landing_url": "https://arxiv.org/abs/2309.14324v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14324"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper describes a neural codec language model that processes discrete codes for voice conversion, so the research centers on discrete audio token generation and modeling which satisfies the inclusion criteria and no exclusion applies."
    },
    "round-A_JuniorNano_reasoning": "Paper describes a neural codec language model that processes discrete codes for voice conversion, so the research centers on discrete audio token generation and modeling which satisfies the inclusion criteria and no exclusion applies.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
    "abstract": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
    "metadata": {
      "arxiv_id": "2309.14509",
      "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models",
      "summary": "Computation in a typical Transformer-based large language model (LLM) can be characterized by batch size, hidden dimension, number of layers, and sequence length. Until now, system works for accelerating LLM training have focused on the first three dimensions: data parallelism for batch size, tensor parallelism for hidden size and pipeline parallelism for model depth or layers. These widely studied forms of parallelism are not targeted or optimized for long sequence Transformer models. Given practical application needs for long sequence LLM, renewed attentions are being drawn to sequence parallelism. However, existing works in sequence parallelism are constrained by memory-communication inefficiency, limiting their scalability to long sequence large models. In this work, we introduce DeepSpeed-Ulysses, a novel, portable and effective methodology for enabling highly efficient and scalable LLM training with extremely long sequence length. DeepSpeed-Ulysses at its core partitions input data along the sequence dimension and employs an efficient all-to-all collective communication for attention computation. Theoretical communication analysis shows that whereas other methods incur communication overhead as sequence length increases, DeepSpeed-Ulysses maintains constant communication volume when sequence length and compute devices are increased proportionally. Furthermore, experimental evaluations show that DeepSpeed-Ulysses trains 2.5x faster with 4x longer sequence length than the existing method SOTA baseline.",
      "authors": [
        "Sam Ade Jacobs",
        "Masahiro Tanaka",
        "Chengming Zhang",
        "Minjia Zhang",
        "Shuaiwen Leon Song",
        "Samyam Rajbhandari",
        "Yuxiong He"
      ],
      "published": "2023-09-25T20:15:57Z",
      "updated": "2023-10-04T16:51:13Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.DC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.14509v2",
      "landing_url": "https://arxiv.org/abs/2309.14509v2",
      "doi": "https://doi.org/10.48550/arXiv.2309.14509"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe system optimizations for long sequence transformer training without any discussion of discrete audio tokenization, so it fails the inclusion criteria and meets the exclusion (non-audio sequence) criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe system optimizations for long sequence transformer training without any discussion of discrete audio tokenization, so it fails the inclusion criteria and meets the exclusion (non-audio sequence) criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Faster Relative Entropy Coding with Greedy Rejection Coding",
    "abstract": "Relative entropy coding (REC) algorithms encode a sample from a target distribution $Q$ using a proposal distribution $P$ using as few bits as possible. Unlike entropy coding, REC does not assume discrete distributions or require quantisation. As such, it can be naturally integrated into communication pipelines such as learnt compression and differentially private federated learning. Unfortunately, despite their practical benefits, REC algorithms have not seen widespread application, due to their prohibitively slow runtimes or restrictive assumptions. In this paper, we make progress towards addressing these issues. We introduce Greedy Rejection Coding (GRC), which generalises the rejection based-algorithm of Harsha et al. (2007) to arbitrary probability spaces and partitioning schemes. We first show that GRC terminates almost surely and returns unbiased samples from $Q$, after which we focus on two of its variants: GRCS and GRCD. We show that for continuous $Q$ and $P$ over $\\mathbb{R}$ with unimodal density ratio $dQ/dP$, the expected runtime of GRCS is upper bounded by $βD_{KL}[Q || P] + O(1)$ where $β\\approx 4.82$, and its expected codelength is optimal. This makes GRCS the first REC algorithm with guaranteed optimal runtime for this class of distributions, up to the multiplicative constant $β$. This significantly improves upon the previous state-of-the-art method, A* coding (Flamich et al., 2022). Under the same assumptions, we experimentally observe and conjecture that the expected runtime and codelength of GRCD are upper bounded by $D_{KL}[Q || P] + O(1)$. Finally, we evaluate GRC in a variational autoencoder-based compression pipeline on MNIST, and show that a modified ELBO and an index-compression method can further improve compression efficiency.",
    "metadata": {
      "arxiv_id": "2309.15746",
      "title": "Faster Relative Entropy Coding with Greedy Rejection Coding",
      "summary": "Relative entropy coding (REC) algorithms encode a sample from a target distribution $Q$ using a proposal distribution $P$ using as few bits as possible. Unlike entropy coding, REC does not assume discrete distributions or require quantisation. As such, it can be naturally integrated into communication pipelines such as learnt compression and differentially private federated learning. Unfortunately, despite their practical benefits, REC algorithms have not seen widespread application, due to their prohibitively slow runtimes or restrictive assumptions. In this paper, we make progress towards addressing these issues. We introduce Greedy Rejection Coding (GRC), which generalises the rejection based-algorithm of Harsha et al. (2007) to arbitrary probability spaces and partitioning schemes. We first show that GRC terminates almost surely and returns unbiased samples from $Q$, after which we focus on two of its variants: GRCS and GRCD. We show that for continuous $Q$ and $P$ over $\\mathbb{R}$ with unimodal density ratio $dQ/dP$, the expected runtime of GRCS is upper bounded by $βD_{KL}[Q || P] + O(1)$ where $β\\approx 4.82$, and its expected codelength is optimal. This makes GRCS the first REC algorithm with guaranteed optimal runtime for this class of distributions, up to the multiplicative constant $β$. This significantly improves upon the previous state-of-the-art method, A* coding (Flamich et al., 2022). Under the same assumptions, we experimentally observe and conjecture that the expected runtime and codelength of GRCD are upper bounded by $D_{KL}[Q || P] + O(1)$. Finally, we evaluate GRC in a variational autoencoder-based compression pipeline on MNIST, and show that a modified ELBO and an index-compression method can further improve compression efficiency.",
      "authors": [
        "Gergely Flamich",
        "Stratis Markou",
        "Jose Miguel Hernandez Lobato"
      ],
      "published": "2023-09-27T16:01:05Z",
      "updated": "2023-09-27T16:01:05Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2309.15746v1",
      "landing_url": "https://arxiv.org/abs/2309.15746v1",
      "doi": "https://doi.org/10.48550/arXiv.2309.15746"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on general relative entropy coding for arbitrary distributions and compression theory rather than discrete audio tokens or tokenizers, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on general relative entropy coding for arbitrary distributions and compression theory rather than discrete audio tokens or tokenizers, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers",
    "abstract": "Significance: Dynamic photoacoustic computed tomography (PACT) is a valuable technique for monitoring physiological processes. However, current dynamic PACT techniques are often limited to 2D spatial imaging. While volumetric PACT imagers are commercially available, these systems typically employ a rotating gantry in which the tomographic data are sequentially acquired. Because the object varies during the data-acquisition process, the sequential data-acquisition poses challenges to image reconstruction associated with data incompleteness. The proposed method is highly significant in that it will address these challenges and enable volumetric dynamic PACT imaging with existing imagers. Aim: The aim of this study is to develop a spatiotemporal image reconstruction (STIR) method for dynamic PACT that can be applied to commercially available volumetric PACT imagers that employ a sequential scanning strategy. The proposed method aims to overcome the challenges caused by the limited number of tomographic measurements acquired per frame. Approach: A low-rank matrix estimation-based STIR method (LRME-STIR) is proposed to enable dynamic volumetric PACT. The LRME-STIR method leverages the spatiotemporal redundancies to accurately reconstruct a 4D spatiotemporal image. Results: The numerical studies substantiate the LRME-STIR method's efficacy in reconstructing 4D dynamic images from measurements acquired with a rotating gantry. The experimental study demonstrates the method's ability to faithfully recover the flow of a contrast agent at a frame rate of 0.1 s even when only a single tomographic measurement per frame is available. Conclusions: The LRME-STIR method offers a promising solution to the challenges faced by enabling 4D dynamic imaging using commercially available volumetric imagers. By enabling accurate 4D reconstruction, this method has the potential to advance preclinical research.",
    "metadata": {
      "arxiv_id": "2310.00529",
      "title": "Spatiotemporal Image Reconstruction to Enable High-Frame Rate Dynamic Photoacoustic Tomography with Rotating-Gantry Volumetric Imagers",
      "summary": "Significance: Dynamic photoacoustic computed tomography (PACT) is a valuable technique for monitoring physiological processes. However, current dynamic PACT techniques are often limited to 2D spatial imaging. While volumetric PACT imagers are commercially available, these systems typically employ a rotating gantry in which the tomographic data are sequentially acquired. Because the object varies during the data-acquisition process, the sequential data-acquisition poses challenges to image reconstruction associated with data incompleteness. The proposed method is highly significant in that it will address these challenges and enable volumetric dynamic PACT imaging with existing imagers. Aim: The aim of this study is to develop a spatiotemporal image reconstruction (STIR) method for dynamic PACT that can be applied to commercially available volumetric PACT imagers that employ a sequential scanning strategy. The proposed method aims to overcome the challenges caused by the limited number of tomographic measurements acquired per frame. Approach: A low-rank matrix estimation-based STIR method (LRME-STIR) is proposed to enable dynamic volumetric PACT. The LRME-STIR method leverages the spatiotemporal redundancies to accurately reconstruct a 4D spatiotemporal image. Results: The numerical studies substantiate the LRME-STIR method's efficacy in reconstructing 4D dynamic images from measurements acquired with a rotating gantry. The experimental study demonstrates the method's ability to faithfully recover the flow of a contrast agent at a frame rate of 0.1 s even when only a single tomographic measurement per frame is available. Conclusions: The LRME-STIR method offers a promising solution to the challenges faced by enabling 4D dynamic imaging using commercially available volumetric imagers. By enabling accurate 4D reconstruction, this method has the potential to advance preclinical research.",
      "authors": [
        "Refik M. Cam",
        "Chao Wang",
        "Weylan Thompson",
        "Sergey A. Ermilov",
        "Mark A. Anastasio",
        "Umberto Villa"
      ],
      "published": "2023-10-01T00:18:17Z",
      "updated": "2023-10-01T00:18:17Z",
      "categories": [
        "eess.SP",
        "eess.IV",
        "physics.med-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.00529v1",
      "landing_url": "https://arxiv.org/abs/2310.00529v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.00529"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This study focuses on photoacoustic tomography reconstruction, not on discrete audio-token generation/quantization, so it fails all inclusion criteria and meets exclusion for being unrelated to audio tokens."
    },
    "round-A_JuniorNano_reasoning": "This study focuses on photoacoustic tomography reconstruction, not on discrete audio-token generation/quantization, so it fails all inclusion criteria and meets exclusion for being unrelated to audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
    "abstract": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
    "metadata": {
      "arxiv_id": "2310.01381",
      "title": "DiffAR: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation",
      "summary": "Diffusion models have recently been shown to be relevant for high-quality speech generation. Most work has been focused on generating spectrograms, and as such, they further require a subsequent model to convert the spectrogram to a waveform (i.e., a vocoder). This work proposes a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. We implemented the proposed model for unconditional and conditional speech generation, where the latter can be driven by an input sequence of phonemes, amplitudes, and pitch values. Working on the waveform directly has some empirical advantages. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.",
      "authors": [
        "Roi Benita",
        "Michael Elad",
        "Joseph Keshet"
      ],
      "published": "2023-10-02T17:42:22Z",
      "updated": "2024-03-10T22:31:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.01381v3",
      "landing_url": "https://arxiv.org/abs/2310.01381v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.01381"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper proposes an autoregressive diffusion model for raw waveform generation without discussing any discrete tokenization/quantization schema, so it fails to meet the discrete-audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes an autoregressive diffusion model for raw waveform generation without discussing any discrete tokenization/quantization schema, so it fails to meet the discrete-audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers",
    "abstract": "In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models.",
    "metadata": {
      "arxiv_id": "2310.03686",
      "title": "DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers",
      "summary": "In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models.",
      "authors": [
        "Anna Langedijk",
        "Hosein Mohebbi",
        "Gabriele Sarti",
        "Willem Zuidema",
        "Jaap Jumelet"
      ],
      "published": "2023-10-05T17:04:59Z",
      "updated": "2024-04-03T12:09:26Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.03686v2",
      "landing_url": "https://arxiv.org/abs/2310.03686v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.03686"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "DecoderLens focuses on interpreting encoder-decoder transformers for QA, reasoning, speech recognition, and translation using hidden states, without addressing discrete audio tokenization, codec quantization, or token-level design/evaluation, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "DecoderLens focuses on interpreting encoder-decoder transformers for QA, reasoning, speech recognition, and translation using hidden states, without addressing discrete audio tokenization, codec quantization, or token-level design/evaluation, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information",
    "abstract": "We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel conditions in terms of the various quality measures considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We have made the source code of the proposed method public to enable further research, and the reproducibility of the results.",
    "metadata": {
      "arxiv_id": "2310.04311",
      "title": "Distributed Deep Joint Source-Channel Coding with Decoder-Only Side Information",
      "summary": "We consider low-latency image transmission over a noisy wireless channel when correlated side information is present only at the receiver side (the Wyner-Ziv scenario). In particular, we are interested in developing practical schemes using a data-driven joint source-channel coding (JSCC) approach, which has been previously shown to outperform conventional separation-based approaches in the practical finite blocklength regimes, and to provide graceful degradation with channel quality. We propose a novel neural network architecture that incorporates the decoder-only side information at multiple stages at the receiver side. Our results demonstrate that the proposed method succeeds in integrating the side information, yielding improved performance at all channel conditions in terms of the various quality measures considered here, especially at low channel signal-to-noise ratios (SNRs) and small bandwidth ratios (BRs). We have made the source code of the proposed method public to enable further research, and the reproducibility of the results.",
      "authors": [
        "Selim F. Yilmaz",
        "Ezgi Ozyilkan",
        "Deniz Gunduz",
        "Elza Erkip"
      ],
      "published": "2023-10-06T15:17:45Z",
      "updated": "2024-02-27T18:10:02Z",
      "categories": [
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04311v2",
      "landing_url": "https://arxiv.org/abs/2310.04311v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.04311"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on image transmission JSCC without any discrete audio tokenization, so it fails the core inclusion of audio-token research and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on image transmission JSCC without any discrete audio tokenization, so it fails the core inclusion of audio-token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
    "abstract": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
    "metadata": {
      "arxiv_id": "2310.04358",
      "title": "Transferring speech-generic and depression-specific knowledge for Alzheimer's disease detection",
      "summary": "The detection of Alzheimer's disease (AD) from spontaneous speech has attracted increasing attention while the sparsity of training data remains an important issue. This paper handles the issue by knowledge transfer, specifically from both speech-generic and depression-specific knowledge. The paper first studies sequential knowledge transfer from generic foundation models pretrained on large amounts of speech and text data. A block-wise analysis is performed for AD diagnosis based on the representations extracted from different intermediate blocks of different foundation models. Apart from the knowledge from speech-generic representations, this paper also proposes to simultaneously transfer the knowledge from a speech depression detection task based on the high comorbidity rates of depression and AD. A parallel knowledge transfer framework is studied that jointly learns the information shared between these two tasks. Experimental results show that the proposed method improves AD and depression detection, and produces a state-of-the-art F1 score of 0.928 for AD diagnosis on the commonly used ADReSSo dataset.",
      "authors": [
        "Ziyun Cui",
        "Wen Wu",
        "Wei-Qiang Zhang",
        "Ji Wu",
        "Chao Zhang"
      ],
      "published": "2023-10-06T16:28:07Z",
      "updated": "2023-10-06T16:28:07Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.04358v1",
      "landing_url": "https://arxiv.org/abs/2310.04358v1",
      "doi": "https://doi.org/10.1109/ASRU57964.2023.10389785"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on transferring speech- and depression-related knowledge for Alzheimer’s detection using pretrained models without discussing discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on transferring speech- and depression-related knowledge for Alzheimer’s detection using pretrained models without discussing discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
    "abstract": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
    "metadata": {
      "arxiv_id": "2310.05224",
      "title": "Generative Spoken Language Model based on continuous word-sized audio tokens",
      "summary": "In NLP, text language models based on words or subwords are known to outperform their character-based counterparts. Yet, in the speech community, the standard input of spoken LMs are 20ms or 40ms-long discrete units (shorter than a phoneme). Taking inspiration from word-based LM, we introduce a Generative Spoken Language Model (GSLM) based on word-size continuous-valued audio embeddings that can generate diverse and expressive language output. This is obtained by replacing lookup table for lexical types with a Lexical Embedding function, the cross entropy loss by a contrastive loss, and multinomial sampling by k-NN sampling. The resulting model is the first generative language model based on word-size continuous embeddings. Its performance is on par with discrete unit GSLMs regarding generation quality as measured by automatic metrics and subjective human judgements. Moreover, it is five times more memory efficient thanks to its large 200ms units. In addition, the embeddings before and after the Lexical Embedder are phonetically and semantically interpretable.",
      "authors": [
        "Robin Algayres",
        "Yossi Adi",
        "Tu Anh Nguyen",
        "Jade Copet",
        "Gabriel Synnaeve",
        "Benoit Sagot",
        "Emmanuel Dupoux"
      ],
      "published": "2023-10-08T16:46:14Z",
      "updated": "2023-10-08T16:46:14Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.05224v1",
      "landing_url": "https://arxiv.org/abs/2310.05224v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.05224"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper centers on continuous word-size audio embeddings rather than deriving or evaluating discrete audio tokens with a finite vocabulary, so it fails to meet the discrete-token inclusion requirements and triggers the exclusion of continuous representations."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on continuous word-size audio embeddings rather than deriving or evaluating discrete audio tokens with a finite vocabulary, so it fails to meet the discrete-token inclusion requirements and triggers the exclusion of continuous representations.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions",
    "abstract": "We present the JVNV, a Japanese emotional speech corpus with verbal content and nonverbal vocalizations whose scripts are generated by a large-scale language model. Existing emotional speech corpora lack not only proper emotional scripts but also nonverbal vocalizations (NVs) that are essential expressions in spoken language to express emotions. We propose an automatic script generation method to produce emotional scripts by providing seed words with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using prompt engineering. We select 514 scripts with balanced phoneme coverage from the generated candidate scripts with the assistance of emotion confidence scores and language fluency scores. We demonstrate the effectiveness of JVNV by showing that JVNV has better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora. We then benchmark JVNV on emotional text-to-speech synthesis using discrete codes to represent NVs. We show that there still exists a gap between the performance of synthesizing read-aloud speech and emotional speech, and adding NVs in the speech makes the task even harder, which brings new challenges for this task and makes JVNV a valuable resource for relevant works in the future. To our best knowledge, JVNV is the first speech corpus that generates scripts automatically using large language models.",
    "metadata": {
      "arxiv_id": "2310.06072",
      "title": "JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions",
      "summary": "We present the JVNV, a Japanese emotional speech corpus with verbal content and nonverbal vocalizations whose scripts are generated by a large-scale language model. Existing emotional speech corpora lack not only proper emotional scripts but also nonverbal vocalizations (NVs) that are essential expressions in spoken language to express emotions. We propose an automatic script generation method to produce emotional scripts by providing seed words with sentiment polarity and phrases of nonverbal vocalizations to ChatGPT using prompt engineering. We select 514 scripts with balanced phoneme coverage from the generated candidate scripts with the assistance of emotion confidence scores and language fluency scores. We demonstrate the effectiveness of JVNV by showing that JVNV has better phoneme coverage and emotion recognizability than previous Japanese emotional speech corpora. We then benchmark JVNV on emotional text-to-speech synthesis using discrete codes to represent NVs. We show that there still exists a gap between the performance of synthesizing read-aloud speech and emotional speech, and adding NVs in the speech makes the task even harder, which brings new challenges for this task and makes JVNV a valuable resource for relevant works in the future. To our best knowledge, JVNV is the first speech corpus that generates scripts automatically using large language models.",
      "authors": [
        "Detai Xin",
        "Junfeng Jiang",
        "Shinnosuke Takamichi",
        "Yuki Saito",
        "Akiko Aizawa",
        "Hiroshi Saruwatari"
      ],
      "published": "2023-10-09T18:27:13Z",
      "updated": "2023-10-09T18:27:13Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.06072v1",
      "landing_url": "https://arxiv.org/abs/2310.06072v1",
      "doi": "https://doi.org/10.1109/ACCESS.2024.3360885"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Corpus focuses on emotional speech scripts and NVs generated by LLMs but does not address discrete audio tokenization/codec/quantization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Corpus focuses on emotional speech scripts and NVs generated by LLMs but does not address discrete audio tokenization/codec/quantization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
    "abstract": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
    "metadata": {
      "arxiv_id": "2310.07246",
      "title": "Vec-Tok Speech: speech vectorization and tokenization for neural speech generation",
      "summary": "Language models (LMs) have recently flourished in natural language processing and computer vision, generating high-fidelity texts or images in various tasks. In contrast, the current speech generative models are still struggling regarding speech quality and task generalization. This paper presents Vec-Tok Speech, an extensible framework that resembles multiple speech generation tasks, generating expressive and high-fidelity speech. Specifically, we propose a novel speech codec based on speech vectors and semantic tokens. Speech vectors contain acoustic details contributing to high-fidelity speech reconstruction, while semantic tokens focus on the linguistic content of speech, facilitating language modeling. Based on the proposed speech codec, Vec-Tok Speech leverages an LM to undertake the core of speech generation. Moreover, Byte-Pair Encoding (BPE) is introduced to reduce the token length and bit rate for lower exposure bias and longer context coverage, improving the performance of LMs. Vec-Tok Speech can be used for intra- and cross-lingual zero-shot voice conversion (VC), zero-shot speaking style transfer text-to-speech (TTS), speech-to-speech translation (S2ST), speech denoising, and speaker de-identification and anonymization. Experiments show that Vec-Tok Speech, built on 50k hours of speech, performs better than other SOTA models. Code will be available at https://github.com/BakerBunker/VecTok .",
      "authors": [
        "Xinfa Zhu",
        "Yuanjun Lv",
        "Yi Lei",
        "Tao Li",
        "Wendi He",
        "Hongbin Zhou",
        "Heng Lu",
        "Lei Xie"
      ],
      "published": "2023-10-11T07:23:27Z",
      "updated": "2023-10-12T05:49:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.07246v2",
      "landing_url": "https://arxiv.org/abs/2310.07246v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.07246"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Vec-Tok Speech clearly defines and evaluates discrete speech vectors and semantic tokens produced by a codec, uses BPE for tokenization, and demonstrates LM-based modeling for generation across multiple tasks, so it meets all inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "Vec-Tok Speech clearly defines and evaluates discrete speech vectors and semantic tokens produced by a codec, uses BPE for tokenization, and demonstrates LM-based modeling for generation across multiple tasks, so it meets all inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models",
    "abstract": "A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute-binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities. To train CompA-CLAP, we first propose improvements to contrastive training with composition-aware hard negatives, allowing for more focused training. Next, we propose a novel modular contrastive loss that helps the model learn fine-grained compositional understanding and overcomes the acute scarcity of openly available compositional audios. CompA-CLAP significantly improves over all our baseline models on the CompA benchmark, indicating its superior compositional reasoning capabilities.",
    "metadata": {
      "arxiv_id": "2310.08753",
      "title": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language Models",
      "summary": "A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute-binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities. To train CompA-CLAP, we first propose improvements to contrastive training with composition-aware hard negatives, allowing for more focused training. Next, we propose a novel modular contrastive loss that helps the model learn fine-grained compositional understanding and overcomes the acute scarcity of openly available compositional audios. CompA-CLAP significantly improves over all our baseline models on the CompA benchmark, indicating its superior compositional reasoning capabilities.",
      "authors": [
        "Sreyan Ghosh",
        "Ashish Seth",
        "Sonal Kumar",
        "Utkarsh Tyagi",
        "Chandra Kiran Evuru",
        "S. Ramaneswaran",
        "S. Sakshi",
        "Oriol Nieto",
        "Ramani Duraiswami",
        "Dinesh Manocha"
      ],
      "published": "2023-10-12T22:43:38Z",
      "updated": "2024-07-30T18:58:01Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.08753v4",
      "landing_url": "https://arxiv.org/abs/2310.08753v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.08753"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on compositional reasoning benchmarks and contrastive fine-tuning for audio-language models without proposing or using any discrete audio tokenization/quantization scheme, so it fails to satisfy the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on compositional reasoning benchmarks and contrastive fine-tuning for audio-language models without proposing or using any discrete audio tokenization/quantization scheme, so it fails to satisfy the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
    "abstract": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
    "metadata": {
      "arxiv_id": "2310.09382",
      "title": "LL-VQ-VAE: Learnable Lattice Vector-Quantization For Efficient Representations",
      "summary": "In this paper we introduce learnable lattice vector quantization and demonstrate its effectiveness for learning discrete representations. Our method, termed LL-VQ-VAE, replaces the vector quantization layer in VQ-VAE with lattice-based discretization. The learnable lattice imposes a structure over all discrete embeddings, acting as a deterrent against codebook collapse, leading to high codebook utilization. Compared to VQ-VAE, our method obtains lower reconstruction errors under the same training conditions, trains in a fraction of the time, and with a constant number of parameters (equal to the embedding dimension $D$), making it a very scalable approach. We demonstrate these results on the FFHQ-1024 dataset and include FashionMNIST and Celeb-A.",
      "authors": [
        "Ahmed Khalil",
        "Robert Piechocki",
        "Raul Santos-Rodriguez"
      ],
      "published": "2023-10-13T20:03:18Z",
      "updated": "2023-10-13T20:03:18Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09382v1",
      "landing_url": "https://arxiv.org/abs/2310.09382v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09382"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work focuses on lattice VQ for image datasets (FFHQ, FashionMNIST, Celeb-A) rather than learning discrete audio tokens or codecs, so it fails to meet the inclusion criterion of audio-token-centric research and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "This work focuses on lattice VQ for image datasets (FFHQ, FashionMNIST, Celeb-A) rather than learning discrete audio tokens or codecs, so it fails to meet the inclusion criterion of audio-token-centric research and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
    "abstract": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
    "metadata": {
      "arxiv_id": "2310.09570",
      "title": "Energy-Efficient Multi-Codec Bitrate-Ladder Estimation for Adaptive Video Streaming",
      "summary": "With the emergence of multiple modern video codecs, streaming service providers are forced to encode, store, and transmit bitrate ladders of multiple codecs separately, consequently suffering from additional energy costs for encoding, storage, and transmission. To tackle this issue, we introduce an online energy-efficient Multi-Codec Bitrate ladder Estimation scheme (MCBE) for adaptive video streaming applications. In MCBE, quality representations within the bitrate ladder of new-generation codecs (e.g., High Efficiency Video Coding (HEVC), Alliance for Open Media Video 1 (AV1)) that lie below the predicted rate-distortion curve of the Advanced Video Coding (AVC) codec are removed. Moreover, perceptual redundancy between representations of the bitrate ladders of the considered codecs is also minimized based on a Just Noticeable Difference (JND) threshold. Therefore, random forest-based models predict the VMAF score of bitrate ladder representations of each codec. In a live streaming session where all clients support the decoding of AVC, HEVC, and AV1, MCBE achieves impressive results, reducing cumulative encoding energy by 56.45%, storage energy usage by 94.99%, and transmission energy usage by 77.61% (considering a JND of six VMAF points). These energy reductions are in comparison to a baseline bitrate ladder encoding based on current industry practice.",
      "authors": [
        "Vignesh V Menon",
        "Reza Farahani",
        "Prajit T Rajendran",
        "Samira Afzal",
        "Klaus Schoeffmann",
        "Christian Timmerer"
      ],
      "published": "2023-10-14T12:05:18Z",
      "updated": "2023-10-14T12:05:18Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.09570v1",
      "landing_url": "https://arxiv.org/abs/2310.09570v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.09570"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper addresses bitrate ladder estimation for video streaming and not the creation/analysis of discrete audio tokens, so it fails all inclusion criteria and triggers exclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper addresses bitrate ladder estimation for video streaming and not the creation/analysis of discrete audio tokens, so it fails all inclusion criteria and triggers exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
    "abstract": "Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.",
    "metadata": {
      "arxiv_id": "2310.10054",
      "title": "NASH: A Simple Unified Framework of Structured Pruning for Accelerating Encoder-Decoder Language Models",
      "summary": "Structured pruning methods have proven effective in reducing the model size and accelerating inference speed in various network architectures such as Transformers. Despite the versatility of encoder-decoder models in numerous NLP tasks, the structured pruning methods on such models are relatively less explored compared to encoder-only models. In this study, we investigate the behavior of the structured pruning of the encoder-decoder models in the decoupled pruning perspective of the encoder and decoder component, respectively. Our findings highlight two insights: (1) the number of decoder layers is the dominant factor of inference speed, and (2) low sparsity in the pruned encoder network enhances generation quality. Motivated by these findings, we propose a simple and effective framework, NASH, that narrows the encoder and shortens the decoder networks of encoder-decoder models. Extensive experiments on diverse generation and inference tasks validate the effectiveness of our method in both speedup and output quality.",
      "authors": [
        "Jongwoo Ko",
        "Seungjoon Park",
        "Yujin Kim",
        "Sumyeong Ahn",
        "Du-Seong Chang",
        "Euijai Ahn",
        "Se-Young Yun"
      ],
      "published": "2023-10-16T04:27:36Z",
      "updated": "2023-10-16T04:27:36Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10054v1",
      "landing_url": "https://arxiv.org/abs/2310.10054v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10054"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper studies structured pruning for encoder-decoder language models without any mention of discrete audio token generation/quantization, so it fails to meet the topic’s inclusion criteria and matches exclusion (non-audio discrete sequence)."
    },
    "round-A_JuniorNano_reasoning": "This paper studies structured pruning for encoder-decoder language models without any mention of discrete audio token generation/quantization, so it fails to meet the topic’s inclusion criteria and matches exclusion (non-audio discrete sequence).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards image compression with perfect realism at ultra-low bitrates",
    "abstract": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
    "metadata": {
      "arxiv_id": "2310.10325",
      "title": "Towards image compression with perfect realism at ultra-low bitrates",
      "summary": "Image codecs are typically optimized to trade-off bitrate \\vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods.",
      "authors": [
        "Marlène Careil",
        "Matthew J. Muckley",
        "Jakob Verbeek",
        "Stéphane Lathuilière"
      ],
      "published": "2023-10-16T12:08:35Z",
      "updated": "2024-03-19T09:54:41Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10325v2",
      "landing_url": "https://arxiv.org/abs/2310.10325v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.10325"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe image compression with diffusion-based decoding rather than research on discrete audio tokens or tokenizers, so it fails the audio-focused inclusion criteria and matches exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe image compression with diffusion-based decoding rather than research on discrete audio tokens or tokenizers, so it fails the audio-focused inclusion criteria and matches exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
    "abstract": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
    "metadata": {
      "arxiv_id": "2310.10803",
      "title": "SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic Organization in HuBERT",
      "summary": "Data-driven unit discovery in self-supervised learning (SSL) of speech has embarked on a new era of spoken language processing. Yet, the discovered units often remain in phonetic space and the units beyond phonemes are largely underexplored. Here, we demonstrate that a syllabic organization emerges in learning sentence-level representation of speech. In particular, we adopt \"self-distillation\" objective to fine-tune the pretrained HuBERT with an aggregator token that summarizes the entire sentence. Without any supervision, the resulting model draws definite boundaries in speech, and the representations across frames exhibit salient syllabic structures. We demonstrate that this emergent structure largely corresponds to the ground truth syllables. Furthermore, we propose a new benchmark task, Spoken Speech ABX, for evaluating sentence-level representation of speech. When compared to previous models, our model outperforms in both unsupervised syllable discovery and learning sentence-level representation. Together, we demonstrate that the self-distillation of HuBERT gives rise to syllabic organization without relying on external labels or modalities, and potentially provides novel data-driven units for spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Abdelrahman Mohamed",
        "Shang-Wen Li",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2023-10-16T20:05:36Z",
      "updated": "2025-04-10T11:20:55Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10803v3",
      "landing_url": "https://arxiv.org/abs/2310.10803v3",
      "doi": "https://doi.org/10.48550/arXiv.2310.10803"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on emergent syllabic structure in HuBERT sentence representations via self-distillation but never describes encoding continuous audio into a finite discrete vocabulary/token sequence, so it fails the discrete-token inclusion requirements despite relevant SSL context."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on emergent syllabic structure in HuBERT sentence representations via self-distillation but never describes encoding continuous audio into a finite discrete vocabulary/token sequence, so it fails the discrete-token inclusion requirements despite relevant SSL context.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "A High Fidelity and Low Complexity Neural Audio Coding",
    "abstract": "Audio coding is an essential module in the real-time communication system. Neural audio codecs can compress audio samples with a low bitrate due to the strong modeling and generative capabilities of deep neural networks. To address the poor high-frequency expression and high computational cost and storage consumption, we proposed an integrated framework that utilizes a neural network to model wide-band components and adopts traditional signal processing to compress high-band components according to psychological hearing knowledge. Inspired by auditory perception theory, a perception-based loss function is designed to improve harmonic modeling. Besides, generative adversarial network (GAN) compression is proposed for the first time for neural audio codecs. Our method is superior to prior advanced neural codecs across subjective and objective metrics and allows real-time inference on desktop and mobile.",
    "metadata": {
      "arxiv_id": "2310.10992",
      "title": "A High Fidelity and Low Complexity Neural Audio Coding",
      "summary": "Audio coding is an essential module in the real-time communication system. Neural audio codecs can compress audio samples with a low bitrate due to the strong modeling and generative capabilities of deep neural networks. To address the poor high-frequency expression and high computational cost and storage consumption, we proposed an integrated framework that utilizes a neural network to model wide-band components and adopts traditional signal processing to compress high-band components according to psychological hearing knowledge. Inspired by auditory perception theory, a perception-based loss function is designed to improve harmonic modeling. Besides, generative adversarial network (GAN) compression is proposed for the first time for neural audio codecs. Our method is superior to prior advanced neural codecs across subjective and objective metrics and allows real-time inference on desktop and mobile.",
      "authors": [
        "Wenzhe Liu",
        "Wei Xiao",
        "Meng Wang",
        "Shan Yang",
        "Yupeng Shi",
        "Yuyong Kang",
        "Dan Su",
        "Shidong Shang",
        "Dong Yu"
      ],
      "published": "2023-10-17T04:30:37Z",
      "updated": "2023-10-17T04:30:37Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.10992v1",
      "landing_url": "https://arxiv.org/abs/2310.10992v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.10992"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Abstract describes a neural codec that mixes neural and signal-processing compression but never defines discrete tokens, codebooks, or quantization details, so it fails the discrete-token focus and is better excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a neural codec that mixes neural and signal-processing compression but never defines discrete tokens, codebooks, or quantization details, so it fails the discrete-token focus and is better excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
    "abstract": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.",
    "metadata": {
      "arxiv_id": "2310.12109",
      "title": "Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture",
      "summary": "Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.",
      "authors": [
        "Daniel Y. Fu",
        "Simran Arora",
        "Jessica Grogan",
        "Isys Johnson",
        "Sabri Eyuboglu",
        "Armin W. Thomas",
        "Benjamin Spector",
        "Michael Poli",
        "Atri Rudra",
        "Christopher Ré"
      ],
      "published": "2023-10-18T17:06:22Z",
      "updated": "2023-10-18T17:06:22Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.12109v1",
      "landing_url": "https://arxiv.org/abs/2310.12109v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.12109"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper introduces a GEMM-based transformer alternative for general language and vision modeling without any focus on discrete audio tokenization methods or evaluations, so it fails to meet inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a GEMM-based transformer alternative for general language and vision modeling without any focus on discrete audio tokenization methods or evaluations, so it fails to meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sequence Length Independent Norm-Based Generalization Bounds for Transformers",
    "abstract": "This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.",
    "metadata": {
      "arxiv_id": "2310.13088",
      "title": "Sequence Length Independent Norm-Based Generalization Bounds for Transformers",
      "summary": "This paper provides norm-based generalization bounds for the Transformer architecture that do not depend on the input sequence length. We employ a covering number based approach to prove our bounds. We use three novel covering number bounds for the function class of bounded linear transformations to upper bound the Rademacher complexity of the Transformer. Furthermore, we show this generalization bound applies to the common Transformer training technique of masking and then predicting the masked word. We also run a simulated study on a sparse majority data set that empirically validates our theoretical findings.",
      "authors": [
        "Jacob Trauger",
        "Ambuj Tewari"
      ],
      "published": "2023-10-19T18:31:09Z",
      "updated": "2023-10-19T18:31:09Z",
      "categories": [
        "stat.ML",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.13088v1",
      "landing_url": "https://arxiv.org/abs/2310.13088v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.13088"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on theoretical generalization bounds for Transformers and has no connection to discrete audio tokens, so it fails to meet the inclusion criteria and matches the exclusion of non-audio-token work."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on theoretical generalization bounds for Transformers and has no connection to discrete audio tokens, so it fails to meet the inclusion criteria and matches the exclusion of non-audio-token work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
    "abstract": "Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.",
    "metadata": {
      "arxiv_id": "2310.14170",
      "title": "Learning Invariant Molecular Representation in Latent Discrete Space",
      "summary": "Molecular representation learning lays the foundation for drug discovery. However, existing methods suffer from poor out-of-distribution (OOD) generalization, particularly when data for training and testing originate from different environments. To address this issue, we propose a new framework for learning molecular representations that exhibit invariance and robustness against distribution shifts. Specifically, we propose a strategy called ``first-encoding-then-separation'' to identify invariant molecule features in the latent space, which deviates from conventional practices. Prior to the separation step, we introduce a residual vector quantization module that mitigates the over-fitting to training data distributions while preserving the expressivity of encoders. Furthermore, we design a task-agnostic self-supervised learning objective to encourage precise invariance identification, which enables our method widely applicable to a variety of tasks, such as regression and multi-label classification. Extensive experiments on 18 real-world molecular datasets demonstrate that our model achieves stronger generalization against state-of-the-art baselines in the presence of various distribution shifts. Our code is available at https://github.com/HICAI-ZJU/iMoLD.",
      "authors": [
        "Xiang Zhuang",
        "Qiang Zhang",
        "Keyan Ding",
        "Yatao Bian",
        "Xiao Wang",
        "Jingsong Lv",
        "Hongyang Chen",
        "Huajun Chen"
      ],
      "published": "2023-10-22T04:06:44Z",
      "updated": "2023-10-22T04:06:44Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14170v1",
      "landing_url": "https://arxiv.org/abs/2310.14170v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.14170"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on molecular representation learning with invariant features, which has no relation to discrete audio token generation/quantization or any audio signal research, so it clearly violates the inclusion criteria and matches exclusion criteria for non-audio tokens."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on molecular representation learning with invariant features, which has no relation to discrete audio token generation/quantization or any audio signal research, so it clearly violates the inclusion criteria and matches exclusion criteria for non-audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
    "abstract": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
    "metadata": {
      "arxiv_id": "2310.14580",
      "title": "Acoustic BPE for Speech Generation with Discrete Tokens",
      "summary": "Discrete audio tokens derived from self-supervised learning models have gained widespread usage in speech generation. However, current practice of directly utilizing audio tokens poses challenges for sequence modeling due to the length of the token sequence. Additionally, this approach places the burden on the model to establish correlations between tokens, further complicating the modeling process. To address this issue, we propose acoustic BPE which encodes frequent audio token patterns by utilizing byte-pair encoding. Acoustic BPE effectively reduces the sequence length and leverages the prior morphological information present in token sequence, which alleviates the modeling challenges of token correlation. Through comprehensive investigations on a speech language model trained with acoustic BPE, we confirm the notable advantages it offers, including faster inference and improved syntax capturing capabilities. In addition, we propose a novel rescore method to select the optimal synthetic speech among multiple candidates generated by rich-diversity TTS system. Experiments prove that rescore selection aligns closely with human preference, which highlights acoustic BPE's potential to other speech generation tasks.",
      "authors": [
        "Feiyu Shen",
        "Yiwei Guo",
        "Chenpeng Du",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-10-23T05:38:41Z",
      "updated": "2024-01-15T05:53:31Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14580v4",
      "landing_url": "https://arxiv.org/abs/2310.14580v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.14580"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper proposes acoustic BPE to compress discrete audio token sequences produced by self-supervised models, with clear focus on tokenization and modeling improvements plus evaluation, so it satisfies the inclusion criteria and hits none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "Paper proposes acoustic BPE to compress discrete audio token sequences produced by self-supervised models, with clear focus on tokenization and modeling improvements plus evaluation, so it satisfies the inclusion criteria and hits none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
    "abstract": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
    "metadata": {
      "arxiv_id": "2310.14837",
      "title": "Harnessing Attention Mechanisms: Efficient Sequence Reduction using Attention-based Autoencoders",
      "summary": "Many machine learning models use the manipulation of dimensions as a driving force to enable models to identify and learn important features in data. In the case of sequential data this manipulation usually happens on the token dimension level. Despite the fact that many tasks require a change in sequence length itself, the step of sequence length reduction usually happens out of necessity and in a single step. As far as we are aware, no model uses the sequence length reduction step as an additional opportunity to tune the models performance. In fact, sequence length manipulation as a whole seems to be an overlooked direction. In this study we introduce a novel attention-based method that allows for the direct manipulation of sequence lengths. To explore the method's capabilities, we employ it in an autoencoder model. The autoencoder reduces the input sequence to a smaller sequence in latent space. It then aims to reproduce the original sequence from this reduced form. In this setting, we explore the methods reduction performance for different input and latent sequence lengths. We are able to show that the autoencoder retains all the significant information when reducing the original sequence to half its original size. When reducing down to as low as a quarter of its original size, the autoencoder is still able to reproduce the original sequence with an accuracy of around 90%.",
      "authors": [
        "Daniel Biermann",
        "Fabrizio Palumbo",
        "Morten Goodwin",
        "Ole-Christoffer Granmo"
      ],
      "published": "2023-10-23T11:57:44Z",
      "updated": "2023-10-23T11:57:44Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.14837v1",
      "landing_url": "https://arxiv.org/abs/2310.14837v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.14837"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on general attention-based sequence reduction in autoencoders without any mention of discrete audio tokenization or codec/quantization designs, so it fails the inclusion criteria and matches the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Focuses on general attention-based sequence reduction in autoencoders without any mention of discrete audio tokenization or codec/quantization designs, so it fails the inclusion criteria and matches the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
    "abstract": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
    "metadata": {
      "arxiv_id": "2310.15163",
      "title": "Convex Hull Prediction Methods for Bitrate Ladder Construction: Design, Evaluation, and Comparison",
      "summary": "HTTP adaptive streaming (HAS) has emerged as a prevalent approach for over-the-top (OTT) video streaming services due to its ability to deliver a seamless user experience. A fundamental component of HAS is the bitrate ladder, which comprises a set of encoding parameters (e.g., bitrate-resolution pairs) used to encode the source video into multiple representations. This adaptive bitrate ladder enables the client's video player to dynamically adjust the quality of the video stream in real-time based on fluctuations in network conditions, ensuring uninterrupted playback by selecting the most suitable representation for the available bandwidth. The most straightforward approach involves using a fixed bitrate ladder for all videos, consisting of pre-determined bitrate-resolution pairs known as one-size-fits-all. Conversely, the most reliable technique relies on intensively encoding all resolutions over a wide range of bitrates to build the convex hull, thereby optimizing the bitrate ladder by selecting the representations from the convex hull for each specific video. Several techniques have been proposed to predict content-based ladders without performing a costly, exhaustive search encoding. This paper provides a comprehensive review of various convex hull prediction methods, including both conventional and learning-based approaches. Furthermore, we conduct a benchmark study of several handcrafted- and DL-based approaches for predicting content-optimized convex hulls across multiple codec settings. The considered methods are evaluated on our proposed large-scale dataset, which includes 300 UHD video shots encoded with software and hardware encoders using three state-of-the-art video standards, including AVC/H.264, HEVC/H.265, and VVC/H.266, at various bitrate points. Our analysis provides valuable insights and establishes baseline performance for future research in this field.",
      "authors": [
        "Ahmed Telili",
        "Wassim Hamidouche",
        "Hadi Amirpour",
        "Sid Ahmed Fezza",
        "Luce Morin",
        "Christian Timmerer"
      ],
      "published": "2023-10-23T17:58:24Z",
      "updated": "2025-03-11T06:48:06Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15163v4",
      "landing_url": "https://arxiv.org/abs/2310.15163v4",
      "doi": "https://doi.org/10.48550/arXiv.2310.15163"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on convex-hull bitrate ladder prediction for video streaming rather than discrete audio token creation/quantization/tokenizer evaluation, so it fails to meet the inclusion criteria and violates the topic scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on convex-hull bitrate ladder prediction for video streaming rather than discrete audio token creation/quantization/tokenizer evaluation, so it fails to meet the inclusion criteria and violates the topic scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
    "abstract": "In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.",
    "metadata": {
      "arxiv_id": "2310.15296",
      "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM",
      "summary": "In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications.",
      "authors": [
        "Weijie Xu",
        "Wenxiang Hu",
        "Fanyou Wu",
        "Srinivasan Sengamedu"
      ],
      "published": "2023-10-23T19:03:04Z",
      "updated": "2023-12-23T07:05:20Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15296v2",
      "landing_url": "https://arxiv.org/abs/2310.15296v2",
      "doi": "https://doi.org/10.18653/v1/2023.findings-emnlp.606"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on topic modeling with diffusion-enhanced LLM embeddings, without any discrete audio codec/token definitions or quantization focus, so it falls outside the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on topic modeling with diffusion-enhanced LLM embeddings, without any discrete audio codec/token definitions or quantization focus, so it falls outside the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation",
    "abstract": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozière et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance training efficiency in terms of time, we adopt curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert takes only 6 hours to train on one single 80Gb A100 HBM. With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is release here: https://github.com/sade-adrien/SteloCoder.",
    "metadata": {
      "arxiv_id": "2310.15539",
      "title": "SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation",
      "summary": "With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozière et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance training efficiency in terms of time, we adopt curriculum learning strategy and use self-instruct data for efficient fine-tuning. As a result, each expert takes only 6 hours to train on one single 80Gb A100 HBM. With experiments on XLCoST datasets, SteloCoder achieves an average of 73.76 CodeBLEU score in multi-programming language-to-Python translation, surpassing the top performance from the leaderboard by at least 3.5. This accomplishment is attributed to only 45M extra parameters with StarCoder as the backbone and 32 hours of valid training on one 80GB A100 HBM. The source code is release here: https://github.com/sade-adrien/SteloCoder.",
      "authors": [
        "Jialing Pan",
        "Adrien Sadé",
        "Jin Kim",
        "Eric Soriano",
        "Guillem Sole",
        "Sylvain Flamant"
      ],
      "published": "2023-10-24T06:04:28Z",
      "updated": "2023-12-15T06:40:41Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.15539v2",
      "landing_url": "https://arxiv.org/abs/2310.15539v2",
      "doi": "https://doi.org/10.48550/arXiv.2310.15539"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe LLM code translation and MoE training, which has nothing to do with discrete audio tokens, tokenizers/codecs, or any audio-specific evaluation, so it fails inclusion and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe LLM code translation and MoE training, which has nothing to do with discrete audio tokens, tokenizers/codecs, or any audio-specific evaluation, so it fails inclusion and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning",
    "abstract": "The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distribution among source and unmeasured domains in training. Compared to existing ADA methods, ADA-CBF can correctly identify confounders in domain-invariant features, thereby eliminating the confounder biases in the extracted features from PLMs. The confounder classifier in ADA-CBF is designed as a plug-and-play and can be applied in the confounder measurable, unmeasurable, or partially measurable environments. Empirical results on natural language processing and computer vision downstream tasks show that ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.",
    "metadata": {
      "arxiv_id": "2310.16062",
      "title": "Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning",
      "summary": "The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes an adversarial domain adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The ADA-CBF includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distribution among source and unmeasured domains in training. Compared to existing ADA methods, ADA-CBF can correctly identify confounders in domain-invariant features, thereby eliminating the confounder biases in the extracted features from PLMs. The confounder classifier in ADA-CBF is designed as a plug-and-play and can be applied in the confounder measurable, unmeasurable, or partially measurable environments. Empirical results on natural language processing and computer vision downstream tasks show that ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.",
      "authors": [
        "Shuoran Jiang",
        "Qingcai Chen",
        "Yang Xiang",
        "Youcheng Pan",
        "Xiangping Wu"
      ],
      "published": "2023-10-24T09:11:45Z",
      "updated": "2023-10-24T09:11:45Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16062v1",
      "landing_url": "https://arxiv.org/abs/2310.16062v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16062"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on PLM-based adversarial domain adaptation for NLP/CV without discrete audio token generation or evaluation, so it should be excluded (Score 1)."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on PLM-based adversarial domain adaptation for NLP/CV without discrete audio token generation or evaluation, so it should be excluded (Score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ArTST: Arabic Text and Speech Transformer",
    "abstract": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
    "metadata": {
      "arxiv_id": "2310.16621",
      "title": "ArTST: Arabic Text and Speech Transformer",
      "summary": "We present ArTST, a pre-trained Arabic text and speech transformer for supporting open-source speech technologies for the Arabic language. The model architecture follows the unified-modal framework, SpeechT5, that was recently released for English, and is focused on Modern Standard Arabic (MSA), with plans to extend the model for dialectal and code-switched Arabic in future editions. We pre-trained the model from scratch on MSA speech and text data, and fine-tuned it for the following tasks: Automatic Speech Recognition (ASR), Text-To-Speech synthesis (TTS), and spoken dialect identification. In our experiments comparing ArTST with SpeechT5, as well as with previously reported results in these tasks, ArTST performs on a par with or exceeding the current state-of-the-art in all three tasks. Moreover, we find that our pre-training is conducive for generalization, which is particularly evident in the low-resource TTS task. The pre-trained model as well as the fine-tuned ASR and TTS models are released for research use.",
      "authors": [
        "Hawau Olamide Toyin",
        "Amirbek Djanibekov",
        "Ajinkya Kulkarni",
        "Hanan Aldarmaki"
      ],
      "published": "2023-10-25T13:20:54Z",
      "updated": "2023-10-25T13:20:54Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.16621v1",
      "landing_url": "https://arxiv.org/abs/2310.16621v1",
      "doi": "https://doi.org/10.48550/arXiv.2310.16621"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on a unified text-and-speech transformer for Arabic ASR/TTS/dialect ID without discussing discrete audio tokenization, quantization, or finite vocabulary representations, so it fails the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a unified text-and-speech transformer for Arabic ASR/TTS/dialect ID without discussing discrete audio tokenization, quantization, or finite vocabulary representations, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DecoderTracker: Decoder-Only Method for Multiple-Object Tracking",
    "abstract": "Decoder-only methods, such as GPT, have demonstrated superior performance in many areas compared to traditional encoder-decoder structure transformer methods. Over the years, end-to-end methods based on the traditional transformer structure, like MOTR, have achieved remarkable performance in multi-object tracking. However,the substantial computational resource consumption of these methods, coupled with the optimization challenges posed by dynamic data, results in less favorable inference speeds and training times. To address the aforementioned issues, this paper optimized the network architecture and proposed an effective training strategy to mitigate the problem of prolonged training times, thereby developing DecoderTracker, a novel end-to-end tracking method. Subsequently, to tackle the optimization challenges arising from dynamic data, this paper introduced DecoderTracker+ by incorporating a Fixed-Size Query Memory and refining certain attention layers. Our methods, without any bells and whistles, outperforms MOTR on multiple benchmarks, \\textcolor{black}{featuring a 2 to 3 times faster inference than MOTR}, respectively. The proposed method is implemented in open-source code, accessible at https://github.com/liaopan-lp/MO-YOLO.",
    "metadata": {
      "arxiv_id": "2310.17170",
      "title": "DecoderTracker: Decoder-Only Method for Multiple-Object Tracking",
      "summary": "Decoder-only methods, such as GPT, have demonstrated superior performance in many areas compared to traditional encoder-decoder structure transformer methods. Over the years, end-to-end methods based on the traditional transformer structure, like MOTR, have achieved remarkable performance in multi-object tracking. However,the substantial computational resource consumption of these methods, coupled with the optimization challenges posed by dynamic data, results in less favorable inference speeds and training times. To address the aforementioned issues, this paper optimized the network architecture and proposed an effective training strategy to mitigate the problem of prolonged training times, thereby developing DecoderTracker, a novel end-to-end tracking method. Subsequently, to tackle the optimization challenges arising from dynamic data, this paper introduced DecoderTracker+ by incorporating a Fixed-Size Query Memory and refining certain attention layers. Our methods, without any bells and whistles, outperforms MOTR on multiple benchmarks, \\textcolor{black}{featuring a 2 to 3 times faster inference than MOTR}, respectively. The proposed method is implemented in open-source code, accessible at https://github.com/liaopan-lp/MO-YOLO.",
      "authors": [
        "Liao Pan",
        "Yang Feng",
        "Zhao Wenhui",
        "Yua Jinwen",
        "Zhang Dingwen"
      ],
      "published": "2023-10-26T05:49:44Z",
      "updated": "2025-07-09T02:15:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2310.17170v6",
      "landing_url": "https://arxiv.org/abs/2310.17170v6",
      "doi": "https://doi.org/10.48550/arXiv.2310.17170"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on decoder-only multi-object tracking models and does not address discrete audio token generation, quantization, or codebook evaluation, so it fails to meet the inclusion criteria targeting audio-token research."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on decoder-only multi-object tracking models and does not address discrete audio token generation, quantization, or codebook evaluation, so it fails to meet the inclusion criteria targeting audio-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flooding Regularization for Stable Training of Generative Adversarial Networks",
    "abstract": "Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also show that by restricting the discriminator's loss to be no less than the flood level, the training proceeds stably even when the flood level is somewhat high.",
    "metadata": {
      "arxiv_id": "2311.00318",
      "title": "Flooding Regularization for Stable Training of Generative Adversarial Networks",
      "summary": "Generative Adversarial Networks (GANs) have shown remarkable performance in image generation. However, GAN training suffers from the problem of instability. One of the main approaches to address this problem is to modify the loss function, often using regularization terms in addition to changing the type of adversarial losses. This paper focuses on directly regularizing the adversarial loss function. We propose a method that applies flooding, an overfitting suppression method in supervised learning, to GANs to directly prevent the discriminator's loss from becoming excessively low. Flooding requires tuning the flood level, but when applied to GANs, we propose that the appropriate range of flood level settings is determined by the adversarial loss function, supported by theoretical analysis of GANs using the binary cross entropy loss. We experimentally verify that flooding stabilizes GAN training and can be combined with other stabilization techniques. We also show that by restricting the discriminator's loss to be no less than the flood level, the training proceeds stably even when the flood level is somewhat high.",
      "authors": [
        "Iu Yahiro",
        "Takashi Ishida",
        "Naoto Yokoya"
      ],
      "published": "2023-11-01T06:02:59Z",
      "updated": "2024-03-18T07:21:27Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00318v2",
      "landing_url": "https://arxiv.org/abs/2311.00318v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.00318"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on GAN loss regularization for image generation and does not address discrete audio tokens or their quantization/tokenizer design, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on GAN loss regularization for image generation and does not address discrete audio tokens or their quantization/tokenizer design, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
    "abstract": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
    "metadata": {
      "arxiv_id": "2311.00945",
      "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
      "summary": "We propose Easy End-to-End Diffusion-based Text to Speech, a simple and efficient end-to-end text-to-speech model based on diffusion. E3 TTS directly takes plain text as input and generates an audio waveform through an iterative refinement process. Unlike many prior work, E3 TTS does not rely on any intermediate representations like spectrogram features or alignment information. Instead, E3 TTS models the temporal structure of the waveform through the diffusion process. Without relying on additional conditioning information, E3 TTS could support flexible latent structure within the given audio. This enables E3 TTS to be easily adapted for zero-shot tasks such as editing without any additional training. Experiments show that E3 TTS can generate high-fidelity audio, approaching the performance of a state-of-the-art neural TTS system. Audio samples are available at https://e3tts.github.io.",
      "authors": [
        "Yuan Gao",
        "Nobuyuki Morioka",
        "Yu Zhang",
        "Nanxin Chen"
      ],
      "published": "2023-11-02T02:22:21Z",
      "updated": "2023-11-02T02:22:21Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.00945v1",
      "landing_url": "https://arxiv.org/abs/2311.00945v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.00945"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on end-to-end diffusion TTS producing continuous waveforms without defining or operating on discrete audio tokens/tokenizers, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on end-to-end diffusion TTS producing continuous waveforms without defining or operating on discrete audio tokens/tokenizers, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision",
    "abstract": "This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.",
    "metadata": {
      "arxiv_id": "2311.02333",
      "title": "Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision",
      "summary": "This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.",
      "authors": [
        "Aditya Malusare",
        "Harish Kothandaraman",
        "Dipesh Tamboli",
        "Nadia A. Lanman",
        "Vaneet Aggarwal"
      ],
      "published": "2023-11-04T06:00:56Z",
      "updated": "2024-08-22T20:18:06Z",
      "categories": [
        "cs.LG",
        "q-bio.GN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02333v3",
      "landing_url": "https://arxiv.org/abs/2311.02333v3",
      "doi": "https://doi.org/10.1093/bioadv/vbae117"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on DNA sequence modeling rather than discrete audio tokens or tokenizers, so it violates the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on DNA sequence modeling rather than discrete audio tokens or tokenizers, so it violates the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency",
    "abstract": "In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.",
    "metadata": {
      "arxiv_id": "2311.02772",
      "title": "Attention or Convolution: Transformer Encoders in Audio Language Models for Inference Efficiency",
      "summary": "In this paper, we show that a simple self-supervised pre-trained audio model can achieve comparable inference efficiency to more complicated pre-trained models with speech transformer encoders. These speech transformers rely on mixing convolutional modules with self-attention modules. They achieve state-of-the-art performance on ASR with top efficiency. We first show that employing these speech transformers as an encoder significantly improves the efficiency of pre-trained audio models as well. However, our study shows that we can achieve comparable efficiency with advanced self-attention solely. We demonstrate that this simpler approach is particularly beneficial with a low-bit weight quantization technique of a neural network to improve efficiency. We hypothesize that it prevents propagating the errors between different quantized modules compared to recent speech transformers mixing quantized convolution and the quantized self-attention modules.",
      "authors": [
        "Sungho Jeon",
        "Ching-Feng Yeh",
        "Hakan Inan",
        "Wei-Ning Hsu",
        "Rashi Rungta",
        "Yashar Mehdad",
        "Daniel Bikel"
      ],
      "published": "2023-11-05T21:30:10Z",
      "updated": "2024-02-08T10:09:18Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02772v2",
      "landing_url": "https://arxiv.org/abs/2311.02772v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02772"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on speech transformer encoders and quantization for inference efficiency without proposing or analyzing discrete audio tokenizers/quantized vocabularies, so it fails to meet mandatory discrete-token scope."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on speech transformer encoders and quantization for inference efficiency without proposing or analyzing discrete audio tokenizers/quantized vocabularies, so it fails to meet mandatory discrete-token scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
    "abstract": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
    "metadata": {
      "arxiv_id": "2311.02898",
      "title": "Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction",
      "summary": "We introduce a text-to-speech(TTS) framework based on a neural transducer. We use discretized semantic tokens acquired from wav2vec2.0 embeddings, which makes it easy to adopt a neural transducer for the TTS framework enjoying its monotonic alignment constraints. The proposed model first generates aligned semantic tokens using the neural transducer, then synthesizes a speech sample from the semantic tokens using a non-autoregressive(NAR) speech generator. This decoupled framework alleviates the training complexity of TTS and allows each stage to focus on 1) linguistic and alignment modeling and 2) fine-grained acoustic modeling, respectively. Experimental results on the zero-shot adaptive TTS show that the proposed model exceeds the baselines in speech quality and speaker similarity via objective and subjective measures. We also investigate the inference speed and prosody controllability of our proposed model, showing the potential of the neural transducer for TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Dongjune Lee",
        "Nam Soo Kim"
      ],
      "published": "2023-11-06T06:13:39Z",
      "updated": "2023-11-08T05:52:39Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.02898v2",
      "landing_url": "https://arxiv.org/abs/2311.02898v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.02898"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper centers on discrete semantic tokens derived from wav2vec2 embeddings for TTS, explicitly modeling them with a neural transducer and evaluating reconstruction quality and downstream synthesis metrics, so it matches the inclusion criteria and doesn't hit any exclusions."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on discrete semantic tokens derived from wav2vec2 embeddings for TTS, explicitly modeling them with a neural transducer and evaluating reconstruction quality and downstream synthesis metrics, so it matches the inclusion criteria and doesn't hit any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
    "abstract": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
    "metadata": {
      "arxiv_id": "2311.04534",
      "title": "Loss Masking Is Not Needed in Decoder-only Transformer for Discrete-token-based ASR",
      "summary": "Recently, unified speech-text models, such as SpeechGPT, VioLA, and AudioPaLM, have achieved remarkable performance on various speech tasks. These models discretize speech signals into tokens (speech discretization) and use a shared vocabulary for both text and speech tokens. Then they train a single decoder-only Transformer on a mixture of speech tasks. However, these models rely on the Loss Masking strategy for the ASR task, which ignores the dependency among speech tokens. In this paper, we propose to model speech tokens in an autoregressive way, similar to text. We find that applying the conventional cross-entropy loss on input speech tokens does not consistently improve the ASR performance over the Loss Masking approach. To address this issue, we propose a novel approach denoted Smoothed Label Distillation (SLD), which applies a KL divergence loss with smoothed labels on speech tokens. Our experiments show that SLD effectively models speech tokens and outperforms Loss Masking for decoder-only Transformers in ASR tasks with different speech discretization methods. The source code can be found here: https://github.com/alibaba-damo-academy/SpokenNLP/tree/main/sld",
      "authors": [
        "Qian Chen",
        "Wen Wang",
        "Qinglin Zhang",
        "Siqi Zheng",
        "Shiliang Zhang",
        "Chong Deng",
        "Yukun Ma",
        "Hai Yu",
        "Jiaqing Liu",
        "Chong Zhang"
      ],
      "published": "2023-11-08T08:45:14Z",
      "updated": "2024-02-05T02:42:57Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.04534v2",
      "landing_url": "https://arxiv.org/abs/2311.04534v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.04534"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Abstract describes discrete speech tokenization and modeling within decoder-only Transformers (including a novel Smoothed Label Distillation for token-level ASR), so it clearly centers on discrete audio tokens and their downstream effect, satisfying inclusion criteria and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes discrete speech tokenization and modeling within decoder-only Transformers (including a novel Smoothed Label Distillation for token-level ASR), so it clearly centers on discrete audio tokens and their downstream effect, satisfying inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
    "abstract": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.",
    "metadata": {
      "arxiv_id": "2311.07919",
      "title": "Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models",
      "summary": "Recently, instruction-following audio-language models have received broad attention for audio interaction with humans. However, the absence of pre-trained audio models capable of handling diverse audio types and tasks has hindered progress in this field. Consequently, most existing works have only been able to support a limited range of interaction capabilities. In this paper, we develop the Qwen-Audio model and address this limitation by scaling up audio-language pre-training to cover over 30 tasks and various audio types, such as human speech, natural sounds, music, and songs, to facilitate universal audio understanding abilities. However, directly co-training all tasks and datasets can lead to interference issues, as the textual labels associated with different datasets exhibit considerable variations due to differences in task focus, language, granularity of annotation, and text structure. To overcome the one-to-many interference, we carefully design a multi-task training framework by conditioning on a sequence of hierarchical tags to the decoder for encouraging knowledge sharing and avoiding interference through shared and specified tags respectively. Remarkably, Qwen-Audio achieves impressive performance across diverse benchmark tasks without requiring any task-specific fine-tuning, surpassing its counterparts. Building upon the capabilities of Qwen-Audio, we further develop Qwen-Audio-Chat, which allows for input from various audios and text inputs, enabling multi-turn dialogues and supporting various audio-central scenarios.",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "published": "2023-11-14T05:34:50Z",
      "updated": "2023-12-21T10:20:42Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.07919v2",
      "landing_url": "https://arxiv.org/abs/2311.07919v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.07919"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on scaling instruction-following audio-language pretraining across diverse tasks but never discusses discrete audio tokenization, quantization, or vocabulary-level modeling, so it fails the core inclusion requirement."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on scaling instruction-following audio-language pretraining across diverse tasks but never discusses discrete audio tokenization, quantization, or vocabulary-level modeling, so it fails the core inclusion requirement.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Zero-shot audio captioning with audio-language model guidance and audio context keywords",
    "abstract": "Zero-shot audio captioning aims at automatically generating descriptive textual captions for audio content without prior training for this task. Different from speech recognition which translates audio content that contains spoken language into text, audio captioning is commonly concerned with ambient sounds, or sounds produced by a human performing an action. Inspired by zero-shot image captioning methods, we propose ZerAuCap, a novel framework for summarising such general audio signals in a text caption without requiring task-specific training. In particular, our framework exploits a pre-trained large language model (LLM) for generating the text which is guided by a pre-trained audio-language model to produce captions that describe the audio content. Additionally, we use audio context keywords that prompt the language model to generate text that is broadly relevant to sounds. Our proposed framework achieves state-of-the-art results in zero-shot audio captioning on the AudioCaps and Clotho datasets. Our code is available at https://github.com/ExplainableML/ZerAuCap.",
    "metadata": {
      "arxiv_id": "2311.08396",
      "title": "Zero-shot audio captioning with audio-language model guidance and audio context keywords",
      "summary": "Zero-shot audio captioning aims at automatically generating descriptive textual captions for audio content without prior training for this task. Different from speech recognition which translates audio content that contains spoken language into text, audio captioning is commonly concerned with ambient sounds, or sounds produced by a human performing an action. Inspired by zero-shot image captioning methods, we propose ZerAuCap, a novel framework for summarising such general audio signals in a text caption without requiring task-specific training. In particular, our framework exploits a pre-trained large language model (LLM) for generating the text which is guided by a pre-trained audio-language model to produce captions that describe the audio content. Additionally, we use audio context keywords that prompt the language model to generate text that is broadly relevant to sounds. Our proposed framework achieves state-of-the-art results in zero-shot audio captioning on the AudioCaps and Clotho datasets. Our code is available at https://github.com/ExplainableML/ZerAuCap.",
      "authors": [
        "Leonard Salewski",
        "Stefan Fauth",
        "A. Sophia Koepke",
        "Zeynep Akata"
      ],
      "published": "2023-11-14T18:55:48Z",
      "updated": "2023-11-14T18:55:48Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08396v1",
      "landing_url": "https://arxiv.org/abs/2311.08396v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08396"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on zero-shot audio captioning via LLM guidance and keywords, with no discussion of discrete audio token quantization or vocab-driven codec/SSL token design, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on zero-shot audio captioning via LLM guidance and keywords, with no discussion of discrete audio token quantization or vocab-driven codec/SSL token design, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models",
    "abstract": "Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.",
    "metadata": {
      "arxiv_id": "2311.08623",
      "title": "DEED: Dynamic Early Exit on Decoder for Accelerating Encoder-Decoder Transformer Models",
      "summary": "Encoder-decoder transformer models have achieved great success on various vision-language (VL) tasks, but they suffer from high inference latency. Typically, the decoder takes up most of the latency because of the auto-regressive decoding. To accelerate the inference, we propose an approach of performing Dynamic Early Exit on Decoder (DEED). We build a multi-exit encoder-decoder transformer model which is trained with deep supervision so that each of its decoder layers is capable of generating plausible predictions. In addition, we leverage simple yet practical techniques, including shared generation head and adaptation modules, to keep accuracy when exiting at shallow decoder layers. Based on the multi-exit model, we perform step-level dynamic early exit during inference, where the model may decide to use fewer decoder layers based on its confidence of the current layer at each individual decoding step. Considering different number of decoder layers may be used at different decoding steps, we compute deeper-layer decoder features of previous decoding steps just-in-time, which ensures the features from different decoding steps are semantically aligned. We evaluate our approach with two state-of-the-art encoder-decoder transformer models on various VL tasks. We show our approach can reduce overall inference latency by 30%-60% with comparable or even higher accuracy compared to baselines.",
      "authors": [
        "Peng Tang",
        "Pengkai Zhu",
        "Tian Li",
        "Srikar Appalaraju",
        "Vijay Mahadevan",
        "R. Manmatha"
      ],
      "published": "2023-11-15T01:01:02Z",
      "updated": "2023-11-15T01:01:02Z",
      "categories": [
        "cs.CV",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08623v1",
      "landing_url": "https://arxiv.org/abs/2311.08623v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.08623"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on dynamic early exit for encoder-decoder transformers on vision-language tasks with latency reduction, which has nothing to do with discrete audio token generation/quantization or evaluations, so exclude outright."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on dynamic early exit for encoder-decoder transformers on vision-language tasks with latency reduction, which has nothing to do with discrete audio token generation/quantization or evaluations, so exclude outright.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Texture and Noise Dual Adaptation for Infrared Image Super-Resolution",
    "abstract": "Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \\url{https://github.com/yongsongH/DASRGAN}.",
    "metadata": {
      "arxiv_id": "2311.08816",
      "title": "Texture and Noise Dual Adaptation for Infrared Image Super-Resolution",
      "summary": "Recent efforts have explored leveraging visible light images to enrich texture details in infrared (IR) super-resolution. However, this direct adaptation approach often becomes a double-edged sword, as it improves texture at the cost of introducing noise and blurring artifacts. To address these challenges, we propose the Target-oriented Domain Adaptation SRGAN (DASRGAN), an innovative framework specifically engineered for robust IR super-resolution model adaptation. DASRGAN operates on the synergy of two key components: 1) Texture-Oriented Adaptation (TOA) to refine texture details meticulously, and 2) Noise-Oriented Adaptation (NOA), dedicated to minimizing noise transfer. Specifically, TOA uniquely integrates a specialized discriminator, incorporating a prior extraction branch, and employs a Sobel-guided adversarial loss to align texture distributions effectively. Concurrently, NOA utilizes a noise adversarial loss to distinctly separate the generative and Gaussian noise pattern distributions during adversarial training. Our extensive experiments confirm DASRGAN's superiority. Comparative analyses against leading methods across multiple benchmarks and upsampling factors reveal that DASRGAN sets new state-of-the-art performance standards. Code are available at \\url{https://github.com/yongsongH/DASRGAN}.",
      "authors": [
        "Yongsong Huang",
        "Tomo Miyazaki",
        "Xiaofeng Liu",
        "Yafei Dong",
        "Shinichiro Omachi"
      ],
      "published": "2023-11-15T09:35:07Z",
      "updated": "2025-02-20T08:51:25Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.08816v2",
      "landing_url": "https://arxiv.org/abs/2311.08816v2",
      "doi": "https://doi.org/10.48550/arXiv.2311.08816"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on infrared image super-resolution and GAN-based texture/noise adaptation, with no discussion of discrete audio tokens, quantization, vocabularies, or token-level modeling, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on infrared image super-resolution and GAN-based texture/noise adaptation, with no discussion of discrete audio tokens, quantization, vocabularies, or token-level modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers",
    "abstract": "We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.",
    "metadata": {
      "arxiv_id": "2311.15475",
      "title": "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers",
      "summary": "We introduce MeshGPT, a new approach for generating triangle meshes that reflects the compactness typical of artist-created meshes, in contrast to dense triangle meshes extracted by iso-surfacing methods from neural fields. Inspired by recent advances in powerful large language models, we adopt a sequence-based approach to autoregressively generate triangle meshes as sequences of triangles. We first learn a vocabulary of latent quantized embeddings, using graph convolutions, which inform these embeddings of the local mesh geometry and topology. These embeddings are sequenced and decoded into triangles by a decoder, ensuring that they can effectively reconstruct the mesh. A transformer is then trained on this learned vocabulary to predict the index of the next embedding given previous embeddings. Once trained, our model can be autoregressively sampled to generate new triangle meshes, directly generating compact meshes with sharp edges, more closely imitating the efficient triangulation patterns of human-crafted meshes. MeshGPT demonstrates a notable improvement over state of the art mesh generation methods, with a 9% increase in shape coverage and a 30-point enhancement in FID scores across various categories.",
      "authors": [
        "Yawar Siddiqui",
        "Antonio Alliegro",
        "Alexey Artemov",
        "Tatiana Tommasi",
        "Daniele Sirigatti",
        "Vladislav Rosov",
        "Angela Dai",
        "Matthias Nießner"
      ],
      "published": "2023-11-27T01:20:11Z",
      "updated": "2023-11-27T01:20:11Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2311.15475v1",
      "landing_url": "https://arxiv.org/abs/2311.15475v1",
      "doi": "https://doi.org/10.48550/arXiv.2311.15475"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "MeshGPT generates triangle meshes from visual data and does not address discrete audio tokenization or quantized audio codec/vocabulary design, so it fails the inclusion criteria and meets exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "MeshGPT generates triangle meshes from visual data and does not address discrete audio tokenization or quantized audio codec/vocabulary design, so it fails the inclusion criteria and meets exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bitstream Organization for Parallel Entropy Coding on Neural Network-based Video Codecs",
    "abstract": "Video compression systems must support increasing bandwidth and data throughput at low cost and power, and can be limited by entropy coding bottlenecks. Efficiency can be greatly improved by parallelizing coding, which can be done at much larger scales with new neural-based codecs, but with some compression loss related to data organization. We analyze the bit rate overhead needed to support multiple bitstreams for concurrent decoding, and for its minimization propose a method for compressing parallel-decoding entry points, using bidirectional bitstream packing, and a new form of jointly optimizing arithmetic coding termination. It is shown that those techniques significantly lower the overhead, making it easier to reduce it to a small fraction of the average bitstream size, like, for example, less than 1% and 0.1% when the average number of bitstream bytes is respectively larger than 95 and 1,200 bytes.",
    "metadata": {
      "arxiv_id": "2312.00921",
      "title": "Bitstream Organization for Parallel Entropy Coding on Neural Network-based Video Codecs",
      "summary": "Video compression systems must support increasing bandwidth and data throughput at low cost and power, and can be limited by entropy coding bottlenecks. Efficiency can be greatly improved by parallelizing coding, which can be done at much larger scales with new neural-based codecs, but with some compression loss related to data organization. We analyze the bit rate overhead needed to support multiple bitstreams for concurrent decoding, and for its minimization propose a method for compressing parallel-decoding entry points, using bidirectional bitstream packing, and a new form of jointly optimizing arithmetic coding termination. It is shown that those techniques significantly lower the overhead, making it easier to reduce it to a small fraction of the average bitstream size, like, for example, less than 1% and 0.1% when the average number of bitstream bytes is respectively larger than 95 and 1,200 bytes.",
      "authors": [
        "Amir Said",
        "Hoang Le",
        "Farzad Farhadzadeh"
      ],
      "published": "2023-12-01T20:50:17Z",
      "updated": "2023-12-01T20:50:17Z",
      "categories": [
        "eess.IV",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.00921v1",
      "landing_url": "https://arxiv.org/abs/2312.00921v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.00921"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract discuss bitstream organization for neural video codecs, not discrete audio-tokenization, so it fails the topic inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract discuss bitstream organization for neural video codecs, not discrete audio-tokenization, so it fails the topic inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
    "abstract": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
    "metadata": {
      "arxiv_id": "2312.02147",
      "title": "Rejuvenating image-GPT as Strong Visual Representation Learners",
      "summary": "This paper enhances image-GPT (iGPT), one of the pioneering works that introduce autoregressive pretraining to predict the next pixels for visual representation learning. Two simple yet essential changes are made. First, we shift the prediction target from raw pixels to semantic tokens, enabling a higher-level understanding of visual content. Second, we supplement the autoregressive modeling by instructing the model to predict not only the next tokens but also the visible tokens. This pipeline is particularly effective when semantic tokens are encoded by discriminatively trained models, such as CLIP. We introduce this novel approach as D-iGPT. Extensive experiments showcase that D-iGPT excels as a strong learner of visual representations: A notable achievement is its compelling performance on the ImageNet-1K dataset -- by training on publicly available datasets, D-iGPT unprecedentedly achieves \\textbf{90.0\\%} top-1 accuracy with a vanilla ViT-H. Additionally, D-iGPT shows strong generalization on the downstream task. Code is available at https://github.com/OliverRensu/D-iGPT.",
      "authors": [
        "Sucheng Ren",
        "Zeyu Wang",
        "Hongru Zhu",
        "Junfei Xiao",
        "Alan Yuille",
        "Cihang Xie"
      ],
      "published": "2023-12-04T18:59:20Z",
      "updated": "2024-07-05T05:07:08Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.02147v2",
      "landing_url": "https://arxiv.org/abs/2312.02147v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.02147"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses solely on autoregressive vision modeling with semantic image tokens and does not study any discrete audio tokens, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses solely on autoregressive vision modeling with semantic image tokens and does not study any discrete audio tokens, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations",
    "abstract": "Anomaly detection is a critical and challenging task that aims to identify data points deviating from normal patterns and distributions within a dataset. Various methods have been proposed using a one-class-one-model approach, but these techniques often face practical problems such as memory inefficiency and the requirement of sufficient data for training. In particular, few-shot anomaly detection presents significant challenges in industrial applications, where limited samples are available before mass production. In this paper, we propose a few-shot anomaly detection method that integrates adversarial training loss to obtain more robust and generalized feature representations. We utilize the adversarial loss previously employed in domain adaptation to align feature distributions between source and target domains, to enhance feature robustness and generalization in few-shot anomaly detection tasks. We hypothesize that adversarial loss is effective when applied to features that should have similar characteristics, such as those from the same layer in a Siamese network's parallel branches or input-output pairs of reconstruction-based methods. Experimental results demonstrate that the proposed method generally achieves better performance when utilizing the adversarial loss.",
    "metadata": {
      "arxiv_id": "2312.03005",
      "title": "Few-Shot Anomaly Detection with Adversarial Loss for Robust Feature Representations",
      "summary": "Anomaly detection is a critical and challenging task that aims to identify data points deviating from normal patterns and distributions within a dataset. Various methods have been proposed using a one-class-one-model approach, but these techniques often face practical problems such as memory inefficiency and the requirement of sufficient data for training. In particular, few-shot anomaly detection presents significant challenges in industrial applications, where limited samples are available before mass production. In this paper, we propose a few-shot anomaly detection method that integrates adversarial training loss to obtain more robust and generalized feature representations. We utilize the adversarial loss previously employed in domain adaptation to align feature distributions between source and target domains, to enhance feature robustness and generalization in few-shot anomaly detection tasks. We hypothesize that adversarial loss is effective when applied to features that should have similar characteristics, such as those from the same layer in a Siamese network's parallel branches or input-output pairs of reconstruction-based methods. Experimental results demonstrate that the proposed method generally achieves better performance when utilizing the adversarial loss.",
      "authors": [
        "Jae Young Lee",
        "Wonjun Lee",
        "Jaehyun Choi",
        "Yongkwi Lee",
        "Young Seog Yoon"
      ],
      "published": "2023-12-04T09:45:02Z",
      "updated": "2023-12-04T09:45:02Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03005v1",
      "landing_url": "https://arxiv.org/abs/2312.03005v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.03005"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on few-shot anomaly detection using adversarial loss for robust feature representations, which does not involve discrete audio tokens, tokenizers, codecs, or quantization, so it fails to meet the inclusion criteria and falls under the exclusion of studies that do not target discrete audio token research."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on few-shot anomaly detection using adversarial loss for robust feature representations, which does not involve discrete audio tokens, tokenizers, codecs, or quantization, so it fails to meet the inclusion criteria and falls under the exclusion of studies that do not target discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
    "abstract": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
    "metadata": {
      "arxiv_id": "2312.03406",
      "title": "Does Vector Quantization Fail in Spatio-Temporal Forecasting? Exploring a Differentiable Sparse Soft-Vector Quantization Approach",
      "summary": "Spatio-temporal forecasting is crucial in various fields and requires a careful balance between identifying subtle patterns and filtering out noise. Vector quantization (VQ) appears well-suited for this purpose, as it quantizes input vectors into a set of codebook vectors or patterns. Although VQ has shown promise in various computer vision tasks, it surprisingly falls short in enhancing the accuracy of spatio-temporal forecasting. We attribute this to two main issues: inaccurate optimization due to non-differentiability and limited representation power in hard-VQ. To tackle these challenges, we introduce Differentiable Sparse Soft-Vector Quantization (SVQ), the first VQ method to enhance spatio-temporal forecasting. SVQ balances detail preservation with noise reduction, offering full differentiability and a solid foundation in sparse regression. Our approach employs a two-layer MLP and an extensive codebook to streamline the sparse regression process, significantly cutting computational costs while simplifying training and improving performance. Empirical studies on five spatio-temporal benchmark datasets show SVQ achieves state-of-the-art results, including a 7.9% improvement on the WeatherBench-S temperature dataset and an average mean absolute error reduction of 9.4% in video prediction benchmarks (Human3.6M, KTH, and KittiCaltech), along with a 17.3% enhancement in image quality (LPIPS). Code is publicly available at https://github.com/Pachark/SVQ-Forecasting.",
      "authors": [
        "Chao Chen",
        "Tian Zhou",
        "Yanjun Zhao",
        "Hui Liu",
        "Liang Sun",
        "Rong Jin"
      ],
      "published": "2023-12-06T10:42:40Z",
      "updated": "2025-05-18T09:11:15Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.03406v4",
      "landing_url": "https://arxiv.org/abs/2312.03406v4",
      "doi": "https://doi.org/10.48550/arXiv.2312.03406"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vector quantization for spatio-temporal forecasting in vision-related datasets, not on discrete audio token generation or evaluation, so it fails the audio-specific inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization for spatio-temporal forecasting in vision-related datasets, not on discrete audio token generation or evaluation, so it fails the audio-specific inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Improved Masking Strategy for Self-supervised Masked Reconstruction in Human Activity Recognition",
    "abstract": "Masked reconstruction serves as a fundamental pretext task for self-supervised learning, enabling the model to enhance its feature extraction capabilities by reconstructing the masked segments from extensive unlabeled data. In human activity recognition, this pretext task employed a masking strategy centered on the time dimension. However, this masking strategy fails to fully exploit the inherent characteristics of wearable sensor data and overlooks the inter-channel information coupling, thereby limiting its potential as a powerful pretext task. To address these limitations, we propose a novel masking strategy called Channel Masking. It involves masking the sensor data along the channel dimension, thereby compelling the encoder to extract channel-related features while performing the masked reconstruction task. Moreover, Channel Masking can be seamlessly integrated with masking strategies along the time dimension, thereby motivating the self-supervised model to undertake the masked reconstruction task in both the time and channel dimensions. Integrated masking strategies are named Time-Channel Masking and Span-Channel Masking. Finally, we optimize the reconstruction loss function to incorporate the reconstruction loss in both the time and channel dimensions. We evaluate proposed masking strategies on three public datasets, and experimental results show that the proposed strategies outperform prior strategies in both self-supervised and semi-supervised scenarios.",
    "metadata": {
      "arxiv_id": "2312.04147",
      "title": "An Improved Masking Strategy for Self-supervised Masked Reconstruction in Human Activity Recognition",
      "summary": "Masked reconstruction serves as a fundamental pretext task for self-supervised learning, enabling the model to enhance its feature extraction capabilities by reconstructing the masked segments from extensive unlabeled data. In human activity recognition, this pretext task employed a masking strategy centered on the time dimension. However, this masking strategy fails to fully exploit the inherent characteristics of wearable sensor data and overlooks the inter-channel information coupling, thereby limiting its potential as a powerful pretext task. To address these limitations, we propose a novel masking strategy called Channel Masking. It involves masking the sensor data along the channel dimension, thereby compelling the encoder to extract channel-related features while performing the masked reconstruction task. Moreover, Channel Masking can be seamlessly integrated with masking strategies along the time dimension, thereby motivating the self-supervised model to undertake the masked reconstruction task in both the time and channel dimensions. Integrated masking strategies are named Time-Channel Masking and Span-Channel Masking. Finally, we optimize the reconstruction loss function to incorporate the reconstruction loss in both the time and channel dimensions. We evaluate proposed masking strategies on three public datasets, and experimental results show that the proposed strategies outperform prior strategies in both self-supervised and semi-supervised scenarios.",
      "authors": [
        "Jinqiang Wang",
        "Tao Zhu",
        "Huansheng Ning"
      ],
      "published": "2023-12-07T09:02:11Z",
      "updated": "2023-12-07T09:02:11Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.04147v1",
      "landing_url": "https://arxiv.org/abs/2312.04147v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.04147"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about masking strategies for self-supervised human activity recognition on wearable sensors, with no discrete audio token generation or evaluation, so it fails the inclusion focus and meets the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper is about masking strategies for self-supervised human activity recognition on wearable sensors, with no discrete audio token generation or evaluation, so it fails the inclusion focus and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
    "abstract": "Recently, diffusion models have become popular tools for image synthesis because of their high-quality outputs. However, like other large-scale models, they may leak private information about their training data. Here, we demonstrate a privacy vulnerability of diffusion models through a \\emph{membership inference (MI) attack}, which aims to identify whether a target example belongs to the training set when given the trained diffusion model. Our proposed MI attack learns quantile regression models that predict (a quantile of) the distribution of reconstruction loss on examples not used in training. This allows us to define a granular hypothesis test for determining the membership of a point in the training set, based on thresholding the reconstruction loss of that point using a custom threshold tailored to the example. We also provide a simple bootstrap technique that takes a majority membership prediction over ``a bag of weak attackers'' which improves the accuracy over individual quantile regression models. We show that our attack outperforms the prior state-of-the-art attack while being substantially less computationally expensive -- prior attacks required training multiple ``shadow models'' with the same architecture as the model under attack, whereas our attack requires training only much smaller models.",
    "metadata": {
      "arxiv_id": "2312.05140",
      "title": "Membership Inference Attacks on Diffusion Models via Quantile Regression",
      "summary": "Recently, diffusion models have become popular tools for image synthesis because of their high-quality outputs. However, like other large-scale models, they may leak private information about their training data. Here, we demonstrate a privacy vulnerability of diffusion models through a \\emph{membership inference (MI) attack}, which aims to identify whether a target example belongs to the training set when given the trained diffusion model. Our proposed MI attack learns quantile regression models that predict (a quantile of) the distribution of reconstruction loss on examples not used in training. This allows us to define a granular hypothesis test for determining the membership of a point in the training set, based on thresholding the reconstruction loss of that point using a custom threshold tailored to the example. We also provide a simple bootstrap technique that takes a majority membership prediction over ``a bag of weak attackers'' which improves the accuracy over individual quantile regression models. We show that our attack outperforms the prior state-of-the-art attack while being substantially less computationally expensive -- prior attacks required training multiple ``shadow models'' with the same architecture as the model under attack, whereas our attack requires training only much smaller models.",
      "authors": [
        "Shuai Tang",
        "Zhiwei Steven Wu",
        "Sergul Aydore",
        "Michael Kearns",
        "Aaron Roth"
      ],
      "published": "2023-12-08T16:21:24Z",
      "updated": "2023-12-08T16:21:24Z",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.05140v1",
      "landing_url": "https://arxiv.org/abs/2312.05140v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.05140"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on membership inference attacks on diffusion models for images and contains no content about discrete audio token generation, quantization, or tokenizer evaluation, so it does not meet any inclusion criteria and violates the scope."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on membership inference attacks on diffusion models for images and contains no content about discrete audio token generation, quantization, or tokenizer evaluation, so it does not meet any inclusion criteria and violates the scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding and Leveraging the Learning Phases of Neural Networks",
    "abstract": "The learning dynamics of deep neural networks are not well understood. The information bottleneck (IB) theory proclaimed separate fitting and compression phases. But they have since been heavily debated. We comprehensively analyze the learning dynamics by investigating a layer's reconstruction ability of the input and prediction performance based on the evolution of parameters during training. We empirically show the existence of three phases using common datasets and architectures such as ResNet and VGG: (i) near constant reconstruction loss, (ii) decrease, and (iii) increase. We also derive an empirically grounded data model and prove the existence of phases for single-layer networks. Technically, our approach leverages classical complexity analysis. It differs from IB by relying on measuring reconstruction loss rather than information theoretic measures to relate information of intermediate layers and inputs. Our work implies a new best practice for transfer learning: We show empirically that the pre-training of a classifier should stop well before its performance is optimal.",
    "metadata": {
      "arxiv_id": "2312.06887",
      "title": "Understanding and Leveraging the Learning Phases of Neural Networks",
      "summary": "The learning dynamics of deep neural networks are not well understood. The information bottleneck (IB) theory proclaimed separate fitting and compression phases. But they have since been heavily debated. We comprehensively analyze the learning dynamics by investigating a layer's reconstruction ability of the input and prediction performance based on the evolution of parameters during training. We empirically show the existence of three phases using common datasets and architectures such as ResNet and VGG: (i) near constant reconstruction loss, (ii) decrease, and (iii) increase. We also derive an empirically grounded data model and prove the existence of phases for single-layer networks. Technically, our approach leverages classical complexity analysis. It differs from IB by relying on measuring reconstruction loss rather than information theoretic measures to relate information of intermediate layers and inputs. Our work implies a new best practice for transfer learning: We show empirically that the pre-training of a classifier should stop well before its performance is optimal.",
      "authors": [
        "Johannes Schneider",
        "Mohit Prabhushankar"
      ],
      "published": "2023-12-11T23:20:58Z",
      "updated": "2023-12-14T14:32:45Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.06887v2",
      "landing_url": "https://arxiv.org/abs/2312.06887v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.06887"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies neural network learning phases with reconstruction loss but has no focus on discrete audio tokenizers/codecs or finite-vocabulary audio sequences, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper studies neural network learning phases with reconstruction loss but has no focus on discrete audio tokenizers/codecs or finite-vocabulary audio sequences, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction",
    "abstract": "Canonical relation extraction aims to extract relational triples from sentences, where the triple elements (entity pairs and their relationship) are mapped to the knowledge base. Recently, methods based on the encoder-decoder architecture are proposed and achieve promising results. However, these methods cannot well utilize the entity information, which is merely used as augmented training data. Moreover, they are incapable of representing novel entities, since no embeddings have been learned for them. In this paper, we propose a novel framework, Bi-Encoder-Decoder (BED), to solve the above issues. Specifically, to fully utilize entity information, we employ an encoder to encode semantics of this information, leading to high-quality entity representations. For novel entities, given a trained entity encoder, their representations can be easily generated. Experimental results on two datasets show that, our method achieves a significant performance improvement over the previous state-of-the-art and handle novel entities well without retraining.",
    "metadata": {
      "arxiv_id": "2312.07088",
      "title": "BED: Bi-Encoder-Decoder Model for Canonical Relation Extraction",
      "summary": "Canonical relation extraction aims to extract relational triples from sentences, where the triple elements (entity pairs and their relationship) are mapped to the knowledge base. Recently, methods based on the encoder-decoder architecture are proposed and achieve promising results. However, these methods cannot well utilize the entity information, which is merely used as augmented training data. Moreover, they are incapable of representing novel entities, since no embeddings have been learned for them. In this paper, we propose a novel framework, Bi-Encoder-Decoder (BED), to solve the above issues. Specifically, to fully utilize entity information, we employ an encoder to encode semantics of this information, leading to high-quality entity representations. For novel entities, given a trained entity encoder, their representations can be easily generated. Experimental results on two datasets show that, our method achieves a significant performance improvement over the previous state-of-the-art and handle novel entities well without retraining.",
      "authors": [
        "Nantao Zheng",
        "Siyu Long",
        "Xinyu Dai"
      ],
      "published": "2023-12-12T09:14:55Z",
      "updated": "2023-12-12T09:14:55Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07088v1",
      "landing_url": "https://arxiv.org/abs/2312.07088v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.07088"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on canonical relation extraction in text, with no discussion of discrete audio token generation or codec/quantization design, so it fails the inclusion criteria and violates the audio token focus."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on canonical relation extraction in text, with no discussion of discrete audio token generation or codec/quantization design, so it fails the inclusion criteria and violates the audio token focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)",
    "abstract": "Video captioning in Nepali, a language written in the Devanagari script, presents a unique challenge due to the lack of existing academic work in this domain. This work develops a novel encoder-decoder paradigm for Nepali video captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models are used in the model to produce related textual descriptions based on features retrieved from video frames using CNNs. Using Google Translate and manual post-editing, a Nepali video captioning dataset is generated from the Microsoft Research Video Description Corpus (MSVD) dataset created using Google Translate, and manual post-editing work. The efficiency of the model for Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE measures, which are used to assess its performance.",
    "metadata": {
      "arxiv_id": "2312.07418",
      "title": "Attention Based Encoder Decoder Model for Video Captioning in Nepali (2023)",
      "summary": "Video captioning in Nepali, a language written in the Devanagari script, presents a unique challenge due to the lack of existing academic work in this domain. This work develops a novel encoder-decoder paradigm for Nepali video captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models are used in the model to produce related textual descriptions based on features retrieved from video frames using CNNs. Using Google Translate and manual post-editing, a Nepali video captioning dataset is generated from the Microsoft Research Video Description Corpus (MSVD) dataset created using Google Translate, and manual post-editing work. The efficiency of the model for Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE measures, which are used to assess its performance.",
      "authors": [
        "Kabita Parajuli",
        "Shashidhar Ram Joshi"
      ],
      "published": "2023-12-12T16:39:12Z",
      "updated": "2024-05-19T15:39:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07418v3",
      "landing_url": "https://arxiv.org/abs/2312.07418v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.07418"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Work focuses on encoder-decoder video captioning with text (Nepali) and translation, not on discrete audio token generation/quantization or token modeling, so it fails inclusion and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Work focuses on encoder-decoder video captioning with text (Nepali) and translation, not on discrete audio token generation/quantization or token modeling, so it fails inclusion and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bitrate Ladder Construction using Visual Information Fidelity",
    "abstract": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
    "metadata": {
      "arxiv_id": "2312.07780",
      "title": "Bitrate Ladder Construction using Visual Information Fidelity",
      "summary": "Recently proposed perceptually optimized per-title video encoding methods provide better BD-rate savings than fixed bitrate-ladder approaches that have been employed in the past. However, a disadvantage of per-title encoding is that it requires significant time and energy to compute bitrate ladders. Over the past few years, a variety of methods have been proposed to construct optimal bitrate ladders including using low-level features to predict cross-over bitrates, optimal resolutions for each bitrate, predicting visual quality, etc. Here, we deploy features drawn from Visual Information Fidelity (VIF) (VIF features) extracted from uncompressed videos to predict the visual quality (VMAF) of compressed videos. We present multiple VIF feature sets extracted from different scales and subbands of a video to tackle the problem of bitrate ladder construction. Comparisons are made against a fixed bitrate ladder and a bitrate ladder obtained from exhaustive encoding using Bjontegaard delta metrics.",
      "authors": [
        "Krishna Srikar Durbha",
        "Hassene Tmar",
        "Cosmin Stejerean",
        "Ioannis Katsavounidis",
        "Alan C. Bovik"
      ],
      "published": "2023-12-12T22:48:50Z",
      "updated": "2024-02-29T03:44:59Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.07780v2",
      "landing_url": "https://arxiv.org/abs/2312.07780v2",
      "doi": "https://doi.org/10.1109/PCS60826.2024.10566405"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on bitrate ladder construction for video using VIF features rather than discrete audio token generation, quantization, or vocabularies, so it fails the inclusion criteria and hits no relevant content."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on bitrate ladder construction for video using VIF features rather than discrete audio token generation, quantization, or vocabularies, so it fails the inclusion criteria and hits no relevant content.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
    "abstract": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
    "metadata": {
      "arxiv_id": "2312.08676",
      "title": "SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross Attention",
      "summary": "Zero-shot voice conversion (VC) aims to transfer the source speaker timbre to arbitrary unseen target speaker timbre, while keeping the linguistic content unchanged. Although the voice of generated speech can be controlled by providing the speaker embedding of the target speaker, the speaker similarity still lags behind the ground truth recordings. In this paper, we propose SEF-VC, a speaker embedding free voice conversion model, which is designed to learn and incorporate speaker timbre from reference speech via a powerful position-agnostic cross-attention mechanism, and then reconstruct waveform from HuBERT semantic tokens in a non-autoregressive manner. The concise design of SEF-VC enhances its training stability and voice conversion performance. Objective and subjective evaluations demonstrate the superiority of SEF-VC to generate high-quality speech with better similarity to target reference than strong zero-shot VC baselines, even for very short reference speeches.",
      "authors": [
        "Junjie Li",
        "Yiwei Guo",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2023-12-14T06:26:55Z",
      "updated": "2024-01-30T14:11:29Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.08676v2",
      "landing_url": "https://arxiv.org/abs/2312.08676v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.08676"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on zero-shot voice conversion using HuBERT semantic tokens as inputs but does not propose or analyze discrete token generation/quantization methods, so it fails to meet the discrete-token–centric inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on zero-shot voice conversion using HuBERT semantic tokens as inputs but does not propose or analyze discrete token generation/quantization methods, so it fails to meet the discrete-token–centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Tokenize Anything via Prompting",
    "abstract": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
    "metadata": {
      "arxiv_id": "2312.09128",
      "title": "Tokenize Anything via Prompting",
      "summary": "We present a unified, promptable model capable of simultaneously segmenting, recognizing, and captioning anything. Unlike SAM, we aim to build a versatile region representation in the wild via visual prompting. To achieve this, we train a generalizable model with massive segmentation masks, \\eg, SA-1B masks, and semantic priors from a pre-trained CLIP model with 5 billion parameters. Specifically, we construct a promptable image decoder by adding a semantic token to each mask token. The semantic token is responsible for learning the semantic priors in a predefined concept space. Through joint optimization of segmentation on mask tokens and concept prediction on semantic tokens, our model exhibits strong regional recognition and localization capabilities. For example, an additional 38M-parameter causal text decoder trained from scratch sets a new record with a CIDEr score of 164.7 on the Visual Genome region captioning task. We believe this model can be a versatile region-level image tokenizer, capable of encoding general-purpose region context for a broad range of visual perception tasks. Code and models are available at {\\footnotesize \\url{https://github.com/baaivision/tokenize-anything}}.",
      "authors": [
        "Ting Pan",
        "Lulu Tang",
        "Xinlong Wang",
        "Shiguang Shan"
      ],
      "published": "2023-12-14T17:01:02Z",
      "updated": "2024-07-17T04:34:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09128v2",
      "landing_url": "https://arxiv.org/abs/2312.09128v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09128"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vision-region segmentation and captioning rather than any discrete audio tokenization pipeline, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vision-region segmentation and captioning rather than any discrete audio tokenization pipeline, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
    "abstract": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
    "metadata": {
      "arxiv_id": "2312.09747",
      "title": "SELM: Speech Enhancement Using Discrete Tokens and Language Models",
      "summary": "Language models (LMs) have shown superior performances in various speech generation tasks recently, demonstrating their powerful ability for semantic context modeling. Given the intrinsic similarity between speech generation and speech enhancement, harnessing semantic information holds potential advantages for speech enhancement tasks. In light of this, we propose SELM, a novel paradigm for speech enhancement, which integrates discrete tokens and leverages language models. SELM comprises three stages: encoding, modeling, and decoding. We transform continuous waveform signals into discrete tokens using pre-trained self-supervised learning (SSL) models and a k-means tokenizer. Language models then capture comprehensive contextual information within these tokens. Finally, a detokenizer and HiFi-GAN restore them into enhanced speech. Experimental results demonstrate that SELM achieves comparable performance in objective metrics alongside superior results in subjective perception. Our demos are available https://honee-w.github.io/SELM/.",
      "authors": [
        "Ziqian Wang",
        "Xinfa Zhu",
        "Zihan Zhang",
        "YuanJun Lv",
        "Ning Jiang",
        "Guoqing Zhao",
        "Lei Xie"
      ],
      "published": "2023-12-15T12:36:05Z",
      "updated": "2024-01-07T09:02:52Z",
      "categories": [
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09747v2",
      "landing_url": "https://arxiv.org/abs/2312.09747v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.09747"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "SELM clearly fits by quantizing speech into discrete SSL-based tokens modeled by an LM and decoding them for enhancement, so it meets the inclusion focus on discrete audio tokens and provides relevant reconstruction/testing context before inclusion."
    },
    "round-A_JuniorNano_reasoning": "SELM clearly fits by quantizing speech into discrete SSL-based tokens modeled by an LM and decoding them for enhancement, so it meets the inclusion focus on discrete audio tokens and provides relevant reconstruction/testing context before inclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
    "abstract": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
    "metadata": {
      "arxiv_id": "2312.09911",
      "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
      "summary": "Amphion is an open-source toolkit for Audio, Music, and Speech Generation, targeting to ease the way for junior researchers and engineers into these fields. It presents a unified framework that includes diverse generation tasks and models, with the added bonus of being easily extendable for new incorporation. The toolkit is designed with beginner-friendly workflows and pre-trained models, allowing both beginners and seasoned researchers to kick-start their projects with relative ease. The initial release of Amphion v0.1 supports a range of tasks including Text to Speech (TTS), Text to Audio (TTA), and Singing Voice Conversion (SVC), supplemented by essential components like data preprocessing, state-of-the-art vocoders, and evaluation metrics. This paper presents a high-level overview of Amphion. Amphion is open-sourced at https://github.com/open-mmlab/Amphion.",
      "authors": [
        "Xueyao Zhang",
        "Liumeng Xue",
        "Yicheng Gu",
        "Yuancheng Wang",
        "Jiaqi Li",
        "Haorui He",
        "Chaoren Wang",
        "Songting Liu",
        "Xi Chen",
        "Junan Zhang",
        "Zihao Fang",
        "Haopeng Chen",
        "Tze Ying Tang",
        "Lexiao Zou",
        "Mingxuan Wang",
        "Jun Han",
        "Kai Chen",
        "Haizhou Li",
        "Zhizheng Wu"
      ],
      "published": "2023-12-15T16:23:21Z",
      "updated": "2024-09-16T11:35:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.09911v3",
      "landing_url": "https://arxiv.org/abs/2312.09911v3",
      "doi": "https://doi.org/10.48550/arXiv.2312.09911"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes an open-source audio generation toolkit and not discrete tokenization/quantization methods or evaluations, so it fails the inclusion focus on explicit discrete audio token research."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes an open-source audio generation toolkit and not discrete tokenization/quantization methods or evaluations, so it fails the inclusion focus on explicit discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "T2M-HiFiGPT: Generating High Quality Human Motion from Textual Descriptions with Residual Discrete Representations",
    "abstract": "In this study, we introduce T2M-HiFiGPT, a novel conditional generative framework for synthesizing human motion from textual descriptions. This framework is underpinned by a Residual Vector Quantized Variational AutoEncoder (RVQ-VAE) and a double-tier Generative Pretrained Transformer (GPT) architecture. We demonstrate that our CNN-based RVQ-VAE is capable of producing highly accurate 2D temporal-residual discrete motion representations. Our proposed double-tier GPT structure comprises a temporal GPT and a residual GPT. The temporal GPT efficiently condenses information from previous frames and textual descriptions into a 1D context vector. This vector then serves as a context prompt for the residual GPT, which generates the final residual discrete indices. These indices are subsequently transformed back into motion data by the RVQ-VAE decoder. To mitigate the exposure bias issue, we employ straightforward code corruption techniques for RVQ and a conditional dropout strategy, resulting in enhanced synthesis performance. Remarkably, T2M-HiFiGPT not only simplifies the generative process but also surpasses existing methods in both performance and parameter efficacy, including the latest diffusion-based and GPT-based models. On the HumanML3D and KIT-ML datasets, our framework achieves exceptional results across nearly all primary metrics. We further validate the efficacy of our framework through comprehensive ablation studies on the HumanML3D dataset, examining the contribution of each component. Our findings reveal that RVQ-VAE is more adept at capturing precise 3D human motion with comparable computational demand compared to its VQ-VAE counterparts. As a result, T2M-HiFiGPT enables the generation of human motion with significantly increased accuracy, outperforming recent state-of-the-art approaches such as T2M-GPT and Att-T2M.",
    "metadata": {
      "arxiv_id": "2312.10628",
      "title": "T2M-HiFiGPT: Generating High Quality Human Motion from Textual Descriptions with Residual Discrete Representations",
      "summary": "In this study, we introduce T2M-HiFiGPT, a novel conditional generative framework for synthesizing human motion from textual descriptions. This framework is underpinned by a Residual Vector Quantized Variational AutoEncoder (RVQ-VAE) and a double-tier Generative Pretrained Transformer (GPT) architecture. We demonstrate that our CNN-based RVQ-VAE is capable of producing highly accurate 2D temporal-residual discrete motion representations. Our proposed double-tier GPT structure comprises a temporal GPT and a residual GPT. The temporal GPT efficiently condenses information from previous frames and textual descriptions into a 1D context vector. This vector then serves as a context prompt for the residual GPT, which generates the final residual discrete indices. These indices are subsequently transformed back into motion data by the RVQ-VAE decoder. To mitigate the exposure bias issue, we employ straightforward code corruption techniques for RVQ and a conditional dropout strategy, resulting in enhanced synthesis performance. Remarkably, T2M-HiFiGPT not only simplifies the generative process but also surpasses existing methods in both performance and parameter efficacy, including the latest diffusion-based and GPT-based models. On the HumanML3D and KIT-ML datasets, our framework achieves exceptional results across nearly all primary metrics. We further validate the efficacy of our framework through comprehensive ablation studies on the HumanML3D dataset, examining the contribution of each component. Our findings reveal that RVQ-VAE is more adept at capturing precise 3D human motion with comparable computational demand compared to its VQ-VAE counterparts. As a result, T2M-HiFiGPT enables the generation of human motion with significantly increased accuracy, outperforming recent state-of-the-art approaches such as T2M-GPT and Att-T2M.",
      "authors": [
        "Congyi Wang"
      ],
      "published": "2023-12-17T06:58:31Z",
      "updated": "2023-12-24T01:31:26Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.10628v2",
      "landing_url": "https://arxiv.org/abs/2312.10628v2",
      "doi": "https://doi.org/10.48550/arXiv.2312.10628"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on human motion generation using residual discrete motion representations rather than discrete audio tokens/codecs, so it fails to match the required audio-token topic and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on human motion generation using residual discrete motion representations rather than discrete audio tokens/codecs, so it fails to match the required audio-token topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AE-Flow: AutoEncoder Normalizing Flow",
    "abstract": "Recently normalizing flows have been gaining traction in text-to-speech (TTS) and voice conversion (VC) due to their state-of-the-art (SOTA) performance. Normalizing flows are unsupervised generative models. In this paper, we introduce supervision to the training process of normalizing flows, without the need for parallel data. We call this training paradigm AutoEncoder Normalizing Flow (AE-Flow). It adds a reconstruction loss forcing the model to use information from the conditioning to reconstruct an audio sample. Our goal is to understand the impact of each component and find the right combination of the negative log-likelihood (NLL) and the reconstruction loss in training normalizing flows with coupling blocks. For that reason we will compare flow-based mapping model trained with: (i) NLL loss, (ii) NLL and reconstruction losses, as well as (iii) reconstruction loss only. Additionally, we compare our model with SOTA VC baseline. The models are evaluated in terms of naturalness, speaker similarity, intelligibility in many-to-many and many-to-any VC settings. The results show that the proposed training paradigm systematically improves speaker similarity and naturalness when compared to regular training methods of normalizing flows. Furthermore, we show that our method improves speaker similarity and intelligibility over the state-of-the-art.",
    "metadata": {
      "arxiv_id": "2312.16552",
      "title": "AE-Flow: AutoEncoder Normalizing Flow",
      "summary": "Recently normalizing flows have been gaining traction in text-to-speech (TTS) and voice conversion (VC) due to their state-of-the-art (SOTA) performance. Normalizing flows are unsupervised generative models. In this paper, we introduce supervision to the training process of normalizing flows, without the need for parallel data. We call this training paradigm AutoEncoder Normalizing Flow (AE-Flow). It adds a reconstruction loss forcing the model to use information from the conditioning to reconstruct an audio sample. Our goal is to understand the impact of each component and find the right combination of the negative log-likelihood (NLL) and the reconstruction loss in training normalizing flows with coupling blocks. For that reason we will compare flow-based mapping model trained with: (i) NLL loss, (ii) NLL and reconstruction losses, as well as (iii) reconstruction loss only. Additionally, we compare our model with SOTA VC baseline. The models are evaluated in terms of naturalness, speaker similarity, intelligibility in many-to-many and many-to-any VC settings. The results show that the proposed training paradigm systematically improves speaker similarity and naturalness when compared to regular training methods of normalizing flows. Furthermore, we show that our method improves speaker similarity and intelligibility over the state-of-the-art.",
      "authors": [
        "Jakub Mosiński",
        "Piotr Biliński",
        "Thomas Merritt",
        "Abdelhamid Ezzerg",
        "Daniel Korzekwa"
      ],
      "published": "2023-12-27T12:29:21Z",
      "updated": "2023-12-27T12:29:21Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2312.16552v1",
      "landing_url": "https://arxiv.org/abs/2312.16552v1",
      "doi": "https://doi.org/10.48550/arXiv.2312.16552"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on training normalizing flows with reconstruction loss for VC/TTS using continuous representations, without discrete tokenization, quantization, or vocabulary details, so it fails to meet inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on training normalizing flows with reconstruction loss for VC/TTS using continuous representations, without discrete tokenization, quantization, or vocabulary details, so it fails to meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation",
    "abstract": "Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention. However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment. Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource. Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lacking in prior TTA works. Through comprehensive ablation studies and innovative cross-attention map visualizations, we provide insightful assessments of text-audio alignment in TTA. Our findings reveal Auffusion's superior capability in generating audios that accurately match textual descriptions, which further demonstrated in several related tasks, such as audio style transfer, inpainting and other manipulations. Our implementation and demos are available at https://auffusion.github.io.",
    "metadata": {
      "arxiv_id": "2401.01044",
      "title": "Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation",
      "summary": "Recent advancements in diffusion models and large language models (LLMs) have significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning AIGC application designed to generate audio from natural language prompts, is attracting increasing attention. However, existing TTA studies often struggle with generation quality and text-audio alignment, especially for complex textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I) diffusion models, we introduce Auffusion, a TTA system adapting T2I model frameworks to TTA task, by effectively leveraging their inherent generative strengths and precise cross-modal alignment. Our objective and subjective evaluations demonstrate that Auffusion surpasses previous TTA approaches using limited data and computational resource. Furthermore, previous studies in T2I recognizes the significant impact of encoder choice on cross-modal alignment, like fine-grained details and object bindings, while similar evaluation is lacking in prior TTA works. Through comprehensive ablation studies and innovative cross-attention map visualizations, we provide insightful assessments of text-audio alignment in TTA. Our findings reveal Auffusion's superior capability in generating audios that accurately match textual descriptions, which further demonstrated in several related tasks, such as audio style transfer, inpainting and other manipulations. Our implementation and demos are available at https://auffusion.github.io.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-01-02T05:42:14Z",
      "updated": "2024-01-02T05:42:14Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01044v1",
      "landing_url": "https://arxiv.org/abs/2401.01044v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01044"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The abstract focuses on diffusion/LLM-based text-to-audio without describing discrete audio tokens, tokenizers, or codec details, so it fails the inclusion criteria and should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on diffusion/LLM-based text-to-audio without describing discrete audio tokens, tokenizers, or codec details, so it fails the inclusion criteria and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MOC-RVQ: Multilevel Codebook-Assisted Digital Generative Semantic Communication",
    "abstract": "Vector quantization-based image semantic communication systems have successfully boosted transmission efficiency, but face challenges with conflicting requirements between codebook design and digital constellation modulation. Traditional codebooks need wide index ranges, while modulation favors few discrete states. To address this, we propose a multilevel generative semantic communication system with a two-stage training framework. In the first stage, we train a high-quality codebook, using a multi-head octonary codebook (MOC) to compress the index range. In addition, a residual vector quantization (RVQ) mechanism is also integrated for effective multilevel communication. In the second stage, a noise reduction block (NRB) based on Swin Transformer is introduced, coupled with the multilevel codebook from the first stage, serving as a high-quality semantic knowledge base (SKB) for generative feature restoration. Finally, to simulate modern image transmission scenarios, we employ a diverse collection of high-resolution 2K images as the test set. The experimental results consistently demonstrate the superior performance of MOC-RVQ over conventional methods such as BPG or JPEG. Additionally, MOC-RVQ achieves comparable performance to an analog JSCC scheme, while needing only one-sixth of the channel bandwidth ratio (CBR) and being directly compatible with digital transmission systems.",
    "metadata": {
      "arxiv_id": "2401.01272",
      "title": "MOC-RVQ: Multilevel Codebook-Assisted Digital Generative Semantic Communication",
      "summary": "Vector quantization-based image semantic communication systems have successfully boosted transmission efficiency, but face challenges with conflicting requirements between codebook design and digital constellation modulation. Traditional codebooks need wide index ranges, while modulation favors few discrete states. To address this, we propose a multilevel generative semantic communication system with a two-stage training framework. In the first stage, we train a high-quality codebook, using a multi-head octonary codebook (MOC) to compress the index range. In addition, a residual vector quantization (RVQ) mechanism is also integrated for effective multilevel communication. In the second stage, a noise reduction block (NRB) based on Swin Transformer is introduced, coupled with the multilevel codebook from the first stage, serving as a high-quality semantic knowledge base (SKB) for generative feature restoration. Finally, to simulate modern image transmission scenarios, we employ a diverse collection of high-resolution 2K images as the test set. The experimental results consistently demonstrate the superior performance of MOC-RVQ over conventional methods such as BPG or JPEG. Additionally, MOC-RVQ achieves comparable performance to an analog JSCC scheme, while needing only one-sixth of the channel bandwidth ratio (CBR) and being directly compatible with digital transmission systems.",
      "authors": [
        "Yingbin Zhou",
        "Yaping Sun",
        "Guanying Chen",
        "Xiaodong Xu",
        "Hao Chen",
        "Binhong Huang",
        "Shuguang Cui",
        "Ping Zhang"
      ],
      "published": "2024-01-02T16:17:43Z",
      "updated": "2024-09-28T10:48:14Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01272v2",
      "landing_url": "https://arxiv.org/abs/2401.01272v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.01272"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on multilevel image semantic communication using vector quantized codebooks, not discrete audio token generation or evaluation, so it fails the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on multilevel image semantic communication using vector quantized codebooks, not discrete audio token generation or evaluation, so it fails the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
    "abstract": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
    "metadata": {
      "arxiv_id": "2401.01498",
      "title": "Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction",
      "summary": "We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similarity, both objectively and subjectively. We also delve into the inference speed and prosody control capabilities of our approach, highlighting the potential of neural transducers in TTS frameworks.",
      "authors": [
        "Minchan Kim",
        "Myeonghun Jeong",
        "Byoung Jin Choi",
        "Semin Kim",
        "Joun Yeop Lee",
        "Nam Soo Kim"
      ],
      "published": "2024-01-03T02:03:36Z",
      "updated": "2024-01-03T02:03:36Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01498v1",
      "landing_url": "https://arxiv.org/abs/2401.01498v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01498"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper uses quantized semantic tokens from wav2vec2.0 as discrete units and centers the pipeline on token-level modeling and generation, so it meets inclusion criteria fully."
    },
    "round-A_JuniorNano_reasoning": "Paper uses quantized semantic tokens from wav2vec2.0 as discrete units and centers the pipeline on token-level modeling and generation, so it meets inclusion criteria fully.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
    "abstract": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
    "metadata": {
      "arxiv_id": "2401.01755",
      "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
      "summary": "Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.",
      "authors": [
        "Muyang Du",
        "Chuan Liu",
        "Junjie Lai"
      ],
      "published": "2024-01-03T14:17:35Z",
      "updated": "2024-01-03T14:17:35Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.01755v1",
      "landing_url": "https://arxiv.org/abs/2401.01755v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.01755"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discrete audio tokenization or quantized vocab-based modeling is described—FastPitch operates on continuous mel features—so it fails to meet inclusion criteria (it focuses on continuous speech synthesis latency)."
    },
    "round-A_JuniorNano_reasoning": "No discrete audio tokenization or quantized vocab-based modeling is described—FastPitch operates on continuous mel features—so it fails to meet inclusion criteria (it focuses on continuous speech synthesis latency).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pheme: Efficient and Conversational Speech Generation",
    "abstract": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
    "metadata": {
      "arxiv_id": "2401.02839",
      "title": "Pheme: Efficient and Conversational Speech Generation",
      "summary": "In recent years, speech generation has seen remarkable progress, now achieving one-shot generation capability that is often virtually indistinguishable from real human voice. Integrating such advancements in speech generation with large language models might revolutionize a wide range of applications. However, certain applications, such as assistive conversational systems, require natural and conversational speech generation tools that also operate efficiently in real time. Current state-of-the-art models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs, require large neural components and extensive training data to work well. In contrast, MQTTS aims to build more compact conversational TTS models while capitalizing on smaller-scale real-life conversational speech data. However, its autoregressive nature yields high inference latency and thus limits its real-time usage. In order to mitigate the current limitations of the state-of-the-art TTS models while capitalizing on their strengths, in this work we introduce the Pheme model series that 1) offers compact yet high-performing models, 2) allows for parallel speech generation of 3) natural conversational speech, and 4) it can be trained efficiently on smaller-scale conversational data, cutting data demands by more than 10x but still matching the quality of the autoregressive TTS models. We also show that through simple teacher-student distillation we can meet significant improvements in voice quality for single-speaker setups on top of pretrained Pheme checkpoints, relying solely on synthetic speech generated by much larger teacher models. Audio samples and pretrained models are available online.",
      "authors": [
        "Paweł Budzianowski",
        "Taras Sereda",
        "Tomasz Cichy",
        "Ivan Vulić"
      ],
      "published": "2024-01-05T14:47:20Z",
      "updated": "2024-01-05T14:47:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.02839v1",
      "landing_url": "https://arxiv.org/abs/2401.02839v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.02839"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on efficient conversational TTS and distillation but never discusses discrete audio-token/codec design, quantization, token vocabularies, or evaluations tied to tokens, so it fails the inclusion scope and fits exclusion rule about continuous representations only."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on efficient conversational TTS and distillation but never discusses discrete audio-token/codec design, quantization, token vocabularies, or evaluations tied to tokens, so it fails the inclusion scope and fits exclusion rule about continuous representations only.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
    "abstract": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
    "metadata": {
      "arxiv_id": "2401.03195",
      "title": "Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features",
      "summary": "Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.",
      "authors": [
        "Ali Falahati",
        "Mohammad Karim Safavi",
        "Ardavan Elahi",
        "Farhad Pakdaman",
        "Moncef Gabbouj"
      ],
      "published": "2024-01-06T11:37:20Z",
      "updated": "2024-03-14T03:59:19Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03195v2",
      "landing_url": "https://arxiv.org/abs/2401.03195v2",
      "doi": "https://doi.org/10.1109/MVIP62238.2024.10491154"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on bitrate ladder prediction for video using transfer learning and spatio-temporal features, which neither discusses discrete audio tokens nor describes any tokenizer/quantization pipeline, so it fails all inclusion criteria and matches exclusion criteria about unrelated continuous feature work."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on bitrate ladder prediction for video using transfer learning and spatio-temporal features, which neither discusses discrete audio tokens nor describes any tokenizer/quantization pipeline, so it fails all inclusion criteria and matches exclusion criteria about unrelated continuous feature work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Creating Personalized Synthetic Voices from Articulation Impaired Speech Using Augmented Reconstruction Loss",
    "abstract": "This research is about the creation of personalized synthetic voices for head and neck cancer survivors. It is focused particularly on tongue cancer patients whose speech might exhibit severe articulation impairment. Our goal is to restore normal articulation in the synthesized speech, while maximally preserving the target speaker's individuality in terms of both the voice timbre and speaking style. This is formulated as a task of learning from noisy labels. We propose to augment the commonly used speech reconstruction loss with two additional terms. The first term constitutes a regularization loss that mitigates the impact of distorted articulation in the training speech. The second term is a consistency loss that encourages correct articulation in the generated speech. These additional loss terms are obtained from frame-level articulation scores of original and generated speech, which are derived using a separately trained phone classifier. Experimental results on a real case of tongue cancer patient confirm that the synthetic voice achieves comparable articulation quality to unimpaired natural speech, while effectively maintaining the target speaker's individuality. Audio samples are available at https://myspeechproject.github.io/ArticulationRepair/.",
    "metadata": {
      "arxiv_id": "2401.03816",
      "title": "Creating Personalized Synthetic Voices from Articulation Impaired Speech Using Augmented Reconstruction Loss",
      "summary": "This research is about the creation of personalized synthetic voices for head and neck cancer survivors. It is focused particularly on tongue cancer patients whose speech might exhibit severe articulation impairment. Our goal is to restore normal articulation in the synthesized speech, while maximally preserving the target speaker's individuality in terms of both the voice timbre and speaking style. This is formulated as a task of learning from noisy labels. We propose to augment the commonly used speech reconstruction loss with two additional terms. The first term constitutes a regularization loss that mitigates the impact of distorted articulation in the training speech. The second term is a consistency loss that encourages correct articulation in the generated speech. These additional loss terms are obtained from frame-level articulation scores of original and generated speech, which are derived using a separately trained phone classifier. Experimental results on a real case of tongue cancer patient confirm that the synthetic voice achieves comparable articulation quality to unimpaired natural speech, while effectively maintaining the target speaker's individuality. Audio samples are available at https://myspeechproject.github.io/ArticulationRepair/.",
      "authors": [
        "Yusheng Tian",
        "Jingyu Li",
        "Tan Lee"
      ],
      "published": "2024-01-08T11:10:50Z",
      "updated": "2024-01-08T11:10:50Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.03816v1",
      "landing_url": "https://arxiv.org/abs/2401.03816v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.03816"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on articulation loss for personalized synthetic voice using reconstruction/consistency losses derived from articulation scores, with no mention of discrete token/codec/tokenizer design or evaluation, so it fails to meet the discrete-audio-token inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on articulation loss for personalized synthetic voice using reconstruction/consistency losses derived from articulation scores, with no mention of discrete token/codec/tokenizer design or evaluation, so it fails to meet the discrete-audio-token inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
    "abstract": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
    "metadata": {
      "arxiv_id": "2401.04405",
      "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation",
      "summary": "Adaptive video streaming requires efficient bitrate ladder construction to meet heterogeneous network conditions and end-user demands. Per-title optimized encoding typically traverses numerous encoding parameters to search the Pareto-optimal operating points for each video. Recently, researchers have attempted to predict the content-optimized bitrate ladder for pre-encoding overhead reduction. However, existing methods commonly estimate the encoding parameters on the Pareto front and still require subsequent pre-encodings. In this paper, we propose to directly predict the optimal transcoding resolution at each preset bitrate for efficient bitrate ladder construction. We adopt a Temporal Attentive Gated Recurrent Network to capture spatial-temporal features and predict transcoding resolutions as a multi-task classification problem. We demonstrate that content-optimized bitrate ladders can thus be efficiently determined without any pre-encoding. Our method well approximates the ground-truth bitrate-resolution pairs with a slight Bjøntegaard Delta rate loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.",
      "authors": [
        "Jinhai Yang",
        "Mengxi Guo",
        "Shijie Zhao",
        "Junlin Li",
        "Li Zhang"
      ],
      "published": "2024-01-09T08:01:47Z",
      "updated": "2024-01-09T08:01:47Z",
      "categories": [
        "cs.MM",
        "cs.AI",
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04405v1",
      "landing_url": "https://arxiv.org/abs/2401.04405v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.04405"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on bitrate ladder prediction for video transcoding rather than discrete audio token generation/quantization, so it fails all inclusion criteria and meets exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on bitrate ladder prediction for video transcoding rather than discrete audio token generation/quantization, so it fails all inclusion criteria and meets exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
    "abstract": "Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.",
    "metadata": {
      "arxiv_id": "2401.04658",
      "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
      "summary": "Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.",
      "authors": [
        "Zhen Qin",
        "Weigao Sun",
        "Dong Li",
        "Xuyang Shen",
        "Weixuan Sun",
        "Yiran Zhong"
      ],
      "published": "2024-01-09T16:27:28Z",
      "updated": "2024-01-15T14:57:29Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.04658v2",
      "landing_url": "https://arxiv.org/abs/2401.04658v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.04658"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on attention mechanisms for LLMs and does not describe discrete audio token generation/quantization or anything related to audio tokenization, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on attention mechanisms for LLMs and does not describe discrete audio token generation/quantization or anything related to audio tokenization, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
    "abstract": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
    "metadata": {
      "arxiv_id": "2401.07333",
      "title": "ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering",
      "summary": "The language model (LM) approach based on acoustic and linguistic prompts, such as VALL-E, has achieved remarkable progress in the field of zero-shot audio generation. However, existing methods still have some limitations: 1) repetitions, transpositions, and omissions in the output synthesized speech due to limited alignment constraints between audio and phoneme tokens; 2) challenges of fine-grained control over the synthesized speech with autoregressive (AR) language model; 3) infinite silence generation due to the nature of AR-based decoding, especially under the greedy strategy. To alleviate these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot text-to-speech (TTS) framework, which enables fine-grained control over synthesized audio at the phoneme level. The key to ELLA-V is interleaving sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of the corresponding acoustic tokens. The experimental findings reveal that our model outperforms VALL-E in terms of accuracy and delivers more stable results using both greedy and sampling-based decoding strategies. The code of ELLA-V will be open-sourced after cleanups. Audio samples are available at https://ereboas.github.io/ELLAV/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "published": "2024-01-14T17:43:55Z",
      "updated": "2024-01-14T17:43:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.07333v1",
      "landing_url": "https://arxiv.org/abs/2401.07333v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.07333"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper focuses on interleaved sequences of acoustic and phoneme tokens in a neural codec language model for zero-shot TTS, so it clearly addresses discrete audio-token modeling and avoids any exclusion issues, making it fully includable."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on interleaved sequences of acoustic and phoneme tokens in a neural codec language model for zero-shot TTS, so it clearly addresses discrete audio-token modeling and avoids any exclusion issues, making it fully includable.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TLIC: Learned Image Compression with ROI-Weighted Distortion and Bit Allocation",
    "abstract": "This short paper describes our method for the track of image compression. To achieve better perceptual quality, we use the adversarial loss to generate realistic textures, use region of interest (ROI) mask to guide the bit allocation for different regions. Our Team name is TLIC.",
    "metadata": {
      "arxiv_id": "2401.08154",
      "title": "TLIC: Learned Image Compression with ROI-Weighted Distortion and Bit Allocation",
      "summary": "This short paper describes our method for the track of image compression. To achieve better perceptual quality, we use the adversarial loss to generate realistic textures, use region of interest (ROI) mask to guide the bit allocation for different regions. Our Team name is TLIC.",
      "authors": [
        "Wei Jiang",
        "Yongqi Zhai",
        "Hangyu Li",
        "Ronggang Wang"
      ],
      "published": "2024-01-16T06:53:03Z",
      "updated": "2024-03-23T08:44:14Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.08154v3",
      "landing_url": "https://arxiv.org/abs/2401.08154v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.08154"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract discuss image compression with ROI weighting and adversarial loss, which has nothing to do with discrete audio tokenization or quantized audio codecs, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract discuss image compression with ROI weighting and adversarial loss, which has nothing to do with discrete audio tokenization or quantized audio codecs, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
    "abstract": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
    "metadata": {
      "arxiv_id": "2401.11857",
      "title": "Adversarial speech for voice privacy protection from Personalized Speech generation",
      "summary": "The rapid progress in personalized speech generation technology, including personalized text-to-speech (TTS) and voice conversion (VC), poses a challenge in distinguishing between generated and real speech for human listeners, resulting in an urgent demand in protecting speakers' voices from malicious misuse. In this regard, we propose a speaker protection method based on adversarial attacks. The proposed method perturbs speech signals by minimally altering the original speech while rendering downstream speech generation models unable to accurately generate the voice of the target speaker. For validation, we employ the open-source pre-trained YourTTS model for speech generation and protect the target speaker's speech in the white-box scenario. Automatic speaker verification (ASV) evaluations were carried out on the generated speech as the assessment of the voice protection capability. Our experimental results show that we successfully perturbed the speaker encoder of the YourTTS model using the gradient-based I-FGSM adversarial perturbation method. Furthermore, the adversarial perturbation is effective in preventing the YourTTS model from generating the speech of the target speaker. Audio samples can be found in https://voiceprivacy.github.io/Adeversarial-Speech-with-YourTTS.",
      "authors": [
        "Shihao Chen",
        "Liping Chen",
        "Jie Zhang",
        "KongAik Lee",
        "Zhenhua Ling",
        "Lirong Dai"
      ],
      "published": "2024-01-22T11:26:59Z",
      "updated": "2024-01-22T11:26:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.11857v1",
      "landing_url": "https://arxiv.org/abs/2401.11857v1",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10447699"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on adversarial perturbations for speaker protection from personalized speech generation without any discussion of discrete quantized audio tokenization or codec-based token modeling, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on adversarial perturbations for speaker protection from personalized speech generation without any discussion of discrete quantized audio tokenization or codec-based token modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ScoreDec: A Phase-preserving High-Fidelity Audio Codec with A Generalized Score-based Diffusion Post-filter",
    "abstract": "Although recent mainstream waveform-domain end-to-end (E2E) neural audio codecs achieve impressive coded audio quality with a very low bitrate, the quality gap between the coded and natural audio is still significant. A generative adversarial network (GAN) training is usually required for these E2E neural codecs because of the difficulty of direct phase modeling. However, such adversarial learning hinders these codecs from preserving the original phase information. To achieve human-level naturalness with a reasonable bitrate, preserve the original phase, and get rid of the tricky and opaque GAN training, we develop a score-based diffusion post-filter (SPF) in the complex spectral domain and combine our previous AudioDec with the SPF to propose ScoreDec, which can be trained using only spectral and score-matching losses. Both the objective and subjective experimental results show that ScoreDec with a 24~kbps bitrate encodes and decodes full-band 48~kHz speech with human-level naturalness and well-preserved phase information.",
    "metadata": {
      "arxiv_id": "2401.12160",
      "title": "ScoreDec: A Phase-preserving High-Fidelity Audio Codec with A Generalized Score-based Diffusion Post-filter",
      "summary": "Although recent mainstream waveform-domain end-to-end (E2E) neural audio codecs achieve impressive coded audio quality with a very low bitrate, the quality gap between the coded and natural audio is still significant. A generative adversarial network (GAN) training is usually required for these E2E neural codecs because of the difficulty of direct phase modeling. However, such adversarial learning hinders these codecs from preserving the original phase information. To achieve human-level naturalness with a reasonable bitrate, preserve the original phase, and get rid of the tricky and opaque GAN training, we develop a score-based diffusion post-filter (SPF) in the complex spectral domain and combine our previous AudioDec with the SPF to propose ScoreDec, which can be trained using only spectral and score-matching losses. Both the objective and subjective experimental results show that ScoreDec with a 24~kbps bitrate encodes and decodes full-band 48~kHz speech with human-level naturalness and well-preserved phase information.",
      "authors": [
        "Yi-Chiao Wu",
        "Dejan Marković",
        "Steven Krenn",
        "Israel D. Gebru",
        "Alexander Richard"
      ],
      "published": "2024-01-22T17:54:19Z",
      "updated": "2024-01-22T17:54:19Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.12160v1",
      "landing_url": "https://arxiv.org/abs/2401.12160v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.12160"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on a diffusion post-filter for a neural codec and never discusses quantized/discrete token representations or vocabulary-level modeling, so it fails the discrete audio token inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a diffusion post-filter for a neural codec and never discusses quantized/discrete token representations or vocabulary-level modeling, so it fails the discrete audio token inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Viewport Prediction, Bitrate Selection, and Beamforming Design for THz-Enabled 360° Video Streaming",
    "abstract": "360° videos require significant bandwidth to provide an immersive viewing experience. Wireless systems using terahertz (THz) frequency band can meet this high data rate demand. However, self-blockage is a challenge in such systems. To ensure reliable transmission, this paper explores THz-enabled 360° video streaming through multiple multi-antenna access points (APs). Guaranteeing users' quality of experience (QoE) requires accurate viewport prediction to determine which video tiles to send, followed by asynchronous bitrate selection for those tiles and beamforming design at the APs. To address users' privacy and data heterogeneity, we propose a content-based viewport prediction framework, wherein users' head movement prediction models are trained using a personalized federated learning (PFL) algorithm. To address asynchronous decision-making for tile bitrates and dynamic THz link connections, we formulate the optimization of bitrate selection and beamforming as a macro-action decentralized partially observable Markov decision process (MacDec-POMDP) problem. To efficiently tackle this problem for multiple users, we develop two deep reinforcement learning (DRL) algorithms based on multi-agent actor-critic methods and propose a hierarchical learning framework to train the actor and critic networks. Experimental results show that our proposed approach provides a higher QoE when compared with three benchmark algorithms.",
    "metadata": {
      "arxiv_id": "2401.13114",
      "title": "Viewport Prediction, Bitrate Selection, and Beamforming Design for THz-Enabled 360° Video Streaming",
      "summary": "360° videos require significant bandwidth to provide an immersive viewing experience. Wireless systems using terahertz (THz) frequency band can meet this high data rate demand. However, self-blockage is a challenge in such systems. To ensure reliable transmission, this paper explores THz-enabled 360° video streaming through multiple multi-antenna access points (APs). Guaranteeing users' quality of experience (QoE) requires accurate viewport prediction to determine which video tiles to send, followed by asynchronous bitrate selection for those tiles and beamforming design at the APs. To address users' privacy and data heterogeneity, we propose a content-based viewport prediction framework, wherein users' head movement prediction models are trained using a personalized federated learning (PFL) algorithm. To address asynchronous decision-making for tile bitrates and dynamic THz link connections, we formulate the optimization of bitrate selection and beamforming as a macro-action decentralized partially observable Markov decision process (MacDec-POMDP) problem. To efficiently tackle this problem for multiple users, we develop two deep reinforcement learning (DRL) algorithms based on multi-agent actor-critic methods and propose a hierarchical learning framework to train the actor and critic networks. Experimental results show that our proposed approach provides a higher QoE when compared with three benchmark algorithms.",
      "authors": [
        "Mehdi Setayesh",
        "Vincent W. S. Wong"
      ],
      "published": "2024-01-23T21:50:56Z",
      "updated": "2024-12-07T20:37:59Z",
      "categories": [
        "eess.IV",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13114v2",
      "landing_url": "https://arxiv.org/abs/2401.13114v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.13114"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe THz video streaming without any focus on discrete audio tokens, tokenizers, quantization, or evaluations of discrete audio units, so it fails the inclusion criteria and hits exclusion as non-audio token work."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe THz video streaming without any focus on discrete audio tokens, tokenizers, quantization, or evaluations of discrete audio units, so it fails the inclusion criteria and hits exclusion as non-audio token work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
    "abstract": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
    "metadata": {
      "arxiv_id": "2401.13527",
      "title": "SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation",
      "summary": "Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate that SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable proficiency in capturing and modeling speech's semantic and perceptual dimensions. Code and models are available at https://github.com/0nutation/SpeechGPT.",
      "authors": [
        "Dong Zhang",
        "Xin Zhang",
        "Jun Zhan",
        "Shimin Li",
        "Yaqian Zhou",
        "Xipeng Qiu"
      ],
      "published": "2024-01-24T15:25:01Z",
      "updated": "2024-01-25T17:24:52Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13527v2",
      "landing_url": "https://arxiv.org/abs/2401.13527v2",
      "doi": "https://doi.org/10.48550/arXiv.2401.13527"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Abstract focuses on semantic/perceptual modeling and speech generation with LLMs/flow matching without describing discrete audio tokenizers, quantization, or token-level evaluation, so it fails the inclusion criterion despite being in speech generation."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on semantic/perceptual modeling and speech generation with LLMs/flow matching without describing discrete audio tokenizers, quantization, or token-level evaluation, so it fails the inclusion criterion despite being in speech generation.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Text to speech synthesis",
    "abstract": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
    "metadata": {
      "arxiv_id": "2401.13891",
      "title": "Text to speech synthesis",
      "summary": "Text-to-speech (TTS) synthesis is a technology that converts written text into spoken words, enabling a natural and accessible means of communication. This abstract explores the key aspects of TTS synthesis, encompassing its underlying technologies, applications, and implications for various sectors. The technology utilizes advanced algorithms and linguistic models to convert textual information into life like speech, allowing for enhanced user experiences in diverse contexts such as accessibility tools, navigation systems, and virtual assistants. The abstract delves into the challenges and advancements in TTS synthesis, including considerations for naturalness, multilingual support, and emotional expression in synthesized speech.",
      "authors": [
        "Harini s",
        "Manoj G M"
      ],
      "published": "2024-01-25T02:13:45Z",
      "updated": "2024-01-25T02:13:45Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.13891v1",
      "landing_url": "https://arxiv.org/abs/2401.13891v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.13891"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract only describes general TTS technology without any mention of discrete audio token generation, quantization, or token-based modeling, so it fails to meet the discrete-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract only describes general TTS technology without any mention of discrete audio token generation, quantization, or token-based modeling, so it fails to meet the discrete-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech",
    "abstract": "Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate.",
    "metadata": {
      "arxiv_id": "2401.14321",
      "title": "VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech",
      "summary": "Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate.",
      "authors": [
        "Chenpeng Du",
        "Yiwei Guo",
        "Hankun Wang",
        "Yifan Yang",
        "Zhikang Niu",
        "Shuai Wang",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-01-25T17:19:01Z",
      "updated": "2025-03-14T00:10:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14321v5",
      "landing_url": "https://arxiv.org/abs/2401.14321v5",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890943"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes a generative transducer for decoder-only TTS focusing on alignment/robustness rather than any discrete audio tokenization/quantization, so it fails to meet the discrete token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a generative transducer for decoder-only TTS focusing on alignment/robustness rather than any discrete audio tokenization/quantization, so it fails to meet the discrete token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling",
    "abstract": "Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs' performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.",
    "metadata": {
      "arxiv_id": "2401.14556",
      "title": "Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling",
      "summary": "Pre-trained language models based on masked language modeling (MLM) excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, recent decoder-only large language models (LLMs) perform on par with smaller MLM-based encoders. Although their performance improves with scale, LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL). We hypothesize that LLMs' poor SL performance stems from causal masking, which prevents the model from attending to tokens on the right of the current token. Yet, how exactly and to what extent LLMs' performance on SL can be improved remains unclear. We explore techniques for improving the SL performance of open LLMs on IE tasks by applying layer-wise removal of the causal mask (CM) during LLM fine-tuning. This approach yields performance gains competitive with state-of-the-art SL models, matching or outperforming the results of CM removal from all blocks. Our findings hold for diverse SL tasks, demonstrating that open LLMs with layer-dependent CM removal outperform strong MLM-based encoders and even instruction-tuned LLMs.",
      "authors": [
        "David Dukić",
        "Jan Šnajder"
      ],
      "published": "2024-01-25T22:50:48Z",
      "updated": "2024-06-06T12:33:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.14556v3",
      "landing_url": "https://arxiv.org/abs/2401.14556v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.14556"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe fine-tuning decoder-only LLMs for textual sequence labeling, so it does not focus on discrete audio token generation or quantization as required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe fine-tuning decoder-only LLMs for textual sequence labeling, so it does not focus on discrete audio token generation or quantization as required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MunTTS: A Text-to-Speech System for Mundari",
    "abstract": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
    "metadata": {
      "arxiv_id": "2401.15579",
      "title": "MunTTS: A Text-to-Speech System for Mundari",
      "summary": "We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work addresses the gap in linguistic technology for underrepresented languages by collecting and processing data to build a speech synthesis system. We begin our study by gathering a substantial dataset of Mundari text and speech and train end-to-end speech models. We also delve into the methods used for training our models, ensuring they are efficient and effective despite the data constraints. We evaluate our system with native speakers and objective metrics, demonstrating its potential as a tool for preserving and promoting the Mundari language in the digital age.",
      "authors": [
        "Varun Gumma",
        "Rishav Hada",
        "Aditya Yadavalli",
        "Pamir Gogoi",
        "Ishani Mondal",
        "Vivek Seshadri",
        "Kalika Bali"
      ],
      "published": "2024-01-28T06:27:17Z",
      "updated": "2024-01-28T06:27:17Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.15579v1",
      "landing_url": "https://arxiv.org/abs/2401.15579v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.15579"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes a TTS system for Mundari without any mention of discrete audio tokenization, codec/token design, or discrete vocabulary modeling, so it fails to meet the core inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a TTS system for Mundari without any mention of discrete audio tokenization, codec/token design, or discrete vocabulary modeling, so it fails to meet the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
    "abstract": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
    "metadata": {
      "arxiv_id": "2401.16812",
      "title": "SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics",
      "summary": "While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.",
      "authors": [
        "Takaaki Saeki",
        "Soumi Maiti",
        "Shinnosuke Takamichi",
        "Shinji Watanabe",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-01-30T08:26:28Z",
      "updated": "2024-09-01T14:34:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.16812v3",
      "landing_url": "https://arxiv.org/abs/2401.16812v3",
      "doi": "https://doi.org/10.48550/arXiv.2401.16812"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on reference-aware evaluation metrics for speech generation using dense/semantic representations rather than proposing or assessing discrete audio tokenization or token-level modeling, so it does not meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on reference-aware evaluation metrics for speech generation using dense/semantic representations rather than proposing or assessing discrete audio tokenization or token-level modeling, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning",
    "abstract": "We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP employs two acoustic representation models, EnCodec and CLAP, along with a pretrained language model, BART. We also introduce a new training objective called masked codec modeling that improves acoustic awareness of the pretrained language model. Experimental results on AudioCaps and Clotho demonstrate that our model surpasses the performance of baseline models. Source code will be available at https://github.com/jaeyeonkim99/EnCLAP . An online demo is available at https://huggingface.co/spaces/enclap-team/enclap .",
    "metadata": {
      "arxiv_id": "2401.17690",
      "title": "EnCLAP: Combining Neural Audio Codec and Audio-Text Joint Embedding for Automated Audio Captioning",
      "summary": "We propose EnCLAP, a novel framework for automated audio captioning. EnCLAP employs two acoustic representation models, EnCodec and CLAP, along with a pretrained language model, BART. We also introduce a new training objective called masked codec modeling that improves acoustic awareness of the pretrained language model. Experimental results on AudioCaps and Clotho demonstrate that our model surpasses the performance of baseline models. Source code will be available at https://github.com/jaeyeonkim99/EnCLAP . An online demo is available at https://huggingface.co/spaces/enclap-team/enclap .",
      "authors": [
        "Jaeyeon Kim",
        "Jaeyoon Jung",
        "Jinjoo Lee",
        "Sang Hoon Woo"
      ],
      "published": "2024-01-31T09:23:16Z",
      "updated": "2024-01-31T09:23:16Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2401.17690v1",
      "landing_url": "https://arxiv.org/abs/2401.17690v1",
      "doi": "https://doi.org/10.48550/arXiv.2401.17690"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Title/abstract describe an audio-captioning system built on EnCodec/CLAP but the study’s core focus is downstream captioning performance rather than defining or evaluating discrete audio token generation/tokenizer mechanisms, so it likely fails the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe an audio-captioning system built on EnCodec/CLAP but the study’s core focus is downstream captioning performance rather than defining or evaluating discrete audio token generation/tokenizer mechanisms, so it likely fails the inclusion scope.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Exploring the limits of decoder-only models trained on public speech recognition corpora",
    "abstract": "The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.",
    "metadata": {
      "arxiv_id": "2402.00235",
      "title": "Exploring the limits of decoder-only models trained on public speech recognition corpora",
      "summary": "The emergence of industrial-scale speech recognition (ASR) models such as Whisper and USM, trained on 1M hours of weakly labelled and 12M hours of audio only proprietary data respectively, has led to a stronger need for large scale public ASR corpora and competitive open source pipelines. Unlike the said models, large language models are typically based on Transformer decoders, and it remains unclear if decoder-only models trained on public data alone can deliver competitive performance. In this work, we investigate factors such as choice of training datasets and modeling components necessary for obtaining the best performance using public English ASR corpora alone. Our Decoder-Only Transformer for ASR (DOTA) model comprehensively outperforms the encoder-decoder open source replication of Whisper (OWSM) on nearly all English ASR benchmarks and outperforms Whisper large-v3 on 7 out of 15 test sets. We release our codebase and model checkpoints under permissive license.",
      "authors": [
        "Ankit Gupta",
        "George Saon",
        "Brian Kingsbury"
      ],
      "published": "2024-01-31T23:29:42Z",
      "updated": "2024-01-31T23:29:42Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.00235v1",
      "landing_url": "https://arxiv.org/abs/2402.00235v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.00235"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about training a decoder-only ASR model on public corpora and does not describe discrete audio tokenization, quantization, vocabularies, or token-level evaluations, so it fails the inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper is about training a decoder-only ASR model on public corpora and does not describe discrete audio tokenization, quantization, vocabularies, or token-level evaluations, so it fails the inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PAM: Prompting Audio-Language Models for Audio Quality Assessment",
    "abstract": "While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other \"reference-free\" metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.",
    "metadata": {
      "arxiv_id": "2402.00282",
      "title": "PAM: Prompting Audio-Language Models for Audio Quality Assessment",
      "summary": "While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other \"reference-free\" metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.",
      "authors": [
        "Soham Deshmukh",
        "Dareen Alharthi",
        "Benjamin Elizalde",
        "Hannes Gamper",
        "Mahmoud Al Ismail",
        "Rita Singh",
        "Bhiksha Raj",
        "Huaming Wang"
      ],
      "published": "2024-02-01T02:15:59Z",
      "updated": "2024-02-01T02:15:59Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.00282v1",
      "landing_url": "https://arxiv.org/abs/2402.00282v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.00282"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on using audio-language models for quality assessment without discussing discrete-token codecs or tokenizer/quantization designs, so it does not meet the discrete audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on using audio-language models for quality assessment without discussing discrete-token codecs or tokenizer/quantization designs, so it does not meet the discrete audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
    "abstract": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
    "metadata": {
      "arxiv_id": "2402.01271",
      "title": "An Intra-BRNN and GB-RVQ Based END-TO-END Neural Audio Codec",
      "summary": "Recently, neural networks have proven to be effective in performing speech coding task at low bitrates. However, under-utilization of intra-frame correlations and the error of quantizer specifically degrade the reconstructed audio quality. To improve the coding quality, we present an end-to-end neural speech codec, namely CBRC (Convolutional and Bidirectional Recurrent neural Codec). An interleaved structure using 1D-CNN and Intra-BRNN is designed to exploit the intra-frame correlations more efficiently. Furthermore, Group-wise and Beam-search Residual Vector Quantizer (GB-RVQ) is used to reduce the quantization noise. CBRC encodes audio every 20ms with no additional latency, which is suitable for real-time communication. Experimental results demonstrate the superiority of the proposed codec when comparing CBRC at 3kbps with Opus at 12kbps.",
      "authors": [
        "Linping Xu",
        "Jiawei Jiang",
        "Dejun Zhang",
        "Xianjun Xia",
        "Li Chen",
        "Yijian Xiao",
        "Piao Ding",
        "Shenyi Song",
        "Sixing Yin",
        "Ferdous Sohel"
      ],
      "published": "2024-02-02T09:55:15Z",
      "updated": "2024-02-02T09:55:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01271v1",
      "landing_url": "https://arxiv.org/abs/2402.01271v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.01271"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "CBRC is an end-to-end neural speech codec that explicitly uses vector quantization (GB-RVQ) to produce discrete codes for real-time communication with reported quality comparisons, so it matches the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "CBRC is an end-to-end neural speech codec that explicitly uses vector quantization (GB-RVQ) to produce discrete codes for real-time communication with reported quality comparisons, so it matches the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
    "abstract": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
    "metadata": {
      "arxiv_id": "2402.01708",
      "title": "Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators",
      "summary": "The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields a taxonomy that can support appropriate policy interventions and decision making for the responsible development and release of speech generation models.",
      "authors": [
        "Wiebke Hutiri",
        "Oresiti Papakyriakopoulos",
        "Alice Xiang"
      ],
      "published": "2024-01-25T11:47:06Z",
      "updated": "2024-05-15T15:26:42Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CY",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01708v2",
      "landing_url": "https://arxiv.org/abs/2402.01708v2",
      "doi": "https://doi.org/10.1145/3630106.3658911"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on ethical/safety taxonomy of speech generation incidents rather than discrete audio token codecs, quantization, vocabularies, or token-level modeling, so it fails the inclusion criteria and matches the exclusion of non-token-research."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on ethical/safety taxonomy of speech generation incidents rather than discrete audio token codecs, quantization, vocabularies, or token-level modeling, so it fails the inclusion criteria and matches the exclusion of non-token-research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
    "abstract": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
    "metadata": {
      "arxiv_id": "2402.01831",
      "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities",
      "summary": "Augmenting large language models (LLMs) to understand audio -- including non-speech sounds and non-verbal speech -- is critically important for diverse real-world applications of LLMs. In this paper, we propose Audio Flamingo, a novel audio language model with 1) strong audio understanding abilities, 2) the ability to quickly adapt to unseen tasks via in-context learning and retrieval, and 3) strong multi-turn dialogue abilities. We introduce a series of training techniques, architecture design, and data strategies to enhance our model with these abilities. Extensive evaluations across various audio understanding tasks confirm the efficacy of our method, setting new state-of-the-art benchmarks. Our demo website is https://audioflamingo.github.io/ and the code is open-sourced at https://github.com/NVIDIA/audio-flamingo.",
      "authors": [
        "Zhifeng Kong",
        "Arushi Goel",
        "Rohan Badlani",
        "Wei Ping",
        "Rafael Valle",
        "Bryan Catanzaro"
      ],
      "published": "2024-02-02T18:58:34Z",
      "updated": "2024-05-28T05:44:53Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.01831v3",
      "landing_url": "https://arxiv.org/abs/2402.01831v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.01831"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract only describes an audio-LLM with dialogue abilities but doesn’t mention any discrete audio token/quantization/tokenizer design or evaluation, so it fails the inclusion focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract only describes an audio-LLM with dialogue abilities but doesn’t mention any discrete audio token/quantization/tokenizer design or evaluation, so it fails the inclusion focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
    "abstract": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
    "metadata": {
      "arxiv_id": "2402.02302",
      "title": "Predicting positive transfer for improved low-resource speech recognition using acoustic pseudo-tokens",
      "summary": "While massively multilingual speech models like wav2vec 2.0 XLSR-128 can be directly fine-tuned for automatic speech recognition (ASR), downstream performance can still be relatively poor on languages that are under-represented in the pre-training data. Continued pre-training on 70-200 hours of untranscribed speech in these languages can help -- but what about languages without that much recorded data? For such cases, we show that supplementing the target language with data from a similar, higher-resource 'donor' language can help. For example, continued pre-training on only 10 hours of low-resource Punjabi supplemented with 60 hours of donor Hindi is almost as good as continued pretraining on 70 hours of Punjabi. By contrast, sourcing data from less similar donors like Bengali does not improve ASR performance. To inform donor language selection, we propose a novel similarity metric based on the sequence distribution of induced acoustic units: the Acoustic Token Distribution Similarity (ATDS). Across a set of typologically different target languages (Punjabi, Galician, Iban, Setswana), we show that the ATDS between the target language and its candidate donors precisely predicts target language ASR performance.",
      "authors": [
        "Nay San",
        "Georgios Paraskevopoulos",
        "Aryaman Arora",
        "Xiluo He",
        "Prabhjot Kaur",
        "Oliver Adams",
        "Dan Jurafsky"
      ],
      "published": "2024-02-03T23:54:03Z",
      "updated": "2024-02-03T23:54:03Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02302v1",
      "landing_url": "https://arxiv.org/abs/2402.02302v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.02302"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "The abstract focuses on donor language selection via an Acoustic Token Distribution Similarity metric derived from induced ‘‘acoustic units’’, but it is unclear whether these units are rigorously defined discrete tokens or if token/codec specifications and evaluations are detailed, so I cannot confidently include it."
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on donor language selection via an Acoustic Token Distribution Similarity metric derived from induced ‘‘acoustic units’’, but it is unclear whether these units are rigorously defined discrete tokens or if token/codec specifications and evaluations are detailed, so I cannot confidently include it.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss",
    "abstract": "Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.",
    "metadata": {
      "arxiv_id": "2402.02936",
      "title": "Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss",
      "summary": "Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use the Cubemap Projection (CMP) format as network input. The generator employs gated convolutions to distinguish valid pixels from invalid ones, while a side branch is designed utilizing contextual reconstruction (CR) loss to guide the generators to find the most suitable reference patch for inpainting the missing region. The proposed method is compared with state-of-the-art (SOTA) methods on SUN360 Street View dataset in terms of PSNR and SSIM. Experimental results and ablation study demonstrate that the proposed method outperforms SOTA both quantitatively and qualitatively.",
      "authors": [
        "Li Yu",
        "Yanjun Gao",
        "Farhad Pakdaman",
        "Moncef Gabbouj"
      ],
      "published": "2024-02-05T11:58:08Z",
      "updated": "2024-02-05T11:58:08Z",
      "categories": [
        "eess.IV",
        "cs.CV",
        "cs.LG",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.02936v1",
      "landing_url": "https://arxiv.org/abs/2402.02936v1",
      "doi": "https://doi.org/10.1109/ICASSP48485.2024.10446469"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about panoramic image inpainting using gated convolutions rather than any form of discrete audio tokenization or codec work, so it does not meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper is about panoramic image inpainting using gated convolutions rather than any form of discrete audio tokenization or codec work, so it does not meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
    "abstract": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements. We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
    "metadata": {
      "arxiv_id": "2402.03158",
      "title": "Optimal and Near-Optimal Adaptive Vector Quantization",
      "summary": "Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \\emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.\n  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.",
      "authors": [
        "Ran Ben-Basat",
        "Yaniv Ben-Itzhak",
        "Michael Mitzenmacher",
        "Shay Vargaftik"
      ],
      "published": "2024-02-05T16:27:59Z",
      "updated": "2025-07-31T13:53:50Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT",
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03158v2",
      "landing_url": "https://arxiv.org/abs/2402.03158v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.03158"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract only discusses general adaptive vector quantization for ML workloads, with no reference to audio or discrete audio tokenization requirements, so it fails the inclusion focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract only discusses general adaptive vector quantization for ML workloads, with no reference to audio or discrete audio tokenization requirements, so it fails the inclusion focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
    "abstract": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
    "metadata": {
      "arxiv_id": "2402.03407",
      "title": "Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations",
      "summary": "Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",
      "authors": [
        "Álvaro Martín-Cortinas",
        "Daniel Sáez-Trigueros",
        "Iván Vallés-Pérez",
        "Biel Tura-Vecino",
        "Piotr Biliński",
        "Mateusz Lajszczak",
        "Grzegorz Beringer",
        "Roberto Barra-Chicote",
        "Jaime Lorenzo-Trueba"
      ],
      "published": "2024-02-05T15:08:19Z",
      "updated": "2024-02-05T15:08:19Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03407v1",
      "landing_url": "https://arxiv.org/abs/2402.03407v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03407"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Despite dealing with LLM-based speech generation, the work focuses on self-supervised speaker-disentangled representations without any description of discrete-token/quantization vocabularies or token-level modeling, so it fails the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Despite dealing with LLM-based speech generation, the work focuses on self-supervised speaker-disentangled representations without any description of discrete-token/quantization vocabularies or token-level modeling, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
    "abstract": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
    "metadata": {
      "arxiv_id": "2402.03513",
      "title": "Video Super-Resolution for Optimized Bitrate and Green Online Streaming",
      "summary": "Conventional per-title encoding schemes strive to optimize encoding resolutions to deliver the utmost perceptual quality for each bitrate ladder representation. Nevertheless, maintaining encoding time within an acceptable threshold is equally imperative in online streaming applications. Furthermore, modern client devices are equipped with the capability for fast deep-learning-based video super-resolution (VSR) techniques, enhancing the perceptual quality of the decoded bitstream. This suggests that opting for lower resolutions in representations during the encoding process can curtail the overall energy consumption without substantially compromising perceptual quality. In this context, this paper introduces a video super-resolution-based latency-aware optimized bitrate encoding scheme (ViSOR) designed for online adaptive streaming applications. ViSOR determines the encoding resolution for each target bitrate, ensuring the highest achievable perceptual quality after VSR within the bound of a maximum acceptable latency. Random forest-based prediction models are trained to predict the perceptual quality after VSR and the encoding time for each resolution using the spatiotemporal features extracted for each video segment. Experimental results show that ViSOR targeting fast super-resolution convolutional neural network (FSRCNN) achieves an overall average bitrate reduction of 24.65 % and 32.70 % to maintain the same PSNR and VMAF, compared to the HTTP Live Streaming (HLS) bitrate ladder encoding of 4 s segments using the x265 encoder, when the maximum acceptable latency for each representation is set as two seconds. Considering a just noticeable difference (JND) of six VMAF points, the average cumulative storage consumption and encoding energy for each segment is reduced by 79.32 % and 68.21 %, respectively, contributing towards greener streaming.",
      "authors": [
        "Vignesh V Menon",
        "Prajit T Rajendran",
        "Amritha Premkumar",
        "Benjamin Bross",
        "Detlev Marpe"
      ],
      "published": "2024-02-05T21:01:01Z",
      "updated": "2024-02-05T21:01:01Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03513v1",
      "landing_url": "https://arxiv.org/abs/2402.03513v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03513"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on video super-resolution for bitrate optimization rather than discrete audio token generation/quantization, so it fails all inclusion criteria and hits exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on video super-resolution for bitrate optimization rather than discrete audio token generation/quantization, so it fails all inclusion criteria and hits exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Decoder-Only Image Registration",
    "abstract": "In unsupervised medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images. Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture. For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder. LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether. Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration. Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D. Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources. Our code and pre-trained models are available at https://github.com/xi-jia/LessNet.",
    "metadata": {
      "arxiv_id": "2402.03585",
      "title": "Decoder-Only Image Registration",
      "summary": "In unsupervised medical image registration, the predominant approaches involve the utilization of a encoder-decoder network architecture, allowing for precise prediction of dense, full-resolution displacement fields from given paired images. Despite its widespread use in the literature, we argue for the necessity of making both the encoder and decoder learnable in such an architecture. For this, we propose a novel network architecture, termed LessNet in this paper, which contains only a learnable decoder, while entirely omitting the utilization of a learnable encoder. LessNet substitutes the learnable encoder with simple, handcrafted features, eliminating the need to learn (optimize) network parameters in the encoder altogether. Consequently, this leads to a compact, efficient, and decoder-only architecture for 3D medical image registration. Evaluated on two publicly available brain MRI datasets, we demonstrate that our decoder-only LessNet can effectively and efficiently learn both dense displacement and diffeomorphic deformation fields in 3D. Furthermore, our decoder-only LessNet can achieve comparable registration performance to state-of-the-art methods such as VoxelMorph and TransMorph, while requiring significantly fewer computational resources. Our code and pre-trained models are available at https://github.com/xi-jia/LessNet.",
      "authors": [
        "Xi Jia",
        "Wenqi Lu",
        "Xinxing Cheng",
        "Jinming Duan"
      ],
      "published": "2024-02-05T23:30:37Z",
      "updated": "2024-02-05T23:30:37Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.03585v1",
      "landing_url": "https://arxiv.org/abs/2402.03585v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.03585"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focus on medical image registration rather than discrete audio token/tokenizer work, so it fails every inclusion requirement and matches the exclusion criteria; score:1."
    },
    "round-A_JuniorNano_reasoning": "Focus on medical image registration rather than discrete audio token/tokenizer work, so it fails every inclusion requirement and matches the exclusion criteria; score:1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
    "abstract": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
    "metadata": {
      "arxiv_id": "2402.04563",
      "title": "Attention Guided CAM: Visual Explanations of Vision Transformer Guided by Self-Attention",
      "summary": "Vision Transformer(ViT) is one of the most widely used models in the computer vision field with its great performance on various tasks. In order to fully utilize the ViT-based architecture in various applications, proper visualization methods with a decent localization performance are necessary, but these methods employed in CNN-based models are still not available in ViT due to its unique structure. In this work, we propose an attention-guided visualization method applied to ViT that provides a high-level semantic explanation for its decision. Our method selectively aggregates the gradients directly propagated from the classification output to each self-attention, collecting the contribution of image features extracted from each location of the input image. These gradients are additionally guided by the normalized self-attention scores, which are the pairwise patch correlation scores. They are used to supplement the gradients on the patch-level context information efficiently detected by the self-attention mechanism. This approach of our method provides elaborate high-level semantic explanations with great localization performance only with the class labels. As a result, our method outperforms the previous leading explainability methods of ViT in the weakly-supervised localization task and presents great capability in capturing the full instances of the target class object. Meanwhile, our method provides a visualization that faithfully explains the model, which is demonstrated in the perturbation comparison test.",
      "authors": [
        "Saebom Leem",
        "Hyunseok Seo"
      ],
      "published": "2024-02-07T03:43:56Z",
      "updated": "2024-02-07T03:43:56Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04563v1",
      "landing_url": "https://arxiv.org/abs/2402.04563v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04563"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on ViT explainability for vision data and not on discrete audio tokenization, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on ViT explainability for vision data and not on discrete audio tokenization, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
    "abstract": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
    "metadata": {
      "arxiv_id": "2402.04779",
      "title": "StableMask: Refining Causal Masking in Decoder-only Transformer",
      "summary": "The decoder-only Transformer architecture with causal masking and relative position encoding (RPE) has become the de facto choice in language modeling. Despite its exceptional performance across various tasks, we have identified two limitations: First, it requires all attention scores to be non-zero and sum up to 1, even if the current embedding has sufficient self-contained information. This compels the model to assign disproportional excessive attention to specific tokens. Second, RPE-based Transformers are not universal approximators due to their limited capacity at encoding absolute positional information, which limits their application in position-critical tasks. In this work, we propose StableMask: a parameter-free method to address both limitations by refining the causal mask. It introduces pseudo-attention values to balance attention distributions and encodes absolute positional information via a progressively decreasing mask ratio. StableMask's effectiveness is validated both theoretically and empirically, showing significant enhancements in language models with parameter sizes ranging from 71M to 1.4B across diverse datasets and encoding methods. We further show that it naturally supports (1) efficient extrapolation without special tricks such as StreamingLLM and (2) easy integration with existing attention optimization techniques.",
      "authors": [
        "Qingyu Yin",
        "Xuzheng He",
        "Xiang Zhuang",
        "Yu Zhao",
        "Jianhua Yao",
        "Xiaoyu Shen",
        "Qiang Zhang"
      ],
      "published": "2024-02-07T12:01:02Z",
      "updated": "2024-02-07T12:01:02Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.04779v1",
      "landing_url": "https://arxiv.org/abs/2402.04779v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.04779"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract focus on transformer masking improvements for language modeling rather than any discrete audio token/codebook design, so it clearly fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract focus on transformer masking improvements for language modeling rather than any discrete audio token/codebook design, so it clearly fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Survey on Transformer Compression",
    "abstract": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
    "metadata": {
      "arxiv_id": "2402.05964",
      "title": "A Survey on Transformer Compression",
      "summary": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
      "authors": [
        "Yehui Tang",
        "Yunhe Wang",
        "Jianyuan Guo",
        "Zhijun Tu",
        "Kai Han",
        "Hailin Hu",
        "Dacheng Tao"
      ],
      "published": "2024-02-05T12:16:28Z",
      "updated": "2024-04-07T13:03:58Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.05964v2",
      "landing_url": "https://arxiv.org/abs/2402.05964v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.05964"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract focus on Transformer compression in NLP/CV without any mention of discrete audio tokenization, so it fails the inclusion criteria and matches the exclusion of non-audio token research."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract focus on Transformer compression in NLP/CV without any mention of discrete audio tokenization, so it fails the inclusion criteria and matches the exclusion of non-audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
    "abstract": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
    "metadata": {
      "arxiv_id": "2402.06492",
      "title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings",
      "summary": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show that SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show that SoVQ indeed learns a syntactically clustered embedding space and SAL/SRL induces generalizable attention patterns, which lead to improved systematicity.",
      "authors": [
        "Yichen Jiang",
        "Xiang Zhou",
        "Mohit Bansal"
      ],
      "published": "2024-02-09T15:53:15Z",
      "updated": "2024-02-09T15:53:15Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.06492v1",
      "landing_url": "https://arxiv.org/abs/2402.06492v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.06492"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Transforms systematicity work focuses on text embeddings and attention without any discussion of discrete audio tokenization or quantized audio representations, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Transforms systematicity work focuses on text embeddings and attention without any discussion of discrete audio tokenization or quantized audio representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning",
    "abstract": "In the realm of audio-language pre-training (ALP), the challenge of achieving cross-modal alignment is significant. Moreover, the integration of audio inputs with diverse distributions and task variations poses challenges in developing generic audio-language models. In this study, we present MINT, a novel ALP framework boosting audio-language models through multi-target pre-training and instruction tuning. MINT leverages the strength of frozen pre-trained audio encoders and large language models (LLM) to improve audio-language pre-training, enabling effective transferablility to both audio-text understanding and generation tasks. To address the modality gap, we introduce Bridge-Net, a trainable module that enhances cross-modality alignment and the model's ability to follow instructions for a variety of audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing audio-language representation learning through a multi-target pre-training approach. Subsequently, Bridge-Net further boosts audio-to-language generative learning by integrating a frozen language model with instruction tuning. This integration empowers MINT to extract features in a flexible and effective manner, specifically tailored to the provided instructions for diverse tasks. Experimental results demonstrate that MINT attains superior performance across various audio-language understanding and generation tasks, highlighting its robust generalization capabilities even in zero-shot scenarios.",
    "metadata": {
      "arxiv_id": "2402.07485",
      "title": "MINT: Boosting Audio-Language Model via Multi-Target Pre-Training and Instruction Tuning",
      "summary": "In the realm of audio-language pre-training (ALP), the challenge of achieving cross-modal alignment is significant. Moreover, the integration of audio inputs with diverse distributions and task variations poses challenges in developing generic audio-language models. In this study, we present MINT, a novel ALP framework boosting audio-language models through multi-target pre-training and instruction tuning. MINT leverages the strength of frozen pre-trained audio encoders and large language models (LLM) to improve audio-language pre-training, enabling effective transferablility to both audio-text understanding and generation tasks. To address the modality gap, we introduce Bridge-Net, a trainable module that enhances cross-modality alignment and the model's ability to follow instructions for a variety of audio-text tasks. Bridge-Net is pivotal within MINT, initially enhancing audio-language representation learning through a multi-target pre-training approach. Subsequently, Bridge-Net further boosts audio-to-language generative learning by integrating a frozen language model with instruction tuning. This integration empowers MINT to extract features in a flexible and effective manner, specifically tailored to the provided instructions for diverse tasks. Experimental results demonstrate that MINT attains superior performance across various audio-language understanding and generation tasks, highlighting its robust generalization capabilities even in zero-shot scenarios.",
      "authors": [
        "Hang Zhao",
        "Yifei Xin",
        "Zhesong Yu",
        "Bilei Zhu",
        "Lu Lu",
        "Zejun Ma"
      ],
      "published": "2024-02-12T08:51:06Z",
      "updated": "2024-06-12T03:29:00Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07485v5",
      "landing_url": "https://arxiv.org/abs/2402.07485v5",
      "doi": "https://doi.org/10.48550/arXiv.2402.07485"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on aligning audio and language models via encoder fine-tuning and instruction tuning without proposing or analyzing any discrete audio-token/tokenizer quantization scheme, so it fails to meet the discrete-token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on aligning audio and language models via encoder fine-tuning and instruction tuning without proposing or analyzing any discrete audio-token/tokenizer quantization scheme, so it fails to meet the discrete-token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
    "abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.",
    "metadata": {
      "arxiv_id": "2402.07729",
      "title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension",
      "summary": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\\textbf{A}udio \\textbf{I}nst\\textbf{R}uction \\textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \\textit{foundation} and \\textit{chat} benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research.",
      "authors": [
        "Qian Yang",
        "Jin Xu",
        "Wenrui Liu",
        "Yunfei Chu",
        "Ziyue Jiang",
        "Xiaohuan Zhou",
        "Yichong Leng",
        "Yuanjun Lv",
        "Zhou Zhao",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "published": "2024-02-12T15:41:22Z",
      "updated": "2024-07-26T06:30:47Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.07729v2",
      "landing_url": "https://arxiv.org/abs/2402.07729v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.07729"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Does not describe discrete audio tokens/quantization or tokenization methods—air-bench is just a benchmark for audio-language comprehension—so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Does not describe discrete audio tokens/quantization or tokenization methods—air-bench is just a benchmark for audio-language comprehension—so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
    "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
    "metadata": {
      "arxiv_id": "2402.08093",
      "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data",
      "summary": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for $\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with $\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date, trained on 100K hours of public domain speech data, achieving a new state-of-the-art in speech naturalness. It deploys a 1-billion-parameter autoregressive Transformer that converts raw texts into discrete codes (\"speechcodes\") followed by a convolution-based decoder which converts these speechcodes into waveforms in an incremental, streamable manner. Further, our speechcodes are built using a novel speech tokenization technique that features speaker ID disentanglement and compression with byte-pair encoding. Echoing the widely-reported \"emergent abilities\" of large language models when trained on increasing volume of data, we show that BASE TTS variants built with 10K+ hours and 500M+ parameters begin to demonstrate natural prosody on textually complex sentences. We design and share a specialized dataset to measure these emergent abilities for text-to-speech. We showcase state-of-the-art naturalness of BASE TTS by evaluating against baselines that include publicly available large-scale text-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated by the model can be heard at https://amazon-ltts-paper.com/.",
      "authors": [
        "Mateusz Łajszczak",
        "Guillermo Cámbara",
        "Yang Li",
        "Fatih Beyhan",
        "Arent van Korlaar",
        "Fan Yang",
        "Arnaud Joly",
        "Álvaro Martín-Cortinas",
        "Ammar Abbas",
        "Adam Michalski",
        "Alexis Moinet",
        "Sri Karlapati",
        "Ewa Muszyńska",
        "Haohan Guo",
        "Bartosz Putrycz",
        "Soledad López Gambino",
        "Kayeon Yoo",
        "Elena Sokolova",
        "Thomas Drugman"
      ],
      "published": "2024-02-12T22:21:30Z",
      "updated": "2024-02-15T18:57:26Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.08093v2",
      "landing_url": "https://arxiv.org/abs/2402.08093v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.08093"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The BASE TTS paper centers on codec-based discrete speechcodes with tokenizer/quantization design and evaluation, so it satisfies the inclusion criteria and deserves a 5."
    },
    "round-A_JuniorNano_reasoning": "The BASE TTS paper centers on codec-based discrete speechcodes with tokenizer/quantization design and evaluation, so it satisfies the inclusion criteria and deserves a 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
    "abstract": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
    "metadata": {
      "arxiv_id": "2402.09378",
      "title": "MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech",
      "summary": "Zero-shot text-to-speech (TTS) has gained significant attention due to its powerful voice cloning capabilities, requiring only a few seconds of unseen speaker voice prompts. However, all previous work has been developed for cloud-based systems. Taking autoregressive models as an example, although these approaches achieve high-fidelity voice cloning, they fall short in terms of inference speed, model size, and robustness. Therefore, we propose MobileSpeech, which is a fast, lightweight, and robust zero-shot text-to-speech system based on mobile devices for the first time. Specifically: 1) leveraging discrete codec, we design a parallel speech mask decoder module called SMD, which incorporates hierarchical information from the speech codec and weight mechanisms across different codec layers during the generation process. Moreover, to bridge the gap between text and speech, we introduce a high-level probabilistic mask that simulates the progression of information flow from less to more during speech generation. 2) For speaker prompts, we extract fine-grained prompt duration from the prompt speech and incorporate text, prompt speech by cross attention in SMD. We demonstrate the effectiveness of MobileSpeech on multilingual datasets at different levels, achieving state-of-the-art results in terms of generating speed and speech quality. MobileSpeech achieves RTF of 0.09 on a single A100 GPU and we have successfully deployed MobileSpeech on mobile devices. Audio samples are available at \\url{https://mobilespeech.github.io/} .",
      "authors": [
        "Shengpeng Ji",
        "Ziyue Jiang",
        "Hanting Wang",
        "Jialong Zuo",
        "Zhou Zhao"
      ],
      "published": "2024-02-14T18:24:41Z",
      "updated": "2024-06-02T16:11:18Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09378v2",
      "landing_url": "https://arxiv.org/abs/2402.09378v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.09378"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "MobileSpeech explicitly builds on discrete codec tokens (with hierarchical and probabilistic masks) for zero-shot TTS and evaluates quality/speed, so it matches the discrete audio token focus and provides codec-level design/metrics."
    },
    "round-A_JuniorNano_reasoning": "MobileSpeech explicitly builds on discrete codec tokens (with hierarchical and probabilistic masks) for zero-shot TTS and evaluates quality/speed, so it matches the discrete audio token focus and provides codec-level design/metrics.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Domain Adaptation for Contrastive Audio-Language Models",
    "abstract": "Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.",
    "metadata": {
      "arxiv_id": "2402.09585",
      "title": "Domain Adaptation for Contrastive Audio-Language Models",
      "summary": "Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.",
      "authors": [
        "Soham Deshmukh",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "published": "2024-02-14T21:25:06Z",
      "updated": "2024-07-22T01:04:49Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09585v2",
      "landing_url": "https://arxiv.org/abs/2402.09585v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.09585"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on domain adaptation for contrastive audio-language models using prompt vectors and consistency losses at inference, with no discrete audio-token/tokenizer work, so it fails the inclusion requirement and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on domain adaptation for contrastive audio-language models using prompt vectors and consistency losses at inference, with no discrete audio-token/tokenizer work, so it fails the inclusion requirement and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-word Tokenization for Sequence Compression",
    "abstract": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
    "metadata": {
      "arxiv_id": "2402.09949",
      "title": "Multi-word Tokenization for Sequence Compression",
      "summary": "Large Language Models have proven highly successful at modelling a variety of tasks. However, this comes at a steep computational cost that hinders wider industrial uptake. In this paper, we present MWT: a Multi-Word Tokenizer that goes beyond word boundaries by representing frequent multi-word expressions as single tokens. MWTs produce a more compact and efficient tokenization that yields two benefits: (1) Increase in performance due to a greater coverage of input data given a fixed sequence length budget; (2) Faster and lighter inference due to the ability to reduce the sequence length with negligible drops in performance. Our results show that MWT is more robust across shorter sequence lengths, thus allowing for major speedups via early sequence truncation.",
      "authors": [
        "Leonidas Gee",
        "Leonardo Rigutini",
        "Marco Ernandes",
        "Andrea Zugarini"
      ],
      "published": "2024-02-15T13:52:23Z",
      "updated": "2024-04-04T22:50:25Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.09949v2",
      "landing_url": "https://arxiv.org/abs/2402.09949v2",
      "doi": "https://doi.org/10.18653/v1/2023.emnlp-industry.58"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reject because the study targets NLP multi-word tokenization for text sequences rather than discrete audio tokens or quantized audio tokenization, so it fails to match the inclusion domain."
    },
    "round-A_JuniorNano_reasoning": "Reject because the study targets NLP multi-word tokenization for text sequences rather than discrete audio tokens or quantized audio tokenization, so it fails to match the inclusion domain.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding",
    "abstract": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.",
    "metadata": {
      "arxiv_id": "2402.10533",
      "title": "APCodec: A Neural Audio Codec with Parallel Amplitude and Phase Spectrum Encoding and Decoding",
      "summary": "This paper introduces a novel neural audio codec targeting high waveform sampling rates and low bitrates named APCodec, which seamlessly integrates the strengths of parametric codecs and waveform codecs. The APCodec revolutionizes the process of audio encoding and decoding by concurrently handling the amplitude and phase spectra as audio parametric characteristics like parametric codecs. It is composed of an encoder and a decoder with the modified ConvNeXt v2 network as the backbone, connected by a quantizer based on the residual vector quantization (RVQ) mechanism. The encoder compresses the audio amplitude and phase spectra in parallel, amalgamating them into a continuous latent code at a reduced temporal resolution. This code is subsequently quantized by the quantizer. Ultimately, the decoder reconstructs the audio amplitude and phase spectra in parallel, and the decoded waveform is obtained by inverse short-time Fourier transform. To ensure the fidelity of decoded audio like waveform codecs, spectral-level loss, quantization loss, and generative adversarial network (GAN) based loss are collectively employed for training the APCodec. To support low-latency streamable inference, we employ feed-forward layers and causal deconvolutional layers in APCodec, incorporating a knowledge distillation training strategy to enhance the quality of decoded audio. Experimental results confirm that our proposed APCodec can encode 48 kHz audio at bitrate of just 6 kbps, with no significant degradation in the quality of the decoded audio. At the same bitrate, our proposed APCodec also demonstrates superior decoded audio quality and faster generation speed compared to well-known codecs, such as Encodec, AudioDec and DAC.",
      "authors": [
        "Yang Ai",
        "Xiao-Hang Jiang",
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Zhen-Hua Ling"
      ],
      "published": "2024-02-16T09:38:16Z",
      "updated": "2024-09-24T01:14:07Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.10533v2",
      "landing_url": "https://arxiv.org/abs/2402.10533v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.10533"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper proposes a neural audio codec (APCodec) that includes quantization via residual vector quantization to produce discrete codes for amplitude and phase spectra, reports reconstruction quality and comparisons to other codecs, thus clearly addressing discrete audio tokens with codec design and evaluation."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a neural audio codec (APCodec) that includes quantization via residual vector quantization to produce discrete codes for amplitude and phase spectra, reports reconstruction quality and comparisons to other codecs, thus clearly addressing discrete audio tokens with codec design and evaluation.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
    "abstract": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
    "metadata": {
      "arxiv_id": "2402.11363",
      "title": "Transformer-based de novo peptide sequencing for data-independent acquisition mass spectrometry",
      "summary": "Tandem mass spectrometry (MS/MS) stands as the predominant high-throughput technique for comprehensively analyzing protein content within biological samples. This methodology is a cornerstone driving the advancement of proteomics. In recent years, substantial strides have been made in Data-Independent Acquisition (DIA) strategies, facilitating impartial and non-targeted fragmentation of precursor ions. The DIA-generated MS/MS spectra present a formidable obstacle due to their inherent high multiplexing nature. Each spectrum encapsulates fragmented product ions originating from multiple precursor peptides. This intricacy poses a particularly acute challenge in de novo peptide/protein sequencing, where current methods are ill-equipped to address the multiplexing conundrum. In this paper, we introduce DiaTrans, a deep-learning model based on transformer architecture. It deciphers peptide sequences from DIA mass spectrometry data. Our results show significant improvements over existing STOA methods, including DeepNovo-DIA and PepNet. Casanovo-DIA enhances precision by 15.14% to 34.8%, recall by 11.62% to 31.94% at the amino acid level, and boosts precision by 59% to 81.36% at the peptide level. Integrating DIA data and our DiaTrans model holds considerable promise to uncover novel peptides and more comprehensive profiling of biological samples. Casanovo-DIA is freely available under the GNU GPL license at https://github.com/Biocomputing-Research-Group/DiaTrans.",
      "authors": [
        "Shiva Ebrahimi",
        "Xuan Guo"
      ],
      "published": "2024-02-17T19:04:23Z",
      "updated": "2024-06-26T07:45:33Z",
      "categories": [
        "q-bio.QM",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.11363v3",
      "landing_url": "https://arxiv.org/abs/2402.11363v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.11363"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study targets transformer-based peptide sequencing from mass spectrometry data, which has no relation to discrete audio token generation, codec/semantic unit design, or any audio token evaluation, so it fails all inclusion criteria and matches the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The study targets transformer-based peptide sequencing from mass spectrometry data, which has no relation to discrete audio token generation, codec/semantic unit design, or any audio token evaluation, so it fails all inclusion criteria and matches the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
    "abstract": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
    "metadata": {
      "arxiv_id": "2402.12208",
      "title": "Language-Codec: Bridging Discrete Codec Representations and Speech Language Models",
      "summary": "In recent years, large language models have achieved significant success in generative tasks related to speech, audio, music, and other signal domains. A crucial element of these models is the discrete acoustic codecs, which serve as an intermediate representation replacing the mel-spectrogram. However, there exist several gaps between discrete codecs and downstream speech language models. Specifically, 1) Due to the reconstruction paradigm of the Codec model and the structure of residual vector quantization, the initial channel of the codebooks contains excessive information, making it challenging to directly generate acoustic tokens from weakly supervised signals such as text in downstream tasks. 2) numerous codebooks increases the burden on downstream speech language models. Consequently, leveraging the characteristics of speech language models, we propose Language-Codec. In the Language-Codec, we introduce a Masked Channel Residual Vector Quantization (MCRVQ) mechanism along with improved fourier transform structures and attention blocks, refined discriminator design to address the aforementioned gaps. We compare our method with competing audio compression algorithms and observe significant outperformance across extensive evaluations. Furthermore, we also validate the efficiency of the Language-Codec on downstream speech language models. The source code and pre-trained models can be accessed at https://github.com/jishengpeng/languagecodec .",
      "authors": [
        "Shengpeng Ji",
        "Minghui Fang",
        "Jialong Zuo",
        "Ziyue Jiang",
        "Dingdong Wang",
        "Hanting Wang",
        "Hai Huang",
        "Zhou Zhao"
      ],
      "published": "2024-02-19T15:12:12Z",
      "updated": "2025-06-04T05:50:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.12208v4",
      "landing_url": "https://arxiv.org/abs/2402.12208v4",
      "doi": "https://doi.org/10.48550/arXiv.2402.12208"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Language-Codec introduces discrete acoustic codec tokens with RVQ variants, evaluation and downstream speech-LM validation, so it satisfies the discrete-token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Language-Codec introduces discrete acoustic codec tokens with RVQ variants, evaluation and downstream speech-LM validation, so it satisfies the discrete-token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mode Estimation with Partial Feedback",
    "abstract": "The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.",
    "metadata": {
      "arxiv_id": "2402.13079",
      "title": "Mode Estimation with Partial Feedback",
      "summary": "The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.",
      "authors": [
        "Charles Arnal",
        "Vivien Cabannes",
        "Vianney Perchet"
      ],
      "published": "2024-02-20T15:24:21Z",
      "updated": "2024-02-20T15:24:21Z",
      "categories": [
        "stat.ML",
        "cs.IR",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13079v1",
      "landing_url": "https://arxiv.org/abs/2402.13079v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.13079"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses theoretical mode estimation with partial feedback, not discrete audio token generation/quantization or any audio codec/token focus, so it fails all inclusion criteria and matches exclusion of unrelated topic."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses theoretical mode estimation with partial feedback, not discrete audio token generation/quantization or any audio codec/token focus, so it fails all inclusion criteria and matches exclusion of unrelated topic.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
    "abstract": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
    "metadata": {
      "arxiv_id": "2402.13572",
      "title": "AlgoFormer: An Efficient Transformer Framework with Algorithmic Structures",
      "summary": "Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer, we design a novel transformer framework, dubbed Algorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficient transformer architectures can be designed by leveraging prior knowledge of tasks and the underlying structure of potential algorithms. Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithm representation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer that is responsible for task preprocessing, a looped transformer for iterative optimization algorithms, and a post-transformer for producing the desired results after post-processing. We provide theoretical evidence of the expressive power of the AlgoFormer in solving some challenging problems, mirroring human-designed algorithms. Furthermore, some theoretical and empirical results are presented to show that the designed transformer has the potential to perform algorithm representation and learning. Experimental results demonstrate the empirical superiority of the proposed transformer in that it outperforms the standard transformer and vanilla looped transformer in some specific tasks. An extensive experiment on real language tasks (e.g., neural machine translation of German and English, and text classification) further validates the expressiveness and effectiveness of AlgoFormer.",
      "authors": [
        "Yihang Gao",
        "Chuanyang Zheng",
        "Enze Xie",
        "Han Shi",
        "Tianyang Hu",
        "Yu Li",
        "Michael K. Ng",
        "Zhenguo Li",
        "Zhaoqiang Liu"
      ],
      "published": "2024-02-21T07:07:54Z",
      "updated": "2025-01-10T09:11:39Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.NA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.13572v2",
      "landing_url": "https://arxiv.org/abs/2402.13572v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.13572"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on transformer architectures for algorithmic tasks and language processing rather than discrete audio tokenization, so it fails to meet any inclusion condition (discrete audio tokens) and matches exclusion (non-audio/continuous feature focus)."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on transformer architectures for algorithmic tasks and language processing rather than discrete audio tokenization, so it fails to meet any inclusion condition (discrete audio tokens) and matches exclusion (non-audio/continuous feature focus).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection",
    "abstract": "In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.",
    "metadata": {
      "arxiv_id": "2402.14384",
      "title": "Generative Adversarial Network with Soft-Dynamic Time Warping and Parallel Reconstruction for Energy Time Series Anomaly Detection",
      "summary": "In this paper, we employ a 1D deep convolutional generative adversarial network (DCGAN) for sequential anomaly detection in energy time series data. Anomaly detection involves gradient descent to reconstruct energy sub-sequences, identifying the noise vector that closely generates them through the generator network. Soft-DTW is used as a differentiable alternative for the reconstruction loss and is found to be superior to Euclidean distance. Combining reconstruction loss and the latent space's prior probability distribution serves as the anomaly score. Our novel method accelerates detection by parallel computation of reconstruction of multiple points and shows promise in identifying anomalous energy consumption in buildings, as evidenced by performing experiments on hourly energy time series from 15 buildings.",
      "authors": [
        "Hardik Prabhu",
        "Jayaraman Valadi",
        "Pandarasamy Arjunan"
      ],
      "published": "2024-02-22T08:54:57Z",
      "updated": "2024-02-22T08:54:57Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14384v1",
      "landing_url": "https://arxiv.org/abs/2402.14384v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14384"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on GAN-based anomaly detection in building energy time series without any discrete audio token/tokenizer development or evaluation, so it fails to meet the inclusion criteria and matches the exclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on GAN-based anomaly detection in building energy time series without any discrete audio token/tokenizer development or evaluation, so it fails to meet the inclusion criteria and matches the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding",
    "abstract": "Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the \"vocabulary\" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment \"vocabulary\" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.",
    "metadata": {
      "arxiv_id": "2402.14391",
      "title": "MAPE-PPI: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding",
      "summary": "Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. While existing methods rely heavily on protein sequence for PPI prediction, it is the protein structure that is the key to determine the interactions. To take both protein modalities into account, we define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. In addition, microenvironments defined in previous work are largely based on experimentally assayed physicochemical properties, for which the \"vocabulary\" is usually extremely small. This makes it difficult to cover the diversity and complexity of microenvironments. In this paper, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MPAE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment \"vocabulary\" (i.e., codebook). Moreover, we propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, we can reuse it as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.",
      "authors": [
        "Lirong Wu",
        "Yijun Tian",
        "Yufei Huang",
        "Siyuan Li",
        "Haitao Lin",
        "Nitesh V Chawla",
        "Stan Z. Li"
      ],
      "published": "2024-02-22T09:04:41Z",
      "updated": "2024-02-22T09:04:41Z",
      "categories": [
        "cs.LG",
        "q-bio.BM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.14391v1",
      "landing_url": "https://arxiv.org/abs/2402.14391v1",
      "doi": "https://doi.org/10.48550/arXiv.2402.14391"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on protein microenvironments for PPI prediction and has no relation to discrete audio token modeling, so it clearly fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on protein microenvironments for PPI prediction and has no relation to discrete audio token modeling, so it clearly fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model",
    "abstract": "With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication. The code will be released on https://github.com/lcysyzxdxc/MISC.",
    "metadata": {
      "arxiv_id": "2402.16749",
      "title": "MISC: Ultra-low Bitrate Image Semantic Compression Driven by Large Multimodal Model",
      "summary": "With the evolution of storage and communication protocols, ultra-low bitrate image compression has become a highly demanding topic. However, existing compression algorithms must sacrifice either consistency with the ground truth or perceptual quality at ultra-low bitrate. In recent years, the rapid development of the Large Multimodal Model (LMM) has made it possible to balance these two goals. To solve this problem, this paper proposes a method called Multimodal Image Semantic Compression (MISC), which consists of an LMM encoder for extracting the semantic information of the image, a map encoder to locate the region corresponding to the semantic, an image encoder generates an extremely compressed bitstream, and a decoder reconstructs the image based on the above information. Experimental results show that our proposed MISC is suitable for compressing both traditional Natural Sense Images (NSIs) and emerging AI-Generated Images (AIGIs) content. It can achieve optimal consistency and perception results while saving 50% bitrate, which has strong potential applications in the next generation of storage and communication. The code will be released on https://github.com/lcysyzxdxc/MISC.",
      "authors": [
        "Chunyi Li",
        "Guo Lu",
        "Donghui Feng",
        "Haoning Wu",
        "Zicheng Zhang",
        "Xiaohong Liu",
        "Guangtao Zhai",
        "Weisi Lin",
        "Wenjun Zhang"
      ],
      "published": "2024-02-26T17:11:11Z",
      "updated": "2024-04-17T14:06:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.16749v3",
      "landing_url": "https://arxiv.org/abs/2402.16749v3",
      "doi": "https://doi.org/10.48550/arXiv.2402.16749"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on ultra-low bitrate image compression with multimodal image tokens, which does not address discrete audio token generation/tokenizer for audio signals, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on ultra-low bitrate image compression with multimodal image tokens, which does not address discrete audio token generation/tokenizer for audio signals, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GAN Based Near-Field Channel Estimation for Extremely Large-Scale MIMO Systems",
    "abstract": "Extremely large-scale multiple-input-multiple-output (XL-MIMO) is a promising technique to achieve ultra-high spectral efficiency for future 6G communications. The mixed line-of-sight (LoS) and non-line-of-sight (NLoS) XL-MIMO near-field channel model is adopted to describe the XL-MIMO near-field channel accurately. In this paper, a generative adversarial network (GAN) variant based channel estimation method is proposed for XL-MIMO systems. Specifically, the GAN variant is developed to simultaneously estimate the LoS and NLoS path components of the XL-MIMO channel. The initially estimated channels instead of the received signals are input into the GAN variant as the conditional input to generate the XL-MIMO channels more efficiently. The GAN variant not only learns the mapping from the initially estimated channels to the XL-MIMO channels but also learns an adversarial loss. Moreover, we combine the adversarial loss with a conventional loss function to ensure the correct direction of training the generator. To further enhance the estimation performance, we investigate the impact of the hyper-parameter of the loss function on the performance of our method. Simulation results show that the proposed method outperforms the existing channel estimation approaches in the adopted channel model. In addition, the proposed method surpasses the Cram$\\acute{\\mathbf{e}}$r-Rao lower bound (CRLB) under low pilot overhead.",
    "metadata": {
      "arxiv_id": "2402.17281",
      "title": "GAN Based Near-Field Channel Estimation for Extremely Large-Scale MIMO Systems",
      "summary": "Extremely large-scale multiple-input-multiple-output (XL-MIMO) is a promising technique to achieve ultra-high spectral efficiency for future 6G communications. The mixed line-of-sight (LoS) and non-line-of-sight (NLoS) XL-MIMO near-field channel model is adopted to describe the XL-MIMO near-field channel accurately. In this paper, a generative adversarial network (GAN) variant based channel estimation method is proposed for XL-MIMO systems. Specifically, the GAN variant is developed to simultaneously estimate the LoS and NLoS path components of the XL-MIMO channel. The initially estimated channels instead of the received signals are input into the GAN variant as the conditional input to generate the XL-MIMO channels more efficiently. The GAN variant not only learns the mapping from the initially estimated channels to the XL-MIMO channels but also learns an adversarial loss. Moreover, we combine the adversarial loss with a conventional loss function to ensure the correct direction of training the generator. To further enhance the estimation performance, we investigate the impact of the hyper-parameter of the loss function on the performance of our method. Simulation results show that the proposed method outperforms the existing channel estimation approaches in the adopted channel model. In addition, the proposed method surpasses the Cram$\\acute{\\mathbf{e}}$r-Rao lower bound (CRLB) under low pilot overhead.",
      "authors": [
        "Ming Ye",
        "Xiao Liang",
        "Cunhua Pan",
        "Yinfei Xu",
        "Ming Jiang",
        "Chunguo Li"
      ],
      "published": "2024-02-27T07:52:25Z",
      "updated": "2024-06-17T07:01:29Z",
      "categories": [
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2402.17281v2",
      "landing_url": "https://arxiv.org/abs/2402.17281v2",
      "doi": "https://doi.org/10.48550/arXiv.2402.17281"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on XL-MIMO channel estimation via GANs in wireless communications, which does not address discrete audio tokens or tokenization, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on XL-MIMO channel estimation via GANs in wireless communications, which does not address discrete audio tokens or tokenization, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
    "abstract": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
    "metadata": {
      "arxiv_id": "2403.02002",
      "title": "Fine-Grained Quantitative Emotion Editing for Speech Generation",
      "summary": "It remains a significant challenge how to quantitatively control the expressiveness of speech emotion in speech generation. In this work, we present a novel approach for manipulating the rendering of emotions for speech generation. We propose a hierarchical emotion distribution extractor, i.e. Hierarchical ED, that quantifies the intensity of emotions at different levels of granularity. Support vector machines (SVMs) are employed to rank emotion intensity, resulting in a hierarchical emotional embedding. Hierarchical ED is subsequently integrated into the FastSpeech2 framework, guiding the model to learn emotion intensity at phoneme, word, and utterance levels. During synthesis, users can manually edit the emotional intensity of the generated voices. Both objective and subjective evaluations demonstrate the effectiveness of the proposed network in terms of fine-grained quantitative emotion editing.",
      "authors": [
        "Sho Inoue",
        "Kun Zhou",
        "Shuai Wang",
        "Haizhou Li"
      ],
      "published": "2024-03-04T12:53:15Z",
      "updated": "2024-09-29T15:39:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.02002v2",
      "landing_url": "https://arxiv.org/abs/2403.02002v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.02002"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on emotion editing via hierarchical intensity embedding within FastSpeech2 without describing any discrete audio tokenization or quantization mechanism, so it fails the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on emotion editing via hierarchical intensity embedding within FastSpeech2 without describing any discrete audio tokenization or quantization mechanism, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer",
    "abstract": "Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new direction in generative compression.",
    "metadata": {
      "arxiv_id": "2403.03736",
      "title": "Unifying Generation and Compression: Ultra-low bitrate Image Coding Via Multi-stage Transformer",
      "summary": "Recent progress in generative compression technology has significantly improved the perceptual quality of compressed data. However, these advancements primarily focus on producing high-frequency details, often overlooking the ability of generative models to capture the prior distribution of image content, thus impeding further bitrate reduction in extreme compression scenarios (<0.05 bpp). Motivated by the capabilities of predictive language models for lossless compression, this paper introduces a novel Unified Image Generation-Compression (UIGC) paradigm, merging the processes of generation and compression. A key feature of the UIGC framework is the adoption of vector-quantized (VQ) image models for tokenization, alongside a multi-stage transformer designed to exploit spatial contextual information for modeling the prior distribution. As such, the dual-purpose framework effectively utilizes the learned prior for entropy estimation and assists in the regeneration of lost tokens. Extensive experiments demonstrate the superiority of the proposed UIGC framework over existing codecs in perceptual quality and human perception, particularly in ultra-low bitrate scenarios (<=0.03 bpp), pioneering a new direction in generative compression.",
      "authors": [
        "Naifu Xue",
        "Qi Mao",
        "Zijian Wang",
        "Yuan Zhang",
        "Siwei Ma"
      ],
      "published": "2024-03-06T14:27:02Z",
      "updated": "2024-03-06T14:27:02Z",
      "categories": [
        "cs.CV",
        "cs.LG",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.03736v1",
      "landing_url": "https://arxiv.org/abs/2403.03736v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.03736"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe ultra-low bitrate image compression via multi-stage transformer and VQ image models, so it focuses on visual tokens rather than discrete audio-token generation or codec design, which fails the audio-specific inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe ultra-low bitrate image compression via multi-stage transformer and VQ image models, so it focuses on visual tokens rather than discrete audio-token generation or codec design, which fails the audio-specific inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
    "abstract": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
    "metadata": {
      "arxiv_id": "2403.05010",
      "title": "RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction",
      "summary": "Recent advancements in generative modeling have significantly enhanced the reconstruction of audio waveforms from various representations. While diffusion models are adept at this task, they are hindered by latency issues due to their operation at the individual sample point level and the need for numerous sampling steps. In this study, we introduce RFWave, a cutting-edge multi-band Rectified Flow approach designed to reconstruct high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens. RFWave uniquely generates complex spectrograms and operates at the frame level, processing all subbands simultaneously to boost efficiency. Leveraging Rectified Flow, which targets a straight transport trajectory, RFWave achieves reconstruction with just 10 sampling steps. Our empirical evaluations show that RFWave not only provides outstanding reconstruction quality but also offers vastly superior computational efficiency, enabling audio generation at speeds up to 160 times faster than real-time on a GPU. An online demonstration is available at: https://rfwave-demo.github.io/rfwave/.",
      "authors": [
        "Peng Liu",
        "Dongyang Dai",
        "Zhiyong Wu"
      ],
      "published": "2024-03-08T03:16:47Z",
      "updated": "2024-10-07T02:08:05Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.05010v3",
      "landing_url": "https://arxiv.org/abs/2403.05010v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.05010"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper centers on continuous waveform reconstruction from Mel-spectrograms or described “discrete acoustic tokens” but lacks any tokenization/quantization specs or evaluation of discrete vocabularies, so it better to exclude."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on continuous waveform reconstruction from Mel-spectrograms or described “discrete acoustic tokens” but lacks any tokenization/quantization specs or evaluation of discrete vocabularies, so it better to exclude.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
    "abstract": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
    "metadata": {
      "arxiv_id": "2403.06536",
      "title": "Multi-Scale Implicit Transformer with Re-parameterize for Arbitrary-Scale Super-Resolution",
      "summary": "Recently, the methods based on implicit neural representations have shown excellent capabilities for arbitrary-scale super-resolution (ASSR). Although these methods represent the features of an image by generating latent codes, these latent codes are difficult to adapt for different magnification factors of super-resolution, which seriously affects their performance. Addressing this, we design Multi-Scale Implicit Transformer (MSIT), consisting of an Multi-scale Neural Operator (MSNO) and Multi-Scale Self-Attention (MSSA). Among them, MSNO obtains multi-scale latent codes through feature enhancement, multi-scale characteristics extraction, and multi-scale characteristics merging. MSSA further enhances the multi-scale characteristics of latent codes, resulting in better performance. Furthermore, to improve the performance of network, we propose the Re-Interaction Module (RIM) combined with the cumulative training strategy to improve the diversity of learned information for the network. We have systematically introduced multi-scale characteristics for the first time in ASSR, extensive experiments are performed to validate the effectiveness of MSIT, and our method achieves state-of-the-art performance in arbitrary super-resolution tasks.",
      "authors": [
        "Jinchen Zhu",
        "Mingjian Zhang",
        "Ling Zheng",
        "Shizhuang Weng"
      ],
      "published": "2024-03-11T09:23:20Z",
      "updated": "2024-03-11T09:23:20Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.06536v1",
      "landing_url": "https://arxiv.org/abs/2403.06536v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.06536"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about arbitrary-scale image super-resolution using implicit transformers and does not address discrete audio tokens or tokenization for audio, so it fails inclusion and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper is about arbitrary-scale image super-resolution using implicit transformers and does not address discrete audio tokens or tokenization for audio, so it fails inclusion and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
    "abstract": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
    "metadata": {
      "arxiv_id": "2403.07355",
      "title": "Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems",
      "summary": "This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction performance under a given feedback overhead.",
      "authors": [
        "Junyong Shin",
        "Yujin Kang",
        "Yo-Seb Jeon"
      ],
      "published": "2024-03-12T06:28:41Z",
      "updated": "2024-03-13T02:29:29Z",
      "categories": [
        "eess.SP",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.07355v2",
      "landing_url": "https://arxiv.org/abs/2403.07355v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.07355"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vector quantization for CSI feedback in MIMO wireless systems rather than on discrete audio token generation, tokenizer design, or evaluation, so it fails to meet the audio-token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization for CSI feedback in MIMO wireless systems rather than on discrete audio token generation, tokenizer design, or evaluation, so it fails to meet the audio-token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
    "abstract": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
    "metadata": {
      "arxiv_id": "2403.08206",
      "title": "Discrete Semantic Tokenization for Deep CTR Prediction",
      "summary": "Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings, subsequently caching them to optimize processing time at the expense of space. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.",
      "authors": [
        "Qijiong Liu",
        "Hengchang Hu",
        "Jiahao Wu",
        "Jieming Zhu",
        "Min-Yen Kan",
        "Xiao-Ming Wu"
      ],
      "published": "2024-03-13T03:03:15Z",
      "updated": "2024-03-21T15:17:46Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.08206v2",
      "landing_url": "https://arxiv.org/abs/2403.08206v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.08206"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract discuss CTR prediction and semantic tokenization for recommender systems, not discrete audio tokens, so it fails the inclusion criteria (and falls under the exclusion for non-audio sequences)."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract discuss CTR prediction and semantic tokenization for recommender systems, not discrete audio tokens, so it fails the inclusion criteria (and falls under the exclusion for non-audio sequences).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
    "abstract": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
    "metadata": {
      "arxiv_id": "2403.09673",
      "title": "FoldToken: Learning Protein Language via Vector Quantization and Beyond",
      "summary": "Is there a foreign language describing protein sequences and structures simultaneously? Protein structures, represented by continuous 3D points, have long posed a challenge due to the contrasting modeling paradigms of discrete sequences. We introduce \\textbf{FoldTokenizer} to represent protein sequence-structure as discrete symbols. This innovative approach involves projecting residue types and structures into a discrete space, guided by a reconstruction loss for information preservation. We refer to the learned discrete symbols as \\textbf{FoldToken}, and the sequence of FoldTokens serves as a new protein language, transforming the protein sequence-structure into a unified modality. We apply the created protein language on general backbone inpainting and antibody design tasks, building the first GPT-style model (\\textbf{FoldGPT}) for sequence-structure co-generation with promising results. Key to our success is the substantial enhancement of the vector quantization module, Soft Conditional Vector Quantization (\\textbf{SoftCVQ}).",
      "authors": [
        "Zhangyang Gao",
        "Cheng Tan",
        "Jue Wang",
        "Yufei Huang",
        "Lirong Wu",
        "Stan Z. Li"
      ],
      "published": "2024-02-04T12:18:51Z",
      "updated": "2024-03-19T05:29:23Z",
      "categories": [
        "q-bio.BM",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.09673v2",
      "landing_url": "https://arxiv.org/abs/2403.09673v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.09673"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe protein structure tokenization and not audio-derived discrete tokens, so it fails the inclusion scope and matches exclusion criteria for non-audio sequences."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe protein structure tokenization and not audio-derived discrete tokens, so it fails the inclusion scope and matches exclusion criteria for non-audio sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
    "abstract": "Transformer-based NLP models are powerful but have high computational costs that limit deployment. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and decomposable tasks where multiple outputs are required for a single shared input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes the output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding and increasing the operational intensity (ratio of numbers of arithmetic operation to memory access) of decoding process by sharing the input key-value cache. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks, with comparable or better performance.",
    "metadata": {
      "arxiv_id": "2403.13112",
      "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
      "summary": "Transformer-based NLP models are powerful but have high computational costs that limit deployment. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and decomposable tasks where multiple outputs are required for a single shared input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes the output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding and increasing the operational intensity (ratio of numbers of arithmetic operation to memory access) of decoding process by sharing the input key-value cache. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks, with comparable or better performance.",
      "authors": [
        "Bo-Ru Lu",
        "Nikita Haduong",
        "Chien-Yu Lin",
        "Hao Cheng",
        "Noah A. Smith",
        "Mari Ostendorf"
      ],
      "published": "2024-03-19T19:27:23Z",
      "updated": "2024-11-16T20:39:46Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.13112v3",
      "landing_url": "https://arxiv.org/abs/2403.13112v3",
      "doi": "https://doi.org/10.48550/arXiv.2403.13112"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Transformer encoder-decoder efficiency study focuses on NLP tasks with text outputs and does not cover discrete audio token generation/quantization or codec-token modeling, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Transformer encoder-decoder efficiency study focuses on NLP tasks with text outputs and does not cover discrete audio token generation/quantization or codec-token modeling, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Agentic AI: The Era of Semantic Decoding",
    "abstract": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
    "metadata": {
      "arxiv_id": "2403.14562",
      "title": "Agentic AI: The Era of Semantic Decoding",
      "summary": "Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.",
      "authors": [
        "Maxime Peyrard",
        "Martin Josifoski",
        "Robert West"
      ],
      "published": "2024-03-21T17:06:17Z",
      "updated": "2025-04-29T15:24:52Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.HC",
        "cs.MA"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.14562v2",
      "landing_url": "https://arxiv.org/abs/2403.14562v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.14562"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: The paper discusses abstract semantic orchestration of LLMs and not discrete audio tokenization or codec/quantization methods, so it fails to meet any inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper discusses abstract semantic orchestration of LLMs and not discrete audio tokenization or codec/quantization methods, so it fails to meet any inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Encoding of lexical tone in self-supervised models of spoken language",
    "abstract": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
    "metadata": {
      "arxiv_id": "2403.16865",
      "title": "Encoding of lexical tone in self-supervised models of spoken language",
      "summary": "Interpretability research has shown that self-supervised Spoken Language Models (SLMs) encode a wide variety of features in human speech from the acoustic, phonetic, phonological, syntactic and semantic levels, to speaker characteristics. The bulk of prior research on representations of phonology has focused on segmental features such as phonemes; the encoding of suprasegmental phonology (such as tone and stress patterns) in SLMs is not yet well understood. Tone is a suprasegmental feature that is present in more than half of the world's languages. This paper aims to analyze the tone encoding capabilities of SLMs, using Mandarin and Vietnamese as case studies. We show that SLMs encode lexical tone to a significant degree even when they are trained on data from non-tonal languages. We further find that SLMs behave similarly to native and non-native human participants in tone and consonant perception studies, but they do not follow the same developmental trajectory.",
      "authors": [
        "Gaofei Shen",
        "Michaela Watkins",
        "Afra Alishahi",
        "Arianna Bisazza",
        "Grzegorz Chrupała"
      ],
      "published": "2024-03-25T15:28:38Z",
      "updated": "2024-04-03T12:59:20Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.16865v2",
      "landing_url": "https://arxiv.org/abs/2403.16865v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.16865"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper analyzes tone encoding in self-supervised spoken language models but never addresses discrete audio token quantization/codec/tokenizer design as required, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes tone encoding in self-supervised spoken language models but never addresses discrete audio token quantization/codec/tokenizer design as required, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
    "abstract": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
    "metadata": {
      "arxiv_id": "2403.17378",
      "title": "Low-Latency Neural Speech Phase Prediction based on Parallel Estimation Architecture and Anti-Wrapping Losses for Speech Generation Tasks",
      "summary": "This paper presents a novel neural speech phase prediction model which predicts wrapped phase spectra directly from amplitude spectra. The proposed model is a cascade of a residual convolutional network and a parallel estimation architecture. The parallel estimation architecture is a core module for direct wrapped phase prediction. This architecture consists of two parallel linear convolutional layers and a phase calculation formula, imitating the process of calculating the phase spectra from the real and imaginary parts of complex spectra and strictly restricting the predicted phase values to the principal value interval. To avoid the error expansion issue caused by phase wrapping, we design anti-wrapping training losses defined between the predicted wrapped phase spectra and natural ones by activating the instantaneous phase error, group delay error and instantaneous angular frequency error using an anti-wrapping function. We mathematically demonstrate that the anti-wrapping function should possess three properties, namely parity, periodicity and monotonicity. We also achieve low-latency streamable phase prediction by combining causal convolutions and knowledge distillation training strategies. For both analysis-synthesis and specific speech generation tasks, experimental results show that our proposed neural speech phase prediction model outperforms the iterative phase estimation algorithms and neural network-based phase prediction methods in terms of phase prediction precision, efficiency and robustness. Compared with HiFi-GAN-based waveform reconstruction method, our proposed model also shows outstanding efficiency advantages while ensuring the quality of synthesized speech. To the best of our knowledge, we are the first to directly predict speech phase spectra from amplitude spectra only via neural networks.",
      "authors": [
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-03-26T04:53:15Z",
      "updated": "2024-03-26T04:53:15Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.17378v1",
      "landing_url": "https://arxiv.org/abs/2403.17378v1",
      "doi": "https://doi.org/10.48550/arXiv.2403.17378"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on neural phase prediction from amplitude spectra without proposing or evaluating any discrete audio tokenization or quantization scheme, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on neural phase prediction from amplitude spectra without proposing or evaluating any discrete audio tokenization or quantization scheme, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "D'OH: Decoder-Only Random Hypernetworks for Implicit Neural Representations",
    "abstract": "Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no offline training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging redundancies that exist between layers. We propose to use a novel runtime decoder-only hypernetwork - that uses no offline training data - to better exploit cross-layer parameter redundancy. Previous applications of hypernetworks with deep implicit functions have employed feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the optimization of runtime deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH). By directly changing the latent code dimension, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures.",
    "metadata": {
      "arxiv_id": "2403.19163",
      "title": "D'OH: Decoder-Only Random Hypernetworks for Implicit Neural Representations",
      "summary": "Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no offline training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging redundancies that exist between layers. We propose to use a novel runtime decoder-only hypernetwork - that uses no offline training data - to better exploit cross-layer parameter redundancy. Previous applications of hypernetworks with deep implicit functions have employed feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the optimization of runtime deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH). By directly changing the latent code dimension, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures.",
      "authors": [
        "Cameron Gordon",
        "Lachlan Ewen MacDonald",
        "Hemanth Saratchandran",
        "Simon Lucey"
      ],
      "published": "2024-03-28T06:18:12Z",
      "updated": "2024-10-11T08:57:52Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2403.19163v2",
      "landing_url": "https://arxiv.org/abs/2403.19163v2",
      "doi": "https://doi.org/10.48550/arXiv.2403.19163"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on runtime hypernetwork optimization for implicit neural representations without any mention of discrete audio tokens, codecs, quantized vocabularies, or tokenizer design, so it fails to meet the inclusion criteria targeting discrete audio token research."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on runtime hypernetwork optimization for implicit neural representations without any mention of discrete audio tokens, codecs, quantized vocabularies, or tokenizer design, so it fails to meet the inclusion criteria targeting discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Boosting Cardiac Color Doppler Frame Rates with Deep Learning",
    "abstract": "Color Doppler echocardiography enables visualization of blood flow within the heart. However, the limited frame rate impedes the quantitative assessment of blood velocity throughout the cardiac cycle, thereby compromising a comprehensive analysis of ventricular filling. Concurrently, deep learning is demonstrating promising outcomes in post-processing of echocardiographic data for various applications. This work explores the use of deep learning models for intracardiac Doppler velocity estimation from a reduced number of filtered I/Q signals. We used a supervised learning approach by simulating patient-based cardiac color Doppler acquisitions and proposed data augmentation strategies to enlarge the training dataset. We implemented architectures based on convolutional neural networks. In particular, we focused on comparing the U-Net model and the recent ConvNeXt models, alongside assessing real-valued versus complex-valued representations. We found that both models outperformed the state-of-the-art autocorrelator method, effectively mitigating aliasing and noise. We did not observe significant differences between the use of real and complex data. Finally, we validated the models on in vitro and in vivo experiments. All models produced quantitatively comparable results to the baseline and were more robust to noise. ConvNeXt emerged as the sole model to achieve high-quality results on in vivo aliased samples. These results demonstrate the interest of supervised deep learning methods for Doppler velocity estimation from a reduced number of acquisitions.",
    "metadata": {
      "arxiv_id": "2404.00067",
      "title": "Boosting Cardiac Color Doppler Frame Rates with Deep Learning",
      "summary": "Color Doppler echocardiography enables visualization of blood flow within the heart. However, the limited frame rate impedes the quantitative assessment of blood velocity throughout the cardiac cycle, thereby compromising a comprehensive analysis of ventricular filling. Concurrently, deep learning is demonstrating promising outcomes in post-processing of echocardiographic data for various applications. This work explores the use of deep learning models for intracardiac Doppler velocity estimation from a reduced number of filtered I/Q signals. We used a supervised learning approach by simulating patient-based cardiac color Doppler acquisitions and proposed data augmentation strategies to enlarge the training dataset. We implemented architectures based on convolutional neural networks. In particular, we focused on comparing the U-Net model and the recent ConvNeXt models, alongside assessing real-valued versus complex-valued representations. We found that both models outperformed the state-of-the-art autocorrelator method, effectively mitigating aliasing and noise. We did not observe significant differences between the use of real and complex data. Finally, we validated the models on in vitro and in vivo experiments. All models produced quantitatively comparable results to the baseline and were more robust to noise. ConvNeXt emerged as the sole model to achieve high-quality results on in vivo aliased samples. These results demonstrate the interest of supervised deep learning methods for Doppler velocity estimation from a reduced number of acquisitions.",
      "authors": [
        "Julia Puig",
        "Denis Friboulet",
        "Hang Jung Ling",
        "François Varray",
        "Jonathan Porée",
        "Jean Provost",
        "Damien Garcia",
        "Fabien Millioz"
      ],
      "published": "2024-03-28T08:52:52Z",
      "updated": "2024-08-21T08:07:49Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00067v2",
      "landing_url": "https://arxiv.org/abs/2404.00067v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.00067"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on deep learning for echocardiographic Doppler velocity estimation without any discussion of discrete audio tokenization, vocabularies, or quantization of audio representations, so it fails to match requirements for discrete audio token methods."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on deep learning for echocardiographic Doppler velocity estimation without any discussion of discrete audio tokenization, vocabularies, or quantization of audio representations, so it fails to match requirements for discrete audio token methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transfer Learning with Reconstruction Loss",
    "abstract": "In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidden layer in the model. The proposed approach encourages the learned features to be general and transferable, and therefore can be readily used for efficient transfer learning. For numerical simulations, three applications are studied: transfer learning on classifying MNIST handwritten digits, the device-to-device wireless network power allocation, and the multiple-input-single-output network downlink beamforming and localization. Simulation results suggest that the proposed approach is highly efficient in data and model complexity, is resilient to over-fitting, and has competitive performances.",
    "metadata": {
      "arxiv_id": "2404.00505",
      "title": "Transfer Learning with Reconstruction Loss",
      "summary": "In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidden layer in the model. The proposed approach encourages the learned features to be general and transferable, and therefore can be readily used for efficient transfer learning. For numerical simulations, three applications are studied: transfer learning on classifying MNIST handwritten digits, the device-to-device wireless network power allocation, and the multiple-input-single-output network downlink beamforming and localization. Simulation results suggest that the proposed approach is highly efficient in data and model complexity, is resilient to over-fitting, and has competitive performances.",
      "authors": [
        "Wei Cui",
        "Wei Yu"
      ],
      "published": "2024-03-31T00:22:36Z",
      "updated": "2024-04-12T00:16:43Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.NI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.00505v2",
      "landing_url": "https://arxiv.org/abs/2404.00505v2",
      "doi": "https://doi.org/10.1109/TMLCN.2024.3384329"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on transfer learning across optimization tasks and adds a reconstruction loss for shared hidden features, with no discussion of discrete audio tokenizers, quantization, or vocabularies, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on transfer learning across optimization tasks and adds a reconstruction loss for shared hidden features, with no discussion of discrete audio tokenizers, quantization, or vocabularies, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
    "abstract": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
    "metadata": {
      "arxiv_id": "2404.02781",
      "title": "CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech",
      "summary": "With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.",
      "authors": [
        "Jaehyeon Kim",
        "Keon Lee",
        "Seungjun Chung",
        "Jaewoong Cho"
      ],
      "published": "2024-04-03T14:52:20Z",
      "updated": "2024-04-03T14:52:20Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.02781v1",
      "landing_url": "https://arxiv.org/abs/2404.02781v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.02781"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "CLaM-TTS clearly targets discrete neural audio codec tokens, compressing multiple token streams and evaluating naturalness/intelligibility/speaker similarity, so it satisfies the inclusion criteria and no exclusion applies."
    },
    "round-A_JuniorNano_reasoning": "CLaM-TTS clearly targets discrete neural audio codec tokens, compressing multiple token streams and evaluating naturalness/intelligibility/speaker similarity, so it satisfies the inclusion criteria and no exclusion applies.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
    "abstract": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
    "metadata": {
      "arxiv_id": "2404.03663",
      "title": "Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips",
      "summary": "Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as ``Meta-SpikeFormer\", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer in \\citet{yao2023spike} into a meta architecture, and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0\\% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7\\%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design. Source code and models are available at \\url{https://github.com/BICLab/Spike-Driven-Transformer-V2}.",
      "authors": [
        "Man Yao",
        "Jiakui Hu",
        "Tianxiang Hu",
        "Yifan Xu",
        "Zhaokun Zhou",
        "Yonghong Tian",
        "Bo Xu",
        "Guoqi Li"
      ],
      "published": "2024-02-15T13:26:18Z",
      "updated": "2024-02-15T13:26:18Z",
      "categories": [
        "cs.NE",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.03663v1",
      "landing_url": "https://arxiv.org/abs/2404.03663v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.03663"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe neuromorphic vision Transformer SNNs, not discrete audio token generation or codec/semantic token design, so it fails the inclusion focus and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe neuromorphic vision Transformer SNNs, not discrete audio token generation or codec/semantic token design, so it fails the inclusion focus and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model",
    "abstract": "This manuscript studies the unsupervised change point detection problem in time series of graphs using a decoder-only latent space model. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that bridges the observed graphs and latent representations. The prior distributions of the latent spaces are learned from the observed data as empirical Bayes to assist change point detection. Specifically, the model parameters are estimated via maximum approximate likelihood, with a Group Fused Lasso regularization imposed on the prior parameters. The augmented Lagrangian is solved via Alternating Direction Method of Multipliers, and Langevin Dynamics are recruited for posterior inference. Simulation studies show good performance of the latent space model in supporting change point detection and real data experiments yield change points that align with significant events.",
    "metadata": {
      "arxiv_id": "2404.04719",
      "title": "Change Point Detection in Dynamic Graphs with Decoder-only Latent Space Model",
      "summary": "This manuscript studies the unsupervised change point detection problem in time series of graphs using a decoder-only latent space model. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that bridges the observed graphs and latent representations. The prior distributions of the latent spaces are learned from the observed data as empirical Bayes to assist change point detection. Specifically, the model parameters are estimated via maximum approximate likelihood, with a Group Fused Lasso regularization imposed on the prior parameters. The augmented Lagrangian is solved via Alternating Direction Method of Multipliers, and Langevin Dynamics are recruited for posterior inference. Simulation studies show good performance of the latent space model in supporting change point detection and real data experiments yield change points that align with significant events.",
      "authors": [
        "Yik Lun Kei",
        "Jialiang Li",
        "Hangjian Li",
        "Yanzhen Chen",
        "Oscar Hernan Madrid Padilla"
      ],
      "published": "2024-04-06T19:50:37Z",
      "updated": "2025-04-17T09:01:12Z",
      "categories": [
        "stat.ME"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04719v4",
      "landing_url": "https://arxiv.org/abs/2404.04719v4",
      "doi": "https://doi.org/10.48550/arXiv.2404.04719"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe unsupervised change-point detection on dynamic graphs with latent-space modeling, which has no connection to discrete audio tokenization research, so it clearly violates inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe unsupervised change-point detection on dynamic graphs with latent-space modeling, which has no connection to discrete audio tokenization research, so it clearly violates inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gull: A Generative Multifunctional Audio Codec",
    "abstract": "We introduce Gull, a generative multifunctional audio codec. Gull is a general purpose neural audio compression and decompression model which can be applied to a wide range of tasks and applications such as real-time communication, audio super-resolution, and codec language models. The key components of Gull include (1) universal-sample-rate modeling via subband modeling schemes motivated by recent progress in audio source separation, (2) gain-shape representations motivated by traditional audio codecs, (3) improved residual vector quantization modules, (4) elastic decoder network that enables user-defined model size and complexity during inference time, (5) built-in ability for audio super-resolution without the increase of bitrate. We compare Gull with existing traditional and neural audio codecs and show that Gull is able to achieve on par or better performance across various sample rates, bitrates and model complexities in both subjective and objective evaluation metrics.",
    "metadata": {
      "arxiv_id": "2404.04947",
      "title": "Gull: A Generative Multifunctional Audio Codec",
      "summary": "We introduce Gull, a generative multifunctional audio codec. Gull is a general purpose neural audio compression and decompression model which can be applied to a wide range of tasks and applications such as real-time communication, audio super-resolution, and codec language models. The key components of Gull include (1) universal-sample-rate modeling via subband modeling schemes motivated by recent progress in audio source separation, (2) gain-shape representations motivated by traditional audio codecs, (3) improved residual vector quantization modules, (4) elastic decoder network that enables user-defined model size and complexity during inference time, (5) built-in ability for audio super-resolution without the increase of bitrate. We compare Gull with existing traditional and neural audio codecs and show that Gull is able to achieve on par or better performance across various sample rates, bitrates and model complexities in both subjective and objective evaluation metrics.",
      "authors": [
        "Yi Luo",
        "Jianwei Yu",
        "Hangting Chen",
        "Rongzhi Gu",
        "Chao Weng"
      ],
      "published": "2024-04-07T12:57:46Z",
      "updated": "2024-06-07T07:03:30Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.04947v2",
      "landing_url": "https://arxiv.org/abs/2404.04947v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.04947"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The abstract describes a neural audio codec (Gull) with quantization, residual vector quantization, and codec-style tokenization for compression and modeling, so it directly addresses discrete audio tokens with evaluations, hence it should be included (score 5)."
    },
    "round-A_JuniorNano_reasoning": "The abstract describes a neural audio codec (Gull) with quantization, residual vector quantization, and codec-style tokenization for compression and modeling, so it directly addresses discrete audio tokens with evaluations, hence it should be included (score 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Videos",
    "abstract": "In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses multiple challenges in the task, including the need for discriminability among frame representations, high output temporal resolution to maintain prediction precision, and the necessity to capture information at different temporal scales to handle events with varying dynamics. It tackles these challenges through its specifically designed architecture, featuring an encoder-decoder for leveraging multiple temporal scales and achieving high output temporal resolution, along with temporal modules designed to increase token discriminability. Leveraging these characteristics, T-DEED achieves SOTA performance on the FigureSkating and FineDiving datasets. Code is available at https://github.com/arturxe2/T-DEED.",
    "metadata": {
      "arxiv_id": "2404.05392",
      "title": "T-DEED: Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in Sports Videos",
      "summary": "In this paper, we introduce T-DEED, a Temporal-Discriminability Enhancer Encoder-Decoder for Precise Event Spotting in sports videos. T-DEED addresses multiple challenges in the task, including the need for discriminability among frame representations, high output temporal resolution to maintain prediction precision, and the necessity to capture information at different temporal scales to handle events with varying dynamics. It tackles these challenges through its specifically designed architecture, featuring an encoder-decoder for leveraging multiple temporal scales and achieving high output temporal resolution, along with temporal modules designed to increase token discriminability. Leveraging these characteristics, T-DEED achieves SOTA performance on the FigureSkating and FineDiving datasets. Code is available at https://github.com/arturxe2/T-DEED.",
      "authors": [
        "Artur Xarles",
        "Sergio Escalera",
        "Thomas B. Moeslund",
        "Albert Clapés"
      ],
      "published": "2024-04-08T10:51:29Z",
      "updated": "2024-04-11T13:36:58Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05392v2",
      "landing_url": "https://arxiv.org/abs/2404.05392v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.05392"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No mention of discrete audio-token generation or quantized vocabularies; the paper focuses on video event spotting with encoder-decoder architecture, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "No mention of discrete audio-token generation or quantized vocabularies; the paper focuses on video event spotting with encoder-decoder architecture, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MLP Can Be A Good Transformer Learner",
    "abstract": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
    "metadata": {
      "arxiv_id": "2404.05657",
      "title": "MLP Can Be A Good Transformer Learner",
      "summary": "Self-attention mechanism is the key of the Transformer but often criticized for its computation demands. Previous token pruning works motivate their methods from the view of computation redundancy but still need to load the full network and require same memory costs. This paper introduces a novel strategy that simplifies vision transformers and reduces computational load through the selective removal of non-essential attention layers, guided by entropy considerations. We identify that regarding the attention layer in bottom blocks, their subsequent MLP layers, i.e. two feed-forward layers, can elicit the same entropy quantity. Meanwhile, the accompanied MLPs are under-exploited since they exhibit smaller feature entropy compared to those MLPs in the top blocks. Therefore, we propose to integrate the uninformative attention layers into their subsequent counterparts by degenerating them into identical mapping, yielding only MLP in certain transformer blocks. Experimental results on ImageNet-1k show that the proposed method can remove 40% attention layer of DeiT-B, improving throughput and memory bound without performance compromise. Code is available at https://github.com/sihaoevery/lambda_vit.",
      "authors": [
        "Sihao Lin",
        "Pumeng Lyu",
        "Dongrui Liu",
        "Tao Tang",
        "Xiaodan Liang",
        "Andy Song",
        "Xiaojun Chang"
      ],
      "published": "2024-04-08T16:40:15Z",
      "updated": "2024-04-08T16:40:15Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.05657v1",
      "landing_url": "https://arxiv.org/abs/2404.05657v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.05657"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on simplifying vision transformer attention layers with MLPs for ImageNet, so it does not address discrete audio token generation, quantization, or evaluation, thus it fails all inclusion criteria and falls under exclusion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on simplifying vision transformer attention layers with MLPs for ImageNet, so it does not address discrete audio token generation, quantization, or evaluation, thus it fails all inclusion criteria and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
    "abstract": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
    "metadata": {
      "arxiv_id": "2404.06079",
      "title": "The X-LANCE Technical Report for Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge",
      "summary": "Discrete speech tokens have been more and more popular in multiple speech processing fields, including automatic speech recognition (ASR), text-to-speech (TTS) and singing voice synthesis (SVS). In this paper, we describe the systems developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track both with the whole training set and only 1h training data, with the highest UTMOS score and lowest bitrate among all submissions.",
      "authors": [
        "Yiwei Guo",
        "Chenrun Wang",
        "Yifan Yang",
        "Hankun Wang",
        "Ziyang Ma",
        "Chenpeng Du",
        "Shuai Wang",
        "Hanzheng Li",
        "Shuai Fan",
        "Hui Zhang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-09T07:37:41Z",
      "updated": "2024-04-10T00:33:25Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06079v2",
      "landing_url": "https://arxiv.org/abs/2404.06079v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.06079"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The report details discrete speech token systems for TTS/SVS/ASR, describing codec/token-based models and leaderboard results so it satisfies the criteria for inclusion; score 5."
    },
    "round-A_JuniorNano_reasoning": "The report details discrete speech token systems for TTS/SVS/ASR, describing codec/token-based models and leaderboard results so it satisfies the criteria for inclusion; score 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
    "abstract": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
    "metadata": {
      "arxiv_id": "2404.06690",
      "title": "CoVoMix: Advancing Zero-Shot Speech Generation for Human-like Multi-talker Conversations",
      "summary": "Recent advancements in zero-shot text-to-speech (TTS) modeling have led to significant strides in generating high-fidelity and diverse speech. However, dialogue generation, along with achieving human-like naturalness in speech, continues to be a challenge. In this paper, we introduce CoVoMix: Conversational Voice Mixture Generation, a novel model for zero-shot, human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix first converts dialogue text into multiple streams of discrete tokens, with each token stream representing semantic information for individual talkers. These token streams are then fed into a flow-matching based acoustic model to generate mixed mel-spectrograms. Finally, the speech waveforms are produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of metrics for measuring the effectiveness of dialogue modeling and generation. Our experimental results show that CoVoMix can generate dialogues that are not only human-like in their naturalness and coherence but also involve multiple talkers engaging in multiple rounds of conversation. This is exemplified by instances generated in a single channel where one speaker's utterance is seamlessly mixed with another's interjections or laughter, indicating the latter's role as an attentive listener. Audio samples are available at https://aka.ms/covomix.",
      "authors": [
        "Leying Zhang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Midia Yousefi",
        "Yanmin Qian",
        "Jinyu Li",
        "Lei He",
        "Sheng Zhao",
        "Michael Zeng"
      ],
      "published": "2024-04-10T02:32:58Z",
      "updated": "2024-12-15T16:30:54Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.06690v3",
      "landing_url": "https://arxiv.org/abs/2404.06690v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.06690"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on zero-shot multi-speaker TTS and converts dialogue text into discrete semantic token streams, but there is no mention of learning/quantizing discrete audio tokens or codecs as required by the inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on zero-shot multi-speaker TTS and converts dialogue text into discrete semantic token streams, but there is no mention of learning/quantizing discrete audio tokens or codecs as required by the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GLID: Pre-training a Generalist Encoder-Decoder Vision Model",
    "abstract": "This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks. While self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown success in transfer learning, task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training. GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be fine-tuned on various vision tasks with minimal task-specific architecture modifications. In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as \"query-to-answer\" problems, including the pre-training pretext task and other downstream tasks. We pre-train a task-agnostic encoder-decoder with query-mask pairs. During fine-tuning, GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads. This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks. GLID achieves competitive performance on various vision tasks, including object detection, image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer.",
    "metadata": {
      "arxiv_id": "2404.07603",
      "title": "GLID: Pre-training a Generalist Encoder-Decoder Vision Model",
      "summary": "This paper proposes a GeneraLIst encoder-Decoder (GLID) pre-training method for better handling various downstream computer vision tasks. While self-supervised pre-training approaches, e.g., Masked Autoencoder, have shown success in transfer learning, task-specific sub-architectures are still required to be appended for different downstream tasks, which cannot enjoy the benefits of large-scale pre-training. GLID overcomes this challenge by allowing the pre-trained generalist encoder-decoder to be fine-tuned on various vision tasks with minimal task-specific architecture modifications. In the GLID training scheme, pre-training pretext task and other downstream tasks are modeled as \"query-to-answer\" problems, including the pre-training pretext task and other downstream tasks. We pre-train a task-agnostic encoder-decoder with query-mask pairs. During fine-tuning, GLID maintains the pre-trained encoder-decoder and queries, only replacing the topmost linear transformation layer with task-specific linear heads. This minimizes the pretrain-finetune architecture inconsistency and enables the pre-trained model to better adapt to downstream tasks. GLID achieves competitive performance on various vision tasks, including object detection, image segmentation, pose estimation, and depth estimation, outperforming or matching specialist models such as Mask2Former, DETR, ViTPose, and BinsFormer.",
      "authors": [
        "Jihao Liu",
        "Jinliang Zheng",
        "Yu Liu",
        "Hongsheng Li"
      ],
      "published": "2024-04-11T09:43:07Z",
      "updated": "2024-04-11T09:43:07Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.07603v1",
      "landing_url": "https://arxiv.org/abs/2404.07603v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.07603"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about vision encoder-decoder pretraining without any audio tokenization, so it fails the audio discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper is about vision encoder-decoder pretraining without any audio tokenization, so it fails the audio discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation",
    "abstract": "Referring image segmentation aims to segment an object referred to by natural language expression from an image. The primary challenge lies in the efficient propagation of fine-grained semantic information from textual features to visual features. Many recent works utilize a Transformer to address this challenge. However, conventional transformer decoders can distort linguistic information with deeper layers, leading to suboptimal results. In this paper, we introduce CRFormer, a model that iteratively calibrates multi-modal features in the transformer decoder. We start by generating language queries using vision features, emphasizing different aspects of the input language. Then, we propose a novel Calibration Decoder (CDec) wherein the multi-modal features can iteratively calibrated by the input language features. In the Calibration Decoder, we use the output of each decoder layer and the original language features to generate new queries for continuous calibration, which gradually updates the language features. Based on CDec, we introduce a Language Reconstruction Module and a reconstruction loss. This module leverages queries from the final layer of the decoder to reconstruct the input language and compute the reconstruction loss. This can further prevent the language information from being lost or distorted. Our experiments consistently show the superior performance of our approach across RefCOCO, RefCOCO+, and G-Ref datasets compared to state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2404.08281",
      "title": "Calibration & Reconstruction: Deep Integrated Language for Referring Image Segmentation",
      "summary": "Referring image segmentation aims to segment an object referred to by natural language expression from an image. The primary challenge lies in the efficient propagation of fine-grained semantic information from textual features to visual features. Many recent works utilize a Transformer to address this challenge. However, conventional transformer decoders can distort linguistic information with deeper layers, leading to suboptimal results. In this paper, we introduce CRFormer, a model that iteratively calibrates multi-modal features in the transformer decoder. We start by generating language queries using vision features, emphasizing different aspects of the input language. Then, we propose a novel Calibration Decoder (CDec) wherein the multi-modal features can iteratively calibrated by the input language features. In the Calibration Decoder, we use the output of each decoder layer and the original language features to generate new queries for continuous calibration, which gradually updates the language features. Based on CDec, we introduce a Language Reconstruction Module and a reconstruction loss. This module leverages queries from the final layer of the decoder to reconstruct the input language and compute the reconstruction loss. This can further prevent the language information from being lost or distorted. Our experiments consistently show the superior performance of our approach across RefCOCO, RefCOCO+, and G-Ref datasets compared to state-of-the-art methods.",
      "authors": [
        "Yichen Yan",
        "Xingjian He",
        "Sihan Chen",
        "Jing Liu"
      ],
      "published": "2024-04-12T07:13:32Z",
      "updated": "2024-04-12T07:13:32Z",
      "categories": [
        "cs.CV",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.08281v1",
      "landing_url": "https://arxiv.org/abs/2404.08281v1",
      "doi": "https://doi.org/10.1145/3652583.3658095"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper addresses referring image segmentation with multimodal transformers and does not involve discrete audio tokens, so it clearly fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper addresses referring image segmentation with multimodal transformers and does not involve discrete audio tokens, so it clearly fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction",
    "abstract": "Large language models (LLMs) have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving LLM inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing LLM serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of LLMs and enable efficient interactive LLM serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict LLM output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.",
    "metadata": {
      "arxiv_id": "2404.08509",
      "title": "Efficient Interactive LLM Serving with Proxy Model-based Sequence Length Prediction",
      "summary": "Large language models (LLMs) have been driving a new wave of interactive AI applications across numerous domains. However, efficiently serving LLM inference requests is challenging due to their unpredictable execution times originating from the autoregressive nature of generative models. Existing LLM serving systems exploit first-come-first-serve (FCFS) scheduling, suffering from head-of-line blocking issues. To address the non-deterministic nature of LLMs and enable efficient interactive LLM serving, we present a speculative shortest-job-first (SSJF) scheduler that uses a light proxy model to predict LLM output sequence lengths. Our open-source SSJF implementation does not require changes to memory management or batching strategies. Evaluations on real-world datasets and production workload traces show that SSJF reduces average job completion times by 30.5-39.6% and increases throughput by 2.2-3.6x compared to FCFS schedulers, across no batching, dynamic batching, and continuous batching settings.",
      "authors": [
        "Haoran Qiu",
        "Weichao Mao",
        "Archit Patke",
        "Shengkun Cui",
        "Saurabh Jha",
        "Chen Wang",
        "Hubertus Franke",
        "Zbigniew T. Kalbarczyk",
        "Tamer Başar",
        "Ravishankar K. Iyer"
      ],
      "published": "2024-04-12T14:46:15Z",
      "updated": "2024-11-25T17:35:07Z",
      "categories": [
        "cs.DC",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.08509v2",
      "landing_url": "https://arxiv.org/abs/2404.08509v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.08509"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses solely on scheduling LLM inference based on sequence length prediction and mentions no discrete audio tokenization, codec, or token-level modeling, so it fails all inclusion criteria and matches the exclusion for non-audio token work."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses solely on scheduling LLM inference based on sequence length prediction and mentions no discrete audio tokenization, codec, or token-level modeling, so it fails all inclusion criteria and matches the exclusion for non-audio token work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
    "abstract": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
    "metadata": {
      "arxiv_id": "2404.11532",
      "title": "Select and Reorder: A Novel Approach for Neural Sign Language Production",
      "summary": "Sign languages, often categorised as low-resource languages, face significant challenges in achieving accurate translation due to the scarcity of parallel annotated datasets. This paper introduces Select and Reorder (S&R), a novel approach that addresses data scarcity by breaking down the translation process into two distinct steps: Gloss Selection (GS) and Gloss Reordering (GR). Our method leverages large spoken language models and the substantial lexical overlap between source spoken languages and target sign languages to establish an initial alignment. Both steps make use of Non-AutoRegressive (NAR) decoding for reduced computation and faster inference speeds. Through this disentanglement of tasks, we achieve state-of-the-art BLEU and Rouge scores on the Meine DGS Annotated (mDGS) dataset, demonstrating a substantial BLUE-1 improvement of 37.88% in Text to Gloss (T2G) Translation. This innovative approach paves the way for more effective translation models for sign languages, even in resource-constrained settings.",
      "authors": [
        "Harry Walsh",
        "Ben Saunders",
        "Richard Bowden"
      ],
      "published": "2024-04-17T16:25:19Z",
      "updated": "2024-04-17T16:25:19Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.11532v1",
      "landing_url": "https://arxiv.org/abs/2404.11532v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.11532"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on sign language gloss translation method and does not address discrete audio token generation, quantization, vocabularies, or codec evaluations, so it fails to meet inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on sign language gloss translation method and does not address discrete audio token generation, quantization, vocabularies, or codec evaluations, so it fails to meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HybridFlow: Infusing Continuity into Masked Codebook for Extreme Low-Bitrate Image Compression",
    "abstract": "This paper investigates the challenging problem of learned image compression (LIC) with extreme low bitrates. Previous LIC methods based on transmitting quantized continuous features often yield blurry and noisy reconstruction due to the severe quantization loss. While previous LIC methods based on learned codebooks that discretize visual space usually give poor-fidelity reconstruction due to the insufficient representation power of limited codewords in capturing faithful details. We propose a novel dual-stream framework, HyrbidFlow, which combines the continuous-feature-based and codebook-based streams to achieve both high perceptual quality and high fidelity under extreme low bitrates. The codebook-based stream benefits from the high-quality learned codebook priors to provide high quality and clarity in reconstructed images. The continuous feature stream targets at maintaining fidelity details. To achieve the ultra low bitrate, a masked token-based transformer is further proposed, where we only transmit a masked portion of codeword indices and recover the missing indices through token generation guided by information from the continuous feature stream. We also develop a bridging correction network to merge the two streams in pixel decoding for final image reconstruction, where the continuous stream features rectify biases of the codebook-based pixel decoder to impose reconstructed fidelity details. Experimental results demonstrate superior performance across several datasets under extremely low bitrates, compared with existing single-stream codebook-based or continuous-feature-based LIC methods.",
    "metadata": {
      "arxiv_id": "2404.13372",
      "title": "HybridFlow: Infusing Continuity into Masked Codebook for Extreme Low-Bitrate Image Compression",
      "summary": "This paper investigates the challenging problem of learned image compression (LIC) with extreme low bitrates. Previous LIC methods based on transmitting quantized continuous features often yield blurry and noisy reconstruction due to the severe quantization loss. While previous LIC methods based on learned codebooks that discretize visual space usually give poor-fidelity reconstruction due to the insufficient representation power of limited codewords in capturing faithful details. We propose a novel dual-stream framework, HyrbidFlow, which combines the continuous-feature-based and codebook-based streams to achieve both high perceptual quality and high fidelity under extreme low bitrates. The codebook-based stream benefits from the high-quality learned codebook priors to provide high quality and clarity in reconstructed images. The continuous feature stream targets at maintaining fidelity details. To achieve the ultra low bitrate, a masked token-based transformer is further proposed, where we only transmit a masked portion of codeword indices and recover the missing indices through token generation guided by information from the continuous feature stream. We also develop a bridging correction network to merge the two streams in pixel decoding for final image reconstruction, where the continuous stream features rectify biases of the codebook-based pixel decoder to impose reconstructed fidelity details. Experimental results demonstrate superior performance across several datasets under extremely low bitrates, compared with existing single-stream codebook-based or continuous-feature-based LIC methods.",
      "authors": [
        "Lei Lu",
        "Yanyue Xie",
        "Wei Jiang",
        "Wei Wang",
        "Xue Lin",
        "Yanzhi Wang"
      ],
      "published": "2024-04-20T13:19:08Z",
      "updated": "2024-04-20T13:19:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.13372v1",
      "landing_url": "https://arxiv.org/abs/2404.13372v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.13372"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on hybrid image compression without addressing discrete audio token generation, so it fails the audio-token inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on hybrid image compression without addressing discrete audio token generation, so it fails the audio-token inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards smaller, faster decoder-only transformers: Architectural variants and their implications",
    "abstract": "In recent times, the research on Large Language Models (LLMs) has grown exponentially, predominantly focusing on models underpinned by the transformer architecture, as established by [1], and further developed through the decoder-only variations by [2]. Contemporary efforts in this field primarily aim to enhance model capabilities by scaling up both the architecture and data volumes utilized during training. However, the exploration into reduce these model sizes while preserving their efficacy remains scant. In this study, we introduce three modifications to the decoder-only transformer architecture, namely ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). These variants demonstrate comparable performance to the conventional architecture in language generation, yet benefit from reduced model sizes and faster training processes. We open-source the model weights and the complete codebase for these implementation for further research.",
    "metadata": {
      "arxiv_id": "2404.14462",
      "title": "Towards smaller, faster decoder-only transformers: Architectural variants and their implications",
      "summary": "In recent times, the research on Large Language Models (LLMs) has grown exponentially, predominantly focusing on models underpinned by the transformer architecture, as established by [1], and further developed through the decoder-only variations by [2]. Contemporary efforts in this field primarily aim to enhance model capabilities by scaling up both the architecture and data volumes utilized during training. However, the exploration into reduce these model sizes while preserving their efficacy remains scant. In this study, we introduce three modifications to the decoder-only transformer architecture, namely ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). These variants demonstrate comparable performance to the conventional architecture in language generation, yet benefit from reduced model sizes and faster training processes. We open-source the model weights and the complete codebase for these implementation for further research.",
      "authors": [
        "Sathya Krishnan Suresh",
        "Shunmugapriya P"
      ],
      "published": "2024-04-22T06:19:46Z",
      "updated": "2024-10-08T09:20:56Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14462v4",
      "landing_url": "https://arxiv.org/abs/2404.14462v4",
      "doi": "https://doi.org/10.48550/arXiv.2404.14462"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on decoder-only transformer variants for LLM efficiency with no mention of discrete audio tokenization, so it fails to meet inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on decoder-only transformer variants for LLM efficiency with no mention of discrete audio tokenization, so it fails to meet inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
    "abstract": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
    "metadata": {
      "arxiv_id": "2404.14774",
      "title": "CoST: Contrastive Quantization based Semantic Tokenization for Generative Recommendation",
      "summary": "Embedding-based retrieval serves as a dominant approach to candidate item matching for industrial recommender systems. With the success of generative AI, generative retrieval has recently emerged as a new retrieval paradigm for recommendation, which casts item retrieval as a generation problem. Its model consists of two stages: semantic tokenization and autoregressive generation. The first stage involves item tokenization that constructs discrete semantic tokens to index items, while the second stage autoregressively generates semantic tokens of candidate items. Therefore, semantic tokenization serves as a crucial preliminary step for training generative recommendation models. Existing research usually employs a vector quantizier with reconstruction loss (e.g., RQ-VAE) to obtain semantic tokens of items, but this method fails to capture the essential neighborhood relationships that are vital for effective item modeling in recommender systems. In this paper, we propose a contrastive quantization-based semantic tokenization approach, named CoST, which harnesses both item relationships and semantic information to learn semantic tokens. Our experimental results highlight the significant impact of semantic tokenization on generative recommendation performance, with CoST achieving up to a 43% improvement in Recall@5 and 44% improvement in NDCG@5 on the MIND dataset over previous baselines.",
      "authors": [
        "Jieming Zhu",
        "Mengqun Jin",
        "Qijiong Liu",
        "Zexuan Qiu",
        "Zhenhua Dong",
        "Xiu Li"
      ],
      "published": "2024-04-23T06:29:48Z",
      "updated": "2024-09-07T16:11:36Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.14774v2",
      "landing_url": "https://arxiv.org/abs/2404.14774v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.14774"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on generating semantic tokens for recommender systems rather than discretizing audio signals; it therefore fails the requirement to center on discrete audio-token modeling and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on generating semantic tokens for recommender systems rather than discretizing audio signals; it therefore fails the requirement to center on discrete audio-token modeling and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Out-of-Distribution Detection using Maximum Entropy Coding",
    "abstract": "Given a default distribution $P$ and a set of test data $x^M=\\{x_1,x_2,\\ldots,x_M\\}$ this paper seeks to answer the question if it was likely that $x^M$ was generated by $P$. For discrete distributions, the definitive answer is in principle given by Kolmogorov-Martin-Löf randomness. In this paper we seek to generalize this to continuous distributions. We consider a set of statistics $T_1(x^M),T_2(x^M),\\ldots$. To each statistic we associate its maximum entropy distribution and with this a universal source coder. The maximum entropy distributions are subsequently combined to give a total codelength, which is compared with $-\\log P(x^M)$. We show that this approach satisfied a number of theoretical properties. For real world data $P$ usually is unknown. We transform data into a standard distribution in the latent space using a bidirectional generate network and use maximum entropy coding there. We compare the resulting method to other methods that also used generative neural networks to detect anomalies. In most cases, our results show better performance.",
    "metadata": {
      "arxiv_id": "2404.17023",
      "title": "Out-of-Distribution Detection using Maximum Entropy Coding",
      "summary": "Given a default distribution $P$ and a set of test data $x^M=\\{x_1,x_2,\\ldots,x_M\\}$ this paper seeks to answer the question if it was likely that $x^M$ was generated by $P$. For discrete distributions, the definitive answer is in principle given by Kolmogorov-Martin-Löf randomness. In this paper we seek to generalize this to continuous distributions. We consider a set of statistics $T_1(x^M),T_2(x^M),\\ldots$. To each statistic we associate its maximum entropy distribution and with this a universal source coder. The maximum entropy distributions are subsequently combined to give a total codelength, which is compared with $-\\log P(x^M)$. We show that this approach satisfied a number of theoretical properties.\n  For real world data $P$ usually is unknown. We transform data into a standard distribution in the latent space using a bidirectional generate network and use maximum entropy coding there. We compare the resulting method to other methods that also used generative neural networks to detect anomalies. In most cases, our results show better performance.",
      "authors": [
        "Mojtaba Abolfazli",
        "Mohammad Zaeri Amirani",
        "Anders Høst-Madsen",
        "June Zhang",
        "Andras Bratincsak"
      ],
      "published": "2024-04-25T20:28:43Z",
      "updated": "2024-04-25T20:28:43Z",
      "categories": [
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.17023v1",
      "landing_url": "https://arxiv.org/abs/2404.17023v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.17023"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on maximum entropy coding for out-of-distribution detection with generative models, not on discrete audio token generation/quantization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on maximum entropy coding for out-of-distribution detection with generative models, not on discrete audio token generation/quantization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Compressed Image Captioning using CNN-based Encoder-Decoder Framework",
    "abstract": "In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications. But one particularly exciting application is image captioning. The potential impact of effective image captioning is vast. It can significantly boost the accuracy of search engines, making it easier to find relevant information. Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digital content. However, despite its promise, image captioning presents several challenges. One major hurdle is extracting meaningful visual information from images and transforming it into coherent language. This requires bridging the gap between the visual and linguistic domains, a task that demands sophisticated algorithms and models. Our project is focused on addressing these challenges by developing an automatic image captioning architecture that combines the strengths of convolutional neural networks (CNNs) and encoder-decoder models. The CNN model is used to extract the visual features from images, and later, with the help of the encoder-decoder framework, captions are generated. We also did a performance comparison where we delved into the realm of pre-trained CNN models, experimenting with multiple architectures to understand their performance variations. In our quest for optimization, we also explored the integration of frequency regularization techniques to compress the \"AlexNet\" and \"EfficientNetB0\" model. We aimed to see if this compressed model could maintain its effectiveness in generating image captions while being more resource-efficient.",
    "metadata": {
      "arxiv_id": "2404.18062",
      "title": "Compressed Image Captioning using CNN-based Encoder-Decoder Framework",
      "summary": "In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications. But one particularly exciting application is image captioning. The potential impact of effective image captioning is vast. It can significantly boost the accuracy of search engines, making it easier to find relevant information. Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digital content. However, despite its promise, image captioning presents several challenges. One major hurdle is extracting meaningful visual information from images and transforming it into coherent language. This requires bridging the gap between the visual and linguistic domains, a task that demands sophisticated algorithms and models. Our project is focused on addressing these challenges by developing an automatic image captioning architecture that combines the strengths of convolutional neural networks (CNNs) and encoder-decoder models. The CNN model is used to extract the visual features from images, and later, with the help of the encoder-decoder framework, captions are generated. We also did a performance comparison where we delved into the realm of pre-trained CNN models, experimenting with multiple architectures to understand their performance variations. In our quest for optimization, we also explored the integration of frequency regularization techniques to compress the \"AlexNet\" and \"EfficientNetB0\" model. We aimed to see if this compressed model could maintain its effectiveness in generating image captions while being more resource-efficient.",
      "authors": [
        "Md Alif Rahman Ridoy",
        "M Mahmud Hasan",
        "Shovon Bhowmick"
      ],
      "published": "2024-04-28T03:47:48Z",
      "updated": "2024-04-28T03:47:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.18062v1",
      "landing_url": "https://arxiv.org/abs/2404.18062v1",
      "doi": "https://doi.org/10.48550/arXiv.2404.18062"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about image captioning with CNN encoder-decoder models and compressed visual features, so it does not address discrete audio tokenization or audio-specific codec/token evaluations, making it clearly outside the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper is about image captioning with CNN encoder-decoder models and compressed visual features, so it does not address discrete audio tokenization or audio-specific codec/token evaluations, making it clearly outside the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
    "abstract": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
    "metadata": {
      "arxiv_id": "2404.19441",
      "title": "ESC: Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers",
      "summary": "Neural speech codecs aim to compress input signals into minimal bits while maintaining content quality in a low-latency manner. However, existing neural codecs often trade model complexity for reconstruction performance. These codecs primarily use convolutional blocks for feature transformation, which are not inherently suited for capturing the local redundancies in speech signals. To compensate, they require either adversarial discriminators or a large number of model parameters to enhance audio quality. In response to these challenges, we introduce the Efficient Speech Codec (ESC), a lightweight, parameter-efficient speech codec based on a cross-scale residual vector quantization scheme and transformers. Our model employs mirrored hierarchical window transformer blocks and performs step-wise decoding from coarse-to-fine feature representations. To enhance bitrate efficiency, we propose a novel combination of vector quantization techniques along with a pre-training paradigm. Extensive experiments demonstrate that ESC can achieve high-fidelity speech reconstruction with significantly lower model complexity, making it a promising alternative to existing convolutional audio codecs.",
      "authors": [
        "Yuzhe Gu",
        "Enmao Diao"
      ],
      "published": "2024-04-30T10:44:33Z",
      "updated": "2024-10-03T12:23:26Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19441v3",
      "landing_url": "https://arxiv.org/abs/2404.19441v3",
      "doi": "https://doi.org/10.48550/arXiv.2404.19441"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The ESC codec centers on vector-quantized discrete codes for speech reconstruction and reports quality evaluations, so it matches the inclusion criteria with no exclusion issues."
    },
    "round-A_JuniorNano_reasoning": "The ESC codec centers on vector-quantized discrete codes for speech reconstruction and reports quality evaluations, so it matches the inclusion criteria with no exclusion issues.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Attention-Constrained Inference for Robust Decoder-Only Text-to-Speech",
    "abstract": "Recent popular decoder-only text-to-speech models are known for their ability of generating natural-sounding speech. However, such models sometimes suffer from word skipping and repeating due to the lack of explicit monotonic alignment constraints. In this paper, we notice from the attention maps that some particular attention heads of the decoder-only model indicate the alignments between speech and text. We call the attention maps of those heads Alignment-Emerged Attention Maps (AEAMs). Based on this discovery, we propose a novel inference method without altering the training process, named Attention-Constrained Inference (ACI), to facilitate monotonic synthesis. It first identifies AEAMs using the Attention Sweeping algorithm and then applies constraining masks on AEAMs. Our experimental results on decoder-only TTS model VALL-E show that the WER of synthesized speech is reduced by up to 20.5% relatively with ACI while the naturalness and speaker similarity are comparable.",
    "metadata": {
      "arxiv_id": "2404.19723",
      "title": "Attention-Constrained Inference for Robust Decoder-Only Text-to-Speech",
      "summary": "Recent popular decoder-only text-to-speech models are known for their ability of generating natural-sounding speech. However, such models sometimes suffer from word skipping and repeating due to the lack of explicit monotonic alignment constraints. In this paper, we notice from the attention maps that some particular attention heads of the decoder-only model indicate the alignments between speech and text. We call the attention maps of those heads Alignment-Emerged Attention Maps (AEAMs). Based on this discovery, we propose a novel inference method without altering the training process, named Attention-Constrained Inference (ACI), to facilitate monotonic synthesis. It first identifies AEAMs using the Attention Sweeping algorithm and then applies constraining masks on AEAMs. Our experimental results on decoder-only TTS model VALL-E show that the WER of synthesized speech is reduced by up to 20.5% relatively with ACI while the naturalness and speaker similarity are comparable.",
      "authors": [
        "Hankun Wang",
        "Chenpeng Du",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-04-30T17:17:07Z",
      "updated": "2024-10-18T11:54:14Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2404.19723v2",
      "landing_url": "https://arxiv.org/abs/2404.19723v2",
      "doi": "https://doi.org/10.48550/arXiv.2404.19723"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on inference constraints for decoder-only TTS to avoid word skipping/repeating without discussing discrete audio-token vocabularies, quantization, or token-level evaluations, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on inference constraints for decoder-only TTS to avoid word skipping/repeating without discussing discrete audio-token vocabularies, quantization, or token-level evaluations, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound",
    "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.",
    "metadata": {
      "arxiv_id": "2405.00233",
      "title": "SemantiCodec: An Ultra Low Bitrate Semantic Audio Codec for General Sound",
      "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modelling techniques to audio data. However, traditional codecs often operate at high bitrates or within narrow domains such as speech and lack the semantic clues required for efficient language modelling. Addressing these challenges, we introduce SemantiCodec, a novel codec designed to compress audio into fewer than a hundred tokens per second across diverse audio types, including speech, general sound, and music, without compromising quality. SemantiCodec features a dual-encoder architecture: a semantic encoder using a self-supervised pre-trained Audio Masked Autoencoder (AudioMAE), discretized using k-means clustering on extensive audio data, and an acoustic encoder to capture the remaining details. The semantic and acoustic encoder outputs are used to reconstruct audio via a diffusion-model-based decoder. SemantiCodec is presented in three variants with token rates of 25, 50, and 100 per second, supporting a range of ultra-low bit rates between 0.31 kbps and 1.40 kbps. Experimental results demonstrate that SemantiCodec significantly outperforms the state-of-the-art Descript codec on reconstruction quality. Our results also suggest that SemantiCodec contains significantly richer semantic information than all evaluated state-of-the-art audio codecs, even at significantly lower bitrates. Our code and demos are available at https://haoheliu.github.io/SemantiCodec/.",
      "authors": [
        "Haohe Liu",
        "Xuenan Xu",
        "Yi Yuan",
        "Mengyue Wu",
        "Wenwu Wang",
        "Mark D. Plumbley"
      ],
      "published": "2024-04-30T22:51:36Z",
      "updated": "2024-11-28T12:31:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.MM",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.00233v2",
      "landing_url": "https://arxiv.org/abs/2405.00233v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3506286"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "SemantiCodec clearly generates discrete semantic and acoustic tokens via quantization of SSL representations and neural codec outputs, evaluates reconstruction quality versus baselines, and targets audio token modeling, so it fully meets the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "SemantiCodec clearly generates discrete semantic and acoustic tokens via quantization of SSL representations and neural codec outputs, evaluates reconstruction quality versus baselines, and targets audio token modeling, so it fully meets the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "One-Shot Wyner-Ziv Compression of a Uniform Source",
    "abstract": "In this paper, we consider the one-shot version of the classical Wyner-Ziv problem where a source is compressed in a lossy fashion when only the decoder has access to a correlated side information. Following the entropy-constrained quantization framework, we assume a scalar quantizer followed by variable length entropy coding. We consider compression of a uniform source, motivated by its role in the compression of processes with low-dimensional features embedded within a high-dimensional ambient space. We find upper and lower bounds to the entropy-distortion functions of the uniform source for quantized and noisy side information, and illustrate tightness of the bounds at high compression rates.",
    "metadata": {
      "arxiv_id": "2405.01774",
      "title": "One-Shot Wyner-Ziv Compression of a Uniform Source",
      "summary": "In this paper, we consider the one-shot version of the classical Wyner-Ziv problem where a source is compressed in a lossy fashion when only the decoder has access to a correlated side information. Following the entropy-constrained quantization framework, we assume a scalar quantizer followed by variable length entropy coding. We consider compression of a uniform source, motivated by its role in the compression of processes with low-dimensional features embedded within a high-dimensional ambient space. We find upper and lower bounds to the entropy-distortion functions of the uniform source for quantized and noisy side information, and illustrate tightness of the bounds at high compression rates.",
      "authors": [
        "Oğuzhan Kubilay Ülger",
        "Elza Erkip"
      ],
      "published": "2024-05-02T23:12:48Z",
      "updated": "2024-05-02T23:12:48Z",
      "categories": [
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.01774v1",
      "landing_url": "https://arxiv.org/abs/2405.01774v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.01774"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies one-shot Wyner-Ziv lossy compression for a uniform source with scalar quantization, but it does not concern discrete audio tokens, tokenizers, vocabularies, or evaluations of codec/semantic units, so it fails the inclusion topic and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper studies one-shot Wyner-Ziv lossy compression for a uniform source with scalar quantization, but it does not concern discrete audio tokens, tokenizers, vocabularies, or evaluations of codec/semantic units, so it fails the inclusion topic and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
    "abstract": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
    "metadata": {
      "arxiv_id": "2405.02330",
      "title": "Adaptive Semantic Token Selection for AI-native Goal-oriented Communications",
      "summary": "In this paper, we propose a novel design for AI-native goal-oriented communications, exploiting transformer neural networks under dynamic inference constraints on bandwidth and computation. Transformers have become the standard architecture for pretraining large-scale vision and text models, and preliminary results have shown promising performance also in deep joint source-channel coding (JSCC). Here, we consider a dynamic model where communication happens over a channel with variable latency and bandwidth constraints. Leveraging recent works on conditional computation, we exploit the structure of the transformer blocks and the multihead attention operator to design a trainable semantic token selection mechanism that learns to select relevant tokens (e.g., image patches) from the input signal. This is done dynamically, on a per-input basis, with a rate that can be chosen as an additional input by the user. We show that our model improves over state-of-the-art token selection mechanisms, exhibiting high accuracy for a wide range of latency and bandwidth constraints, without the need for deploying multiple architectures tailored to each constraint. Last, but not least, the proposed token selection mechanism helps extract powerful semantics that are easy to understand and explain, paving the way for interpretable-by-design models for the next generation of AI-native communication systems.",
      "authors": [
        "Alessio Devoto",
        "Simone Petruzzi",
        "Jary Pomponi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2024-04-25T13:49:50Z",
      "updated": "2024-04-25T13:49:50Z",
      "categories": [
        "cs.IT",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.02330v1",
      "landing_url": "https://arxiv.org/abs/2405.02330v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.02330"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on transformer token selection in goal-oriented communications (likely vision/text) rather than quantized discrete audio tokens, so it fails the audio-token inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on transformer token selection in goal-oriented communications (likely vision/text) rather than quantized discrete audio tokens, so it fails the audio-token inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
    "abstract": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
    "metadata": {
      "arxiv_id": "2405.03110",
      "title": "Vector Quantization for Recommender Systems: A Review and Outlook",
      "summary": "Vector quantization, renowned for its unparalleled feature compression capabilities, has been a prominent topic in signal processing and machine learning research for several decades and remains widely utilized today. With the emergence of large models and generative AI, vector quantization has gained popularity in recommender systems, establishing itself as a preferred solution. This paper starts with a comprehensive review of vector quantization techniques. It then explores systematic taxonomies of vector quantization methods for recommender systems (VQ4Rec), examining their applications from multiple perspectives. Further, it provides a thorough introduction to research efforts in diverse recommendation scenarios, including efficiency-oriented approaches and quality-oriented approaches. Finally, the survey analyzes the remaining challenges and anticipates future trends in VQ4Rec, including the challenges associated with the training of vector quantization, the opportunities presented by large language models, and emerging trends in multimodal recommender systems. We hope this survey can pave the way for future researchers in the recommendation community and accelerate their exploration in this promising field.",
      "authors": [
        "Qijiong Liu",
        "Xiaoyu Dong",
        "Jiaren Xiao",
        "Nuo Chen",
        "Hengchang Hu",
        "Jieming Zhu",
        "Chenxu Zhu",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-06T02:06:26Z",
      "updated": "2024-05-06T02:06:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.03110v1",
      "landing_url": "https://arxiv.org/abs/2405.03110v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.03110"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This survey focuses on vector quantization for recommender systems rather than discrete audio token generation or evaluation, so it fails the inclusion requirements and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "This survey focuses on vector quantization for recommender systems rather than discrete audio token generation or evaluation, so it fails the inclusion requirements and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
    "abstract": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
    "metadata": {
      "arxiv_id": "2405.04752",
      "title": "HILCodec: High-Fidelity and Lightweight Neural Audio Codec",
      "summary": "The recent advancement of end-to-end neural audio codecs enables compressing audio at very low bitrates while reconstructing the output audio with high fidelity. Nonetheless, such improvements often come at the cost of increased model complexity. In this paper, we identify and address the problems of existing neural audio codecs. We show that the performance of the SEANet-based codec does not increase consistently as the network depth increases. We analyze the root cause of such a phenomenon and suggest a variance-constrained design. Also, we reveal various distortions in previous waveform domain discriminators and propose a novel distortion-free discriminator. The resulting model, HILCodec, is a real-time streaming audio codec that demonstrates state-of-the-art quality across various bitrates and audio types.",
      "authors": [
        "Sunghwan Ahn",
        "Beom Jun Woo",
        "Min Hyun Han",
        "Chanyeong Moon",
        "Nam Soo Kim"
      ],
      "published": "2024-05-08T01:40:13Z",
      "updated": "2024-09-24T07:40:54Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.04752v2",
      "landing_url": "https://arxiv.org/abs/2405.04752v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.04752"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper proposes a neural audio codec with quantization-based discrete codes, analyzes architectural trade-offs, and presents codec quality evaluations, so it aligns with the discrete audio token topic and lacks exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a neural audio codec with quantization-based discrete codes, analyzes architectural trade-offs, and presents codec quality evaluations, so it aligns with the discrete audio token topic and lacks exclusion triggers.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Restricted Randomized Benchmarking with Universal Gates of Fixed Sequence Length",
    "abstract": "The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible. Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation. We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one. This makes our protocol highly resource efficient and practical for small qubit numbers. We exemplify our protocol for creating Haar-randomness in the case of single and two qubits. Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique. We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise.",
    "metadata": {
      "arxiv_id": "2405.05215",
      "title": "Restricted Randomized Benchmarking with Universal Gates of Fixed Sequence Length",
      "summary": "The standard randomized benchmarking protocol requires access to often complex operations that are not always directly accessible. Compiler optimization does not always ensure equal sequence length of the directly accessible universal gates for each random operation. We introduce a version of the RB protocol that creates Haar-randomness using a directly accessible universal gate set of equal sequence length rather than relying upon a t-design or even an approximate one. This makes our protocol highly resource efficient and practical for small qubit numbers. We exemplify our protocol for creating Haar-randomness in the case of single and two qubits. Benchmarking our result with the standard RB protocol, allows us to calculate the overestimation of the average gate fidelity as compared to the standard technique. We augment our findings with a noise analysis which demonstrates that our method could be an effective tool for building accurate models of experimental noise.",
      "authors": [
        "Mohsen Mehrani",
        "Kasra Masoudi",
        "Rawad Mezher",
        "Elham Kashefi",
        "Debasis Sadhukhan"
      ],
      "published": "2024-05-08T17:07:56Z",
      "updated": "2024-05-08T17:07:56Z",
      "categories": [
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.05215v1",
      "landing_url": "https://arxiv.org/abs/2405.05215v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.05215"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes randomized benchmarking for quantum gates without any mention of discrete audio tokens, tokenizers, quantization, or audio-language modeling, so it fails every inclusion criterion and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper describes randomized benchmarking for quantum gates without any mention of discrete audio tokens, tokenizers, quantization, or audio-language modeling, so it fails every inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
    "abstract": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
    "metadata": {
      "arxiv_id": "2405.06266",
      "title": "A Multi-Channel Spatial-Temporal Transformer Model for Traffic Flow Forecasting",
      "summary": "Traffic flow forecasting is a crucial task in transportation management and planning. The main challenges for traffic flow forecasting are that (1) as the length of prediction time increases, the accuracy of prediction will decrease; (2) the predicted results greatly rely on the extraction of temporal and spatial dependencies from the road networks. To overcome the challenges mentioned above, we propose a multi-channel spatial-temporal transformer model for traffic flow forecasting, which improves the accuracy of the prediction by fusing results from different channels of traffic data. Our approach leverages graph convolutional network to extract spatial features from each channel while using a transformer-based architecture to capture temporal dependencies across channels. We introduce an adaptive adjacency matrix to overcome limitations in feature extraction from fixed topological structures. Experimental results on six real-world datasets demonstrate that introducing a multi-channel mechanism into the temporal model enhances performance and our proposed model outperforms state-of-the-art models in terms of accuracy.",
      "authors": [
        "Jianli Xiao",
        "Baichao Long"
      ],
      "published": "2024-05-10T06:37:07Z",
      "updated": "2024-05-10T06:37:07Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.06266v1",
      "landing_url": "https://arxiv.org/abs/2405.06266v1",
      "doi": "https://doi.org/10.1016/j.ins.2024.120648"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Traffic forecasting paper has nothing to do with discrete audio tokenization or codec/token evaluation, so it clearly fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Traffic forecasting paper has nothing to do with discrete audio tokenization or codec/token evaluation, so it clearly fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
    "abstract": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
    "metadata": {
      "arxiv_id": "2405.07682",
      "title": "FastSAG: Towards Fast Non-Autoregressive Singing Accompaniment Generation",
      "summary": "Singing Accompaniment Generation (SAG), which generates instrumental music to accompany input vocals, is crucial to developing human-AI symbiotic art creation systems. The state-of-the-art method, SingSong, utilizes a multi-stage autoregressive (AR) model for SAG, however, this method is extremely slow as it generates semantic and acoustic tokens recursively, and this makes it impossible for real-time applications. In this paper, we aim to develop a Fast SAG method that can create high-quality and coherent accompaniments. A non-AR diffusion-based framework is developed, which by carefully designing the conditions inferred from the vocal signals, generates the Mel spectrogram of the target accompaniment directly. With diffusion and Mel spectrogram modeling, the proposed method significantly simplifies the AR token-based SingSong framework, and largely accelerates the generation. We also design semantic projection, prior projection blocks as well as a set of loss functions, to ensure the generated accompaniment has semantic and rhythm coherence with the vocal signal. By intensive experimental studies, we demonstrate that the proposed method can generate better samples than SingSong, and accelerate the generation by at least 30 times. Audio samples and code are available at https://fastsag.github.io/.",
      "authors": [
        "Jianyi Chen",
        "Wei Xue",
        "Xu Tan",
        "Zhen Ye",
        "Qifeng Liu",
        "Yike Guo"
      ],
      "published": "2024-05-13T12:14:54Z",
      "updated": "2024-05-13T12:14:54Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.07682v1",
      "landing_url": "https://arxiv.org/abs/2405.07682v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.07682"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focusing on diffusion-based Mel-spectrogram generation without describing any discrete tokenization/quantization or token-level modeling makes it outside the discrete audio token scope, so exclude."
    },
    "round-A_JuniorNano_reasoning": "Focusing on diffusion-based Mel-spectrogram generation without describing any discrete tokenization/quantization or token-level modeling makes it outside the discrete audio token scope, so exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffTF++: 3D-aware Diffusion Transformer for Large-Vocabulary 3D Generation",
    "abstract": "Generating diverse and high-quality 3D assets automatically poses a fundamental yet challenging task in 3D computer vision. Despite extensive efforts in 3D generation, existing optimization-based approaches struggle to produce large-scale 3D assets efficiently. Meanwhile, feed-forward methods often focus on generating only a single category or a few categories, limiting their generalizability. Therefore, we introduce a diffusion-based feed-forward framework to address these challenges with a single model. To handle the large diversity and complexity in geometry and texture across categories efficiently, we 1) adopt improved triplane to guarantee efficiency; 2) introduce the 3D-aware transformer to aggregate the generalized 3D knowledge with specialized 3D features; and 3) devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge. Building upon our 3D-aware Diffusion model with TransFormer, DiffTF, we propose a stronger version for 3D generation, i.e., DiffTF++. It boils down to two parts: multi-view reconstruction loss and triplane refinement. Specifically, we utilize multi-view reconstruction loss to fine-tune the diffusion model and triplane decoder, thereby avoiding the negative influence caused by reconstruction errors and improving texture synthesis. By eliminating the mismatch between the two stages, the generative performance is enhanced, especially in texture. Additionally, a 3D-aware refinement process is introduced to filter out artifacts and refine triplanes, resulting in the generation of more intricate and reasonable details. Extensive experiments on ShapeNet and OmniObject3D convincingly demonstrate the effectiveness of our proposed modules and the state-of-the-art 3D object generation performance with large diversity, rich semantics, and high quality.",
    "metadata": {
      "arxiv_id": "2405.08055",
      "title": "DiffTF++: 3D-aware Diffusion Transformer for Large-Vocabulary 3D Generation",
      "summary": "Generating diverse and high-quality 3D assets automatically poses a fundamental yet challenging task in 3D computer vision. Despite extensive efforts in 3D generation, existing optimization-based approaches struggle to produce large-scale 3D assets efficiently. Meanwhile, feed-forward methods often focus on generating only a single category or a few categories, limiting their generalizability. Therefore, we introduce a diffusion-based feed-forward framework to address these challenges with a single model. To handle the large diversity and complexity in geometry and texture across categories efficiently, we 1) adopt improved triplane to guarantee efficiency; 2) introduce the 3D-aware transformer to aggregate the generalized 3D knowledge with specialized 3D features; and 3) devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge. Building upon our 3D-aware Diffusion model with TransFormer, DiffTF, we propose a stronger version for 3D generation, i.e., DiffTF++. It boils down to two parts: multi-view reconstruction loss and triplane refinement. Specifically, we utilize multi-view reconstruction loss to fine-tune the diffusion model and triplane decoder, thereby avoiding the negative influence caused by reconstruction errors and improving texture synthesis. By eliminating the mismatch between the two stages, the generative performance is enhanced, especially in texture. Additionally, a 3D-aware refinement process is introduced to filter out artifacts and refine triplanes, resulting in the generation of more intricate and reasonable details. Extensive experiments on ShapeNet and OmniObject3D convincingly demonstrate the effectiveness of our proposed modules and the state-of-the-art 3D object generation performance with large diversity, rich semantics, and high quality.",
      "authors": [
        "Ziang Cao",
        "Fangzhou Hong",
        "Tong Wu",
        "Liang Pan",
        "Ziwei Liu"
      ],
      "published": "2024-05-13T17:59:51Z",
      "updated": "2024-05-13T17:59:51Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08055v1",
      "landing_url": "https://arxiv.org/abs/2405.08055v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.08055"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe 3D object generation via diffusion on triplanes and transformers, with no focus on generating or quantizing discrete audio tokens, so it fails the inclusion criteria outright."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe 3D object generation via diffusion on triplanes and transformers, with no focus on generating or quantizing discrete audio tokens, so it fails the inclusion criteria outright.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
    "abstract": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
    "metadata": {
      "arxiv_id": "2405.08417",
      "title": "Neural Speech Coding for Real-time Communications using Constant Bitrate Scalar Quantization",
      "summary": "Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder is learned. This allows for efficient transmission of the input audio signal. The learned discrete representation of neural codecs is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose and analyze simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. For real-time speech communication applications, these neural codecs are required to operate at low complexity, low latency and at low bitrates. We address those challenges by proposing a new causal network architecture that is based on SQ and a Short-Time Fourier Transform (STFT) representation. The proposed method performs particularly well in the very low complexity and low bitrate regime.",
      "authors": [
        "Andreas Brendel",
        "Nicola Pia",
        "Kishan Gupta",
        "Lyonel Behringer",
        "Guillaume Fuchs",
        "Markus Multrus"
      ],
      "published": "2024-05-14T08:23:30Z",
      "updated": "2024-09-19T12:02:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08417v2",
      "landing_url": "https://arxiv.org/abs/2405.08417v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.08417"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper clearly focuses on learning neural codec quantization strategies to generate discrete codes (scalar quantization tokens) for low-bitrate speech reconstruction, matching the discrete audio token inclusion criteria, so it is better to include."
    },
    "round-A_JuniorNano_reasoning": "The paper clearly focuses on learning neural codec quantization strategies to generate discrete codes (scalar quantization tokens) for low-bitrate speech reconstruction, matching the discrete audio token inclusion criteria, so it is better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
    "abstract": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
    "metadata": {
      "arxiv_id": "2405.08429",
      "title": "TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection",
      "summary": "Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.",
      "authors": [
        "Martín Bayón-Gutiérrez",
        "María Teresa García-Ordás",
        "Héctor Alaiz Moretón",
        "Jose Aveleira-Mata",
        "Sergio Rubio Martín",
        "José Alberto Benítez-Andrades"
      ],
      "published": "2024-05-14T08:45:34Z",
      "updated": "2024-05-14T08:45:34Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.08429v1",
      "landing_url": "https://arxiv.org/abs/2405.08429v1",
      "doi": "https://doi.org/10.1093/jigpal/jzae048"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on road surface detection using camera and LiDAR features, with no mention of discrete audio tokenization, encoders/decoders for audio, vocabularies, or token evaluation, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on road surface detection using camera and LiDAR features, with no mention of discrete audio tokenization, encoders/decoders for audio, vocabularies, or token evaluation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
    "abstract": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
    "metadata": {
      "arxiv_id": "2405.09508",
      "title": "Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming",
      "summary": "This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer models in replicating cross-language structural priming, a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Our findings indicate that transformers outperform RNNs in generating primed sentence structures, with accuracy rates that exceed 25.84\\% to 33. 33\\%. This challenges the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggests a role for cue-based retrieval mechanisms. This work contributes to our understanding of how computational models may reflect human cognitive processes across diverse language families.",
      "authors": [
        "Demi Zhang",
        "Bushi Xiao",
        "Chao Gao",
        "Sangpil Youm",
        "Bonnie J Dorr"
      ],
      "published": "2024-05-15T17:01:02Z",
      "updated": "2024-10-15T20:24:00Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09508v2",
      "landing_url": "https://arxiv.org/abs/2405.09508v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.09508"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about modeling cross-language structural priming with RNN/Transformer on text, so it does not involve discrete audio token generation/quantization or evaluation as required, thus excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper is about modeling cross-language structural priming with RNN/Transformer on text, so it does not involve discrete audio token generation/quantization or evaluation as required, thus excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
    "abstract": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
    "metadata": {
      "arxiv_id": "2405.09768",
      "title": "Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model",
      "summary": "Recent advances in generative language modeling applied to discrete speech tokens presented a new avenue for text-to-speech (TTS) synthesis. These speech language models (SLMs), similarly to their textual counterparts, are scalable, probabilistic, and context-aware. While they can produce diverse and natural outputs, they sometimes face issues such as unintelligibility and the inclusion of non-speech noises or hallucination. As the adoption of this innovative paradigm in speech synthesis increases, there is a clear need for an in-depth evaluation of its capabilities and limitations. In this paper, we evaluate TTS from a discrete token-based SLM, through both automatic metrics and listening tests. We examine five key dimensions: speaking style, intelligibility, speaker consistency, prosodic variation, spontaneous behaviour. Our results highlight the model's strength in generating varied prosody and spontaneous outputs. It is also rated higher in naturalness and context appropriateness in listening tests compared to a conventional TTS. However, the model's performance in intelligibility and speaker consistency lags behind traditional TTS. Additionally, we show that increasing the scale of SLMs offers a modest boost in robustness. Our findings aim to serve as a benchmark for future advancements in generative SLMs for speech synthesis.",
      "authors": [
        "Siyang Wang",
        "Éva Székely"
      ],
      "published": "2024-05-16T02:18:41Z",
      "updated": "2024-05-16T02:18:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09768v1",
      "landing_url": "https://arxiv.org/abs/2405.09768v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09768"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Evaluation details a discrete token-based speech language model for TTS, discussing codec/token modeling and offering intelligibility, prosody, and listening test benchmarks, so it satisfies all inclusion criteria with no exclusion reasons."
    },
    "round-A_JuniorNano_reasoning": "Evaluation details a discrete token-based speech language model for TTS, discussing codec/token modeling and offering intelligibility, prosody, and listening test benchmarks, so it satisfies all inclusion criteria with no exclusion reasons.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
    "abstract": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
    "metadata": {
      "arxiv_id": "2405.09789",
      "title": "LeMeViT: Efficient Vision Transformer with Learnable Meta Tokens for Remote Sensing Image Interpretation",
      "summary": "Due to spatial redundancy in remote sensing images, sparse tokens containing rich information are usually involved in self-attention (SA) to reduce the overall token numbers within the calculation, avoiding the high computational cost issue in Vision Transformers. However, such methods usually obtain sparse tokens by hand-crafted or parallel-unfriendly designs, posing a challenge to reach a better balance between efficiency and performance. Different from them, this paper proposes to use learnable meta tokens to formulate sparse tokens, which effectively learn key information meanwhile improving the inference speed. Technically, the meta tokens are first initialized from image tokens via cross-attention. Then, we propose Dual Cross-Attention (DCA) to promote information exchange between image tokens and meta tokens, where they serve as query and key (value) tokens alternatively in a dual-branch structure, significantly reducing the computational complexity compared to self-attention. By employing DCA in the early stages with dense visual tokens, we obtain the hierarchical architecture LeMeViT with various sizes. Experimental results in classification and dense prediction tasks show that LeMeViT has a significant $1.7 \\times$ speedup, fewer parameters, and competitive performance compared to the baseline models, and achieves a better trade-off between efficiency and performance.",
      "authors": [
        "Wentao Jiang",
        "Jing Zhang",
        "Di Wang",
        "Qiming Zhang",
        "Zengmao Wang",
        "Bo Du"
      ],
      "published": "2024-05-16T03:26:06Z",
      "updated": "2024-05-16T03:26:06Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.09789v1",
      "landing_url": "https://arxiv.org/abs/2405.09789v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.09789"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on vision transformers for remote sensing, not on discrete audio token generation/quantization, so it fails to meet the discrete-audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on vision transformers for remote sensing, not on discrete audio token generation/quantization, so it fails to meet the discrete-audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accelerating Relative Entropy Coding with Space Partitioning",
    "abstract": "Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times greater than what previous methods can manage, and reduces the bitrate by approximately 5-15% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10, compared to previous methods, significantly improving the practicality of REC for neural compression.",
    "metadata": {
      "arxiv_id": "2405.12203",
      "title": "Accelerating Relative Entropy Coding with Space Partitioning",
      "summary": "Relative entropy coding (REC) algorithms encode a random sample following a target distribution $Q$, using a coding distribution $P$ shared between the sender and receiver. Sadly, general REC algorithms suffer from prohibitive encoding times, at least on the order of $2^{D_{\\text{KL}}[Q||P]}$, and faster algorithms are limited to very specific settings. This work addresses this issue by introducing a REC scheme utilizing space partitioning to reduce runtime in practical scenarios. We provide theoretical analyses of our method and demonstrate its effectiveness with both toy examples and practical applications. Notably, our method successfully handles REC tasks with $D_{\\text{KL}}[Q||P]$ about three times greater than what previous methods can manage, and reduces the bitrate by approximately 5-15% in VAE-based lossless compression on MNIST and INR-based lossy compression on CIFAR-10, compared to previous methods, significantly improving the practicality of REC for neural compression.",
      "authors": [
        "Jiajun He",
        "Gergely Flamich",
        "José Miguel Hernández-Lobato"
      ],
      "published": "2024-05-20T17:41:19Z",
      "updated": "2024-10-29T10:14:32Z",
      "categories": [
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.12203v3",
      "landing_url": "https://arxiv.org/abs/2405.12203v3",
      "doi": "https://doi.org/10.48550/arXiv.2405.12203"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on general relative entropy coding acceleration and neural compression results without any discussion of discrete audio tokenization or tokenizer/quantization details tied to audio signals, so it fails the inclusion criteria (non-audio, non-token focus)."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on general relative entropy coding acceleration and neural compression results without any discussion of discrete audio tokenization or tokenizer/quantization details tied to audio signals, so it fails the inclusion criteria (non-audio, non-token focus).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum",
    "abstract": "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.",
    "metadata": {
      "arxiv_id": "2405.13226",
      "title": "Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum",
      "summary": "Large language models (LLMs) are commonly trained on datasets consisting of fixed-length token sequences. These datasets are created by randomly concatenating documents of various lengths and then chunking them into sequences of a predetermined target length (concat-and-chunk). Recent attention implementations mask cross-document attention, reducing the effective length of a chunk of tokens. Additionally, training on long sequences becomes computationally prohibitive due to the quadratic cost of attention. In this study, we introduce dataset decomposition, a novel variable sequence length training technique, to tackle these challenges. We decompose a dataset into a union of buckets, each containing sequences of the same size extracted from a unique document. During training, we use variable sequence length and batch-size, sampling simultaneously from all buckets with a curriculum. In contrast to the concat-and-chunk baseline, which incurs a fixed attention cost at every step of training, our proposed method incurs a computational cost proportional to the actual document lengths at each step, resulting in significant savings in training time. We train an 8k context-length 1B model at the same cost as a 2k context-length model trained with the baseline approach. Experiments on a web-scale corpus demonstrate that our approach significantly enhances performance on standard language evaluations and long-context benchmarks, reaching target accuracy with up to 6x faster training compared to the baseline. Our method not only enables efficient pretraining on long sequences but also scales effectively with dataset size. Lastly, we shed light on a critical yet less studied aspect of training large language models: the distribution and curriculum of sequence lengths, which results in a non-negligible difference in performance.",
      "authors": [
        "Hadi Pouransari",
        "Chun-Liang Li",
        "Jen-Hao Rick Chang",
        "Pavan Kumar Anasosalu Vasu",
        "Cem Koc",
        "Vaishaal Shankar",
        "Oncel Tuzel"
      ],
      "published": "2024-05-21T22:26:01Z",
      "updated": "2025-01-06T23:51:47Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.13226v2",
      "landing_url": "https://arxiv.org/abs/2405.13226v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.13226"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on efficient LLM dataset bucketing and curriculum for text sequences, with no treatment of discrete audio token generation/quantization, so it fails inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on efficient LLM dataset bucketing and curriculum for text sequences, with no treatment of discrete audio token generation/quantization, so it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LG-VQ: Language-Guided Codebook Learning",
    "abstract": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
    "metadata": {
      "arxiv_id": "2405.14206",
      "title": "LG-VQ: Language-Guided Codebook Learning",
      "summary": "Vector quantization (VQ) is a key technique in high-resolution and high-fidelity image synthesis, which aims to learn a codebook to encode an image with a sequence of discrete codes and then generate an image in an auto-regression manner. Although existing methods have shown superior performance, most methods prefer to learn a single-modal codebook (\\emph{e.g.}, image), resulting in suboptimal performance when the codebook is applied to multi-modal downstream tasks (\\emph{e.g.}, text-to-image, image captioning) due to the existence of modal gaps. In this paper, we propose a novel language-guided codebook learning framework, called LG-VQ, which aims to learn a codebook that can be aligned with the text to improve the performance of multi-modal downstream tasks. Specifically, we first introduce pre-trained text semantics as prior knowledge, then design two novel alignment modules (\\emph{i.e.}, Semantic Alignment Module, and Relationship Alignment Module) to transfer such prior knowledge into codes for achieving codebook text alignment. In particular, our LG-VQ method is model-agnostic, which can be easily integrated into existing VQ models. Experimental results show that our method achieves superior performance on reconstruction and various multi-modal downstream tasks.",
      "authors": [
        "Guotao Liang",
        "Baoquan Zhang",
        "Yaowei Wang",
        "Xutao Li",
        "Yunming Ye",
        "Huaibin Wang",
        "Chuyao Luo",
        "Kola Ye",
        "linfeng Luo"
      ],
      "published": "2024-05-23T06:04:40Z",
      "updated": "2024-10-09T04:30:30Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.14206v2",
      "landing_url": "https://arxiv.org/abs/2405.14206v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.14206"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on language-guided image codebook learning for multi-modal vision tasks rather than discrete audio-token generation, so it fails the inclusion criteria and matches the exclusion of non-audio discrete sequences."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on language-guided image codebook learning for multi-modal vision tasks rather than discrete audio-token generation, so it fails the inclusion criteria and matches the exclusion of non-audio discrete sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
    "abstract": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.",
    "metadata": {
      "arxiv_id": "2405.14862",
      "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
      "summary": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.",
      "authors": [
        "Dawid J. Kopiczko",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
      ],
      "published": "2024-05-23T17:59:22Z",
      "updated": "2025-08-28T17:59:31Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.14862v2",
      "landing_url": "https://arxiv.org/abs/2405.14862v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.14862"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on decoder-only LLM attention mechanisms without any mention of discrete audio token generation, quantization, or token-based codec modeling, so it fails every inclusion criterion and matches exclusion by being unrelated to audio tokens."
    },
    "round-A_JuniorNano_reasoning": "Focuses on decoder-only LLM attention mechanisms without any mention of discrete audio token generation, quantization, or token-based codec modeling, so it fails every inclusion criterion and matches exclusion by being unrelated to audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MuDreamer: Learning Predictive World Models without Reconstruction",
    "abstract": "The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using a pixel reconstruction loss. However, while the reconstruction loss is essential to Dreamer's performance, it also necessitates modeling unnecessary information. Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential. In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world model without the need for reconstructing input signals. Rather than relying on pixel reconstruction, hidden representations are instead learned by predicting the environment value function and previously selected actions. Similar to predictive self-supervised methods for images, we find that the use of batch normalization is crucial to prevent learning collapse. We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability. We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches, replacing the environment background with task-irrelevant real-world videos. Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.",
    "metadata": {
      "arxiv_id": "2405.15083",
      "title": "MuDreamer: Learning Predictive World Models without Reconstruction",
      "summary": "The DreamerV3 agent recently demonstrated state-of-the-art performance in diverse domains, learning powerful world models in latent space using a pixel reconstruction loss. However, while the reconstruction loss is essential to Dreamer's performance, it also necessitates modeling unnecessary information. Consequently, Dreamer sometimes fails to perceive crucial elements which are necessary for task-solving when visual distractions are present in the observation, significantly limiting its potential. In this paper, we present MuDreamer, a robust reinforcement learning agent that builds upon the DreamerV3 algorithm by learning a predictive world model without the need for reconstructing input signals. Rather than relying on pixel reconstruction, hidden representations are instead learned by predicting the environment value function and previously selected actions. Similar to predictive self-supervised methods for images, we find that the use of batch normalization is crucial to prevent learning collapse. We also study the effect of KL balancing between model posterior and prior losses on convergence speed and learning stability. We evaluate MuDreamer on the commonly used DeepMind Visual Control Suite and demonstrate stronger robustness to visual distractions compared to DreamerV3 and other reconstruction-free approaches, replacing the environment background with task-irrelevant real-world videos. Our method also achieves comparable performance on the Atari100k benchmark while benefiting from faster training.",
      "authors": [
        "Maxime Burchi",
        "Radu Timofte"
      ],
      "published": "2024-05-23T22:09:01Z",
      "updated": "2024-05-23T22:09:01Z",
      "categories": [
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15083v1",
      "landing_url": "https://arxiv.org/abs/2405.15083v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15083"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "MuDreamer focuses on learning world models for reinforcement learning from visual inputs without involving any discrete audio tokenizer/codecs or token quantization for audio, so it does not meet the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "MuDreamer focuses on learning world models for reinforcement learning from visual inputs without involving any discrete audio tokenizer/codecs or token quantization for audio, so it does not meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
    "abstract": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
    "metadata": {
      "arxiv_id": "2405.15310",
      "title": "Spectraformer: A Unified Random Feature Framework for Transformer",
      "summary": "Linearization of attention using various kernel approximation and kernel learning techniques has shown promise. Past methods used a subset of combinations of component functions and weight matrices within the random feature paradigm. We identify the need for a systematic comparison of different combinations of weight matrices and component functions for attention learning in Transformer. Hence, we introduce Spectraformer, a unified framework for approximating and learning the kernel function in the attention mechanism of the Transformer. Our empirical results demonstrate, for the first time, that a random feature-based approach can achieve performance comparable to top-performing sparse and low-rank methods on the challenging Long Range Arena benchmark. Thus, we establish a new state-of-the-art for random feature-based efficient Transformers. The framework also produces many variants that offer different advantages in accuracy, training time, and memory consumption. Our code is available at: https://github.com/cruiseresearchgroup/spectraformer .",
      "authors": [
        "Duke Nguyen",
        "Du Yin",
        "Aditya Joshi",
        "Flora Salim"
      ],
      "published": "2024-05-24T07:52:53Z",
      "updated": "2025-09-23T03:21:45Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15310v5",
      "landing_url": "https://arxiv.org/abs/2405.15310v5",
      "doi": "https://doi.org/10.1145/3768161"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This transformer attention random-feature method has no connection to discrete audio token tokenizers/codecs and does not define or evaluate finite-vocabulary audio tokens, so it fails all inclusion criteria and meets exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "This transformer attention random-feature method has no connection to discrete audio token tokenizers/codecs and does not define or evaluate finite-vocabulary audio tokens, so it fails all inclusion criteria and meets exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
    "abstract": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
    "metadata": {
      "arxiv_id": "2405.15405",
      "title": "Transformer-based Federated Learning for Multi-Label Remote Sensing Image Classification",
      "summary": "Federated learning (FL) aims to collaboratively learn deep learning model parameters from decentralized data archives (i.e., clients) without accessing training data on clients. However, the training data across clients might be not independent and identically distributed (non-IID), which may result in difficulty in achieving optimal model convergence. In this work, we investigate the capability of state-of-the-art transformer architectures (which are MLP-Mixer, ConvMixer, PoolFormer) to address the challenges related to non-IID training data across various clients in the context of FL for multi-label classification (MLC) problems in remote sensing (RS). The considered transformer architectures are compared among themselves and with the ResNet-50 architecture in terms of their: 1) robustness to training data heterogeneity; 2) local training complexity; and 3) aggregation complexity under different non-IID levels. The experimental results obtained on the BigEarthNet-S2 benchmark archive demonstrate that the considered architectures increase the generalization ability with the cost of higher local training and aggregation complexities. On the basis of our analysis, some guidelines are derived for a proper selection of transformer architecture in the context of FL for RS MLC. The code of this work is publicly available at https://git.tu-berlin.de/rsim/FL-Transformer.",
      "authors": [
        "Barış Büyüktaş",
        "Kenneth Weitzel",
        "Sebastian Völkers",
        "Felix Zailskas",
        "Begüm Demir"
      ],
      "published": "2024-05-24T10:13:49Z",
      "updated": "2024-05-24T10:13:49Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15405v1",
      "landing_url": "https://arxiv.org/abs/2405.15405v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15405"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on federated learning for remote sensing image classification without any discrete audio token generation or tokenizer details, so it fails the inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on federated learning for remote sensing image classification without any discrete audio token generation or tokenizer details, so it fails the inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
    "abstract": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
    "metadata": {
      "arxiv_id": "2405.15562",
      "title": "Transformer-XL for Long Sequence Tasks in Robotic Learning from Demonstration",
      "summary": "This paper presents an innovative application of Transformer-XL for long sequence tasks in robotic learning from demonstrations (LfD). The proposed framework effectively integrates multi-modal sensor inputs, including RGB-D images, LiDAR, and tactile sensors, to construct a comprehensive feature vector. By leveraging the advanced capabilities of Transformer-XL, particularly its attention mechanism and position encoding, our approach can handle the inherent complexities and long-term dependencies of multi-modal sensory data. The results of an extensive empirical evaluation demonstrate significant improvements in task success rates, accuracy, and computational efficiency compared to conventional methods such as Long Short-Term Memory (LSTM) networks and Convolutional Neural Networks (CNNs). The findings indicate that the Transformer-XL-based framework not only enhances the robot's perception and decision-making abilities but also provides a robust foundation for future advancements in robotic learning from demonstrations.",
      "authors": [
        "Gao Tianci"
      ],
      "published": "2024-05-24T13:49:31Z",
      "updated": "2024-05-24T13:49:31Z",
      "categories": [
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15562v1",
      "landing_url": "https://arxiv.org/abs/2405.15562v1",
      "doi": "https://doi.org/10.1007/978-3-032-04758-8_3"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on Transformer-XL for robotic learning from demonstration with multi-modal sensors, not on discrete audio token generation/quantization/tokenization, so it fails to meet any inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on Transformer-XL for robotic learning from demonstration with multi-modal sensors, not on discrete audio token generation/quantization/tokenization, so it fails to meet any inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
    "abstract": "Vision Transformers (ViTs) are pivotal for foundational models in scientific imagery, including Earth science applications, due to their capability to process large sequence lengths. While transformers for text has inspired scaling sequence lengths in ViTs, yet adapting these for ViTs introduces unique challenges. We develop distributed sequence parallelism for ViTs, enabling them to handle up to 1M tokens. Our approach, leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation with model sharding, is the first to apply sequence parallelism in ViT training, achieving a 94% batch scaling efficiency on 2,048 AMD-MI250X GPUs. Evaluating sequence parallelism in ViTs, particularly in models up to 10B parameters, highlighted substantial bottlenecks. We countered these with hybrid sequence, pipeline, tensor parallelism, and flash attention strategies, to scale beyond single GPU memory limits. Our method significantly enhances climate modeling accuracy by 20% in temperature predictions, marking the first training of a transformer model on a full-attention matrix over 188K sequence length.",
    "metadata": {
      "arxiv_id": "2405.15780",
      "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier",
      "summary": "Vision Transformers (ViTs) are pivotal for foundational models in scientific imagery, including Earth science applications, due to their capability to process large sequence lengths. While transformers for text has inspired scaling sequence lengths in ViTs, yet adapting these for ViTs introduces unique challenges. We develop distributed sequence parallelism for ViTs, enabling them to handle up to 1M tokens. Our approach, leveraging DeepSpeed-Ulysses and Long-Sequence-Segmentation with model sharding, is the first to apply sequence parallelism in ViT training, achieving a 94% batch scaling efficiency on 2,048 AMD-MI250X GPUs. Evaluating sequence parallelism in ViTs, particularly in models up to 10B parameters, highlighted substantial bottlenecks. We countered these with hybrid sequence, pipeline, tensor parallelism, and flash attention strategies, to scale beyond single GPU memory limits. Our method significantly enhances climate modeling accuracy by 20% in temperature predictions, marking the first training of a transformer model on a full-attention matrix over 188K sequence length.",
      "authors": [
        "Aristeidis Tsaris",
        "Chengming Zhang",
        "Xiao Wang",
        "Junqi Yin",
        "Siyan Liu",
        "Moetasim Ashfaq",
        "Ming Fan",
        "Jong Youl Choi",
        "Mohamed Wahib",
        "Dan Lu",
        "Prasanna Balaprakash",
        "Feiyi Wang"
      ],
      "published": "2024-04-17T19:57:07Z",
      "updated": "2024-04-17T19:57:07Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.15780v1",
      "landing_url": "https://arxiv.org/abs/2405.15780v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.15780"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses entirely on scaling vision transformers for scientific images and not on discrete audio token generation, quantization, or evaluation, so it fails every inclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses entirely on scaling vision transformers for scientific images and not on discrete audio token generation, quantization, or evaluation, so it fails every inclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
    "abstract": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
    "metadata": {
      "arxiv_id": "2405.16136",
      "title": "C3LLM: Conditional Multimodal Content Generation Using Large Language Models",
      "summary": "We introduce C3LLM (Conditioned-on-Three-Modalities Large Language Models), a novel framework combining three tasks of video-to-audio, audio-to-text, and text-to-audio together. C3LLM adapts the Large Language Model (LLM) structure as a bridge for aligning different modalities, synthesizing the given conditional information, and making multimodal generation in a discrete manner. Our contributions are as follows. First, we adapt a hierarchical structure for audio generation tasks with pre-trained audio codebooks. Specifically, we train the LLM to generate audio semantic tokens from the given conditions, and further use a non-autoregressive transformer to generate different levels of acoustic tokens in layers to better enhance the fidelity of the generated audio. Second, based on the intuition that LLMs were originally designed for discrete tasks with the next-word prediction method, we use the discrete representation for audio generation and compress their semantic meanings into acoustic tokens, similar to adding \"acoustic vocabulary\" to LLM. Third, our method combines the previous tasks of audio understanding, video-to-audio generation, and text-to-audio generation together into one unified model, providing more versatility in an end-to-end fashion. Our C3LLM achieves improved results through various automated evaluation metrics, providing better semantic alignment compared to previous methods.",
      "authors": [
        "Zixuan Wang",
        "Qinkai Duan",
        "Yu-Wing Tai",
        "Chi-Keung Tang"
      ],
      "published": "2024-05-25T09:10:12Z",
      "updated": "2024-05-25T09:10:12Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16136v1",
      "landing_url": "https://arxiv.org/abs/2405.16136v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.16136"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "C3LLM centers on discrete audio semantic tokens generated via codec-inspired quantized outputs and layers of acoustic tokens for multimodal generation, clearly aligning with the inclusion focus on discrete token learning and modeling while providing token definitions and evaluations."
    },
    "round-A_JuniorNano_reasoning": "C3LLM centers on discrete audio semantic tokens generated via codec-inspired quantized outputs and layers of acoustic tokens for multimodal generation, clearly aligning with the inclusion focus on discrete token learning and modeling while providing token definitions and evaluations.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning",
    "abstract": "We present a novel end-to-end framework that generates highly compact (typically 6-15 dimensions), discrete (int4 type), and interpretable node representations, termed node identifiers (node IDs), to tackle inference challenges on large-scale graphs. By employing vector quantization, we compress continuous node embeddings from multiple layers of a Graph Neural Network (GNN) into discrete codes, applicable under both self-supervised and supervised learning paradigms. These node IDs capture high-level abstractions of graph data and offer interpretability that traditional GNN embeddings lack. Extensive experiments on 34 datasets, encompassing node classification, graph classification, link prediction, and attributed graph clustering tasks, demonstrate that the generated node IDs significantly enhance speed and memory efficiency while achieving competitive performance compared to current state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2405.16435",
      "title": "Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning",
      "summary": "We present a novel end-to-end framework that generates highly compact (typically 6-15 dimensions), discrete (int4 type), and interpretable node representations, termed node identifiers (node IDs), to tackle inference challenges on large-scale graphs. By employing vector quantization, we compress continuous node embeddings from multiple layers of a Graph Neural Network (GNN) into discrete codes, applicable under both self-supervised and supervised learning paradigms. These node IDs capture high-level abstractions of graph data and offer interpretability that traditional GNN embeddings lack. Extensive experiments on 34 datasets, encompassing node classification, graph classification, link prediction, and attributed graph clustering tasks, demonstrate that the generated node IDs significantly enhance speed and memory efficiency while achieving competitive performance compared to current state-of-the-art methods.",
      "authors": [
        "Yuankai Luo",
        "Hongkang Li",
        "Qijiong Liu",
        "Lei Shi",
        "Xiao-Ming Wu"
      ],
      "published": "2024-05-26T05:22:38Z",
      "updated": "2024-10-18T06:56:10Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.16435v2",
      "landing_url": "https://arxiv.org/abs/2405.16435v2",
      "doi": "https://doi.org/10.48550/arXiv.2405.16435"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on graph node representations rather than discrete audio-token generation or codec quantization, so it fails the audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on graph node representations rather than discrete audio-token generation or codec quantization, so it fails the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics",
    "abstract": "Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose \\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics.",
    "metadata": {
      "arxiv_id": "2405.17051",
      "title": "BeamVQ: Aligning Space-Time Forecasting Model via Self-training on Physics-aware Metrics",
      "summary": "Data-driven deep learning has emerged as the new paradigm to model complex physical space-time systems. These data-driven methods learn patterns by optimizing statistical metrics and tend to overlook the adherence to physical laws, unlike traditional model-driven numerical methods. Thus, they often generate predictions that are not physically realistic. On the other hand, by sampling a large amount of high quality predictions from a data-driven model, some predictions will be more physically plausible than the others and closer to what will happen in the future. Based on this observation, we propose \\emph{Beam search by Vector Quantization} (BeamVQ) to enhance the physical alignment of data-driven space-time forecasting models. The key of BeamVQ is to train model on self-generated samples filtered with physics-aware metrics. To be flexibly support different backbone architectures, BeamVQ leverages a code bank to transform any encoder-decoder model to the continuous state space into discrete codes. Afterwards, it iteratively employs beam search to sample high-quality sequences, retains those with the highest physics-aware scores, and trains model on the new dataset. Comprehensive experiments show that BeamVQ not only gave an average statistical skill score boost for more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics.",
      "authors": [
        "Hao Wu",
        "Xingjian Shi",
        "Ziyue Huang",
        "Penghao Zhao",
        "Wei Xiong",
        "Jinbao Xue",
        "Yangyu Tao",
        "Xiaomeng Huang",
        "Weiyan Wang"
      ],
      "published": "2024-05-27T11:07:47Z",
      "updated": "2024-05-27T11:07:47Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17051v1",
      "landing_url": "https://arxiv.org/abs/2405.17051v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.17051"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "BeamVQ focuses on space-time forecasting with physics-aware sampling of encoder-decoder predictions without any discrete audio tokenization or codec quantization, so it fails to meet the topic’s criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "BeamVQ focuses on space-time forecasting with physics-aware sampling of encoder-decoder predictions without any discrete audio tokenization or codec quantization, so it fails to meet the topic’s criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness",
    "abstract": "This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN). Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces. Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs. ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification. Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models.",
    "metadata": {
      "arxiv_id": "2405.17361",
      "title": "A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness",
      "summary": "This paper reveals a key insight that a one-layer decoder-only Transformer is equivalent to a two-layer Recurrent Neural Network (RNN). Building on this insight, we propose ARC-Tran, a novel approach for verifying the robustness of decoder-only Transformers against arbitrary perturbation spaces. Compared to ARC-Tran, current robustness verification techniques are limited either to specific and length-preserving perturbations like word substitutions or to recursive models like LSTMs. ARC-Tran addresses these limitations by meticulously managing position encoding to prevent mismatches and by utilizing our key insight to achieve precise and scalable verification. Our evaluation shows that ARC-Tran (1) trains models more robust to arbitrary perturbation spaces than those produced by existing techniques and (2) shows high certification accuracy of the resulting models.",
      "authors": [
        "Yuhao Zhang",
        "Aws Albarghouthi",
        "Loris D'Antoni"
      ],
      "published": "2024-05-27T17:10:04Z",
      "updated": "2024-05-27T17:10:04Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.17361v1",
      "landing_url": "https://arxiv.org/abs/2405.17361v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.17361"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on robustness verification for decoder-only transformers and does not address discrete audio token generation, tokenizer/codec design, or token-level evaluations, so it fails the inclusion criteria and meets the exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on robustness verification for decoder-only transformers and does not address discrete audio token generation, tokenizer/codec design, or token-level evaluations, so it fails the inclusion criteria and meets the exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
    "abstract": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
    "metadata": {
      "arxiv_id": "2405.20410",
      "title": "SeamlessExpressiveLM: Speech Language Model for Expressive Speech-to-Speech Translation with Chain-of-Thought",
      "summary": "Expressive speech-to-speech translation (S2ST) is a key research topic in seamless communication, which focuses on the preservation of semantics and speaker vocal style in translated speech. Early works synthesized speaker style aligned speech in order to directly learn the mapping from speech to target speech spectrogram. Without reliance on style aligned data, recent studies leverage the advances of language modeling (LM) and build cascaded LMs on semantic and acoustic tokens. This work proposes SeamlessExpressiveLM, a single speech language model for expressive S2ST. We decompose the complex source-to-target speech mapping into intermediate generation steps with chain-of-thought prompting. The model is first guided to translate target semantic content and then transfer the speaker style to multi-stream acoustic units. Evaluated on Spanish-to-English and Hungarian-to-English translations, SeamlessExpressiveLM outperforms cascaded LMs in both semantic quality and style transfer, meanwhile achieving better parameter efficiency.",
      "authors": [
        "Hongyu Gong",
        "Bandhav Veluri"
      ],
      "published": "2024-05-30T18:28:31Z",
      "updated": "2024-05-30T18:28:31Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20410v1",
      "landing_url": "https://arxiv.org/abs/2405.20410v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20410"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "The paper mentions learning semantic and acoustic tokens with a single speech language model for expressive S2ST but does not describe discrete tokenization/codec design or evaluations, so it is unclear if it meets the discrete audio token inclusion focus."
    },
    "round-A_JuniorNano_reasoning": "The paper mentions learning semantic and acoustic tokens with a single speech language model for expressive S2ST but does not describe discrete tokenization/codec design or evaluations, so it is unclear if it meets the discrete audio token inclusion focus.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
    "abstract": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
    "metadata": {
      "arxiv_id": "2405.20452",
      "title": "Understanding Encoder-Decoder Structures in Machine Learning Using Information Measures",
      "summary": "We present new results to model and understand the role of encoder-decoder design in machine learning (ML) from an information-theoretic angle. We use two main information concepts, information sufficiency (IS) and mutual information loss (MIL), to represent predictive structures in machine learning. Our first main result provides a functional expression that characterizes the class of probabilistic models consistent with an IS encoder-decoder latent predictive structure. This result formally justifies the encoder-decoder forward stages many modern ML architectures adopt to learn latent (compressed) representations for classification. To illustrate IS as a realistic and relevant model assumption, we revisit some known ML concepts and present some interesting new examples: invariant, robust, sparse, and digital models. Furthermore, our IS characterization allows us to tackle the fundamental question of how much performance (predictive expressiveness) could be lost, using the cross entropy risk, when a given encoder-decoder architecture is adopted in a learning setting. Here, our second main result shows that a mutual information loss quantifies the lack of expressiveness attributed to the choice of a (biased) encoder-decoder ML design. Finally, we address the problem of universal cross-entropy learning with an encoder-decoder design where necessary and sufficiency conditions are established to meet this requirement. In all these results, Shannon's information measures offer new interpretations and explanations for representation learning.",
      "authors": [
        "Jorge F. Silva",
        "Victor Faraggi",
        "Camilo Ramirez",
        "Alvaro Egana",
        "Eduardo Pavez"
      ],
      "published": "2024-05-30T19:58:01Z",
      "updated": "2024-05-30T19:58:01Z",
      "categories": [
        "cs.LG",
        "cs.IT",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2405.20452v1",
      "landing_url": "https://arxiv.org/abs/2405.20452v1",
      "doi": "https://doi.org/10.48550/arXiv.2405.20452"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses general encoder-decoder information-theoretic insights without any mention of audio signals, discrete token vocabularies, quantization, or codec/semantic token techniques required for inclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses general encoder-decoder information-theoretic insights without any mention of audio signals, discrete token vocabularies, quantization, or codec/semantic token techniques required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation",
    "abstract": "Video generation has made remarkable progress in recent years, especially since the advent of the video diffusion models. Many video generation models can produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD). However, most video models can only generate low frame rate videos due to the limited GPU memory as well as the difficulty of modeling a large set of frames. The training videos are always uniformly sampled at a specified interval for temporal compression. Previous methods promote the frame rate by either training a video interpolation model in pixel space as a postprocessing stage or training an interpolation model in latent space for a specific base video model. In this paper, we propose a training-free video interpolation method for generative video diffusion models, which is generalizable to different models in a plug-and-play manner. We investigate the non-linearity in the feature space of video diffusion models and transform a video model into a self-cascaded video diffusion model with incorporating the designed hidden state correction modules. The self-cascaded architecture and the correction module are proposed to retain the temporal consistency between key frames and the interpolated frames. Extensive evaluations are preformed on multiple popular video models to demonstrate the effectiveness of the propose method, especially that our training-free method is even comparable to trained interpolation models supported by huge compute resources and large-scale datasets.",
    "metadata": {
      "arxiv_id": "2406.00908",
      "title": "ZeroSmooth: Training-free Diffuser Adaptation for High Frame Rate Video Generation",
      "summary": "Video generation has made remarkable progress in recent years, especially since the advent of the video diffusion models. Many video generation models can produce plausible synthetic videos, e.g., Stable Video Diffusion (SVD). However, most video models can only generate low frame rate videos due to the limited GPU memory as well as the difficulty of modeling a large set of frames. The training videos are always uniformly sampled at a specified interval for temporal compression. Previous methods promote the frame rate by either training a video interpolation model in pixel space as a postprocessing stage or training an interpolation model in latent space for a specific base video model. In this paper, we propose a training-free video interpolation method for generative video diffusion models, which is generalizable to different models in a plug-and-play manner. We investigate the non-linearity in the feature space of video diffusion models and transform a video model into a self-cascaded video diffusion model with incorporating the designed hidden state correction modules. The self-cascaded architecture and the correction module are proposed to retain the temporal consistency between key frames and the interpolated frames. Extensive evaluations are preformed on multiple popular video models to demonstrate the effectiveness of the propose method, especially that our training-free method is even comparable to trained interpolation models supported by huge compute resources and large-scale datasets.",
      "authors": [
        "Shaoshu Yang",
        "Yong Zhang",
        "Xiaodong Cun",
        "Ying Shan",
        "Ran He"
      ],
      "published": "2024-06-03T00:31:13Z",
      "updated": "2024-06-03T00:31:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00908v1",
      "landing_url": "https://arxiv.org/abs/2406.00908v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.00908"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on video diffusion interpolation and lacks any discussion of discrete audio tokens or tokenizer/quantization design, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on video diffusion interpolation and lacks any discussion of discrete audio tokens or tokenizer/quantization design, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
    "abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
    "metadata": {
      "arxiv_id": "2406.00976",
      "title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer",
      "summary": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce \\textbf{G}enerative \\textbf{P}re-trained \\textbf{S}peech \\textbf{T}ransformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. The code is available at \\url{https://github.com/youngsheen/GPST}.",
      "authors": [
        "Yongxin Zhu",
        "Dan Su",
        "Liqiang He",
        "Linli Xu",
        "Dong Yu"
      ],
      "published": "2024-06-03T04:16:30Z",
      "updated": "2024-11-01T13:54:48Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.00976v2",
      "landing_url": "https://arxiv.org/abs/2406.00976v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.00976"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "GPST explicitly quantizes waveforms into discrete speech representations, integrates them in a hierarchical transformer for generation, and reports evaluation on WER, quality, and speaker similarity, so it matches the discrete audio token inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "GPST explicitly quantizes waveforms into discrete speech representations, integrates them in a hierarchical transformer for generation, and reports evaluation on WER, quality, and speaker similarity, so it matches the discrete audio token inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
    "abstract": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
    "metadata": {
      "arxiv_id": "2406.02092",
      "title": "MaskSR: Masked Language Model for Full-band Speech Restoration",
      "summary": "Speech restoration aims at restoring high quality speech in the presence of a diverse set of distortions. Although several deep learning paradigms have been studied for this task, the power of the recently emerging language models has not been fully explored. In this paper, we propose MaskSR, a masked language model capable of restoring full-band 44.1 kHz speech jointly considering noise, reverb, clipping, and low bandwidth. MaskSR works with discrete acoustic tokens extracted using a pre-trained neural codec. During training, MaskSR is optimized to predict randomly masked tokens extracted from the high quality target speech, conditioned on the corrupted speech with various distortions. During inference, MaskSR reconstructs the target speech tokens with efficient iterative sampling. Extensive experiments show that MaskSR obtains competitive results on both the full-band speech restoration task and also on sub-tasks compared with a wide range of models.",
      "authors": [
        "Xu Li",
        "Qirui Wang",
        "Xiaoyu Liu"
      ],
      "published": "2024-06-04T08:23:57Z",
      "updated": "2024-06-04T08:23:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02092v1",
      "landing_url": "https://arxiv.org/abs/2406.02092v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02092"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Uses discrete acoustic tokens from a neural codec and focuses on token prediction for speech restoration, satisfying the inclusion criteria without any exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Uses discrete acoustic tokens from a neural codec and focuses on token prediction for speech restoration, satisfying the inclusion criteria without any exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
    "abstract": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
    "metadata": {
      "arxiv_id": "2406.02430",
      "title": "Seed-TTS: A Family of High-Quality Versatile Speech Generation Models",
      "summary": "We introduce Seed-TTS, a family of large-scale autoregressive text-to-speech (TTS) models capable of generating speech that is virtually indistinguishable from human speech. Seed-TTS serves as a foundation model for speech generation and excels in speech in-context learning, achieving performance in speaker similarity and naturalness that matches ground truth human speech in both objective and subjective evaluations. With fine-tuning, we achieve even higher subjective scores across these metrics. Seed-TTS offers superior controllability over various speech attributes such as emotion and is capable of generating highly expressive and diverse speech for speakers in the wild. Furthermore, we propose a self-distillation method for speech factorization, as well as a reinforcement learning approach to enhance model robustness, speaker similarity, and controllability. We additionally present a non-autoregressive (NAR) variant of the Seed-TTS model, named $\\text{Seed-TTS}_\\text{DiT}$, which utilizes a fully diffusion-based architecture. Unlike previous NAR-based TTS systems, $\\text{Seed-TTS}_\\text{DiT}$ does not depend on pre-estimated phoneme durations and performs speech generation through end-to-end processing. We demonstrate that this variant achieves comparable performance to the language model-based variant and showcase its effectiveness in speech editing. We encourage readers to listen to demos at \\url{https://bytedancespeech.github.io/seedtts_tech_report}.",
      "authors": [
        "Philip Anastassiou",
        "Jiawei Chen",
        "Jitong Chen",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Ziyi Chen",
        "Jian Cong",
        "Lelai Deng",
        "Chuang Ding",
        "Lu Gao",
        "Mingqing Gong",
        "Peisong Huang",
        "Qingqing Huang",
        "Zhiying Huang",
        "Yuanyuan Huo",
        "Dongya Jia",
        "Chumin Li",
        "Feiya Li",
        "Hui Li",
        "Jiaxin Li",
        "Xiaoyang Li",
        "Xingxing Li",
        "Lin Liu",
        "Shouda Liu",
        "Sichao Liu",
        "Xudong Liu",
        "Yuchen Liu",
        "Zhengxi Liu",
        "Lu Lu",
        "Junjie Pan",
        "Xin Wang",
        "Yuping Wang",
        "Yuxuan Wang",
        "Zhen Wei",
        "Jian Wu",
        "Chao Yao",
        "Yifeng Yang",
        "Yuanhao Yi",
        "Junteng Zhang",
        "Qidi Zhang",
        "Shuo Zhang",
        "Wenjie Zhang",
        "Yang Zhang",
        "Zilin Zhao",
        "Dejian Zhong",
        "Xiaobin Zhuang"
      ],
      "published": "2024-06-04T15:48:29Z",
      "updated": "2024-06-04T15:48:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02430v1",
      "landing_url": "https://arxiv.org/abs/2406.02430v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02430"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Title/abstract describe autoregressive/diffusion TTS models without mentioning any discrete audio tokenization, vocabularies, or quantized token details, so it fails to meet the discrete-token focus required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe autoregressive/diffusion TTS models without mentioning any discrete audio tokenization, vocabularies, or quantized token details, so it fails to meet the discrete-token focus required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions",
    "abstract": "This paper proposes Progressive Inference - a framework to compute input attributions to explain the predictions of decoder-only sequence classification models. Our work is based on the insight that the classification head of a decoder-only Transformer model can be used to make intermediate predictions by evaluating them at different points in the input sequence. Due to the causal attention mechanism, these intermediate predictions only depend on the tokens seen before the inference point, allowing us to obtain the model's prediction on a masked input sub-sequence, with negligible computational overheads. We develop two methods to provide sub-sequence level attributions using this insight. First, we propose Single Pass-Progressive Inference (SP-PI), which computes attributions by taking the difference between consecutive intermediate predictions. Second, we exploit a connection with Kernel SHAP to develop Multi Pass-Progressive Inference (MP-PI). MP-PI uses intermediate predictions from multiple masked versions of the input to compute higher quality attributions. Our studies on a diverse set of models trained on text classification tasks show that SP-PI and MP-PI provide significantly better attributions compared to prior work.",
    "metadata": {
      "arxiv_id": "2406.02625",
      "title": "Progressive Inference: Explaining Decoder-Only Sequence Classification Models Using Intermediate Predictions",
      "summary": "This paper proposes Progressive Inference - a framework to compute input attributions to explain the predictions of decoder-only sequence classification models. Our work is based on the insight that the classification head of a decoder-only Transformer model can be used to make intermediate predictions by evaluating them at different points in the input sequence. Due to the causal attention mechanism, these intermediate predictions only depend on the tokens seen before the inference point, allowing us to obtain the model's prediction on a masked input sub-sequence, with negligible computational overheads. We develop two methods to provide sub-sequence level attributions using this insight. First, we propose Single Pass-Progressive Inference (SP-PI), which computes attributions by taking the difference between consecutive intermediate predictions. Second, we exploit a connection with Kernel SHAP to develop Multi Pass-Progressive Inference (MP-PI). MP-PI uses intermediate predictions from multiple masked versions of the input to compute higher quality attributions. Our studies on a diverse set of models trained on text classification tasks show that SP-PI and MP-PI provide significantly better attributions compared to prior work.",
      "authors": [
        "Sanjay Kariyappa",
        "Freddy Lécué",
        "Saumitra Mishra",
        "Christopher Pond",
        "Daniele Magazzeni",
        "Manuela Veloso"
      ],
      "published": "2024-06-03T21:48:57Z",
      "updated": "2024-06-03T21:48:57Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02625v1",
      "landing_url": "https://arxiv.org/abs/2406.02625v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.02625"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe explaining decoder-only text classifiers via intermediate predictions rather than any work on discrete audio tokens, so they fail the inclusion criterion and match an exclusion (non-audio discrete tokens)."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe explaining decoder-only text classifiers via intermediate predictions rather than any work on discrete audio tokens, so they fail the inclusion criterion and match an exclusion (non-audio discrete tokens).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
    "abstract": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
    "metadata": {
      "arxiv_id": "2406.02770",
      "title": "Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models",
      "summary": "Accurate vessel trajectory prediction is necessary for save and efficient navigation. Deep learning-based prediction models, esp. encoder-decoders, are rarely applied to inland navigation specifically. Approaches from the maritime domain cannot directly be transferred to river navigation due to specific driving behavior influencing factors. Different encoder-decoder architectures, including a transformer encoder-decoder, are compared herein for predicting the next positions of inland vessels, given not only spatio-temporal information from AIS, but also river specific features. The results show that the reformulation of the regression task as classification problem and the inclusion of river specific features yield the lowest displacement errors. The standard LSTM encoder-decoder outperforms the transformer encoder-decoder for the data considered, but is computationally more expensive. In this study for the first time a transformer-based encoder-decoder model is applied to the problem of predicting the ship trajectory. Here, a feature vector using the river-specific context of navigation input parameters is established. Future studies can built on the proposed models, investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization, and use additional river-specific information in the context representation to further increase prediction accuracy.",
      "authors": [
        "Kathrin Donandt",
        "Karim Böttger",
        "Dirk Söffker"
      ],
      "published": "2024-06-04T20:37:30Z",
      "updated": "2024-06-04T20:37:30Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02770v1",
      "landing_url": "https://arxiv.org/abs/2406.02770v1",
      "doi": "https://doi.org/10.1109/ITSC55140.2022.9922148"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on inland vessel trajectory prediction with encoder-decoder models for navigation data rather than the discrete audio token generation/ quantization methods required, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on inland vessel trajectory prediction with encoder-decoder models for navigation data rather than the discrete audio token generation/ quantization methods required, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
    "abstract": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
    "metadata": {
      "arxiv_id": "2406.02897",
      "title": "LiveSpeech: Low-Latency Zero-shot Text-to-Speech via Autoregressive Modeling of Audio Discrete Codes",
      "summary": "Prior works have demonstrated zero-shot text-to-speech by using a generative language model on audio tokens obtained via a neural audio codec. It is still challenging, however, to adapt them to low-latency scenarios. In this paper, we present LiveSpeech - a fully autoregressive language model-based approach for zero-shot text-to-speech, enabling low-latency streaming of the output audio. To allow multiple token prediction within a single decoding step, we propose (1) using adaptive codebook loss weights that consider codebook contribution in each frame and focus on hard instances, and (2) grouping codebooks and processing groups in parallel. Experiments show our proposed models achieve competitive results to state-of-the-art baselines in terms of content accuracy, speaker similarity, audio quality, and inference speed while being suitable for low-latency streaming applications.",
      "authors": [
        "Trung Dang",
        "David Aponte",
        "Dung Tran",
        "Kazuhito Koishida"
      ],
      "published": "2024-06-05T03:36:11Z",
      "updated": "2024-06-10T05:50:53Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.02897v2",
      "landing_url": "https://arxiv.org/abs/2406.02897v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.02897"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "LiveSpeech describes autoregressive modeling over neural codec discrete audio tokens with evaluations including quality and latency, so it satisfies the discrete-token inclusion criteria with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "LiveSpeech describes autoregressive modeling over neural codec discrete audio tokens with evaluations including quality and latency, so it satisfies the discrete-token inclusion criteria with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
    "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
    "metadata": {
      "arxiv_id": "2406.03044",
      "title": "Population Transformer: Learning Population-level Representations of Neural Activity",
      "summary": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address key challenges in scaling models with neural time-series data, namely, sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained temporal embeddings and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight, while achieving similar or better decoding performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained and fine-tuned PopT models to show how they can be used to extract neuroscience insights from large amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability. Code is available at https://github.com/czlwang/PopulationTransformer.",
      "authors": [
        "Geeling Chau",
        "Christopher Wang",
        "Sabera Talukder",
        "Vighnesh Subramaniam",
        "Saraswati Soedarmadji",
        "Yisong Yue",
        "Boris Katz",
        "Andrei Barbu"
      ],
      "published": "2024-06-05T08:15:09Z",
      "updated": "2025-03-28T06:43:28Z",
      "categories": [
        "cs.LG",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03044v4",
      "landing_url": "https://arxiv.org/abs/2406.03044v4",
      "doi": "https://doi.org/10.48550/arXiv.2406.03044"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper addresses neural activity modeling with transformers, not discrete audio tokens or quantization/tokenizer methods, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper addresses neural activity modeling with transformers, not discrete audio tokens or quantization/tokenizer methods, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Distributional Adversarial Loss",
    "abstract": "We initiate the study of a new notion of adversarial loss which we call distributional adversarial loss. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions, and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss. We show sample complexity bounds in the PAC-learning setting for our notion of adversarial loss. Our notion of adversarial loss contrasts the prior work on robust learning that considers a set of points, not distributions, as the perturbation set of each clean example. As an application of our approach, we show how to unify the two lines of work on randomized smoothing and robust learning in the PAC-learning setting and derive sample complexity bounds for randomized smoothing methods. Furthermore, we investigate the role of randomness in achieving robustness against adversarial attacks. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks and show its effectiveness empirically.",
    "metadata": {
      "arxiv_id": "2406.03458",
      "title": "Distributional Adversarial Loss",
      "summary": "We initiate the study of a new notion of adversarial loss which we call distributional adversarial loss. In this notion, we assume for each original example, the allowed adversarial perturbation set is a family of distributions, and the adversarial loss over each example is the maximum loss over all the associated distributions. The goal is to minimize the overall adversarial loss. We show sample complexity bounds in the PAC-learning setting for our notion of adversarial loss. Our notion of adversarial loss contrasts the prior work on robust learning that considers a set of points, not distributions, as the perturbation set of each clean example. As an application of our approach, we show how to unify the two lines of work on randomized smoothing and robust learning in the PAC-learning setting and derive sample complexity bounds for randomized smoothing methods.\n  Furthermore, we investigate the role of randomness in achieving robustness against adversarial attacks. We show a general derandomization technique that preserves the extent of a randomized classifier's robustness against adversarial attacks and show its effectiveness empirically.",
      "authors": [
        "Saba Ahmadi",
        "Siddharth Bhandari",
        "Avrim Blum",
        "Chen Dan",
        "Prabhav Jain"
      ],
      "published": "2024-06-05T17:03:47Z",
      "updated": "2025-06-19T23:07:24Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03458v2",
      "landing_url": "https://arxiv.org/abs/2406.03458v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.03458"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about distributional adversarial loss in PAC-learning and robustness, with no mention of discrete audio tokens or codec/quantization pipelines, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper is about distributional adversarial loss in PAC-learning and robustness, with no mention of discrete audio tokens or codec/quantization pipelines, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
    "abstract": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
    "metadata": {
      "arxiv_id": "2406.03706",
      "title": "Improving Audio Codec-based Zero-Shot Text-to-Speech Synthesis with Multi-Modal Context and Large Language Model",
      "summary": "Recent advances in large language models (LLMs) and development of audio codecs greatly propel the zero-shot TTS. They can synthesize personalized speech with only a 3-second speech of an unseen speaker as acoustic prompt. However, they only support short speech prompts and cannot leverage longer context information, as required in audiobook and conversational TTS scenarios. In this paper, we introduce a novel audio codec-based TTS model to adapt context features with multiple enhancements. Inspired by the success of Qformer, we propose a multi-modal context-enhanced Qformer (MMCE-Qformer) to utilize additional multi-modal context information. Besides, we adapt a pretrained LLM to leverage its understanding ability to predict semantic tokens, and use a SoundStorm to generate acoustic tokens thereby enhancing audio quality and speaker similarity. The extensive objective and subjective evaluations show that our proposed method outperforms baselines across various context TTS scenarios.",
      "authors": [
        "Jinlong Xue",
        "Yayue Deng",
        "Yicheng Han",
        "Yingming Gao",
        "Ya Li"
      ],
      "published": "2024-06-06T03:06:45Z",
      "updated": "2024-06-06T03:06:45Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03706v1",
      "landing_url": "https://arxiv.org/abs/2406.03706v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03706"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper introduces a zero-shot TTS system built around audio codec-based discrete tokens (semantic and acoustic) and evaluates multi-modal context integration, matching the inclusion focus on discrete audio token generation/use plus evaluations, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a zero-shot TTS system built around audio codec-based discrete tokens (semantic and acoustic) and evaluates multi-modal context integration, matching the inclusion focus on discrete audio token generation/use plus evaluations, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Decoder-only Streaming Transformer for Simultaneous Translation",
    "abstract": "Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks.",
    "metadata": {
      "arxiv_id": "2406.03878",
      "title": "Decoder-only Streaming Transformer for Simultaneous Translation",
      "summary": "Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks.",
      "authors": [
        "Shoutao Guo",
        "Shaolei Zhang",
        "Yang Feng"
      ],
      "published": "2024-06-06T09:13:13Z",
      "updated": "2024-06-06T09:13:13Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.03878v1",
      "landing_url": "https://arxiv.org/abs/2406.03878v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.03878"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe a decoder-only streaming transformer for simultaneous machine translation, not research on discrete audio tokenization or related quantized audio representations, so it fails the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe a decoder-only streaming transformer for simultaneous machine translation, not research on discrete audio tokenization or related quantized audio representations, so it fails the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
    "abstract": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
    "metadata": {
      "arxiv_id": "2406.04633",
      "title": "Boosting Diffusion Model for Spectrogram Up-sampling in Text-to-speech: An Empirical Study",
      "summary": "Scaling text-to-speech (TTS) with autoregressive language model (LM) to large-scale datasets by quantizing waveform into discrete speech tokens is making great progress to capture the diversity and expressiveness in human speech, but the speech reconstruction quality from discrete speech token is far from satisfaction depending on the compressed speech token compression ratio. Generative diffusion models trained with score-matching loss and continuous normalized flow trained with flow-matching loss have become prominent in generation of images as well as speech. LM based TTS systems usually quantize speech into discrete tokens and generate these tokens autoregressively, and finally use a diffusion model to up sample coarse-grained speech tokens into fine-grained codec features or mel-spectrograms before reconstructing into waveforms with vocoder, which has a high latency and is not realistic for real time speech applications. In this paper, we systematically investigate varied diffusion models for up sampling stage, which is the main bottleneck for streaming synthesis of LM and diffusion-based architecture, we present the model architecture, objective and subjective metrics to show quality and efficiency improvement.",
      "authors": [
        "Chong Zhang",
        "Yanqing Liu",
        "Yang Zheng",
        "Sheng Zhao"
      ],
      "published": "2024-06-07T04:34:03Z",
      "updated": "2024-06-07T04:34:03Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04633v1",
      "landing_url": "https://arxiv.org/abs/2406.04633v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04633"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Study focuses on diffusion-based up-sampling of already quantized speech tokens rather than proposing or analyzing discrete tokenizers/codecs or their trade-offs, so it fails to meet the discrete token design/evaluation requirement and is better excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on diffusion-based up-sampling of already quantized speech tokens rather than proposing or analyzing discrete tokenizers/codecs or their trade-offs, so it fails to meet the discrete token design/evaluation requirement and is better excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
    "abstract": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
    "metadata": {
      "arxiv_id": "2406.04740",
      "title": "Activation Map-based Vector Quantization for 360-degree Image Semantic Communication",
      "summary": "In virtual reality (VR) applications, 360-degree images play a pivotal role in crafting immersive experiences and offering panoramic views, thus improving user Quality of Experience (QoE). However, the voluminous data generated by 360-degree images poses challenges in network storage and bandwidth. To address these challenges, we propose a novel Activation Map-based Vector Quantization (AM-VQ) framework, which is designed to reduce communication overhead for wireless transmission. The proposed AM-VQ scheme uses the Deep Neural Networks (DNNs) with vector quantization (VQ) to extract and compress semantic features. Particularly, the AM-VQ framework utilizes activation map to adaptively quantize semantic features, thus reducing data distortion caused by quantization operation. To further enhance the reconstruction quality of the 360-degree image, adversarial training with a Generative Adversarial Networks (GANs) discriminator is incorporated. Numerical results show that our proposed AM-VQ scheme achieves better performance than the existing Deep Learning (DL) based coding and the traditional coding schemes under the same transmission symbols.",
      "authors": [
        "Yang Ma",
        "Wenchi Cheng",
        "Jingqing Wang",
        "Wei Zhang"
      ],
      "published": "2024-06-07T08:40:53Z",
      "updated": "2024-06-07T08:40:53Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.04740v1",
      "landing_url": "https://arxiv.org/abs/2406.04740v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.04740"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on vector quantization for 360-degree image semantic communication without any mention of discrete audio tokens, tokenizer/token vocabulary, or audio-specific reconstruction evaluation, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on vector quantization for 360-degree image semantic communication without any mention of discrete audio tokens, tokenizer/token vocabulary, or audio-specific reconstruction evaluation, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
    "abstract": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
    "metadata": {
      "arxiv_id": "2406.05298",
      "title": "Spectral Codecs: Improving Non-Autoregressive Speech Synthesis with Spectrogram-Based Audio Codecs",
      "summary": "Historically, most speech models in machine-learning have used the mel-spectrogram as a speech representation. Recently, discrete audio tokens produced by neural audio codecs have become a popular alternate speech representation for speech synthesis tasks such as text-to-speech (TTS). However, the data distribution produced by such codecs is too complex for some TTS models to predict, typically requiring large autoregressive models to get good quality. Most existing audio codecs use Residual Vector Quantization (RVQ) to compress and reconstruct the time-domain audio signal. We propose a spectral codec which uses Finite Scalar Quantization (FSQ) to compress the mel-spectrogram and reconstruct the time-domain audio signal. A study of objective audio quality metrics and subjective listening tests suggests that our spectral codec has comparable perceptual quality to equivalent audio codecs. We show that FSQ, and the use of spectral speech representations, can both improve the performance of parallel TTS models.",
      "authors": [
        "Ryan Langman",
        "Ante Jukić",
        "Kunal Dhawan",
        "Nithin Rao Koluguri",
        "Jason Li"
      ],
      "published": "2024-06-07T23:47:51Z",
      "updated": "2025-06-04T16:25:54Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05298v2",
      "landing_url": "https://arxiv.org/abs/2406.05298v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.05298"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "This paper clearly studies a neural audio codec that quantizes mel-spectrograms into discrete codes (FSQ spectral codec) and evaluates reconstruction quality and TTS performance, so it satisfies all inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "This paper clearly studies a neural audio codec that quantizes mel-spectrograms into discrete codes (FSQ spectral codec) and evaluates reconstruction quality and TTS performance, so it satisfies all inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
    "abstract": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
    "metadata": {
      "arxiv_id": "2406.05551",
      "title": "Autoregressive Diffusion Transformer for Text-to-Speech Synthesis",
      "summary": "Audio language models have recently emerged as a promising approach for various audio generation tasks, relying on audio tokenizers to encode waveforms into sequences of discrete symbols. Audio tokenization often poses a necessary compromise between code bitrate and reconstruction accuracy. When dealing with low-bitrate audio codes, language models are constrained to process only a subset of the information embedded in the audio, which in turn restricts their generative capabilities. To circumvent these issues, we propose encoding audio as vector sequences in continuous space $\\mathbb R^d$ and autoregressively generating these sequences using a decoder-only diffusion transformer (ARDiT). Our findings indicate that ARDiT excels in zero-shot text-to-speech and exhibits performance that compares to or even surpasses that of state-of-the-art models. High-bitrate continuous speech representation enables almost flawless reconstruction, allowing our model to achieve nearly perfect speech editing. Our experiments reveal that employing Integral Kullback-Leibler (IKL) divergence for distillation at each autoregressive step significantly boosts the perceived quality of the samples. Simultaneously, it condenses the iterative sampling process of the diffusion model into a single step. Furthermore, ARDiT can be trained to predict several continuous vectors in one step, significantly reducing latency during sampling. Impressively, one of our models can generate $170$ ms of $24$ kHz speech per evaluation step with minimal degradation in performance. Audio samples are available at http://ardit-tts.github.io/ .",
      "authors": [
        "Zhijun Liu",
        "Shuai Wang",
        "Sho Inoue",
        "Qibing Bai",
        "Haizhou Li"
      ],
      "published": "2024-06-08T18:57:13Z",
      "updated": "2024-06-08T18:57:13Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.05551v1",
      "landing_url": "https://arxiv.org/abs/2406.05551v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.05551"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "ARDiT focuses on continuous vector-based speech representation and diffusion sampling without producing or evaluating discrete audio tokens/codebooks, so it fails the discrete-token inclusion requirement."
    },
    "round-A_JuniorNano_reasoning": "ARDiT focuses on continuous vector-based speech representation and diffusion sampling without producing or evaluating discrete audio tokens/codebooks, so it fails the discrete-token inclusion requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Efficient Neural Compression with Inference-time Decoding",
    "abstract": "This paper explores the combination of neural network quantization and entropy coding for memory footprint minimization. Edge deployment of quantized models is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth tradeoff, causing dramatic accuracy loss below a certain bitwidth. This accuracy loss can be alleviated thanks to mixed precision quantization, allowing for more flexible bitwidth allocation. However, standard mixed precision benefits remain limited due to the 1-bit frontier, that forces each parameter to be encoded on at least 1 bit of data. This paper introduces an approach that combines mixed precision, zero-point quantization and entropy coding to push the compression boundary of Resnets beyond the 1-bit frontier with an accuracy drop below 1% on the ImageNet benchmark. From an implementation standpoint, a compact decoder architecture features reduced latency, thus allowing for inference-compatible decoding.",
    "metadata": {
      "arxiv_id": "2406.06237",
      "title": "Efficient Neural Compression with Inference-time Decoding",
      "summary": "This paper explores the combination of neural network quantization and entropy coding for memory footprint minimization. Edge deployment of quantized models is hampered by the harsh Pareto frontier of the accuracy-to-bitwidth tradeoff, causing dramatic accuracy loss below a certain bitwidth. This accuracy loss can be alleviated thanks to mixed precision quantization, allowing for more flexible bitwidth allocation. However, standard mixed precision benefits remain limited due to the 1-bit frontier, that forces each parameter to be encoded on at least 1 bit of data. This paper introduces an approach that combines mixed precision, zero-point quantization and entropy coding to push the compression boundary of Resnets beyond the 1-bit frontier with an accuracy drop below 1% on the ImageNet benchmark. From an implementation standpoint, a compact decoder architecture features reduced latency, thus allowing for inference-compatible decoding.",
      "authors": [
        "C. Metz",
        "O. Bichler",
        "A. Dupret"
      ],
      "published": "2024-06-10T13:07:13Z",
      "updated": "2024-06-10T13:07:13Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06237v1",
      "landing_url": "https://arxiv.org/abs/2406.06237v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.06237"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe mixed-precision quantization and entropy coding for image-model compression (ResNets on ImageNet) rather than any encoder/decoder producing discrete audio tokens or related tokenization/quantization of speech/audio representations, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe mixed-precision quantization and entropy coding for image-model compression (ResNets on ImageNet) rather than any encoder/decoder producing discrete audio tokens or related tokenization/quantization of speech/audio representations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
    "abstract": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
    "metadata": {
      "arxiv_id": "2406.06251",
      "title": "Learning Fine-Grained Controllability on Speech Generation via Efficient Fine-Tuning",
      "summary": "As the scale of generative models continues to grow, efficient reuse and adaptation of pre-trained models have become crucial considerations. In this work, we propose Voicebox Adapter, a novel approach that integrates fine-grained conditions into a pre-trained Voicebox speech generation model using a cross-attention module. To ensure a smooth integration of newly added modules with pre-trained ones, we explore various efficient fine-tuning approaches. Our experiment shows that the LoRA with bias-tuning configuration yields the best performance, enhancing controllability without compromising speech quality. Across three fine-grained conditional generation tasks, we demonstrate the effectiveness and resource efficiency of Voicebox Adapter. Follow-up experiments further highlight the robustness of Voicebox Adapter across diverse data setups.",
      "authors": [
        "Chung-Ming Chien",
        "Andros Tjandra",
        "Apoorv Vyas",
        "Matt Le",
        "Bowen Shi",
        "Wei-Ning Hsu"
      ],
      "published": "2024-06-10T13:31:18Z",
      "updated": "2024-06-10T13:31:18Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06251v1",
      "landing_url": "https://arxiv.org/abs/2406.06251v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.06251"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on efficient fine-tuning of a speech generative model, not on discrete audio token/codec generation or semantic unit quantization, so it fails the inclusion criteria and meets exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on efficient fine-tuning of a speech generative model, not on discrete audio token/codec generation or semantic unit quantization, so it fails the inclusion criteria and meets exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
    "abstract": "Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.",
    "metadata": {
      "arxiv_id": "2406.06484",
      "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length",
      "summary": "Transformers with linear attention (i.e., linear transformers) and state-space models have recently been suggested as a viable linear-time alternative to transformers with softmax attention. However, these models still underperform transformers especially on tasks that require in-context retrieval. While more expressive variants of linear transformers which replace the additive update in linear transformers with the delta rule (DeltaNet) have been found to be more effective at associative recall, existing algorithms for training such models do not parallelize over sequence length and are thus inefficient to train on modern hardware. This work describes a hardware-efficient algorithm for training linear transformers with the delta rule, which exploits a memory-efficient representation for computing products of Householder matrices. This algorithm allows us to scale up DeltaNet to standard language modeling settings. We train a 1.3B model for 100B tokens and find that it outperforms recent linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks. We also experiment with two hybrid models which combine DeltaNet layers with (1) sliding-window attention layers every other layer or (2) two global attention layers, and find that these hybrids outperform strong transformer baselines.",
      "authors": [
        "Songlin Yang",
        "Bailin Wang",
        "Yu Zhang",
        "Yikang Shen",
        "Yoon Kim"
      ],
      "published": "2024-06-10T17:24:42Z",
      "updated": "2025-01-15T10:41:40Z",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06484v6",
      "landing_url": "https://arxiv.org/abs/2406.06484v6",
      "doi": "https://doi.org/10.48550/arXiv.2406.06484"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on linear transformer training algorithms for text language modeling and does not address discrete audio token generation, quantization, or any audio codec/tokenizer-specific design required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on linear transformer training algorithms for text language modeling and does not address discrete audio token generation, quantization, or any audio codec/tokenizer-specific design required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
    "abstract": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
    "metadata": {
      "arxiv_id": "2406.06582",
      "title": "Discrete Multimodal Transformers with a Pretrained Large Language Model for Mixed-Supervision Speech Processing",
      "summary": "Recent work on discrete speech tokenization has paved the way for models that can seamlessly perform multiple tasks across modalities, e.g., speech recognition, text to speech, speech to speech translation. Moreover, large language models (LLMs) pretrained from vast text corpora contain rich linguistic information that can improve accuracy in a variety of tasks. In this paper, we present a decoder-only Discrete Multimodal Language Model (DMLM), which can be flexibly applied to multiple tasks (ASR, T2S, S2TT, etc.) and modalities (text, speech, vision). We explore several critical aspects of discrete multi-modal models, including the loss function, weight initialization, mixed training supervision, and codebook. Our results show that DMLM benefits significantly, across multiple tasks and datasets, from a combination of supervised and unsupervised training. Moreover, for ASR, it benefits from initializing DMLM from a pretrained LLM, and from a codebook derived from Whisper activations.",
      "authors": [
        "Viet Anh Trinh",
        "Rosy Southwell",
        "Yiwen Guan",
        "Xinlu He",
        "Zhiyong Wang",
        "Jacob Whitehill"
      ],
      "published": "2024-06-04T20:08:25Z",
      "updated": "2024-06-25T17:44:00Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.06582v2",
      "landing_url": "https://arxiv.org/abs/2406.06582v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.06582"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Reasoning: this work centers on discrete speech tokens, codebook-based quantization, and downstream modeling so it meets the inclusion requirements without hitting any exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: this work centers on discrete speech tokens, codebook-based quantization, and downstream modeling so it meets the inclusion requirements without hitting any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
    "abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
    "metadata": {
      "arxiv_id": "2406.07119",
      "title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text",
      "summary": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts.Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
      "authors": [
        "Aoxiong Yin",
        "Haoyuan Li",
        "Kai Shen",
        "Siliang Tang",
        "Yueting Zhuang"
      ],
      "published": "2024-06-11T10:06:53Z",
      "updated": "2024-06-11T10:06:53Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07119v1",
      "landing_url": "https://arxiv.org/abs/2406.07119v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07119"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on discrete codes for sign language sequences (video) and text-to-sign generation, not on deriving or evaluating finite-vocabulary discrete tokens from audio signals, so it does not match the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on discrete codes for sign language sequences (video) and text-to-sign generation, not on deriving or evaluating finite-vocabulary discrete tokens from audio signals, so it does not match the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
    "abstract": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
    "metadata": {
      "arxiv_id": "2406.07855",
      "title": "VALL-E R: Robust and Efficient Zero-Shot Text-to-Speech Synthesis via Monotonic Alignment",
      "summary": "With the help of discrete neural audio codecs, large language models (LLM) have increasingly been recognized as a promising methodology for zero-shot Text-to-Speech (TTS) synthesis. However, sampling based decoding strategies bring astonishing diversity to generation, but also pose robustness issues such as typos, omissions and repetition. In addition, the high sampling rate of audio also brings huge computational overhead to the inference process of autoregression. To address these issues, we propose VALL-E R, a robust and efficient zero-shot TTS system, building upon the foundation of VALL-E. Specifically, we introduce a phoneme monotonic alignment strategy to strengthen the connection between phonemes and acoustic sequence, ensuring a more precise alignment by constraining the acoustic tokens to match their associated phonemes. Furthermore, we employ a codec-merging approach to downsample the discrete codes in shallow quantization layer, thereby accelerating the decoding speed while preserving the high quality of speech output. Benefiting from these strategies, VALL-E R obtains controllablity over phonemes and demonstrates its strong robustness by approaching the WER of ground truth. In addition, it requires fewer autoregressive steps, with over 60% time reduction during inference. This research has the potential to be applied to meaningful projects, including the creation of speech for those affected by aphasia. Audio samples will be available at: https://aka.ms/valler.",
      "authors": [
        "Bing Han",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Lingwei Meng",
        "Yanming Qian",
        "Yanqing Liu",
        "Sheng Zhao",
        "Jinyu Li",
        "Furu Wei"
      ],
      "published": "2024-06-12T04:09:44Z",
      "updated": "2024-06-12T04:09:44Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.07855v1",
      "landing_url": "https://arxiv.org/abs/2406.07855v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.07855"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Describes improvements to VALL-E with codec-based discrete tokens, phoneme alignment, and codec merging with quality metrics/WER, directly matching the discrete audio token focus so fully meets inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Describes improvements to VALL-E with codec-based discrete tokens, phoneme alignment, and codec merging with quality metrics/WER, directly matching the discrete audio token focus so fully meets inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
    "abstract": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.",
    "metadata": {
      "arxiv_id": "2406.08402",
      "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
      "summary": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.",
      "authors": [
        "Chun-Yi Kuan",
        "Wei-Ping Huang",
        "Hung-yi Lee"
      ],
      "published": "2024-06-12T16:51:54Z",
      "updated": "2024-06-12T16:51:54Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.08402v1",
      "landing_url": "https://arxiv.org/abs/2406.08402v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.08402"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses reliability of large audio-language models and question answering but does not address discrete audio token generation/quantization, so it fails to meet the inclusion focus on discrete token methods."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses reliability of large audio-language models and question answering but does not address discrete audio token generation/quantization, so it fails to meet the inclusion focus on discrete token methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model",
    "abstract": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.",
    "metadata": {
      "arxiv_id": "2406.09869",
      "title": "MMM: Multi-Layer Multi-Residual Multi-Stream Discrete Speech Representation from Self-supervised Learning Model",
      "summary": "Speech discrete representation has proven effective in various downstream applications due to its superior compression rate of the waveform, fast convergence during training, and compatibility with other modalities. Discrete units extracted from self-supervised learning (SSL) models have emerged as a prominent approach for obtaining speech discrete representation. However, while discrete units have shown effectiveness compared to spectral features, they still lag behind continuous SSL representations. In this work, we propose MMM, a multi-layer multi-residual multi-stream discrete units extraction method from SSL. Specifically, we introduce iterative residual vector quantization with K-means for different layers in an SSL model to extract multi-stream speech discrete representation. Through extensive experiments in speech recognition, speech resynthesis, and text-to-speech, we demonstrate the proposed MMM can surpass or on-par with neural codec's performance under various conditions.",
      "authors": [
        "Jiatong Shi",
        "Xutai Ma",
        "Hirofumi Inaguma",
        "Anna Sun",
        "Shinji Watanabe"
      ],
      "published": "2024-06-14T09:29:45Z",
      "updated": "2024-06-14T09:29:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.09869v1",
      "landing_url": "https://arxiv.org/abs/2406.09869v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.09869"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper explicitly targets discrete speech tokens extracted from SSL representations, describes quantization/codebook design (iterative residual VQ) and evaluates them on reconstruction and downstream tasks, so it meets all inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly targets discrete speech tokens extracted from SSL representations, describes quantization/codebook design (iterative residual VQ) and evaluates them on reconstruction and downstream tasks, so it meets all inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
    "abstract": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
    "metadata": {
      "arxiv_id": "2406.10735",
      "title": "How Should We Extract Discrete Audio Tokens from Self-Supervised Models?",
      "summary": "Discrete audio tokens have recently gained attention for their potential to bridge the gap between audio and language processing. Ideal audio tokens must preserve content, paralinguistic elements, speaker identity, and many other audio details. Current audio tokenization methods fall into two categories: Semantic tokens, acquired through quantization of Self-Supervised Learning (SSL) models, and Neural compression-based tokens (codecs). Although previous studies have benchmarked codec models to identify optimal configurations, the ideal setup for quantizing pretrained SSL models remains unclear. This paper explores the optimal configuration of semantic tokens across discriminative and generative tasks. We propose a scalable solution to train a universal vocoder across multiple SSL layers. Furthermore, an attention mechanism is employed to identify task-specific influential layers, enhancing the adaptability and performance of semantic tokens in diverse audio applications.",
      "authors": [
        "Pooneh Mousavi",
        "Jarod Duret",
        "Salah Zaiem",
        "Luca Della Libera",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-15T20:43:07Z",
      "updated": "2024-06-15T20:43:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.10735v1",
      "landing_url": "https://arxiv.org/abs/2406.10735v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.10735"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper focuses on discrete audio tokens derived from quantized SSL representations, evaluates configurations and uses semantic tokens with tasks and reconstruction analysis, satisfying the inclusion criteria with no exclusion trigger."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on discrete audio tokens derived from quantized SSL representations, evaluates configurations and uses semantic tokens with tasks and reconstruction analysis, satisfying the inclusion criteria with no exclusion trigger.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
    "abstract": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
    "metadata": {
      "arxiv_id": "2406.11037",
      "title": "NAST: Noise Aware Speech Tokenization for Speech Language Models",
      "summary": "Speech tokenization is the task of representing speech signals as a sequence of discrete units. Such representations can be later used for various downstream tasks including automatic speech recognition, text-to-speech, etc. More relevant to this study, such representation serves as the basis of Speech Language Models. In this work, we tackle the task of speech tokenization under the noisy setup and present NAST: Noise Aware Speech Tokenization for Speech Language Models. NAST is composed of three main components: (i) a predictor; (ii) a residual encoder; and (iii) a decoder. We evaluate the efficiency of NAST considering several spoken language modeling tasks and show that NAST is superior to the evaluated baselines across all setups. Lastly, we analyze NAST and show its disentanglement properties and robustness to signal variations in the form of noise, reverberation, pitch-shift, and time-stretch. Code and pre-trained models are available at https://github.com/ShovalMessica/NAST.",
      "authors": [
        "Shoval Messica",
        "Yossi Adi"
      ],
      "published": "2024-06-16T18:20:45Z",
      "updated": "2024-06-16T18:20:45Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11037v1",
      "landing_url": "https://arxiv.org/abs/2406.11037v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.11037"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "NAST explicitly proposes a noise-aware speech tokenizer that learns discrete units for speech LMs and evaluates robustness aspects, so it meets all discrete-token inclusion criteria and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "NAST explicitly proposes a noise-aware speech tokenizer that learns discrete units for speech LMs and evaluates robustness aspects, so it meets all discrete-token inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Autoregressive Image Generation without Vector Quantization",
    "abstract": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
    "metadata": {
      "arxiv_id": "2406.11838",
      "title": "Autoregressive Image Generation without Vector Quantization",
      "summary": "Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar.",
      "authors": [
        "Tianhong Li",
        "Yonglong Tian",
        "He Li",
        "Mingyang Deng",
        "Kaiming He"
      ],
      "published": "2024-06-17T17:59:58Z",
      "updated": "2024-11-01T14:45:36Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.11838v3",
      "landing_url": "https://arxiv.org/abs/2406.11838v3",
      "doi": "https://doi.org/10.48550/arXiv.2406.11838"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper targets image generation and does not address discrete audio tokenization or codec-style quantization, so it fails the topic criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper targets image generation and does not address discrete audio tokenization or codec-style quantization, so it fails the topic criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Memory Sequence Length of Data Sampling Impacts the Adaptation of Meta-Reinforcement Learning Agents",
    "abstract": "Fast adaptation to new tasks is extremely important for embodied agents in the real world. Meta-reinforcement learning (meta-RL) has emerged as an effective method to enable fast adaptation in unknown environments. Compared to on-policy meta-RL algorithms, off-policy algorithms rely heavily on efficient data sampling strategies to extract and represent the historical trajectories. However, little is known about how different data sampling methods impact the ability of meta-RL agents to represent unknown environments. Here, we investigate the impact of data sampling strategies on the exploration and adaptability of meta-RL agents. Specifically, we conducted experiments with two types of off-policy meta-RL algorithms based on Thompson sampling and Bayes-optimality theories in continuous control tasks within the MuJoCo environment and sparse reward navigation tasks. Our analysis revealed the long-memory and short-memory sequence sampling strategies affect the representation and adaptive capabilities of meta-RL agents. We found that the algorithm based on Bayes-optimality theory exhibited more robust and better adaptability than the algorithm based on Thompson sampling, highlighting the importance of appropriate data sampling strategies for the agent's representation of an unknown environment, especially in the case of sparse rewards.",
    "metadata": {
      "arxiv_id": "2406.12359",
      "title": "Memory Sequence Length of Data Sampling Impacts the Adaptation of Meta-Reinforcement Learning Agents",
      "summary": "Fast adaptation to new tasks is extremely important for embodied agents in the real world. Meta-reinforcement learning (meta-RL) has emerged as an effective method to enable fast adaptation in unknown environments. Compared to on-policy meta-RL algorithms, off-policy algorithms rely heavily on efficient data sampling strategies to extract and represent the historical trajectories. However, little is known about how different data sampling methods impact the ability of meta-RL agents to represent unknown environments. Here, we investigate the impact of data sampling strategies on the exploration and adaptability of meta-RL agents. Specifically, we conducted experiments with two types of off-policy meta-RL algorithms based on Thompson sampling and Bayes-optimality theories in continuous control tasks within the MuJoCo environment and sparse reward navigation tasks. Our analysis revealed the long-memory and short-memory sequence sampling strategies affect the representation and adaptive capabilities of meta-RL agents. We found that the algorithm based on Bayes-optimality theory exhibited more robust and better adaptability than the algorithm based on Thompson sampling, highlighting the importance of appropriate data sampling strategies for the agent's representation of an unknown environment, especially in the case of sparse rewards.",
      "authors": [
        "Menglong Zhang",
        "Fuyuan Qian",
        "Quanying Liu"
      ],
      "published": "2024-06-18T07:41:40Z",
      "updated": "2024-06-18T07:41:40Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12359v1",
      "landing_url": "https://arxiv.org/abs/2406.12359v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.12359"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper studies meta-reinforcement learning data sampling for continuous control and navigation, which has nothing to do with discrete audio token generation or quantization, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This paper studies meta-reinforcement learning data sampling for continuous control and navigation, which has nothing to do with discrete audio token generation or quantization, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
    "abstract": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
    "metadata": {
      "arxiv_id": "2406.12428",
      "title": "PSLM: Parallel Generation of Text and Speech with LLMs for Low-Latency Spoken Dialogue Systems",
      "summary": "Multimodal language models that process both text and speech have a potential for applications in spoken dialogue systems. However, current models face two major challenges in response generation latency: (1) generating a spoken response requires the prior generation of a written response, and (2) speech sequences are significantly longer than text sequences. This study addresses these issues by extending the input and output sequences of the language model to support the parallel generation of text and speech. Our experiments on spoken question answering tasks demonstrate that our approach improves latency while maintaining the quality of response content. Additionally, we show that latency can be further reduced by generating speech in multiple sequences. Demo samples are available at https://rinnakk.github.io/research/publications/PSLM.",
      "authors": [
        "Kentaro Mitsui",
        "Koh Mitsuda",
        "Toshiaki Wakatsuki",
        "Yukiya Hono",
        "Kei Sawada"
      ],
      "published": "2024-06-18T09:23:54Z",
      "updated": "2024-10-03T05:17:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.12428v2",
      "landing_url": "https://arxiv.org/abs/2406.12428v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.12428"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on reducing latency in joint text and speech generation for dialogue systems but doesn’t describe discrete audio tokenizers/quantization or explicit token specs, so it is better excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on reducing latency in joint text and speech generation for dialogue systems but doesn’t describe discrete audio tokenizers/quantization or explicit token specs, so it is better excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
    "abstract": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
    "metadata": {
      "arxiv_id": "2406.13275",
      "title": "Enhancing Automated Audio Captioning via Large Language Models with Optimized Audio Encoding",
      "summary": "Automated audio captioning (AAC) is an audio-to-text task to describe audio contents in natural language. Recently, the advancements in large language models (LLMs), with improvements in training approaches for audio encoders, have opened up possibilities for improving AAC. Thus, we explore enhancing AAC from three aspects: 1) a pre-trained audio encoder via consistent ensemble distillation (CED) is used to improve the effectivity of acoustic tokens, with a querying transformer (Q-Former) bridging the modality gap to LLM and compress acoustic tokens; 2) we investigate the advantages of using a Llama 2 with 7B parameters as the decoder; 3) another pre-trained LLM corrects text errors caused by insufficient training data and annotation ambiguities. Both the audio encoder and text decoder are optimized by low-rank adaptation (LoRA). Experiments show that each of these enhancements is effective. Our method obtains a 33.0 SPIDEr-FL score, outperforming the winner of DCASE 2023 Task 6A.",
      "authors": [
        "Jizhong Liu",
        "Gang Li",
        "Junbo Zhang",
        "Heinrich Dinkel",
        "Yongqing Wang",
        "Zhiyong Yan",
        "Yujun Wang",
        "Bin Wang"
      ],
      "published": "2024-06-19T07:09:46Z",
      "updated": "2024-06-25T08:07:36Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13275v2",
      "landing_url": "https://arxiv.org/abs/2406.13275v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13275"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on improving automated audio captioning via LLMs and encoder/decoder training without any mention of discrete audio tokenization, quantization, or token-level evaluations; therefore it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on improving automated audio captioning via LLMs and encoder/decoder training without any mention of discrete audio tokenization, quantization, or token-level evaluations; therefore it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Children's Speech Recognition through Discrete Token Enhancement",
    "abstract": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
    "metadata": {
      "arxiv_id": "2406.13431",
      "title": "Children's Speech Recognition through Discrete Token Enhancement",
      "summary": "Children's speech recognition is considered a low-resource task mainly due to the lack of publicly available data. There are several reasons for such data scarcity, including expensive data collection and annotation processes, and data privacy, among others. Transforming speech signals into discrete tokens that do not carry sensitive information but capture both linguistic and acoustic information could be a solution for privacy concerns. In this study, we investigate the integration of discrete speech tokens into children's speech recognition systems as input without significantly degrading the ASR performance. Additionally, we explored single-view and multi-view strategies for creating these discrete labels. Furthermore, we tested the models for generalization capabilities with unseen domain and nativity dataset. Results reveal that the discrete token ASR for children achieves nearly equivalent performance with an approximate 83% reduction in parameters.",
      "authors": [
        "Vrunda N. Sukhadia",
        "Shammur Absar Chowdhury"
      ],
      "published": "2024-06-19T10:45:12Z",
      "updated": "2024-06-24T15:31:59Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.13431v2",
      "landing_url": "https://arxiv.org/abs/2406.13431v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.13431"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The study explicitly investigates discrete speech tokens as input to children’s ASR, including generation strategies and evaluation of generalization, matching the discrete audio token topic and providing token-focused methods/results, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The study explicitly investigates discrete speech tokens as input to children’s ASR, including generation strategies and evaluation of generalization, matching the discrete audio token topic and providing token-focused methods/results, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DASB - Discrete Audio and Speech Benchmark",
    "abstract": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
    "metadata": {
      "arxiv_id": "2406.14294",
      "title": "DASB - Discrete Audio and Speech Benchmark",
      "summary": "Discrete audio tokens have recently gained considerable attention for their potential to connect audio and language processing, enabling the creation of modern multimodal large language models. Ideal audio tokens must effectively preserve phonetic and semantic content along with paralinguistic information, speaker identity, and other details. While several types of audio tokens have been recently proposed, identifying the optimal tokenizer for various tasks is challenging due to the inconsistent evaluation settings in existing studies. To address this gap, we release the Discrete Audio and Speech Benchmark (DASB), a comprehensive leaderboard for benchmarking discrete audio tokens across a wide range of discriminative tasks, including speech recognition, speaker identification and verification, emotion recognition, keyword spotting, and intent classification, as well as generative tasks such as speech enhancement, separation, and text-to-speech. Our results show that, on average, semantic tokens outperform compression tokens across most discriminative and generative tasks. However, the performance gap between semantic tokens and standard continuous representations remains substantial, highlighting the need for further research in this field.",
      "authors": [
        "Pooneh Mousavi",
        "Luca Della Libera",
        "Jarod Duret",
        "Artem Ploujnikov",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2024-06-20T13:23:27Z",
      "updated": "2024-06-21T17:07:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.14294v2",
      "landing_url": "https://arxiv.org/abs/2406.14294v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.14294"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Includes comprehensive evaluation of discrete audio tokens (semantic vs compression) across codec and downstream tasks, matching inclusion requirements with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Includes comprehensive evaluation of discrete audio tokens (semantic vs compression) across codec and downstream tasks, matching inclusion requirements with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "A Cryogenic readout integrated circuit with analog pile-up and in-Pixel ADC for high frame rate Skipper CCD-in-CMOS Sensors",
    "abstract": "The Skipper CCD-in-CMOS Parallel Read-Out Circuit V2 (SPROCKET2) is designed to enable high frame rate readout of Skipper CCD-in-CMOS image sensors. The SPROCKET2 pixel is fabricated in a 65 nm CMOS process and occupies a 60$μ$m $\\times$ 60$μ$m footprint. SPROCKET2 is intended to be heterogeneously integrated with a pixelated Skipper CCD-in-CMOS sensor, such that one readout pixel is connected to a multiplexed array of 16 active image sensor pixels, to match their spatial geometry. Our design benefits from the Skipper CCD-in-CMOS sensor's non-destructive readout capability to achieve exceptionally low noise through multi-sampling and averaging while optimizing for total power consumption. The pixel readout utilizes correlated double sampling to minimize 1/f noise and includes \"pile-up\" of ten successive samples in the analog domain before digitizing at a rate of 66.7 ksps. Measurement results of in-pixel serial SAR ADC show DNL and INL of ~0. 44 LSB and 0.58 LBS respectively. A large area array of 20,000 SPROCKET2 ADC pixels (multiplexed 1:16 to 320,000 sensor pixels) is currently under test. By reading out data over a 10 Gbps optical link, this pixel design enables a frame rate of $\\sim$ 4 kfps for large sensing areas with minimal sensing deadtime. In the highest gain mode, the pixelated ADC has an input-referred resolution of 10$μ$V with a simulated power consumption of 50$μ$W. The pixel operates with constant current draw to minimize power-rail crosstalk.",
    "metadata": {
      "arxiv_id": "2406.15207",
      "title": "A Cryogenic readout integrated circuit with analog pile-up and in-Pixel ADC for high frame rate Skipper CCD-in-CMOS Sensors",
      "summary": "The Skipper CCD-in-CMOS Parallel Read-Out Circuit V2 (SPROCKET2) is designed to enable high frame rate readout of Skipper CCD-in-CMOS image sensors. The SPROCKET2 pixel is fabricated in a 65 nm CMOS process and occupies a 60$μ$m $\\times$ 60$μ$m footprint. SPROCKET2 is intended to be heterogeneously integrated with a pixelated Skipper CCD-in-CMOS sensor, such that one readout pixel is connected to a multiplexed array of 16 active image sensor pixels, to match their spatial geometry. Our design benefits from the Skipper CCD-in-CMOS sensor's non-destructive readout capability to achieve exceptionally low noise through multi-sampling and averaging while optimizing for total power consumption. The pixel readout utilizes correlated double sampling to minimize 1/f noise and includes \"pile-up\" of ten successive samples in the analog domain before digitizing at a rate of 66.7 ksps. Measurement results of in-pixel serial SAR ADC show DNL and INL of ~0. 44 LSB and 0.58 LBS respectively. A large area array of 20,000 SPROCKET2 ADC pixels (multiplexed 1:16 to 320,000 sensor pixels) is currently under test. By reading out data over a 10 Gbps optical link, this pixel design enables a frame rate of $\\sim$ 4 kfps for large sensing areas with minimal sensing deadtime. In the highest gain mode, the pixelated ADC has an input-referred resolution of 10$μ$V with a simulated power consumption of 50$μ$W. The pixel operates with constant current draw to minimize power-rail crosstalk.",
      "authors": [
        "Adam Quinn",
        "Farah Fahim",
        "Davide Braga"
      ],
      "published": "2024-06-21T14:48:37Z",
      "updated": "2024-06-21T14:48:37Z",
      "categories": [
        "physics.ins-det"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15207v1",
      "landing_url": "https://arxiv.org/abs/2406.15207v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15207"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on cryogenic readout hardware for Skipper CCD sensors with analog pile-up and ADC design, which has no relation to discrete audio tokenization or codecs, so it fails both inclusion coverage and relevance criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on cryogenic readout hardware for Skipper CCD sensors with analog pile-up and ADC design, which has no relation to discrete audio tokenization or codecs, so it fails both inclusion coverage and relevance criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
    "abstract": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
    "metadata": {
      "arxiv_id": "2406.15752",
      "title": "TacoLM: GaTed Attention Equipped Codec Language Model are Efficient Zero-Shot Text to Speech Synthesizers",
      "summary": "Neural codec language model (LM) has demonstrated strong capability in zero-shot text-to-speech (TTS) synthesis. However, the codec LM often suffers from limitations in inference speed and stability, due to its auto-regressive nature and implicit alignment between text and audio. In this work, to handle these challenges, we introduce a new variant of neural codec LM, namely TacoLM. Specifically, TacoLM introduces a gated attention mechanism to improve the training and inference efficiency and reduce the model size. Meanwhile, an additional gated cross-attention layer is included for each decoder layer, which improves the efficiency and content accuracy of the synthesized speech. In the evaluation of the Librispeech corpus, the proposed TacoLM achieves a better word error rate, speaker similarity, and mean opinion score, with 90% fewer parameters and 5.2 times speed up, compared with VALL-E. Demo and code is available at https://ereboas.github.io/TacoLM/.",
      "authors": [
        "Yakun Song",
        "Zhuo Chen",
        "Xiaofei Wang",
        "Ziyang Ma",
        "Guanrou Yang",
        "Xie Chen"
      ],
      "published": "2024-06-22T06:39:52Z",
      "updated": "2024-06-22T06:39:52Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.15752v1",
      "landing_url": "https://arxiv.org/abs/2406.15752v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.15752"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "TacoLM builds on neural codec language models that explicitly rely on quantized discrete audio tokens for text-to-speech generation, reporting codec-focused efficiency and quality gains, so it clearly satisfies the discrete-token inclusion requirements with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "TacoLM builds on neural codec language models that explicitly rely on quantized discrete audio tokens for text-to-speech generation, reporting codec-focused efficiency and quality gains, so it clearly satisfies the discrete-token inclusion requirements with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Decoder-only Architecture for Streaming End-to-end Speech Recognition",
    "abstract": "Decoder-only language models (LMs) have been successfully adopted for speech-processing tasks including automatic speech recognition (ASR). The LMs have ample expressiveness and perform efficiently. This efficiency is a suitable characteristic for streaming applications of ASR. In this work, we propose to use a decoder-only architecture for blockwise streaming ASR. In our approach, speech features are compressed using CTC output and context embedding using blockwise speech subnetwork, and are sequentially provided as prompts to the decoder. The decoder estimates the output tokens promptly at each block. To this end, we also propose a novel training scheme using random-length prefix prompts to make the model robust to the truncated prompts caused by blockwise processing. An experimental comparison shows that our proposed decoder-only streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech test-other set while being twice as fast as the baseline model.",
    "metadata": {
      "arxiv_id": "2406.16107",
      "title": "Decoder-only Architecture for Streaming End-to-end Speech Recognition",
      "summary": "Decoder-only language models (LMs) have been successfully adopted for speech-processing tasks including automatic speech recognition (ASR). The LMs have ample expressiveness and perform efficiently. This efficiency is a suitable characteristic for streaming applications of ASR. In this work, we propose to use a decoder-only architecture for blockwise streaming ASR. In our approach, speech features are compressed using CTC output and context embedding using blockwise speech subnetwork, and are sequentially provided as prompts to the decoder. The decoder estimates the output tokens promptly at each block. To this end, we also propose a novel training scheme using random-length prefix prompts to make the model robust to the truncated prompts caused by blockwise processing. An experimental comparison shows that our proposed decoder-only streaming ASR achieves 8% relative word error rate reduction in the LibriSpeech test-other set while being twice as fast as the baseline model.",
      "authors": [
        "Emiru Tsunoo",
        "Hayato Futami",
        "Yosuke Kashiwagi",
        "Siddhant Arora",
        "Shinji Watanabe"
      ],
      "published": "2024-06-23T13:50:08Z",
      "updated": "2024-08-01T13:55:54Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.16107v2",
      "landing_url": "https://arxiv.org/abs/2406.16107v2",
      "doi": "https://doi.org/10.48550/arXiv.2406.16107"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No mention of discrete audio tokens, quantization, or tokenizer/codebook specification, so it does not meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "No mention of discrete audio tokens, quantization, or tokenizer/codebook specification, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
    "abstract": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
    "metadata": {
      "arxiv_id": "2406.17310",
      "title": "High Fidelity Text-to-Speech Via Discrete Tokens Using Token Transducer and Group Masked Language Model",
      "summary": "We propose a novel two-stage text-to-speech (TTS) framework with two types of discrete tokens, i.e., semantic and acoustic tokens, for high-fidelity speech synthesis. It features two core components: the Interpreting module, which processes text and a speech prompt into semantic tokens focusing on linguistic contents and alignment, and the Speaking module, which captures the timbre of the target voice to generate acoustic tokens from semantic tokens, enriching speech reconstruction. The Interpreting stage employs a transducer for its robustness in aligning text to speech. In contrast, the Speaking stage utilizes a Conformer-based architecture integrated with a Grouped Masked Language Model (G-MLM) to boost computational efficiency. Our experiments verify that this innovative structure surpasses the conventional models in the zero-shot scenario in terms of speech quality and speaker similarity.",
      "authors": [
        "Joun Yeop Lee",
        "Myeonghun Jeong",
        "Minchan Kim",
        "Ji-Hyun Lee",
        "Hoon-Young Cho",
        "Nam Soo Kim"
      ],
      "published": "2024-06-25T06:46:47Z",
      "updated": "2024-06-25T06:46:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.17310v1",
      "landing_url": "https://arxiv.org/abs/2406.17310v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.17310"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract describe a discrete-token TTS pipeline with semantic and acoustic tokens, structured tokenizer/quantization modules, and evaluation of quality/speaker similarity, matching the inclusion focus on discrete audio tokens without violating any exclusion rules."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a discrete-token TTS pipeline with semantic and acoustic tokens, structured tokenizer/quantization modules, and evaluation of quality/speaker similarity, matching the inclusion focus on discrete audio tokens without violating any exclusion rules.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study",
    "abstract": "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various speech-related tasks, especially in Automatic Speech Recognition (ASR). These models typically adopt a unified method to model discrete speech and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire speech utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming recognition, incorporating a dedicated boundary token to facilitate streaming recognition and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model's contextual modeling abilities. While achieving streaming speech recognition, experiments on the AISHELL-1 and -2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counterparts.",
    "metadata": {
      "arxiv_id": "2406.18862",
      "title": "Streaming Decoder-Only Automatic Speech Recognition with Discrete Speech Units: A Pilot Study",
      "summary": "Unified speech-text models like SpeechGPT, VioLA, and AudioPaLM have shown impressive performance across various speech-related tasks, especially in Automatic Speech Recognition (ASR). These models typically adopt a unified method to model discrete speech and text tokens, followed by training a decoder-only transformer. However, they are all designed for non-streaming ASR tasks, where the entire speech utterance is needed during decoding. Hence, we introduce a decoder-only model exclusively designed for streaming recognition, incorporating a dedicated boundary token to facilitate streaming recognition and employing causal attention masking during the training phase. Furthermore, we introduce right-chunk attention and various data augmentation techniques to improve the model's contextual modeling abilities. While achieving streaming speech recognition, experiments on the AISHELL-1 and -2 datasets demonstrate the competitive performance of our streaming approach with non-streaming decoder-only counterparts.",
      "authors": [
        "Peikun Chen",
        "Sining Sun",
        "Changhao Shan",
        "Qing Yang",
        "Lei Xie"
      ],
      "published": "2024-06-27T03:16:22Z",
      "updated": "2024-06-27T03:16:22Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2406.18862v1",
      "landing_url": "https://arxiv.org/abs/2406.18862v1",
      "doi": "https://doi.org/10.48550/arXiv.2406.18862"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Title/abstract explicitly focus on decoding discrete speech units for streaming ASR, so it appears to align with the discrete audio token topic and supports token-level modeling, making it better to include."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract explicitly focus on decoding discrete speech units for streaming ASR, so it appears to align with the discrete audio token topic and supports token-level modeling, making it better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
    "abstract": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
    "metadata": {
      "arxiv_id": "2407.00261",
      "title": "Generative Iris Prior Embedded Transformer for Iris Restoration",
      "summary": "Iris restoration from complexly degraded iris images, aiming to improve iris recognition performance, is a challenging problem. Due to the complex degradation, directly training a convolutional neural network (CNN) without prior cannot yield satisfactory results. In this work, we propose a generative iris prior embedded Transformer model (Gformer), in which we build a hierarchical encoder-decoder network employing Transformer block and generative iris prior. First, we tame Transformer blocks to model long-range dependencies in target images. Second, we pretrain an iris generative adversarial network (GAN) to obtain the rich iris prior, and incorporate it into the iris restoration process with our iris feature modulator. Our experiments demonstrate that the proposed Gformer outperforms state-of-the-art methods. Besides, iris recognition performance has been significantly improved after applying Gformer.",
      "authors": [
        "Yubo Huang",
        "Jia Wang",
        "Peipei Li",
        "Liuyu Xiang",
        "Peigang Li",
        "Zhaofeng He"
      ],
      "published": "2024-06-28T23:20:57Z",
      "updated": "2024-09-20T23:54:19Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00261v2",
      "landing_url": "https://arxiv.org/abs/2407.00261v2",
      "doi": "https://doi.org/10.1109/ICME55011.2023.00094"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on iris restoration using CNN/Transformer with GAN priors and has no connection to discrete audio token generation or codec/token tokenizer design, so it clearly fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on iris restoration using CNN/Transformer with GAN priors and has no connection to discrete audio token generation or codec/token tokenizer design, so it clearly fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamically Modulating Visual Place Recognition Sequence Length For Minimum Acceptable Performance Scenarios",
    "abstract": "Mobile robots and autonomous vehicles are often required to function in environments where critical position estimates from sensors such as GPS become uncertain or unreliable. Single image visual place recognition (VPR) provides an alternative for localization but often requires techniques such as sequence matching to improve robustness, which incurs additional computation and latency costs. Even then, the sequence length required to localize at an acceptable performance level varies widely; and simply setting overly long fixed sequence lengths creates unnecessary latency, computational overhead, and can even degrade performance. In these scenarios it is often more desirable to meet or exceed a set target performance at minimal expense. In this paper we present an approach which uses a calibration set of data to fit a model that modulates sequence length for VPR as needed to exceed a target localization performance. We make use of a coarse position prior, which could be provided by any other localization system, and capture the variation in appearance across this region. We use the correlation between appearance variation and sequence length to curate VPR features and fit a multilayer perceptron (MLP) for selecting the optimal length. We demonstrate that this method is effective at modulating sequence length to maximize the number of sections in a dataset which meet or exceed a target performance whilst minimizing the median length used. We show applicability across several datasets and reveal key phenomena like generalization capabilities, the benefits of curating features and the utility of non-state-of-the-art feature extractors with nuanced properties.",
    "metadata": {
      "arxiv_id": "2407.00863",
      "title": "Dynamically Modulating Visual Place Recognition Sequence Length For Minimum Acceptable Performance Scenarios",
      "summary": "Mobile robots and autonomous vehicles are often required to function in environments where critical position estimates from sensors such as GPS become uncertain or unreliable. Single image visual place recognition (VPR) provides an alternative for localization but often requires techniques such as sequence matching to improve robustness, which incurs additional computation and latency costs. Even then, the sequence length required to localize at an acceptable performance level varies widely; and simply setting overly long fixed sequence lengths creates unnecessary latency, computational overhead, and can even degrade performance. In these scenarios it is often more desirable to meet or exceed a set target performance at minimal expense. In this paper we present an approach which uses a calibration set of data to fit a model that modulates sequence length for VPR as needed to exceed a target localization performance. We make use of a coarse position prior, which could be provided by any other localization system, and capture the variation in appearance across this region. We use the correlation between appearance variation and sequence length to curate VPR features and fit a multilayer perceptron (MLP) for selecting the optimal length. We demonstrate that this method is effective at modulating sequence length to maximize the number of sections in a dataset which meet or exceed a target performance whilst minimizing the median length used. We show applicability across several datasets and reveal key phenomena like generalization capabilities, the benefits of curating features and the utility of non-state-of-the-art feature extractors with nuanced properties.",
      "authors": [
        "Connor Malone",
        "Ankit Vora",
        "Thierry Peynot",
        "Michael Milford"
      ],
      "published": "2024-07-01T00:16:35Z",
      "updated": "2024-07-01T00:16:35Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.00863v1",
      "landing_url": "https://arxiv.org/abs/2407.00863v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.00863"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe visual place recognition for robotics, not research on discrete audio tokenization, so it fails to meet any inclusion criteria and instead fits an exclusion case."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe visual place recognition for robotics, not research on discrete audio tokenization, so it fails to meet any inclusion criteria and instead fits an exclusion case.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
    "abstract": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
    "metadata": {
      "arxiv_id": "2407.01290",
      "title": "Hypformer: Exploring Efficient Transformer Fully in Hyperbolic Space",
      "summary": "Hyperbolic geometry have shown significant potential in modeling complex structured data, particularly those with underlying tree-like and hierarchical structures. Despite the impressive performance of various hyperbolic neural networks across numerous domains, research on adapting the Transformer to hyperbolic space remains limited. Previous attempts have mainly focused on modifying self-attention modules in the Transformer. However, these efforts have fallen short of developing a complete hyperbolic Transformer. This stems primarily from: (i) the absence of well-defined modules in hyperbolic space, including linear transformation layers, LayerNorm layers, activation functions, dropout operations, etc. (ii) the quadratic time complexity of the existing hyperbolic self-attention module w.r.t the number of input tokens, which hinders its scalability. To address these challenges, we propose, Hypformer, a novel hyperbolic Transformer based on the Lorentz model of hyperbolic geometry. In Hypformer, we introduce two foundational blocks that define the essential modules of the Transformer in hyperbolic space. Furthermore, we develop a linear self-attention mechanism in hyperbolic space, enabling hyperbolic Transformer to process billion-scale graph data and long-sequence inputs for the first time. Our experimental results confirm the effectiveness and efficiency of Hypformer across various datasets, demonstrating its potential as an effective and scalable solution for large-scale data representation and large models.",
      "authors": [
        "Menglin Yang",
        "Harshit Verma",
        "Delvin Ce Zhang",
        "Jiahong Liu",
        "Irwin King",
        "Rex Ying"
      ],
      "published": "2024-07-01T13:44:38Z",
      "updated": "2025-08-24T12:17:27Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01290v2",
      "landing_url": "https://arxiv.org/abs/2407.01290v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.01290"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper proposes a hyperbolic Transformer architecture and linear self-attention for large-scale structured data, with no focus on discrete audio tokenization, quantization, or codec/token evaluations, so it fails the inclusion criteria and meets exclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a hyperbolic Transformer architecture and linear self-attention for large-scale structured data, with no focus on discrete audio tokenization, quantization, or codec/token evaluations, so it fails the inclusion criteria and meets exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
    "abstract": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
    "metadata": {
      "arxiv_id": "2407.01911",
      "title": "Investigating the Effects of Large-Scale Pseudo-Stereo Data and Different Speech Foundation Model on Dialogue Generative Spoken Language Model",
      "summary": "Recent efforts in Spoken Dialogue Modeling aim to synthesize spoken dialogue without the need for direct transcription, thereby preserving the wealth of non-textual information inherent in speech. However, this approach faces a challenge when speakers talk simultaneously, requiring stereo dialogue data with speakers recorded on separate channels, a notably scarce resource. To address this, we have developed an innovative pipeline capable of transforming single-channel dialogue data into pseudo-stereo data. This expanded our training dataset from a mere 2,000 to an impressive 17,600 hours, significantly enriching the diversity and quality of the training examples available. The inclusion of this pseudo-stereo data has proven to be effective in improving the performance of spoken dialogue language models. Additionally, we explored the use of discrete units of different speech foundation models for spoken dialogue generation.",
      "authors": [
        "Yu-Kuan Fu",
        "Cheng-Kuang Lee",
        "Hsiu-Hsuan Wang",
        "Hung-yi Lee"
      ],
      "published": "2024-07-02T03:22:41Z",
      "updated": "2024-07-02T03:22:41Z",
      "categories": [
        "cs.CL",
        "cs.HC",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.01911v1",
      "landing_url": "https://arxiv.org/abs/2407.01911v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.01911"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Abstract focuses on pseudo-stereo data expansion and testing different speech foundation models for dialogue generation but provides no concrete discrete-token/quantization design or evaluation, so it does not clearly satisfy the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on pseudo-stereo data expansion and testing different speech foundation models for dialogue generation but provides no concrete discrete-token/quantization design or evaluation, so it does not clearly satisfy the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Investigating Decoder-only Large Language Models for Speech-to-text Translation",
    "abstract": "Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT). We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation. Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation. Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data. We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT.",
    "metadata": {
      "arxiv_id": "2407.03169",
      "title": "Investigating Decoder-only Large Language Models for Speech-to-text Translation",
      "summary": "Large language models (LLMs), known for their exceptional reasoning capabilities, generalizability, and fluency across diverse domains, present a promising avenue for enhancing speech-related tasks. In this paper, we focus on integrating decoder-only LLMs to the task of speech-to-text translation (S2TT). We propose a decoder-only architecture that enables the LLM to directly consume the encoded speech representation and generate the text translation. Additionally, we investigate the effects of different parameter-efficient fine-tuning techniques and task formulation. Our model achieves state-of-the-art performance on CoVoST 2 and FLEURS among models trained without proprietary data. We also conduct analyses to validate the design choices of our proposed model and bring insights to the integration of LLMs to S2TT.",
      "authors": [
        "Chao-Wei Huang",
        "Hui Lu",
        "Hongyu Gong",
        "Hirofumi Inaguma",
        "Ilia Kulikov",
        "Ruslan Mavlyutov",
        "Sravya Popuri"
      ],
      "published": "2024-07-03T14:42:49Z",
      "updated": "2024-07-03T14:42:49Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03169v1",
      "landing_url": "https://arxiv.org/abs/2407.03169v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03169"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on decoder-only LLM for S2TT without detailing discrete audio tokenization or quantization, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Focuses on decoder-only LLM for S2TT without detailing discrete audio tokenization or quantization, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations",
    "abstract": "Discrete speech representations have garnered recent attention for their efficacy in training transformer-based models for various speech-related tasks such as automatic speech recognition (ASR), translation, speaker verification, and joint speech-text foundational models. In this work, we present a comprehensive analysis on building ASR systems with discrete codes. We investigate different methods for codec training such as quantization schemes and time-domain vs spectral feature encodings. We further explore ASR training techniques aimed at enhancing performance, training efficiency, and noise robustness. Drawing upon our findings, we introduce a codec ASR pipeline that outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the state-of-the-art results achieved by strong self-supervised models on the 143 languages ML-SUPERB benchmark despite being smaller in size and pretrained on significantly less data.",
    "metadata": {
      "arxiv_id": "2407.03495",
      "title": "Codec-ASR: Training Performant Automatic Speech Recognition Systems with Discrete Speech Representations",
      "summary": "Discrete speech representations have garnered recent attention for their efficacy in training transformer-based models for various speech-related tasks such as automatic speech recognition (ASR), translation, speaker verification, and joint speech-text foundational models. In this work, we present a comprehensive analysis on building ASR systems with discrete codes. We investigate different methods for codec training such as quantization schemes and time-domain vs spectral feature encodings. We further explore ASR training techniques aimed at enhancing performance, training efficiency, and noise robustness. Drawing upon our findings, we introduce a codec ASR pipeline that outperforms Encodec at similar bit-rate. Remarkably, it also surpasses the state-of-the-art results achieved by strong self-supervised models on the 143 languages ML-SUPERB benchmark despite being smaller in size and pretrained on significantly less data.",
      "authors": [
        "Kunal Dhawan",
        "Nithin Rao Koluguri",
        "Ante Jukić",
        "Ryan Langman",
        "Jagadeesh Balam",
        "Boris Ginsburg"
      ],
      "published": "2024-07-03T20:51:41Z",
      "updated": "2024-07-03T20:51:41Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03495v1",
      "landing_url": "https://arxiv.org/abs/2407.03495v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-330"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Focuses on discrete speech representations via codec training and quantization to build performant ASR, clearly targeting discrete audio tokens and evaluating them, so it satisfies all inclusion criteria without hitting any exclusion note."
    },
    "round-A_JuniorNano_reasoning": "Focuses on discrete speech representations via codec training and quantization to build performant ASR, clearly targeting discrete audio tokens and evaluating them, so it satisfies all inclusion criteria without hitting any exclusion note.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
    "abstract": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
    "metadata": {
      "arxiv_id": "2407.03892",
      "title": "On the Effectiveness of Acoustic BPE in Decoder-Only TTS",
      "summary": "Discretizing speech into tokens and generating them by a decoder-only model have been a promising direction for text-to-speech (TTS) and spoken language modeling (SLM). To shorten the sequence length of speech tokens, acoustic byte-pair encoding (BPE) has emerged in SLM that treats speech tokens from self-supervised semantic representations as characters to further compress the token sequence. But the gain in TTS has not been fully investigated, and the proper choice of acoustic BPE remains unclear. In this work, we conduct a comprehensive study on various settings of acoustic BPE to explore its effectiveness in decoder-only TTS models with semantic speech tokens. Experiments on LibriTTS verify that acoustic BPE uniformly increases the intelligibility and diversity of synthesized speech, while showing different features across BPE settings. Hence, acoustic BPE is a favorable tool for decoder-only TTS.",
      "authors": [
        "Bohan Li",
        "Feiyu Shen",
        "Yiwei Guo",
        "Shuai Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-07-04T12:35:32Z",
      "updated": "2024-07-04T12:35:32Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.03892v1",
      "landing_url": "https://arxiv.org/abs/2407.03892v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.03892"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study investigates discrete semantic speech tokens processed via acoustic BPE and evaluates impacts on decoder-only TTS modeling, aligning fully with the discrete audio token inclusion criteria so it should be included."
    },
    "round-A_JuniorNano_reasoning": "Study investigates discrete semantic speech tokens processed via acoustic BPE and evaluates impacts on decoder-only TTS modeling, aligning fully with the discrete audio token inclusion criteria so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
    "abstract": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
    "metadata": {
      "arxiv_id": "2407.04291",
      "title": "Rethinking Speaker Embeddings for Speech Generation: Sub-Center Modeling for Capturing Intra-Speaker Diversity",
      "summary": "Modeling the rich prosodic variations inherent in human speech is essential for generating natural-sounding speech. While speaker embeddings are commonly used as conditioning inputs in personalized speech generation, they are typically optimized for speaker recognition, which encourages the loss of intra-speaker variation. This strategy makes them suboptimal for speech generation in terms of modeling the rich variations at the output speech distribution. In this work, we propose a novel speaker embedding network that employs multiple sub-centers per speaker class during training, instead of a single center as in conventional approaches. This sub-center modeling allows the embedding to capture a broader range of speaker-specific variations while maintaining speaker classification performance. We demonstrate the effectiveness of the proposed embeddings on a voice conversion task, showing improved naturalness and prosodic expressiveness in the synthesized speech.",
      "authors": [
        "Ismail Rasim Ulgen",
        "John H. L. Hansen",
        "Carlos Busso",
        "Berrak Sisman"
      ],
      "published": "2024-07-05T06:54:24Z",
      "updated": "2025-09-18T20:22:33Z",
      "categories": [
        "eess.AS",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.04291v3",
      "landing_url": "https://arxiv.org/abs/2407.04291v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.04291"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on speaker embedding sub-center modeling for prosody in speech generation without describing discrete token/quantization or tokenizer artifacts, so it fails the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on speaker embedding sub-center modeling for prosody in speech generation without describing discrete token/quantization or tokenizer artifacts, so it fails the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
    "abstract": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
    "metadata": {
      "arxiv_id": "2407.05361",
      "title": "Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation",
      "summary": "Recent advancements in speech generation models have been significantly driven by the use of large-scale training data. However, producing highly spontaneous, human-like speech remains a challenge due to the scarcity of large, diverse, and spontaneous speech datasets. In response, we introduce Emilia, the first large-scale, multilingual, and diverse speech generation dataset. Emilia starts with over 101k hours of speech across six languages, covering a wide range of speaking styles to enable more natural and spontaneous speech generation. To facilitate the scale-up of Emilia, we also present Emilia-Pipe, the first open-source preprocessing pipeline designed to efficiently transform raw, in-the-wild speech data into high-quality training data with speech annotations. Experimental results demonstrate the effectiveness of both Emilia and Emilia-Pipe. Demos are available at: https://emilia-dataset.github.io/Emilia-Demo-Page/.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-07-07T13:24:54Z",
      "updated": "2024-09-07T15:08:24Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05361v3",
      "landing_url": "https://arxiv.org/abs/2407.05361v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.05361"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Dataset paper describes large multilingual speech data and preprocessing pipeline for training, but it does not discuss discrete audio tokenization, vocabularies, or quantization so it fails to meet inclusion scope, therefore exclude."
    },
    "round-A_JuniorNano_reasoning": "Dataset paper describes large multilingual speech data and preprocessing pipeline for training, but it does not discuss discrete audio tokenization, vocabularies, or quantization so it fails to meet inclusion scope, therefore exclude.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
    "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
    "metadata": {
      "arxiv_id": "2407.05407",
      "title": "CosyVoice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens",
      "summary": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a scalable zero-shot TTS synthesizer, CosyVoice, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.",
      "authors": [
        "Zhihao Du",
        "Qian Chen",
        "Shiliang Zhang",
        "Kai Hu",
        "Heng Lu",
        "Yexin Yang",
        "Hangrui Hu",
        "Siqi Zheng",
        "Yue Gu",
        "Ziyang Ma",
        "Zhifu Gao",
        "Zhijie Yan"
      ],
      "published": "2024-07-07T15:16:19Z",
      "updated": "2024-07-09T07:42:51Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05407v2",
      "landing_url": "https://arxiv.org/abs/2407.05407v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.05407"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper introduces supervised semantic discrete tokens for TTS, detailing their learning, token-vocoder pipeline, and evaluations that clearly treat tokens as the primary focus, so it meets all inclusion criteria and no exclusions, hence a score of 5."
    },
    "round-A_JuniorNano_reasoning": "The paper introduces supervised semantic discrete tokens for TTS, detailing their learning, token-vocoder pipeline, and evaluations that clearly treat tokens as the primary focus, so it meets all inclusion criteria and no exclusions, hence a score of 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "SVT-AV1 Encoding Bitrate Estimation Using Motion Search Information",
    "abstract": "Enabling high compression efficiency while keeping encoding energy consumption at a low level, requires prioritization of which videos need more sophisticated encoding techniques. However, the effects vary highly based on the content, and information on how good a video can be compressed is required. This can be measured by estimating the encoded bitstream size prior to encoding. We identified the errors between estimated motion vectors from Motion Search, an algorithm that predicts temporal changes in videos, correlates well to the encoded bitstream size. Combining Motion Search with Random Forests, the encoding bitrate can be estimated with a Pearson correlation of above 0.96.",
    "metadata": {
      "arxiv_id": "2407.05900",
      "title": "SVT-AV1 Encoding Bitrate Estimation Using Motion Search Information",
      "summary": "Enabling high compression efficiency while keeping encoding energy consumption at a low level, requires prioritization of which videos need more sophisticated encoding techniques. However, the effects vary highly based on the content, and information on how good a video can be compressed is required. This can be measured by estimating the encoded bitstream size prior to encoding. We identified the errors between estimated motion vectors from Motion Search, an algorithm that predicts temporal changes in videos, correlates well to the encoded bitstream size. Combining Motion Search with Random Forests, the encoding bitrate can be estimated with a Pearson correlation of above 0.96.",
      "authors": [
        "Lena Eichermüller",
        "Gaurang Chaudhari",
        "Ioannis Katsavounidis",
        "Zhijun Lei",
        "Hassene Tmar",
        "Christian Herglotz",
        "André Kaup"
      ],
      "published": "2024-07-08T13:06:33Z",
      "updated": "2024-07-08T13:06:33Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.05900v1",
      "landing_url": "https://arxiv.org/abs/2407.05900v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.05900"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study concerns video encoding bitrate estimation (motion search, random forests) rather than discrete audio token generation/quantization, so it fails the topic inclusion criteria and is excluded."
    },
    "round-A_JuniorNano_reasoning": "The study concerns video encoding bitrate estimation (motion search, random forests) rather than discrete audio token generation/quantization, so it fails the topic inclusion criteria and is excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian",
    "abstract": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models, leaving a gap for less-resourced languages such as Italian. This paper introduces Igea, the first decoder-only language model designed explicitly for biomedical text generation in Italian. Built on the Minerva model and continually pretrained on a diverse corpus of Italian medical texts, Igea is available in three model sizes: 350 million, 1 billion, and 3 billion parameters. The models aim to balance computational efficiency and performance, addressing the challenges of managing the peculiarities of medical terminology in Italian. We evaluate Igea using a mix of in-domain biomedical corpora and general-purpose benchmarks, highlighting its efficacy and retention of general knowledge even after the domain-specific training. This paper discusses the model's development and evaluation, providing a foundation for future advancements in Italian biomedical NLP.",
    "metadata": {
      "arxiv_id": "2407.06011",
      "title": "Igea: a Decoder-Only Language Model for Biomedical Text Generation in Italian",
      "summary": "The development of domain-specific language models has significantly advanced natural language processing applications in various specialized fields, particularly in biomedicine. However, the focus has largely been on English-language models, leaving a gap for less-resourced languages such as Italian. This paper introduces Igea, the first decoder-only language model designed explicitly for biomedical text generation in Italian. Built on the Minerva model and continually pretrained on a diverse corpus of Italian medical texts, Igea is available in three model sizes: 350 million, 1 billion, and 3 billion parameters. The models aim to balance computational efficiency and performance, addressing the challenges of managing the peculiarities of medical terminology in Italian. We evaluate Igea using a mix of in-domain biomedical corpora and general-purpose benchmarks, highlighting its efficacy and retention of general knowledge even after the domain-specific training. This paper discusses the model's development and evaluation, providing a foundation for future advancements in Italian biomedical NLP.",
      "authors": [
        "Tommaso Mario Buonocore",
        "Simone Rancati",
        "Enea Parimbelli"
      ],
      "published": "2024-07-08T15:04:21Z",
      "updated": "2024-07-08T15:04:21Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.06011v1",
      "landing_url": "https://arxiv.org/abs/2407.06011v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.06011"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes Italian biomedical text generation with decoder-only language models, which has no relation to discrete audio tokens/tokenizers, so it clearly fails to meet the inclusion criteria and hits exclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper describes Italian biomedical text generation with decoder-only language models, which has no relation to discrete audio tokens/tokenizers, so it clearly fails to meet the inclusion criteria and hits exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Speech Synthesis without Vector Quantization",
    "abstract": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
    "metadata": {
      "arxiv_id": "2407.08551",
      "title": "Autoregressive Speech Synthesis without Vector Quantization",
      "summary": "We present MELLE, a novel continuous-valued token based language modeling approach for text-to-speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which is typically designed for audio compression and sacrifices fidelity compared to continuous representations. Specifically, (i) instead of cross-entropy loss, we apply regression loss with a proposed spectrogram flux loss function to model the probability distribution of the continuous-valued tokens; (ii) we have incorporated variational inference into MELLE to facilitate sampling mechanisms, thereby enhancing the output diversity and model robustness. Experiments demonstrate that, compared to the two-stage codec language model VALL-E and its variants, the single-stage MELLE mitigates robustness issues by avoiding the inherent flaws of sampling vector-quantized codes, achieves superior performance across multiple metrics, and, most importantly, offers a more streamlined paradigm. The demos of our work are provided at https://aka.ms/melle.",
      "authors": [
        "Lingwei Meng",
        "Long Zhou",
        "Shujie Liu",
        "Sanyuan Chen",
        "Bing Han",
        "Shujie Hu",
        "Yanqing Liu",
        "Jinyu Li",
        "Sheng Zhao",
        "Xixin Wu",
        "Helen Meng",
        "Furu Wei"
      ],
      "published": "2024-07-11T14:36:53Z",
      "updated": "2025-05-27T05:07:56Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.08551v2",
      "landing_url": "https://arxiv.org/abs/2407.08551v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.08551"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper centers on autoregressive generation of continuous mel-spectrogram frames without any discrete token/quantization pipeline, so it fails the criterion of treating discrete audio tokens as the primary research object."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on autoregressive generation of continuous mel-spectrogram frames without any discrete token/quantization pipeline, so it fails the criterion of treating discrete audio tokens as the primary research object.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model",
    "abstract": "Recently learned image compression (LIC) has achieved great progress and even outperformed the traditional approach using DCT or discrete wavelet transform (DWT). However, LIC mainly reduces spatial redundancy in the autoencoder networks and entropy coding, but has not fully removed the frequency-domain correlation explicitly as in DCT or DWT. To leverage the best of both worlds, we propose a surprisingly simple but efficient framework, which introduces the DWT to both the convolution layers and entropy coding of CNN-based LIC. First, in both the core and hyperprior autoencoder networks, we propose a Wavelet-domain Convolution (WeConv) module, which performs convolution after DWT, and then converts the data back to spatial domain via inverse DWT. This module is used at selected layers in a CNN network to reduce the frequency-domain correlation explicitly and make the signal sparser in DWT domain. We also propose a wavelet-domain Channel-wise Auto-Regressive entropy Model (WeChARM), where the output latent representations from the encoder network are first transformed by the DWT, before applying quantization and entropy coding, as in the traditional paradigm. Moreover, the entropy coding is split into two steps. We first code all low-frequency DWT coefficients, and then use them as prior to code high-frequency coefficients. The channel-wise entropy coding is further used in each step. By combining WeConv and WeChARM, the proposed WeConvene scheme achieves superior R-D performance compared to other state-of-the-art LIC methods as well as the latest H.266/VVC. For the Kodak dataset and the baseline network with -0.4% BD-Rate saving over H.266/VVC, introducing WeConv with the simplest Haar transform improves the saving to -4.7%. This is quite impressive given the simplicity of the Haar transform. Enabling Haar-based WeChARM entropy coding further boosts the saving to -8.2%.",
    "metadata": {
      "arxiv_id": "2407.09983",
      "title": "WeConvene: Learned Image Compression with Wavelet-Domain Convolution and Entropy Model",
      "summary": "Recently learned image compression (LIC) has achieved great progress and even outperformed the traditional approach using DCT or discrete wavelet transform (DWT). However, LIC mainly reduces spatial redundancy in the autoencoder networks and entropy coding, but has not fully removed the frequency-domain correlation explicitly as in DCT or DWT. To leverage the best of both worlds, we propose a surprisingly simple but efficient framework, which introduces the DWT to both the convolution layers and entropy coding of CNN-based LIC. First, in both the core and hyperprior autoencoder networks, we propose a Wavelet-domain Convolution (WeConv) module, which performs convolution after DWT, and then converts the data back to spatial domain via inverse DWT. This module is used at selected layers in a CNN network to reduce the frequency-domain correlation explicitly and make the signal sparser in DWT domain. We also propose a wavelet-domain Channel-wise Auto-Regressive entropy Model (WeChARM), where the output latent representations from the encoder network are first transformed by the DWT, before applying quantization and entropy coding, as in the traditional paradigm. Moreover, the entropy coding is split into two steps. We first code all low-frequency DWT coefficients, and then use them as prior to code high-frequency coefficients. The channel-wise entropy coding is further used in each step. By combining WeConv and WeChARM, the proposed WeConvene scheme achieves superior R-D performance compared to other state-of-the-art LIC methods as well as the latest H.266/VVC. For the Kodak dataset and the baseline network with -0.4% BD-Rate saving over H.266/VVC, introducing WeConv with the simplest Haar transform improves the saving to -4.7%. This is quite impressive given the simplicity of the Haar transform. Enabling Haar-based WeChARM entropy coding further boosts the saving to -8.2%.",
      "authors": [
        "Haisheng Fu",
        "Jie Liang",
        "Zhenman Fang",
        "Jingning Han",
        "Feng Liang",
        "Guohe Zhang"
      ],
      "published": "2024-07-13T19:13:38Z",
      "updated": "2024-07-13T19:13:38Z",
      "categories": [
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.09983v1",
      "landing_url": "https://arxiv.org/abs/2407.09983v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.09983"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on learned image compression with wavelet-domain convolutions and entropy modeling rather than any discrete audio tokenization, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on learned image compression with wavelet-domain convolutions and entropy modeling rather than any discrete audio tokenization, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Understanding the Dependence of Perception Model Competency on Regions in an Image",
    "abstract": "While deep neural network (DNN)-based perception models are useful for many applications, these models are black boxes and their outputs are not yet well understood. To confidently enable a real-world, decision-making system to utilize such a perception model without human intervention, we must enable the system to reason about the perception model's level of competency and respond appropriately when the model is incompetent. In order for the system to make an intelligent decision about the appropriate action when the model is incompetent, it would be useful for the system to understand why the model is incompetent. We explore five novel methods for identifying regions in the input image contributing to low model competency, which we refer to as image cropping, segment masking, pixel perturbation, competency gradients, and reconstruction loss. We assess the ability of these five methods to identify unfamiliar objects, recognize regions associated with unseen classes, and identify unexplored areas in an environment. We find that the competency gradients and reconstruction loss methods show great promise in identifying regions associated with low model competency, particularly when aspects of the image that are unfamiliar to the perception model are causing this reduction in competency. Both of these methods boast low computation times and high levels of accuracy in detecting image regions that are unfamiliar to the model, allowing them to provide potential utility in decision-making pipelines. The code for reproducing our methods and results is available on GitHub: https://github.com/sarapohland/explainable-competency.",
    "metadata": {
      "arxiv_id": "2407.10543",
      "title": "Understanding the Dependence of Perception Model Competency on Regions in an Image",
      "summary": "While deep neural network (DNN)-based perception models are useful for many applications, these models are black boxes and their outputs are not yet well understood. To confidently enable a real-world, decision-making system to utilize such a perception model without human intervention, we must enable the system to reason about the perception model's level of competency and respond appropriately when the model is incompetent. In order for the system to make an intelligent decision about the appropriate action when the model is incompetent, it would be useful for the system to understand why the model is incompetent. We explore five novel methods for identifying regions in the input image contributing to low model competency, which we refer to as image cropping, segment masking, pixel perturbation, competency gradients, and reconstruction loss. We assess the ability of these five methods to identify unfamiliar objects, recognize regions associated with unseen classes, and identify unexplored areas in an environment. We find that the competency gradients and reconstruction loss methods show great promise in identifying regions associated with low model competency, particularly when aspects of the image that are unfamiliar to the perception model are causing this reduction in competency. Both of these methods boast low computation times and high levels of accuracy in detecting image regions that are unfamiliar to the model, allowing them to provide potential utility in decision-making pipelines. The code for reproducing our methods and results is available on GitHub: https://github.com/sarapohland/explainable-competency.",
      "authors": [
        "Sara Pohland",
        "Claire Tomlin"
      ],
      "published": "2024-07-15T08:50:13Z",
      "updated": "2024-07-15T08:50:13Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10543v1",
      "landing_url": "https://arxiv.org/abs/2407.10543v1",
      "doi": "https://doi.org/10.1007/978-3-031-63797-1_8"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about identifying low-competency regions for visual perception models and has no focus on discrete audio tokens, so it fails all inclusion criteria and meets exclusion."
    },
    "round-A_JuniorNano_reasoning": "Paper is about identifying low-competency regions for visual perception models and has no focus on discrete audio tokens, so it fails all inclusion criteria and meets exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Representing Rule-based Chatbots with Transformers",
    "abstract": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
    "metadata": {
      "arxiv_id": "2407.10949",
      "title": "Representing Rule-based Chatbots with Transformers",
      "summary": "What kind of internal mechanisms might Transformers use to conduct fluid, natural-sounding conversations? Prior work has illustrated by construction how Transformers can solve various synthetic tasks, such as sorting a list or recognizing formal languages, but it remains unclear how to extend this approach to a conversational setting. In this work, we propose using ELIZA, a classic rule-based chatbot, as a setting for formal, mechanistic analysis of Transformer-based chatbots. ELIZA allows us to formally model key aspects of conversation, including local pattern matching and long-term dialogue state tracking. We first present a theoretical construction of a Transformer that implements the ELIZA chatbot. Building on prior constructions, particularly those for simulating finite-state automata, we show how simpler mechanisms can be composed and extended to produce more sophisticated behavior. Next, we conduct a set of empirical analyses of Transformers trained on synthetically generated ELIZA conversations. Our analysis illustrates the kinds of mechanisms these models tend to prefer--for example, models favor an induction head mechanism over a more precise, position-based copying mechanism; and using intermediate generations to simulate recurrent data structures, akin to an implicit scratchpad or Chain-of-Thought. Overall, by drawing an explicit connection between neural chatbots and interpretable, symbolic mechanisms, our results provide a new framework for the mechanistic analysis of conversational agents.",
      "authors": [
        "Dan Friedman",
        "Abhishek Panigrahi",
        "Danqi Chen"
      ],
      "published": "2024-07-15T17:45:53Z",
      "updated": "2025-02-12T15:18:32Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.10949v2",
      "landing_url": "https://arxiv.org/abs/2407.10949v2",
      "doi": "https://doi.org/10.48550/arXiv.2407.10949"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study analyzes Transformers simulating ELIZA conversationally and does not introduce or evaluate any discrete audio token/codecs, so it fails the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The study analyzes Transformers simulating ELIZA conversationally and does not introduce or evaluate any discrete audio token/codecs, so it fails the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
    "abstract": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
    "metadata": {
      "arxiv_id": "2407.11370",
      "title": "A Pilot Study of GSLM-based Simulation of Foreign Accentuation Only Using Native Speech Corpora",
      "summary": "We propose a method of simulating the human process of foreign accentuation using Generative Spoken Language Model (GSLM) only with native speech corpora. When one listens to spoken words of a foreign language and repeats them, the repeated speech is often with the accent of that listener's L1. This is said to be because the spoken words are mentally represented as a sequence of phonological units of the L1, and those units are used for oral reproduction. We simulate this process by inputting speech of language A into GSLM of language B to add B's accent onto the input speech. The process of running ASR of the L1 for foreign input speech and giving the ASR result to TTS of the L1 can be viewed as a naive implementation of this approach. The results of our experiments show that the synthesized accent of the output speech is highly natural, compared to real samples of A generated by speakers whose L1 is B, and that the degree of accentuation is controllable.",
      "authors": [
        "Kentaro Onda",
        "Joonyong Park",
        "Nobuaki Minematsu",
        "Daisuke Saito"
      ],
      "published": "2024-07-16T04:29:00Z",
      "updated": "2024-07-16T04:29:00Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.11370v1",
      "landing_url": "https://arxiv.org/abs/2407.11370v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.11370"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The abstract describes accent simulation via a GSLM but provides no discrete tokenization/token details or evaluation of codec/token choices, so it fails to satisfy the discrete audio token inclusion criteria and thus should better be excluded."
    },
    "round-A_JuniorNano_reasoning": "The abstract describes accent simulation via a GSLM but provides no discrete tokenization/token details or evaluation of codec/token choices, so it fails to satisfy the discrete audio token inclusion criteria and thus should better be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "TTSDS -- Text-to-Speech Distribution Score",
    "abstract": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
    "metadata": {
      "arxiv_id": "2407.12707",
      "title": "TTSDS -- Text-to-Speech Distribution Score",
      "summary": "Many recently published Text-to-Speech (TTS) systems produce audio close to real speech. However, TTS evaluation needs to be revisited to make sense of the results obtained with the new architectures, approaches and datasets. We propose evaluating the quality of synthetic speech as a combination of multiple factors such as prosody, speaker identity, and intelligibility. Our approach assesses how well synthetic speech mirrors real speech by obtaining correlates of each factor and measuring their distance from both real speech datasets and noise datasets. We benchmark 35 TTS systems developed between 2008 and 2024 and show that our score computed as an unweighted average of factors strongly correlates with the human evaluations from each time period.",
      "authors": [
        "Christoph Minixhofer",
        "Ondřej Klejch",
        "Peter Bell"
      ],
      "published": "2024-07-17T16:30:27Z",
      "updated": "2024-12-02T03:45:42Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.12707v3",
      "landing_url": "https://arxiv.org/abs/2407.12707v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.12707"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on evaluating TTS quality via factors but does not discuss discrete audio tokens or tokenizers/quantization schemes, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on evaluating TTS quality via factors but does not discuss discrete audio tokens or tokenizers/quantization schemes, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by Direct Preference Optimization",
    "abstract": "Extractive question answering over clinical text is a crucial need to help deal with the deluge of clinical text generated in hospitals. While encoder models (e.g., BERT) have been popular for this reading comprehension task, recently encoder-decoder models (e.g., T5) are on the rise. There is also the emergence of preference optimization techniques to align decoder-only LLMs with human preferences. In this paper, we combine encoder-decoder models with the direct preference optimization (DPO) method to improve over prior state of the art for the RadQA radiology question answering task by 12-15 F1 points. To the best of our knowledge, this effort is the first to show that DPO method also works for reading comprehension via novel heuristics to generate preference data without human inputs.",
    "metadata": {
      "arxiv_id": "2407.14000",
      "title": "Clinical Reading Comprehension with Encoder-Decoder Models Enhanced by Direct Preference Optimization",
      "summary": "Extractive question answering over clinical text is a crucial need to help deal with the deluge of clinical text generated in hospitals. While encoder models (e.g., BERT) have been popular for this reading comprehension task, recently encoder-decoder models (e.g., T5) are on the rise. There is also the emergence of preference optimization techniques to align decoder-only LLMs with human preferences. In this paper, we combine encoder-decoder models with the direct preference optimization (DPO) method to improve over prior state of the art for the RadQA radiology question answering task by 12-15 F1 points. To the best of our knowledge, this effort is the first to show that DPO method also works for reading comprehension via novel heuristics to generate preference data without human inputs.",
      "authors": [
        "Md Sultan Al Nahian",
        "Ramakanth Kavuluru"
      ],
      "published": "2024-07-19T03:12:10Z",
      "updated": "2024-07-19T03:12:10Z",
      "categories": [
        "cs.IR",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.14000v1",
      "landing_url": "https://arxiv.org/abs/2407.14000v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.14000"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on clinical reading comprehension and DPO for question answering, not on discrete audio token generation/quantization or evaluations, so it fails all inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on clinical reading comprehension and DPO for question answering, not on discrete audio token generation/quantization or evaluations, so it fails all inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
    "abstract": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
    "metadata": {
      "arxiv_id": "2407.15614",
      "title": "Experimenting with Adaptive Bitrate Algorithms for Virtual Reality Streaming over Wi-Fi",
      "summary": "Interactive Virtual Reality (VR) streaming over Wi-Fi networks encounters significant challenges due to bandwidth fluctuations caused by channel contention and user mobility. Adaptive BitRate (ABR) algorithms dynamically adjust the video encoding bitrate based on the available network capacity, aiming to maximize image quality while mitigating congestion and preserving the user's Quality of Experience (QoE). In this paper, we experiment with ABR algorithms for VR streaming using Air Light VR (ALVR), an open-source VR streaming solution. We extend ALVR with a comprehensive set of metrics that provide a robust characterization of the network's state, enabling more informed bitrate adjustments. To demonstrate the utility of these performance indicators, we develop and test the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR). Results validate the accuracy of the newly implemented network performance metrics and demonstrate NeSt-VR's video bitrate adaptation capabilities.",
      "authors": [
        "Ferran Maura",
        "Miguel Casasnovas",
        "Boris Bellalta"
      ],
      "published": "2024-07-22T13:20:47Z",
      "updated": "2024-09-30T10:37:16Z",
      "categories": [
        "cs.NI",
        "eess.SY"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15614v3",
      "landing_url": "https://arxiv.org/abs/2407.15614v3",
      "doi": "https://doi.org/10.48550/arXiv.2407.15614"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on adaptive bitrate VR streaming over Wi-Fi and does not address discrete audio token generation/quantization or vocabulary-based audio representations, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on adaptive bitrate VR streaming over Wi-Fi and does not address discrete audio token generation/quantization or vocabulary-based audio representations, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
    "abstract": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
    "metadata": {
      "arxiv_id": "2407.15641",
      "title": "Generating Sample-Based Musical Instruments Using Neural Audio Codec Language Models",
      "summary": "In this paper, we propose and investigate the use of neural audio codec language models for the automatic generation of sample-based musical instruments based on text or reference audio prompts. Our approach extends a generative audio framework to condition on pitch across an 88-key spectrum, velocity, and a combined text/audio embedding. We identify maintaining timbral consistency within the generated instruments as a major challenge. To tackle this issue, we introduce three distinct conditioning schemes. We analyze our methods through objective metrics and human listening tests, demonstrating that our approach can produce compelling musical instruments. Specifically, we introduce a new objective metric to evaluate the timbral consistency of the generated instruments and adapt the average Contrastive Language-Audio Pretraining (CLAP) score for the text-to-instrument case, noting that its naive application is unsuitable for assessing this task. Our findings reveal a complex interplay between timbral consistency, the quality of generated samples, and their correspondence to the input prompt.",
      "authors": [
        "Shahan Nercessian",
        "Johannes Imort",
        "Ninon Devis",
        "Frederik Blang"
      ],
      "published": "2024-07-22T13:59:58Z",
      "updated": "2024-07-22T13:59:58Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15641v1",
      "landing_url": "https://arxiv.org/abs/2407.15641v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15641"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The work clearly centers on discrete neural audio codec tokens via a codec language model for instrument generation with evaluations on timbral consistency and CLAP scores, so it satisfies all inclusion criteria without hitting any exclusion issues."
    },
    "round-A_JuniorNano_reasoning": "The work clearly centers on discrete neural audio codec tokens via a codec language model for instrument generation with evaluations on timbral consistency and CLAP scores, so it satisfies all inclusion criteria without hitting any exclusion issues.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
    "abstract": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
    "metadata": {
      "arxiv_id": "2407.15828",
      "title": "J-CHAT: Japanese Large-scale Spoken Dialogue Corpus for Spoken Dialogue Language Modeling",
      "summary": "Spoken dialogue plays a crucial role in human-AI interactions, necessitating dialogue-oriented spoken language models (SLMs). To develop versatile SLMs, large-scale and diverse speech datasets are essential. Additionally, to ensure hiqh-quality speech generation, the data must be spontaneous like in-wild data and must be acoustically clean with noise removed. Despite the critical need, no open-source corpus meeting all these criteria has been available. This study addresses this gap by constructing and releasing a large-scale spoken dialogue corpus, named Japanese Corpus for Human-AI Talks (J-CHAT), which is publicly accessible. Furthermore, this paper presents a language-independent method for corpus construction and describes experiments on dialogue generation using SLMs trained on J-CHAT. Experimental results indicate that the collected data from multiple domains by our method improve the naturalness and meaningfulness of dialogue generation.",
      "authors": [
        "Wataru Nakata",
        "Kentaro Seki",
        "Hitomi Yanaka",
        "Yuki Saito",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-07-22T17:46:50Z",
      "updated": "2024-07-22T17:46:50Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.15828v1",
      "landing_url": "https://arxiv.org/abs/2407.15828v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.15828"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper introduces a spoken dialogue corpus without proposing or evaluating discrete audio tokenization/quantization schemes or token-level modeling, so it does not meet the inclusion focus and should therefore be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper introduces a spoken dialogue corpus without proposing or evaluating discrete audio tokenization/quantization schemes or token-level modeling, so it does not meet the inclusion focus and should therefore be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AutoLegend: A User Feedback-Driven Adaptive Legend Generator for Visualizations",
    "abstract": "We propose AutoLegend to generate interactive visualization legends using online learning with user feedback. AutoLegend accurately extracts symbols and channels from visualizations and then generates quality legends. AutoLegend enables a two-way interaction between legends and interactions, including highlighting, filtering, data retrieval, and retargeting. After analyzing visualization legends from IEEE VIS papers over the past 20 years, we summarized the design space and evaluation metrics for legend design in visualizations, particularly charts. The generation process consists of three interrelated components: a legend search agent, a feedback model, and an adversarial loss model. The search agent determines suitable legend solutions by exploring the design space and receives guidance from the feedback model through scalar scores. The feedback model is continuously updated by the adversarial loss model based on user input. The user study revealed that AutoLegend can learn users' preferences through legend editing.",
    "metadata": {
      "arxiv_id": "2407.16331",
      "title": "AutoLegend: A User Feedback-Driven Adaptive Legend Generator for Visualizations",
      "summary": "We propose AutoLegend to generate interactive visualization legends using online learning with user feedback. AutoLegend accurately extracts symbols and channels from visualizations and then generates quality legends. AutoLegend enables a two-way interaction between legends and interactions, including highlighting, filtering, data retrieval, and retargeting. After analyzing visualization legends from IEEE VIS papers over the past 20 years, we summarized the design space and evaluation metrics for legend design in visualizations, particularly charts. The generation process consists of three interrelated components: a legend search agent, a feedback model, and an adversarial loss model. The search agent determines suitable legend solutions by exploring the design space and receives guidance from the feedback model through scalar scores. The feedback model is continuously updated by the adversarial loss model based on user input. The user study revealed that AutoLegend can learn users' preferences through legend editing.",
      "authors": [
        "Can Liu",
        "Xiyao Mei",
        "Zhibang Jiang",
        "Shaocong Tan",
        "Xiaoru Yuan"
      ],
      "published": "2024-07-23T09:29:17Z",
      "updated": "2024-07-23T09:29:17Z",
      "categories": [
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.16331v1",
      "landing_url": "https://arxiv.org/abs/2407.16331v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.16331"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on interactive visualization legend generation rather than discrete audio tokenization or codec semantics, so it fails to meet any inclusion criteria and squarely hits the exclusions."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on interactive visualization legend generation rather than discrete audio tokenization or codec semantics, so it fails to meet any inclusion criteria and squarely hits the exclusions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
    "abstract": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
    "metadata": {
      "arxiv_id": "2407.19051",
      "title": "Towards a Transformer-Based Pre-trained Model for IoT Traffic Classification",
      "summary": "The classification of IoT traffic is important to improve the efficiency and security of IoT-based networks. As the state-of-the-art classification methods are based on Deep Learning, most of the current results require a large amount of data to be trained. Thereby, in real-life situations, where there is a scarce amount of IoT traffic data, the models would not perform so well. Consequently, these models underperform outside their initial training conditions and fail to capture the complex characteristics of network traffic, rendering them inefficient and unreliable in real-world applications. In this paper, we propose IoT Traffic Classification Transformer (ITCT), a novel approach that utilizes the state-of-the-art transformer-based model named TabTransformer. ITCT, which is pre-trained on a large labeled MQTT-based IoT traffic dataset and may be fine-tuned with a small set of labeled data, showed promising results in various traffic classification tasks. Our experiments demonstrated that the ITCT model significantly outperforms existing models, achieving an overall accuracy of 82%. To support reproducibility and collaborative development, all associated code has been made publicly available.",
      "authors": [
        "Bruna Bazaluk",
        "Mosab Hamdan",
        "Mustafa Ghaleb",
        "Mohammed S. M. Gismalla",
        "Flavio S. Correa da Silva",
        "Daniel Macêdo Batista"
      ],
      "published": "2024-07-26T19:13:11Z",
      "updated": "2024-07-26T19:13:11Z",
      "categories": [
        "cs.NI",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2407.19051v1",
      "landing_url": "https://arxiv.org/abs/2407.19051v1",
      "doi": "https://doi.org/10.48550/arXiv.2407.19051"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on transformer-based IoT traffic classification using network datasets, with no discussion of discrete audio tokens, tokenizers, codec quantization, or audio-language modeling, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on transformer-based IoT traffic classification using network datasets, with no discussion of discrete audio tokens, tokenizers, codec quantization, or audio-language modeling, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Synthetic dual image generation for reduction of labeling efforts in semantic segmentation of micrographs with a customized metric function",
    "abstract": "Training of semantic segmentation models for material analysis requires micrographs and their corresponding masks. It is quite unlikely that perfect masks will be drawn, especially at the edges of objects, and sometimes the amount of data that can be obtained is small, since only a few samples are available. These aspects make it very problematic to train a robust model. We demonstrate a workflow for the improvement of semantic segmentation models of micrographs through the generation of synthetic microstructural images in conjunction with masks. The workflow only requires joining a few micrographs with their respective masks to create the input for a Vector Quantised-Variational AutoEncoder model that includes an embedding space, which is trained such that a generative model (PixelCNN) learns the distribution of each input, transformed into discrete codes, and can be used to sample new codes. The latter will eventually be decoded by VQ-VAE to generate images alongside corresponding masks for semantic segmentation. To evaluate the synthetic data, we have trained U-Net models with different amounts of these synthetic data in conjunction with real data. These models were then evaluated using non-synthetic images only. Additionally, we introduce a customized metric derived from the mean Intersection over Union (mIoU). The proposed metric prevents a few falsely predicted pixels from greatly reducing the value of the mIoU. We have achieved a reduction in sample preparation and acquisition times, as well as the efforts, needed for image processing and labeling tasks, are less when it comes to training semantic segmentation model. The approach could be generalized to various types of image data such that it serves as a user-friendly solution for training models with a small number of real images.",
    "metadata": {
      "arxiv_id": "2408.00707",
      "title": "Synthetic dual image generation for reduction of labeling efforts in semantic segmentation of micrographs with a customized metric function",
      "summary": "Training of semantic segmentation models for material analysis requires micrographs and their corresponding masks. It is quite unlikely that perfect masks will be drawn, especially at the edges of objects, and sometimes the amount of data that can be obtained is small, since only a few samples are available. These aspects make it very problematic to train a robust model. We demonstrate a workflow for the improvement of semantic segmentation models of micrographs through the generation of synthetic microstructural images in conjunction with masks. The workflow only requires joining a few micrographs with their respective masks to create the input for a Vector Quantised-Variational AutoEncoder model that includes an embedding space, which is trained such that a generative model (PixelCNN) learns the distribution of each input, transformed into discrete codes, and can be used to sample new codes. The latter will eventually be decoded by VQ-VAE to generate images alongside corresponding masks for semantic segmentation. To evaluate the synthetic data, we have trained U-Net models with different amounts of these synthetic data in conjunction with real data. These models were then evaluated using non-synthetic images only. Additionally, we introduce a customized metric derived from the mean Intersection over Union (mIoU). The proposed metric prevents a few falsely predicted pixels from greatly reducing the value of the mIoU. We have achieved a reduction in sample preparation and acquisition times, as well as the efforts, needed for image processing and labeling tasks, are less when it comes to training semantic segmentation model. The approach could be generalized to various types of image data such that it serves as a user-friendly solution for training models with a small number of real images.",
      "authors": [
        "Matias Oscar Volman Stern",
        "Dominic Hohs",
        "Andreas Jansche",
        "Timo Bernthaler",
        "Gerhard Schneider"
      ],
      "published": "2024-08-01T16:54:11Z",
      "updated": "2024-08-01T16:54:11Z",
      "categories": [
        "cs.CV",
        "cs.CE",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.00707v1",
      "landing_url": "https://arxiv.org/abs/2408.00707v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.00707"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about synthetic image generation for micrograph segmentation, not discrete audio tokens nor any audio-based quantized tokenization, so it fails inclusion criteria and meets an exclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "The paper is about synthetic image generation for micrograph segmentation, not discrete audio tokens nor any audio-based quantized tokenization, so it fails inclusion criteria and meets an exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
    "abstract": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
    "metadata": {
      "arxiv_id": "2408.01932",
      "title": "Constructing Per-Shot Bitrate Ladders using Visual Information Fidelity",
      "summary": "Video service providers need their delivery systems to be able to adapt to network conditions, user preferences, display settings, and other factors. HTTP Adaptive Streaming (HAS) offers dynamic switching between different video representations to simultaneously enhance bandwidth consumption and users' streaming experiences. Per-shot encoding, pioneered by Netflix, optimizes the encoding parameters on each scene or shot. The Dynamic Optimizer (DO) uses the Video Multi-Method Assessment Fusion (VMAF) perceptual video quality prediction engine to deliver high-quality videos at reduced bitrates. Here we develop a perceptually optimized method of constructing optimal per-shot bitrate and quality ladders, using an ensemble of low-level features and Visual Information Fidelity (VIF) features. During inference, our method predicts the bitrate or quality ladder of a source video without any compression or quality estimation. We compare the performance of our model against other content-adaptive bitrate ladder prediction methods, a fixed bitrate ladder, and reference bitrate ladders constructed via exhaustive encoding using Bjontegaard-delta (BD) metrics. Our proposed method shows excellent gains in bitrate and quality against the fixed bitrate ladder and only small losses against the reference bitrate ladder, while providing significant computational advantages.",
      "authors": [
        "Krishna Srikar Durbha",
        "Alan C. Bovik"
      ],
      "published": "2024-08-04T05:12:21Z",
      "updated": "2025-11-15T01:13:58Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.01932v2",
      "landing_url": "https://arxiv.org/abs/2408.01932v2",
      "doi": "https://doi.org/10.1109/TIP.2025.3625750"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on video bitrate ladder estimation and makes no mention of discrete audio token generation, quantized vocabularies, or token-oriented evaluations, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on video bitrate ladder estimation and makes no mention of discrete audio token generation, quantized vocabularies, or token-oriented evaluations, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
    "abstract": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
    "metadata": {
      "arxiv_id": "2408.03887",
      "title": "Central Kurdish Text-to-Speech Synthesis with Novel End-to-End Transformer Training",
      "summary": "Recent advancements in text-to-speech (TTS) models have aimed to streamline the two-stage process into a single-stage training approach. However, many single-stage models still lag behind in audio quality, particularly when handling Kurdish text and speech. There is a critical need to enhance text-to-speech conversion for the Kurdish language, particularly for the Sorani dialect, which has been relatively neglected and is underrepresented in recent text-to-speech advancements. This study introduces an end-to-end TTS model for efficiently generating high-quality Kurdish audio. The proposed method leverages a variational autoencoder (VAE) that is pre-trained for audio waveform reconstruction and is augmented by adversarial training. This involves aligning the prior distribution established by the pre-trained encoder with the posterior distribution of the text encoder within latent variables. Additionally, a stochastic duration predictor is incorporated to imbue synthesized Kurdish speech with diverse rhythms. By aligning latent distributions and integrating the stochastic duration predictor, the proposed method facilitates the real-time generation of natural Kurdish speech audio, offering flexibility in pitches and rhythms. Empirical evaluation via the mean opinion score (MOS) on a custom dataset confirms the superior performance of our approach (MOS of 3.94) compared with that of a one-stage system and other two-staged systems as assessed through a subjective human evaluation.",
      "authors": [
        "Hawraz A. Ahmad",
        "Tarik A. Rashid"
      ],
      "published": "2024-08-06T07:04:59Z",
      "updated": "2024-08-06T07:04:59Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.03887v1",
      "landing_url": "https://arxiv.org/abs/2408.03887v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.03887"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on an end-to-end Kurdish TTS model using VAE and duration predictors without describing any discrete-token tokenizer/codebook or vocabulary, so it violates the inclusion requirement and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on an end-to-end Kurdish TTS model using VAE and duration predictors without describing any discrete-token tokenizer/codebook or vocabulary, so it violates the inclusion requirement and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs",
    "abstract": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.",
    "metadata": {
      "arxiv_id": "2408.06954",
      "title": "Neural Speech and Audio Coding: Modern AI Technology Meets Traditional Codecs",
      "summary": "This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.",
      "authors": [
        "Minje Kim",
        "Jan Skoglund"
      ],
      "published": "2024-08-13T15:13:21Z",
      "updated": "2025-01-07T04:11:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.06954v2",
      "landing_url": "https://arxiv.org/abs/2408.06954v2",
      "doi": "https://doi.org/10.1109/MSP.2024.3444318"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper discusses hybrid neural coding and codec enhancement but never describes discrete vocabularies, tokenizers, quantization designs, or token-level evaluation, so it does not meet the discrete audio token focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses hybrid neural coding and codec enhancement but never describes discrete vocabularies, tokenizers, quantization designs, or token-level evaluation, so it does not meet the discrete audio token focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Entropy Coding of Unordered Data Structures",
    "abstract": "We present shuffle coding, a general method for optimal compression of sequences of unordered objects using bits-back coding. Data structures that can be compressed using shuffle coding include multisets, graphs, hypergraphs, and others. We release an implementation that can easily be adapted to different data types and statistical models, and demonstrate that our implementation achieves state-of-the-art compression rates on a range of graph datasets including molecular data.",
    "metadata": {
      "arxiv_id": "2408.08837",
      "title": "Entropy Coding of Unordered Data Structures",
      "summary": "We present shuffle coding, a general method for optimal compression of sequences of unordered objects using bits-back coding. Data structures that can be compressed using shuffle coding include multisets, graphs, hypergraphs, and others. We release an implementation that can easily be adapted to different data types and statistical models, and demonstrate that our implementation achieves state-of-the-art compression rates on a range of graph datasets including molecular data.",
      "authors": [
        "Julius Kunze",
        "Daniel Severo",
        "Giulio Zani",
        "Jan-Willem van de Meent",
        "James Townsend"
      ],
      "published": "2024-08-16T16:41:27Z",
      "updated": "2024-08-16T16:41:27Z",
      "categories": [
        "cs.LG",
        "cs.DS",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.08837v1",
      "landing_url": "https://arxiv.org/abs/2408.08837v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.08837"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe generic compression of unordered data structures via shuffle coding, unrelated to discrete audio tokens or tokenizer/codec design, so it fails inclusion criteria for this audio token survey."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe generic compression of unordered data structures via shuffle coding, unrelated to discrete audio tokens or tokenizer/codec design, so it fails inclusion criteria for this audio token survey.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Composable Representations for Audio",
    "abstract": "Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",
    "metadata": {
      "arxiv_id": "2408.09792",
      "title": "Unsupervised Composable Representations for Audio",
      "summary": "Current generative models are able to generate high-quality artefacts but have been shown to struggle with compositional reasoning, which can be defined as the ability to generate complex structures from simpler elements. In this paper, we focus on the problem of compositional representation learning for music data, specifically targeting the fully-unsupervised setting. We propose a simple and extensible framework that leverages an explicit compositional inductive bias, defined by a flexible auto-encoding objective that can leverage any of the current state-of-art generative models. We demonstrate that our framework, used with diffusion models, naturally addresses the task of unsupervised audio source separation, showing that our model is able to perform high-quality separation. Our findings reveal that our proposal achieves comparable or superior performance with respect to other blind source separation methods and, furthermore, it even surpasses current state-of-art supervised baselines on signal-to-interference ratio metrics. Additionally, by learning an a-posteriori masking diffusion model in the space of composable representations, we achieve a system capable of seamlessly performing unsupervised source separation, unconditional generation, and variation generation. Finally, as our proposal works in the latent space of pre-trained neural audio codecs, it also provides a lower computational cost with respect to other neural baselines.",
      "authors": [
        "Giovanni Bindi",
        "Philippe Esling"
      ],
      "published": "2024-08-19T08:41:09Z",
      "updated": "2024-08-19T08:41:09Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.09792v1",
      "landing_url": "https://arxiv.org/abs/2408.09792v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.09792"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on composable latent representations and diffusion-based unsupervised source separation without describing discrete tokenization or quantized vocabularies, so it fails to meet the discrete-audio-token inclusion requirement."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on composable latent representations and diffusion-based unsupervised source separation without describing discrete tokenization or quantized vocabularies, so it fails to meet the discrete-audio-token inclusion requirement.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
    "abstract": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
    "metadata": {
      "arxiv_id": "2408.15676",
      "title": "VoxInstruct: Expressive Human Instruction-to-Speech Generation with Unified Multilingual Codec Language Modelling",
      "summary": "Recent AIGC systems possess the capability to generate digital multimedia content based on human language instructions, such as text, image and video. However, when it comes to speech, existing methods related to human instruction-to-speech generation exhibit two limitations. Firstly, they require the division of inputs into content prompt (transcript) and description prompt (style and speaker), instead of directly supporting human instruction. This division is less natural in form and does not align with other AIGC models. Secondly, the practice of utilizing an independent description prompt to model speech style, without considering the transcript content, restricts the ability to control speech at a fine-grained level. To address these limitations, we propose VoxInstruct, a novel unified multilingual codec language modeling framework that extends traditional text-to-speech tasks into a general human instruction-to-speech task. Our approach enhances the expressiveness of human instruction-guided speech generation and aligns the speech generation paradigm with other modalities. To enable the model to automatically extract the content of synthesized speech from raw text instructions, we introduce speech semantic tokens as an intermediate representation for instruction-to-content guidance. We also incorporate multiple Classifier-Free Guidance (CFG) strategies into our codec language model, which strengthens the generated speech following human instructions. Furthermore, our model architecture and training strategies allow for the simultaneous support of combining speech prompt and descriptive human instruction for expressive speech synthesis, which is a first-of-its-kind attempt. Codes, models and demos are at: https://github.com/thuhcsi/VoxInstruct.",
      "authors": [
        "Yixuan Zhou",
        "Xiaoyu Qin",
        "Zeyu Jin",
        "Shuoyi Zhou",
        "Shun Lei",
        "Songtao Zhou",
        "Zhiyong Wu",
        "Jia Jia"
      ],
      "published": "2024-08-28T09:57:17Z",
      "updated": "2024-08-28T09:57:17Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.15676v1",
      "landing_url": "https://arxiv.org/abs/2408.15676v1",
      "doi": "https://doi.org/10.1145/3664647.3681680"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Uses codec language modeling with speech semantic tokens to represent instruction-derived content and quantized discrete audio tokens, so it clearly addresses the discrete audio token generation and use case and should be included."
    },
    "round-A_JuniorNano_reasoning": "Uses codec language modeling with speech semantic tokens to represent instruction-derived content and quantized discrete audio tokens, so it clearly addresses the discrete audio token generation and use case and should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
    "abstract": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
    "metadata": {
      "arxiv_id": "2408.17131",
      "title": "VQ4DiT: Efficient Post-Training Vector Quantization for Diffusion Transformers",
      "summary": "The Diffusion Transformers Models (DiTs) have transitioned the network architecture from traditional UNets to transformers, demonstrating exceptional capabilities in image generation. Although DiTs have been widely applied to high-definition video generation tasks, their large parameter size hinders inference on edge devices. Vector quantization (VQ) can decompose model weight into a codebook and assignments, allowing extreme weight quantization and significantly reducing memory usage. In this paper, we propose VQ4DiT, a fast post-training vector quantization method for DiTs. We found that traditional VQ methods calibrate only the codebook without calibrating the assignments. This leads to weight sub-vectors being incorrectly assigned to the same assignment, providing inconsistent gradients to the codebook and resulting in a suboptimal result. To address this challenge, VQ4DiT calculates the candidate assignment set for each weight sub-vector based on Euclidean distance and reconstructs the sub-vector based on the weighted average. Then, using the zero-data and block-wise calibration method, the optimal assignment from the set is efficiently selected while calibrating the codebook. VQ4DiT quantizes a DiT XL/2 model on a single NVIDIA A100 GPU within 20 minutes to 5 hours depending on the different quantization settings. Experiments show that VQ4DiT establishes a new state-of-the-art in model size and performance trade-offs, quantizing weights to 2-bit precision while retaining acceptable image generation quality.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Hong Gu",
        "Kedong Xu",
        "Kejie Huang"
      ],
      "published": "2024-08-30T09:15:54Z",
      "updated": "2024-08-30T09:15:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17131v1",
      "landing_url": "https://arxiv.org/abs/2408.17131v1",
      "doi": "https://doi.org/10.48550/arXiv.2408.17131"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on image diffusion transformers and their weight vector quantization without any discrete audio-token/tokenizer context, so it fails to satisfy the audio-centric inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on image diffusion transformers and their weight vector quantization without any discrete audio-token/tokenizer context, so it fails to satisfy the audio-centric inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
    "abstract": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
    "metadata": {
      "arxiv_id": "2408.17175",
      "title": "Codec Does Matter: Exploring the Semantic Shortcoming of Codec for Audio Language Model",
      "summary": "Recent advancements in audio generation have been significantly propelled by the capabilities of Large Language Models (LLMs). The existing research on audio LLM has primarily focused on enhancing the architecture and scale of audio language models, as well as leveraging larger datasets, and generally, acoustic codecs, such as EnCodec, are used for audio tokenization. However, these codecs were originally designed for audio compression, which may lead to suboptimal performance in the context of audio LLM. Our research aims to address the shortcomings of current audio LLM codecs, particularly their challenges in maintaining semantic integrity in generated audio. For instance, existing methods like VALL-E, which condition acoustic token generation on text transcriptions, often suffer from content inaccuracies and elevated word error rates (WER) due to semantic misinterpretations of acoustic tokens, resulting in word skipping and errors. To overcome these issues, we propose a straightforward yet effective approach called X-Codec. X-Codec incorporates semantic features from a pre-trained semantic encoder before the Residual Vector Quantization (RVQ) stage and introduces a semantic reconstruction loss after RVQ. By enhancing the semantic ability of the codec, X-Codec significantly reduces WER in speech synthesis tasks and extends these benefits to non-speech applications, including music and sound generation. Our experiments in text-to-speech, music continuation, and text-to-sound tasks demonstrate that integrating semantic information substantially improves the overall performance of language models in audio generation. Our code and demo are available (Demo: https://x-codec-audio.github.io Code: https://github.com/zhenye234/xcodec)",
      "authors": [
        "Zhen Ye",
        "Peiwen Sun",
        "Jiahe Lei",
        "Hongzhan Lin",
        "Xu Tan",
        "Zheqi Dai",
        "Qiuqiang Kong",
        "Jianyi Chen",
        "Jiahao Pan",
        "Qifeng Liu",
        "Yike Guo",
        "Wei Xue"
      ],
      "published": "2024-08-30T10:24:07Z",
      "updated": "2024-11-27T11:47:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2408.17175v3",
      "landing_url": "https://arxiv.org/abs/2408.17175v3",
      "doi": "https://doi.org/10.48550/arXiv.2408.17175"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "X-Codec explicitly improves a discrete neural codec (RVQ-based) for audio LLMs, adds semantic-aware quantization/loss, and reports evaluations on tokenized generation tasks, so it clearly targets discrete audio tokens and satisfies all inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "X-Codec explicitly improves a discrete neural codec (RVQ-based) for audio LLMs, adds semantic-aware quantization/loss, and reports evaluations on tokenized generation tasks, so it clearly targets discrete audio tokens and satisfies all inclusion requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
    "abstract": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
    "metadata": {
      "arxiv_id": "2409.00750",
      "title": "MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer",
      "summary": "The recent large-scale text-to-speech (TTS) systems are usually grouped as autoregressive and non-autoregressive systems. The autoregressive systems implicitly model duration but exhibit certain deficiencies in robustness and lack of duration controllability. Non-autoregressive systems require explicit alignment information between text and speech during training and predict durations for linguistic units (e.g. phone), which may compromise their naturalness. In this paper, we introduce Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive TTS model that eliminates the need for explicit alignment information between text and speech supervision, as well as phone-level duration prediction. MaskGCT is a two-stage model: in the first stage, the model uses text to predict semantic tokens extracted from a speech self-supervised learning (SSL) model, and in the second stage, the model predicts acoustic tokens conditioned on these semantic tokens. MaskGCT follows the mask-and-predict learning paradigm. During training, MaskGCT learns to predict masked semantic or acoustic tokens based on given conditions and prompts. During inference, the model generates tokens of a specified length in a parallel manner. Experiments with 100K hours of in-the-wild speech demonstrate that MaskGCT outperforms the current state-of-the-art zero-shot TTS systems in terms of quality, similarity, and intelligibility. Audio samples are available at https://maskgct.github.io/. We release our code and model checkpoints at https://github.com/open-mmlab/Amphion/blob/main/models/tts/maskgct.",
      "authors": [
        "Yuancheng Wang",
        "Haoyue Zhan",
        "Liwei Liu",
        "Ruihong Zeng",
        "Haotian Guo",
        "Jiachen Zheng",
        "Qiang Zhang",
        "Xueyao Zhang",
        "Shunsi Zhang",
        "Zhizheng Wu"
      ],
      "published": "2024-09-01T15:26:30Z",
      "updated": "2024-10-20T14:25:49Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00750v3",
      "landing_url": "https://arxiv.org/abs/2409.00750v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.00750"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "MaskGCT explicitly builds semantic and acoustic discrete tokens via SSL quantization and codec stages to model and evaluate speech tokens, so it satisfies the discrete-audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "MaskGCT explicitly builds semantic and acoustic discrete tokens via SSL quantization and codec stages to model and evaluate speech tokens, so it satisfies the discrete-audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
    "abstract": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
    "metadata": {
      "arxiv_id": "2409.00942",
      "title": "VQ-Flow: Taming Normalizing Flows for Multi-Class Anomaly Detection via Hierarchical Vector Quantization",
      "summary": "Normalizing flows, a category of probabilistic models famed for their capabilities in modeling complex data distributions, have exhibited remarkable efficacy in unsupervised anomaly detection. This paper explores the potential of normalizing flows in multi-class anomaly detection, wherein the normal data is compounded with multiple classes without providing class labels. Through the integration of vector quantization (VQ), we empower the flow models to distinguish different concepts of multi-class normal data in an unsupervised manner, resulting in a novel flow-based unified method, named VQ-Flow. Specifically, our VQ-Flow leverages hierarchical vector quantization to estimate two relative codebooks: a Conceptual Prototype Codebook (CPC) for concept distinction and its concomitant Concept-Specific Pattern Codebook (CSPC) to capture concept-specific normal patterns. The flow models in VQ-Flow are conditioned on the concept-specific patterns captured in CSPC, capable of modeling specific normal patterns associated with different concepts. Moreover, CPC further enables our VQ-Flow for concept-aware distribution modeling, faithfully mimicking the intricate multi-class normal distribution through a mixed Gaussian distribution reparametrized on the conceptual prototypes. Through the introduction of vector quantization, the proposed VQ-Flow advances the state-of-the-art in multi-class anomaly detection within a unified training scheme, yielding the Det./Loc. AUROC of 99.5%/98.3% on MVTec AD. The codebase is publicly available at https://github.com/cool-xuan/vqflow.",
      "authors": [
        "Yixuan Zhou",
        "Xing Xu",
        "Zhe Sun",
        "Jingkuan Song",
        "Andrzej Cichocki",
        "Heng Tao Shen"
      ],
      "published": "2024-09-02T05:01:41Z",
      "updated": "2024-09-02T05:01:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.00942v1",
      "landing_url": "https://arxiv.org/abs/2409.00942v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.00942"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on anomaly detection with vector-quantized flows over visual data rather than on discrete audio token generation/quantization, so it fails the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on anomaly detection with vector-quantized flows over visual data rather than on discrete audio token generation/quantization, so it fails the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A multilingual training strategy for low resource Text to Speech",
    "abstract": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
    "metadata": {
      "arxiv_id": "2409.01217",
      "title": "A multilingual training strategy for low resource Text to Speech",
      "summary": "Recent speech technologies have led to produce high quality synthesised speech due to recent advances in neural Text to Speech (TTS). However, such TTS models depend on extensive amounts of data that can be costly to produce and is hardly scalable to all existing languages, especially that seldom attention is given to low resource languages. With techniques such as knowledge transfer, the burden of creating datasets can be alleviated. In this paper, we therefore investigate two aspects; firstly, whether data from social media can be used for a small TTS dataset construction, and secondly whether cross lingual transfer learning (TL) for a low resource language can work with this type of data. In this aspect, we specifically assess to what extent multilingual modeling can be leveraged as an alternative to training on monolingual corporas. To do so, we explore how data from foreign languages may be selected and pooled to train a TTS model for a target low resource language. Our findings show that multilingual pre-training is better than monolingual pre-training at increasing the intelligibility and naturalness of the generated speech.",
      "authors": [
        "Asma Amalas",
        "Mounir Ghogho",
        "Mohamed Chetouani",
        "Rachid Oulad Haj Thami"
      ],
      "published": "2024-09-02T12:53:01Z",
      "updated": "2024-09-02T12:53:01Z",
      "categories": [
        "cs.CL",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01217v1",
      "landing_url": "https://arxiv.org/abs/2409.01217v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01217"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on multilingual low-resource TTS training with social-media data and cross-lingual transfer, without discussing discrete audio tokenization, quantization/codebook design, or any discrete token-driven modeling, so it fails the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multilingual low-resource TTS training with social-media data and cross-lingual transfer, without discussing discrete audio tokenization, quantization/codebook design, or any discrete token-driven modeling, so it fails the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Map-Assisted Remote-Sensing Image Compression at Extremely Low Bitrates",
    "abstract": "Remote-sensing (RS) image compression at extremely low bitrates has always been a challenging task in practical scenarios like edge device storage and narrow bandwidth transmission. Generative models including VAEs and GANs have been explored to compress RS images into extremely low-bitrate streams. However, these generative models struggle to reconstruct visually plausible images due to the highly ill-posed nature of extremely low-bitrate image compression. To this end, we propose an image compression framework that utilizes a pre-trained diffusion model with powerful natural image priors to achieve high-realism reconstructions. However, diffusion models tend to hallucinate small structures and textures due to the significant information loss at limited bitrates. Thus, we introduce vector maps as semantic and structural guidance and propose a novel image compression approach named Map-Assisted Generative Compression (MAGC). MAGC employs a two-stage pipeline to compress and decompress RS images at extremely low bitrates. The first stage maps an image into a latent representation, which is then further compressed in a VAE architecture to save bitrates and serves as implicit guidance in the subsequent diffusion process. The second stage conducts a conditional diffusion model to generate a visually pleasing and semantically accurate result using implicit guidance and explicit semantic guidance. Quantitative and qualitative comparisons show that our method outperforms standard codecs and other learning-based methods in terms of perceptual quality and semantic accuracy. The dataset and code will be publicly available at https://github.com/WHUyyx/MAGC.",
    "metadata": {
      "arxiv_id": "2409.01935",
      "title": "Map-Assisted Remote-Sensing Image Compression at Extremely Low Bitrates",
      "summary": "Remote-sensing (RS) image compression at extremely low bitrates has always been a challenging task in practical scenarios like edge device storage and narrow bandwidth transmission. Generative models including VAEs and GANs have been explored to compress RS images into extremely low-bitrate streams. However, these generative models struggle to reconstruct visually plausible images due to the highly ill-posed nature of extremely low-bitrate image compression. To this end, we propose an image compression framework that utilizes a pre-trained diffusion model with powerful natural image priors to achieve high-realism reconstructions. However, diffusion models tend to hallucinate small structures and textures due to the significant information loss at limited bitrates. Thus, we introduce vector maps as semantic and structural guidance and propose a novel image compression approach named Map-Assisted Generative Compression (MAGC). MAGC employs a two-stage pipeline to compress and decompress RS images at extremely low bitrates. The first stage maps an image into a latent representation, which is then further compressed in a VAE architecture to save bitrates and serves as implicit guidance in the subsequent diffusion process. The second stage conducts a conditional diffusion model to generate a visually pleasing and semantically accurate result using implicit guidance and explicit semantic guidance. Quantitative and qualitative comparisons show that our method outperforms standard codecs and other learning-based methods in terms of perceptual quality and semantic accuracy. The dataset and code will be publicly available at https://github.com/WHUyyx/MAGC.",
      "authors": [
        "Yixuan Ye",
        "Ce Wang",
        "Wanjie Sun",
        "Zhenzhong Chen"
      ],
      "published": "2024-09-03T14:29:54Z",
      "updated": "2024-09-03T14:29:54Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.01935v1",
      "landing_url": "https://arxiv.org/abs/2409.01935v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.01935"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on remote-sensing image compression using diffusion models and maps, so it does not address discrete audio token generation, quantization or token-level evaluations, hence it fails inclusion criteria and meets the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on remote-sensing image compression using diffusion models and maps, so it does not address discrete audio token generation, quantization or token-level evaluations, hence it fails inclusion criteria and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
    "abstract": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
    "metadata": {
      "arxiv_id": "2409.03393",
      "title": "VQ-DeepVSC: A Dual-Stage Vector Quantization Framework for Video Semantic Communication",
      "summary": "In response to the rapid growth of global videomtraffic and the limitations of traditional wireless transmission systems, we propose a novel dual-stage vector quantization framework, VQ-DeepVSC, tailored to enhance video transmission over wireless channels. In the first stage, we design the adaptive keyframe extractor and interpolator, deployed respectively at the transmitter and receiver, which intelligently select key frames to minimize inter-frame redundancy and mitigate the cliff-effect under challenging channel conditions. In the second stage, we propose the semantic vector quantization encoder and decoder, placed respectively at the transmitter and receiver, which efficiently compress key frames using advanced indexing and spatial normalization modules to reduce redundancy. Additionally, we propose adjustable index selection and recovery modules, enhancing compression efficiency and enabling flexible compression ratio adjustment. Compared to the joint source-channel coding (JSCC) framework, the proposed framework exhibits superior compatibility with current digital communication systems. Experimental results demonstrate that VQ-DeepVSC achieves substantial improvements in both Multi-Scale Structural Similarity (MS-SSIM) and Learned Perceptual Image Patch Similarity (LPIPS) metrics than the H.265 standard, particularly under low channel signal-to-noise ratio (SNR) or multi-path channels, highlighting the significantly enhanced transmission capabilities of our approach.",
      "authors": [
        "Yongyi Miao",
        "Zhongdang Li",
        "Yang Wang",
        "Die Hu",
        "Jun Yan",
        "Youfang Wang"
      ],
      "published": "2024-09-05T09:53:53Z",
      "updated": "2024-09-05T09:53:53Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03393v1",
      "landing_url": "https://arxiv.org/abs/2409.03393v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.03393"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on video semantic compression and has no connection to discrete audio-token generation, quantization, or tokenizer-level evaluation, so it fails to meet the audio-token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on video semantic compression and has no connection to discrete audio-token generation, quantization, or tokenizer-level evaluation, so it fails to meet the audio-token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LAST: Language Model Aware Speech Tokenization",
    "abstract": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
    "metadata": {
      "arxiv_id": "2409.03701",
      "title": "LAST: Language Model Aware Speech Tokenization",
      "summary": "Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.",
      "authors": [
        "Arnon Turetzky",
        "Yossi Adi"
      ],
      "published": "2024-09-05T16:57:39Z",
      "updated": "2024-09-10T14:45:15Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.03701v2",
      "landing_url": "https://arxiv.org/abs/2409.03701v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.03701"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study explicitly learns discrete speech tokens aligned with discrete vocabularies and quantization for audio language modeling (including evaluation vs baselines) and thus satisfies the inclusion criteria and no exclusions."
    },
    "round-A_JuniorNano_reasoning": "Study explicitly learns discrete speech tokens aligned with discrete vocabularies and quantization for audio language modeling (including evaluation vs baselines) and thus satisfies the inclusion criteria and no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
    "abstract": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
    "metadata": {
      "arxiv_id": "2409.04016",
      "title": "Investigating Neural Audio Codecs for Speech Language Model-Based Speech Generation",
      "summary": "Neural audio codec tokens serve as the fundamental building blocks for speech language model (SLM)-based speech generation. However, there is no systematic understanding on how the codec system affects the speech generation performance of the SLM. In this work, we examine codec tokens within SLM framework for speech generation to provide insights for effective codec design. We retrain existing high-performing neural codec models on the same data set and loss functions to compare their performance in a uniform setting. We integrate codec tokens into two SLM systems: masked-based parallel speech generation system and an auto-regressive (AR) plus non-auto-regressive (NAR) model-based system. Our findings indicate that better speech reconstruction in codec systems does not guarantee improved speech generation in SLM. A high-quality codec decoder is crucial for natural speech production in SLM, while speech intelligibility depends more on quantization mechanism.",
      "authors": [
        "Jiaqi Li",
        "Dongmei Wang",
        "Xiaofei Wang",
        "Yao Qian",
        "Long Zhou",
        "Shujie Liu",
        "Midia Yousefi",
        "Canrun Li",
        "Chung-Hsien Tsai",
        "Zhen Xiao",
        "Yanqing Liu",
        "Junkun Chen",
        "Sheng Zhao",
        "Jinyu Li",
        "Zhizheng Wu",
        "Michael Zeng"
      ],
      "published": "2024-09-06T04:06:50Z",
      "updated": "2024-09-06T04:06:50Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.04016v1",
      "landing_url": "https://arxiv.org/abs/2409.04016v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.04016"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study systematically compares neural audio codec tokens within speech language model generation, covering codec design, quantization, reconstruction, and generation trade-offs with measurable evaluations, so it clearly fits the discrete audio token criteria."
    },
    "round-A_JuniorNano_reasoning": "Study systematically compares neural audio codec tokens within speech language model generation, covering codec design, quantization, reconstruction, and generation trade-offs with measurable evaluations, so it clearly fits the discrete audio token criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
    "abstract": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
    "metadata": {
      "arxiv_id": "2409.05004",
      "title": "Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion",
      "summary": "Voice conversion (VC) aims to modify the speaker's timbre while retaining speech content. Previous approaches have tokenized the outputs from self-supervised into semantic tokens, facilitating disentanglement of speech content information. Recently, in-context learning (ICL) has emerged in text-to-speech (TTS) systems for effectively modeling specific characteristics such as timbre through context conditioning. This paper proposes an ICL capability enhanced VC system (ICL-VC) employing a mask and reconstruction training strategy based on flow-matching generative models. Augmented with semantic tokens, our experiments on the LibriTTS dataset demonstrate that ICL-VC improves speaker similarity. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-VC system faces challenges in preserving the prosody of the source speech. To mitigate this issue, we propose incorporating prosody embeddings extracted from a pre-trained emotion recognition model into our system. Integration of prosody embeddings notably enhances the system's capability to preserve source speech prosody, as validated on the Emotional Speech Database.",
      "authors": [
        "Zhengyang Chen",
        "Shuai Wang",
        "Mingyang Zhang",
        "Xuechen Liu",
        "Junichi Yamagishi",
        "Yanmin Qian"
      ],
      "published": "2024-09-08T07:24:03Z",
      "updated": "2024-09-10T07:36:03Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05004v2",
      "landing_url": "https://arxiv.org/abs/2409.05004v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05004"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study explicitly uses semantic tokens through self-supervised quantization and discusses tokenization and prosody embeddings with evaluations on LibriTTS and Emotional Speech Database, satisfying discrete audio token scope and providing measurable assessments."
    },
    "round-A_JuniorNano_reasoning": "Study explicitly uses semantic tokens through self-supervised quantization and discusses tokenization and prosody embeddings with evaluations on LibriTTS and Emotional Speech Database, satisfying discrete audio token scope and providing measurable assessments.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec",
    "abstract": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.",
    "metadata": {
      "arxiv_id": "2409.05377",
      "title": "BigCodec: Pushing the Limits of Low-Bitrate Neural Speech Codec",
      "summary": "We present BigCodec, a low-bitrate neural speech codec. While recent neural speech codecs have shown impressive progress, their performance significantly deteriorates at low bitrates (around 1 kbps). Although a low bitrate inherently restricts performance, other factors, such as model capacity, also hinder further improvements. To address this problem, we scale up the model size to 159M parameters that is more than 10 times larger than popular codecs with about 10M parameters. Besides, we integrate sequential models into traditional convolutional architectures to better capture temporal dependency and adopt low-dimensional vector quantization to ensure a high code utilization. Comprehensive objective and subjective evaluations show that BigCodec, with a bitrate of 1.04 kbps, significantly outperforms several existing low-bitrate codecs. Furthermore, BigCodec achieves objective performance comparable to popular codecs operating at 4-6 times higher bitrates, and even delivers better subjective perceptual quality than the ground truth.",
      "authors": [
        "Detai Xin",
        "Xu Tan",
        "Shinnosuke Takamichi",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-09-09T07:18:07Z",
      "updated": "2024-09-09T07:18:07Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05377v1",
      "landing_url": "https://arxiv.org/abs/2409.05377v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.05377"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "BigCodec scales up a neural codec that clearly relies on discrete quantization tokens and provides measurable reconstruction and subjective evaluations at 1 kbps, so it meets the discrete audio token criteria and deserves inclusion."
    },
    "round-A_JuniorNano_reasoning": "BigCodec scales up a neural codec that clearly relies on discrete quantization tokens and provides measurable reconstruction and subjective evaluations at 1 kbps, so it meets the discrete audio token criteria and deserves inclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation",
    "abstract": "Graph neural network(GNN) has been a powerful approach in collaborative filtering(CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning(CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment. To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach.",
    "metadata": {
      "arxiv_id": "2409.05633",
      "title": "Enhancing Graph Contrastive Learning with Reliable and Informative Augmentation for Recommendation",
      "summary": "Graph neural network(GNN) has been a powerful approach in collaborative filtering(CF) due to its ability to model high-order user-item relationships. Recently, to alleviate the data sparsity and enhance representation learning, many efforts have been conducted to integrate contrastive learning(CL) with GNNs. Despite the promising improvements, the contrastive view generation based on structure and representation perturbations in existing methods potentially disrupts the collaborative information in contrastive views, resulting in limited effectiveness of positive alignment. To overcome this issue, we propose CoGCL, a novel framework that aims to enhance graph contrastive learning by constructing contrastive views with stronger collaborative information via discrete codes. The core idea is to map users and items into discrete codes rich in collaborative information for reliable and informative contrastive view generation. To this end, we initially introduce a multi-level vector quantizer in an end-to-end manner to quantize user and item representations into discrete codes. Based on these discrete codes, we enhance the collaborative information of contrastive views by considering neighborhood structure and semantic relevance respectively. For neighborhood structure, we propose virtual neighbor augmentation by treating discrete codes as virtual neighbors, which expands an observed user-item interaction into multiple edges involving discrete codes. Regarding semantic relevance, we identify similar users/items based on shared discrete codes and interaction targets to generate the semantically relevant view. Through these strategies, we construct contrastive views with stronger collaborative information and develop a triple-view graph contrastive learning approach. Extensive experiments on four public datasets demonstrate the effectiveness of our proposed approach.",
      "authors": [
        "Bowen Zheng",
        "Junjie Zhang",
        "Hongyu Lu",
        "Yu Chen",
        "Ming Chen",
        "Wayne Xin Zhao",
        "Ji-Rong Wen"
      ],
      "published": "2024-09-09T14:04:17Z",
      "updated": "2024-12-09T09:44:27Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05633v2",
      "landing_url": "https://arxiv.org/abs/2409.05633v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05633"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper addresses graph contrastive learning for recommendations with discrete codes for collaborative info, not discrete audio tokens or tokenizers, so it fails inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper addresses graph contrastive learning for recommendations with discrete codes for collaborative info, not discrete audio tokens or tokenizers, so it fails inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
    "abstract": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
    "metadata": {
      "arxiv_id": "2409.05784",
      "title": "Vector Quantized Diffusion Model Based Speech Bandwidth Extension",
      "summary": "Recent advancements in neural audio codec (NAC) unlock new potential in audio signal processing. Studies have increasingly explored leveraging the latent features of NAC for various speech signal processing tasks. This paper introduces the first approach to speech bandwidth extension (BWE) that utilizes the discrete features obtained from NAC. By restoring high-frequency details within highly compressed discrete tokens, this approach enhances speech intelligibility and naturalness. Based on Vector Quantized Diffusion, the proposed framework combines the strengths of advanced NAC, diffusion models, and Mamba-2 to reconstruct high-frequency speech components. Extensive experiments demonstrate that this method exhibits superior performance across both log-spectral distance and ViSQOL, significantly improving speech quality.",
      "authors": [
        "Yuan Fang",
        "Jinglin Bai",
        "Jiajie Wang",
        "Xueliang Zhang"
      ],
      "published": "2024-09-09T16:46:54Z",
      "updated": "2024-09-14T14:25:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.05784v2",
      "landing_url": "https://arxiv.org/abs/2409.05784v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.05784"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Uses discrete neural audio codec tokens derived from NAC and evaluates reconstruction quality (log-spectral distance, ViSQOL) for bandwidth extension, so it satisfies the discrete token focus and reporting requirements."
    },
    "round-A_JuniorNano_reasoning": "Uses discrete neural audio codec tokens derived from NAC and evaluates reconstruction quality (log-spectral distance, ViSQOL) for bandwidth extension, so it satisfies the discrete token focus and reporting requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Estimating the Completeness of Discrete Speech Units",
    "abstract": "Representing speech with discrete units has been widely used in speech codec and speech generation. However, there are several unverified claims about self-supervised discrete units, such as disentangling phonetic and speaker information with k-means, or assuming information loss after k-means. In this work, we take an information-theoretic perspective to answer how much information is present (information completeness) and how much information is accessible (information accessibility), before and after residual vector quantization. We show a lower bound for information completeness and estimate completeness on discretized HuBERT representations after residual vector quantization. We find that speaker information is sufficiently present in HuBERT discrete units, and that phonetic information is sufficiently present in the residual, showing that vector quantization does not achieve disentanglement. Our results offer a comprehensive assessment on the choice of discrete units, and suggest that a lot more information in the residual should be mined rather than discarded.",
    "metadata": {
      "arxiv_id": "2409.06109",
      "title": "Estimating the Completeness of Discrete Speech Units",
      "summary": "Representing speech with discrete units has been widely used in speech codec and speech generation. However, there are several unverified claims about self-supervised discrete units, such as disentangling phonetic and speaker information with k-means, or assuming information loss after k-means. In this work, we take an information-theoretic perspective to answer how much information is present (information completeness) and how much information is accessible (information accessibility), before and after residual vector quantization. We show a lower bound for information completeness and estimate completeness on discretized HuBERT representations after residual vector quantization. We find that speaker information is sufficiently present in HuBERT discrete units, and that phonetic information is sufficiently present in the residual, showing that vector quantization does not achieve disentanglement. Our results offer a comprehensive assessment on the choice of discrete units, and suggest that a lot more information in the residual should be mined rather than discarded.",
      "authors": [
        "Sung-Lin Yeh",
        "Hao Tang"
      ],
      "published": "2024-09-09T23:31:56Z",
      "updated": "2024-09-22T18:40:09Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06109v2",
      "landing_url": "https://arxiv.org/abs/2409.06109v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.06109"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper analyzes quantized HuBERT discrete units and their information completeness/accessibility, directly studying self-supervised discrete audio tokens and their encoding/quantization trade-offs, so it clearly matches the inclusion criteria and no exclusion applies."
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes quantized HuBERT discrete units and their information completeness/accessibility, directly studying self-supervised discrete audio tokens and their encoding/quantization trade-offs, so it clearly matches the inclusion criteria and no exclusion applies.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models",
    "abstract": "The Audio Question Answering (AQA) task includes audio event classification, audio captioning, and open-ended reasoning. Recently, AQA has garnered attention due to the advent of Large Audio Language Models (LALMs). Current literature focuses on constructing LALMs by integrating audio encoders with text-only Large Language Models (LLMs) through a projection module. While LALMs excel in general audio understanding, they are limited in temporal reasoning, which may hinder their commercial applications and on-device deployment. This paper addresses these challenges and limitations in audio temporal reasoning. First, we introduce a data augmentation technique for generating reliable audio temporal questions and answers using an LLM. Second, we perform a further fine-tuning of an existing baseline using curriculum learning strategy to specialize in temporal reasoning without compromising performance on fine-tuned tasks. We demonstrate the performance of our model using state-of-the-art LALMs on public audio benchmark datasets. Third, we implement our AQA model on-device locally and investigate its CPU inference for edge applications.",
    "metadata": {
      "arxiv_id": "2409.06223",
      "title": "Enhancing Temporal Understanding in Audio Question Answering for Large Audio Language Models",
      "summary": "The Audio Question Answering (AQA) task includes audio event classification, audio captioning, and open-ended reasoning. Recently, AQA has garnered attention due to the advent of Large Audio Language Models (LALMs). Current literature focuses on constructing LALMs by integrating audio encoders with text-only Large Language Models (LLMs) through a projection module. While LALMs excel in general audio understanding, they are limited in temporal reasoning, which may hinder their commercial applications and on-device deployment. This paper addresses these challenges and limitations in audio temporal reasoning. First, we introduce a data augmentation technique for generating reliable audio temporal questions and answers using an LLM. Second, we perform a further fine-tuning of an existing baseline using curriculum learning strategy to specialize in temporal reasoning without compromising performance on fine-tuned tasks. We demonstrate the performance of our model using state-of-the-art LALMs on public audio benchmark datasets. Third, we implement our AQA model on-device locally and investigate its CPU inference for edge applications.",
      "authors": [
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "published": "2024-09-10T05:26:53Z",
      "updated": "2024-12-13T17:29:55Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.06223v3",
      "landing_url": "https://arxiv.org/abs/2409.06223v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.06223"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on temporal reasoning for large audio language models via data augmentation, fine-tuning, and on-device deployment, but it says nothing about discrete audio tokens, quantization/codecs, or token-level modeling, so it fails the core inclusion criteria and meets the exclusion rule."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on temporal reasoning for large audio language models via data augmentation, fine-tuning, and on-device deployment, but it says nothing about discrete audio tokens, quantization/codecs, or token-level modeling, so it fails the core inclusion criteria and meets the exclusion rule.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
    "abstract": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
    "metadata": {
      "arxiv_id": "2409.07151",
      "title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment",
      "summary": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).",
      "authors": [
        "Tien-Hong Lo",
        "Meng-Ting Tsai",
        "Yao-Ting Sung",
        "Berlin Chen"
      ],
      "published": "2024-09-11T09:55:57Z",
      "updated": "2025-07-26T06:14:20Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07151v2",
      "landing_url": "https://arxiv.org/abs/2409.07151v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07151"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study centers on zero-shot TTS for pronunciation assessment without discussing any discrete audio tokenization, so it fails to meet the inclusion requirements and should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The study centers on zero-shot TTS for pronunciation assessment without discussing any discrete audio tokenization, so it fails to meet the inclusion requirements and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
    "abstract": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
    "metadata": {
      "arxiv_id": "2409.07276",
      "title": "Learning Multi-Aspect Item Palette: A Semantic Tokenization Framework for Generative Recommendation",
      "summary": "Traditional recommendation models often rely on unique item identifiers (IDs) to distinguish between items, which can hinder their ability to effectively leverage item content information and generalize to long-tailed or cold-start items. Recently, semantic tokenization has been proposed as a promising solution that aims to tokenize each item's semantic representation into a sequence of discrete tokens. These semantic tokens have become fundamental in training generative recommendation models. However, existing methods typically rely on RQ-VAE, a residual vector quantizer, for semantic tokenization. This reliance introduces several key limitations, including challenges in embedding extraction, hierarchical coarse-to-fine quantization, and training stability. To address these issues, we introduce LAMIA, a novel approach for multi-aspect semantic tokenization. Unlike RQ-VAE, which uses a single embedding, LAMIA learns an ``item palette''--a collection of independent and semantically parallel embeddings that capture multiple aspects of items. Additionally, LAMIA enhances the semantic encoders through domain-specific tuning using text-based reconstruction tasks, resulting in more representative item palette embeddings. We have conducted extensive experiments to validate the effectiveness of the LAMIA framework across various recommendation tasks and datasets. Our results demonstrate significant improvements in recommendation accuracy over existing methods. To facilitate reproducible research, we will release the source code, data, and configurations.",
      "authors": [
        "Qijiong Liu",
        "Jieming Zhu",
        "Zhaocheng Du",
        "Lu Fan",
        "Zhou Zhao",
        "Xiao-Ming Wu"
      ],
      "published": "2024-09-11T13:49:48Z",
      "updated": "2025-08-05T11:07:31Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07276v3",
      "landing_url": "https://arxiv.org/abs/2409.07276v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.07276"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work focuses on multi-aspect semantic tokenization for recommendation models rather than discrete audio token/codebook design or evaluation, so it fails to meet the audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "This work focuses on multi-aspect semantic tokenization for recommendation models rather than discrete audio token/codebook design or evaluation, so it fails to meet the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
    "abstract": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
    "metadata": {
      "arxiv_id": "2409.07556",
      "title": "SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis",
      "summary": "In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.",
      "authors": [
        "Helin Wang",
        "Meng Yu",
        "Jiarui Hai",
        "Chen Chen",
        "Yuchen Hu",
        "Rilin Chen",
        "Najim Dehak",
        "Dong Yu"
      ],
      "published": "2024-09-11T18:24:07Z",
      "updated": "2025-01-02T03:07:29Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.07556v2",
      "landing_url": "https://arxiv.org/abs/2409.07556v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.07556"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "SSR-Speech clearly builds on neural codec-based discrete audio tokens (via Encodec) for editing/synthesis, evaluates reconstruction quality, and discusses token-level engineering, so it meets the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "SSR-Speech clearly builds on neural codec-based discrete audio tokens (via Encodec) for editing/synthesis, evaluates reconstruction quality, and discusses token-level engineering, so it meets the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
    "abstract": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
    "metadata": {
      "arxiv_id": "2409.08277",
      "title": "Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor",
      "summary": "High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.",
      "authors": [
        "Andrea Conti",
        "Matteo Poggi",
        "Valerio Cambareri",
        "Stefano Mattoccia"
      ],
      "published": "2024-09-12T17:59:46Z",
      "updated": "2024-09-12T17:59:46Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08277v1",
      "landing_url": "https://arxiv.org/abs/2409.08277v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.08277"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Depth-on-demand depth estimation paper focuses on multimodal depth reconstruction, not on discrete audio tokenization, so it fails the inclusion criteria and meets none of the targeted topics."
    },
    "round-A_JuniorNano_reasoning": "Depth-on-demand depth estimation paper focuses on multimodal depth reconstruction, not on discrete audio tokenization, so it fails the inclusion criteria and meets none of the targeted topics.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Investigating Disentanglement in a Phoneme-level Speech Codec for Prosody Modeling",
    "abstract": "Most of the prevalent approaches in speech prosody modeling rely on learning global style representations in a continuous latent space which encode and transfer the attributes of reference speech. However, recent work on neural codecs which are based on Residual Vector Quantization (RVQ) already shows great potential offering distinct advantages. We investigate the prosody modeling capabilities of the discrete space of such an RVQ-VAE model, modifying it to operate on the phoneme-level. We condition both the encoder and decoder of the model on linguistic representations and apply a global speaker embedding in order to factor out both phonetic and speaker information. We conduct an extensive set of investigations based on subjective experiments and objective measures to show that the phoneme-level discrete latent representations obtained this way achieves a high degree of disentanglement, capturing fine-grained prosodic information that is robust and transferable. The latent space turns out to have interpretable structure with its principal components corresponding to pitch and energy.",
    "metadata": {
      "arxiv_id": "2409.08664",
      "title": "Investigating Disentanglement in a Phoneme-level Speech Codec for Prosody Modeling",
      "summary": "Most of the prevalent approaches in speech prosody modeling rely on learning global style representations in a continuous latent space which encode and transfer the attributes of reference speech. However, recent work on neural codecs which are based on Residual Vector Quantization (RVQ) already shows great potential offering distinct advantages. We investigate the prosody modeling capabilities of the discrete space of such an RVQ-VAE model, modifying it to operate on the phoneme-level. We condition both the encoder and decoder of the model on linguistic representations and apply a global speaker embedding in order to factor out both phonetic and speaker information. We conduct an extensive set of investigations based on subjective experiments and objective measures to show that the phoneme-level discrete latent representations obtained this way achieves a high degree of disentanglement, capturing fine-grained prosodic information that is robust and transferable. The latent space turns out to have interpretable structure with its principal components corresponding to pitch and energy.",
      "authors": [
        "Sotirios Karapiperis",
        "Nikolaos Ellinas",
        "Alexandra Vioni",
        "Junkwang Oh",
        "Gunu Jho",
        "Inchul Hwang",
        "Spyros Raptis"
      ],
      "published": "2024-09-13T09:27:05Z",
      "updated": "2024-09-13T09:27:05Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08664v1",
      "landing_url": "https://arxiv.org/abs/2409.08664v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.08664"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper studies phoneme-level RVQ discrete latent tokens for prosody modeling with speaker and linguistic conditioning plus subjective/objective evaluation, so it matches the discrete audio-token inclusion and earns a top score 5."
    },
    "round-A_JuniorNano_reasoning": "Paper studies phoneme-level RVQ discrete latent tokens for prosody modeling with speaker and linguistic conditioning plus subjective/objective evaluation, so it matches the discrete audio-token inclusion and earns a top score 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Text-To-Speech Synthesis In The Wild",
    "abstract": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
    "metadata": {
      "arxiv_id": "2409.08711",
      "title": "Text-To-Speech Synthesis In The Wild",
      "summary": "Traditional Text-to-Speech (TTS) systems rely on studio-quality speech recorded in controlled settings.a Recently, an effort known as noisy-TTS training has emerged, aiming to utilize in-the-wild data. However, the lack of dedicated datasets has been a significant limitation. We introduce the TTS In the Wild (TITW) dataset, which is publicly available, created through a fully automated pipeline applied to the VoxCeleb1 dataset. It comprises two training sets: TITW-Hard, derived from the transcription, segmentation, and selection of raw VoxCeleb1 data, and TITW-Easy, which incorporates additional enhancement and data selection based on DNSMOS. State-of-the-art TTS models achieve over 3.0 UTMOS score with TITW-Easy, while TITW-Hard remains difficult showing UTMOS below 2.8.",
      "authors": [
        "Jee-weon Jung",
        "Wangyou Zhang",
        "Soumi Maiti",
        "Yihan Wu",
        "Xin Wang",
        "Ji-Hoon Kim",
        "Yuta Matsunaga",
        "Seyun Um",
        "Jinchuan Tian",
        "Hye-jin Shim",
        "Nicholas Evans",
        "Joon Son Chung",
        "Shinnosuke Takamichi",
        "Shinji Watanabe"
      ],
      "published": "2024-09-13T10:58:55Z",
      "updated": "2025-06-01T09:29:36Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.08711v2",
      "landing_url": "https://arxiv.org/abs/2409.08711v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.08711"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper describes in-the-wild TTS datasets without any mention of discrete audio tokens, quantization/codebook/tokenizer design, or token-level evaluation, so it fails to match the discrete-token focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper describes in-the-wild TTS datasets without any mention of discrete audio tokens, quantization/codebook/tokenizer design, or token-level evaluation, so it fails to match the discrete-token focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
    "abstract": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
    "metadata": {
      "arxiv_id": "2409.09253",
      "title": "Unleash LLMs Potential for Recommendation by Coordinating Twin-Tower Dynamic Semantic Token Generator",
      "summary": "Owing to the unprecedented capability in semantic understanding and logical reasoning, the pre-trained large language models (LLMs) have shown fantastic potential in developing the next-generation recommender systems (RSs). However, the static index paradigm adopted by current methods greatly restricts the utilization of LLMs capacity for recommendation, leading to not only the insufficient alignment between semantic and collaborative knowledge, but also the neglect of high-order user-item interaction patterns. In this paper, we propose Twin-Tower Dynamic Semantic Recommender (TTDS), the first generative RS which adopts dynamic semantic index paradigm, targeting at resolving the above problems simultaneously. To be more specific, we for the first time contrive a dynamic knowledge fusion framework which integrates a twin-tower semantic token generator into the LLM-based recommender, hierarchically allocating meaningful semantic index for items and users, and accordingly predicting the semantic index of target item. Furthermore, a dual-modality variational auto-encoder is proposed to facilitate multi-grained alignment between semantic and collaborative knowledge. Eventually, a series of novel tuning tasks specially customized for capturing high-order user-item interaction patterns are proposed to take advantages of user historical behavior. Extensive experiments across three public datasets demonstrate the superiority of the proposed methodology in developing LLM-based generative RSs. The proposed TTDS recommender achieves an average improvement of 19.41% in Hit-Rate and 20.84% in NDCG metric, compared with the leading baseline methods.",
      "authors": [
        "Jun Yin",
        "Zhengxin Zeng",
        "Mingzheng Li",
        "Hao Yan",
        "Chaozhuo Li",
        "Weihao Han",
        "Jianjin Zhang",
        "Ruochen Liu",
        "Allen Sun",
        "Denvy Deng",
        "Feng Sun",
        "Qi Zhang",
        "Shirui Pan",
        "Senzhang Wang"
      ],
      "published": "2024-09-14T01:45:04Z",
      "updated": "2024-09-14T01:45:04Z",
      "categories": [
        "cs.IR",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09253v1",
      "landing_url": "https://arxiv.org/abs/2409.09253v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09253"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on LLM-based recommendation systems without any discrete audio tokenization, codec or token-level modeling, so it fails to meet the topic’s inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on LLM-based recommendation systems without any discrete audio tokenization, codec or token-level modeling, so it fails to meet the topic’s inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
    "abstract": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
    "metadata": {
      "arxiv_id": "2409.09357",
      "title": "Joint Semantic Knowledge Distillation and Masked Acoustic Modeling for Full-band Speech Restoration with Improved Intelligibility",
      "summary": "Speech restoration aims at restoring full-band speech with high quality and intelligibility, considering a diverse set of distortions. MaskSR is a recently proposed generative model for this task. As other models of its kind, MaskSR attains high quality but, as we show, intelligibility can be substantially improved. We do so by boosting the speech encoder component of MaskSR with predictions of semantic representations of the target speech, using a pre-trained self-supervised teacher model. Then, a masked language model is conditioned on the learned semantic features to predict acoustic tokens that encode low level spectral details of the target speech. We show that, with the same MaskSR model capacity and inference time, the proposed model, MaskSR2, significantly reduces the word error rate, a typical metric for intelligibility. MaskSR2 also achieves competitive word error rate among other models, while providing superior quality. An ablation study shows the effectiveness of various semantic representations.",
      "authors": [
        "Xiaoyu Liu",
        "Xu Li",
        "Joan Serrà",
        "Santiago Pascual"
      ],
      "published": "2024-09-14T08:09:55Z",
      "updated": "2024-09-14T08:09:55Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.09357v1",
      "landing_url": "https://arxiv.org/abs/2409.09357v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.09357"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes a speech restoration model enhancing MaskSR with semantic predictions but lacks any mention of discrete tokenization, quantization, or finite-vocabulary representations central to the inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a speech restoration model enhancing MaskSR with semantic predictions but lacks any mention of discrete tokenization, quantization, or finite-vocabulary representations central to the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
    "abstract": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
    "metadata": {
      "arxiv_id": "2409.10870",
      "title": "Adaptive Large Language Models By Layerwise Attention Shortcuts",
      "summary": "Transformer architectures are the backbone of the modern AI revolution. However, they are based on simply stacking the same blocks in dozens of layers and processing information sequentially from one block to another. In this paper, we propose to challenge this and introduce adaptive computations for LLM-like setups, which allow the final layer to attend to all of the intermediate layers as it deems fit through the attention mechanism, thereby introducing computational \\textbf{attention shortcuts}. These shortcuts can thus make the architecture depth and context adaptive. We showcase four different datasets, namely acoustic tokens, natural language, and symbolic music, and we achieve superior performance for GPT-like architecture. We give evidence via attention maps that the models learn complex dependencies across layers that are adaptive in context and depth depending on the input tokens.",
      "authors": [
        "Prateek Verma",
        "Mert Pilanci"
      ],
      "published": "2024-09-17T03:46:01Z",
      "updated": "2024-09-17T03:46:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10870v1",
      "landing_url": "https://arxiv.org/abs/2409.10870v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.10870"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on a transformer architecture with layerwise attention shortcuts for LLM-style tasks, without detailing discrete audio-token generation/quantization or token-level modeling, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on a transformer architecture with layerwise attention shortcuts for LLM-style tasks, without detailing discrete audio-token generation/quantization or token-level modeling, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models",
    "abstract": "Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai.",
    "metadata": {
      "arxiv_id": "2409.10999",
      "title": "Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models",
      "summary": "Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai.",
      "authors": [
        "Potsawee Manakul",
        "Guangzhi Sun",
        "Warit Sirichotedumrong",
        "Kasima Tharnpipitchai",
        "Kunat Pipatanakul"
      ],
      "published": "2024-09-17T09:04:03Z",
      "updated": "2025-05-23T10:37:01Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.10999v2",
      "landing_url": "https://arxiv.org/abs/2409.10999v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.10999"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on instruction-tuning audio language models for Thai/English without presenting or evaluating discrete audio token/codec quantization or vocabulary design, so it fails to meet the discrete token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on instruction-tuning audio language models for Thai/English without presenting or evaluating discrete audio token/codec quantization or vocabulary design, so it fails to meet the discrete token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
    "abstract": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
    "metadata": {
      "arxiv_id": "2409.11003",
      "title": "Single-stage TTS with Masked Audio Token Modeling and Semantic Knowledge Distillation",
      "summary": "Audio token modeling has become a powerful framework for speech synthesis, with two-stage approaches employing semantic tokens remaining prevalent. In this paper, we aim to simplify this process by introducing a semantic knowledge distillation method that enables high-quality speech generation in a single stage. Our proposed model improves speech quality, intelligibility, and speaker similarity compared to a single-stage baseline. Although two-stage systems still lead in intelligibility, our model significantly narrows the gap while delivering comparable speech quality. These findings showcase the potential of single-stage models to achieve efficient, high-quality TTS with a more compact and streamlined architecture.",
      "authors": [
        "Gerard I. Gállego",
        "Roy Fejgin",
        "Chunghsin Yeh",
        "Xiaoyu Liu",
        "Gautam Bhattacharya"
      ],
      "published": "2024-09-17T09:08:43Z",
      "updated": "2024-09-17T09:08:43Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11003v1",
      "landing_url": "https://arxiv.org/abs/2409.11003v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11003"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper explicitly focuses on audio token modeling and single-stage TTS using semantic tokens, aligning with the discrete audio token inclusion criteria, so it should definitely be included."
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly focuses on audio token modeling and single-stage TTS using semantic tokens, aligning with the discrete audio token inclusion criteria, so it should definitely be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Source Disentanglement in Neural Audio Codec",
    "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
    "metadata": {
      "arxiv_id": "2409.11228",
      "title": "Learning Source Disentanglement in Neural Audio Codec",
      "summary": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.",
      "authors": [
        "Xiaoyu Bie",
        "Xubo Liu",
        "Gaël Richard"
      ],
      "published": "2024-09-17T14:21:02Z",
      "updated": "2025-02-11T10:35:04Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11228v2",
      "landing_url": "https://arxiv.org/abs/2409.11228v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.11228"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study proposes Source-Disentangled Neural Audio Codec that jointly learns compression and separation, explicitly assigning domain-specific discrete codebooks and evaluating resynthesis/separation quality, so it clearly centers on discrete audio tokens and meets inclusion while avoiding exclusions."
    },
    "round-A_JuniorNano_reasoning": "Study proposes Source-Disentangled Neural Audio Codec that jointly learns compression and separation, explicitly assigning domain-specific discrete codebooks and evaluating resynthesis/separation quality, so it clearly centers on discrete audio tokens and meets inclusion while avoiding exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Tracking Any Point with Frame-Event Fusion Network at High Frame Rate",
    "abstract": "Tracking any point based on image frames is constrained by frame rates, leading to instability in high-speed scenarios and limited generalization in real-world applications. To overcome these limitations, we propose an image-event fusion point tracker, FE-TAP, which combines the contextual information from image frames with the high temporal resolution of events, achieving high frame rate and robust point tracking under various challenging conditions. Specifically, we designed an Evolution Fusion module (EvoFusion) to model the image generation process guided by events. This module can effectively integrate valuable information from both modalities operating at different frequencies. To achieve smoother point trajectories, we employed a transformer-based refinement strategy that updates the point's trajectories and features iteratively. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches, particularly improving expected feature age by 24$\\%$ on EDS datasets. Finally, we qualitatively validated the robustness of our algorithm in real driving scenarios using our custom-designed high-resolution image-event synchronization device. Our source code will be released at https://github.com/ljx1002/FE-TAP.",
    "metadata": {
      "arxiv_id": "2409.11953",
      "title": "Tracking Any Point with Frame-Event Fusion Network at High Frame Rate",
      "summary": "Tracking any point based on image frames is constrained by frame rates, leading to instability in high-speed scenarios and limited generalization in real-world applications. To overcome these limitations, we propose an image-event fusion point tracker, FE-TAP, which combines the contextual information from image frames with the high temporal resolution of events, achieving high frame rate and robust point tracking under various challenging conditions. Specifically, we designed an Evolution Fusion module (EvoFusion) to model the image generation process guided by events. This module can effectively integrate valuable information from both modalities operating at different frequencies. To achieve smoother point trajectories, we employed a transformer-based refinement strategy that updates the point's trajectories and features iteratively. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches, particularly improving expected feature age by 24$\\%$ on EDS datasets. Finally, we qualitatively validated the robustness of our algorithm in real driving scenarios using our custom-designed high-resolution image-event synchronization device. Our source code will be released at https://github.com/ljx1002/FE-TAP.",
      "authors": [
        "Jiaxiong Liu",
        "Bo Wang",
        "Zhen Tan",
        "Jinpu Zhang",
        "Hui Shen",
        "Dewen Hu"
      ],
      "published": "2024-09-18T13:07:19Z",
      "updated": "2024-09-18T13:07:19Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.11953v1",
      "landing_url": "https://arxiv.org/abs/2409.11953v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.11953"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on image-event fusion point tracking rather than any discrete audio token generation, quantization, or evaluation, so it does not meet the audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on image-event fusion point tracking rather than any discrete audio token generation, quantization, or evaluation, so it does not meet the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
    "abstract": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
    "metadata": {
      "arxiv_id": "2409.12117",
      "title": "Low Frame-rate Speech Codec: a Codec Designed for Fast High-quality Speech LLM Training and Inference",
      "summary": "Large language models (LLMs) have significantly advanced audio processing through audio codecs that convert audio into discrete tokens, enabling the application of language modeling techniques to audio data. However, audio codecs often operate at high frame rates, resulting in slow training and inference, especially for autoregressive models. To address this challenge, we present the Low Frame-rate Speech Codec (LFSC): a neural audio codec that leverages finite scalar quantization and adversarial training with large speech language models to achieve high-quality audio compression with a 1.89 kbps bitrate and 21.5 frames per second. We demonstrate that our novel codec can make the inference of LLM-based text-to-speech models around three times faster while improving intelligibility and producing quality comparable to previous models.",
      "authors": [
        "Edresson Casanova",
        "Ryan Langman",
        "Paarth Neekhara",
        "Shehzeen Hussain",
        "Jason Li",
        "Subhankar Ghosh",
        "Ante Jukić",
        "Sang-gil Lee"
      ],
      "published": "2024-09-18T16:39:10Z",
      "updated": "2024-09-18T16:39:10Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12117v1",
      "landing_url": "https://arxiv.org/abs/2409.12117v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12117"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The Low Frame-rate Speech Codec paper clearly proposes a neural audio codec that quantizes audio into discrete codes for use with LLM-based speech modeling, discusses bitrate/frame findings, and evaluates quality/speed tradeoffs, satisfying the discrete-token focus and evaluation requirements."
    },
    "round-A_JuniorNano_reasoning": "The Low Frame-rate Speech Codec paper clearly proposes a neural audio codec that quantizes audio into discrete codes for use with LLM-based speech modeling, discusses bitrate/frame findings, and evaluates quality/speed tradeoffs, satisfying the discrete-token focus and evaluation requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
    "abstract": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
    "metadata": {
      "arxiv_id": "2409.12176",
      "title": "Prosodic Parameter Manipulation in TTS generated speech for Controlled Speech Generation",
      "summary": "This paper explores the manipulation of prosodic parameters in Text-to-Speech (TTS) systems to achieve controlled speech generation. By leveraging advanced speech processing techniques, we compare TTS-generated audio with human-recorded speech to analyze differences in pitch, duration, and energy. Key features are extracted using tools like PyWorld and Librosa, which are then adjusted to align with the prosodic characteristics of natural human speech. The modified features undergo synthesis, producing enhanced TTS outputs that more closely mirror the natural prosody of human speech. This approach aims to enhance the naturalness and expressiveness of TTS systems by providing a framework for precise prosodic parameter adjustments. Our methodology involves feature extraction, prosodic manipulation, and synthesis, followed by comprehensive evaluations to ensure consistency with human speech patterns. The findings demonstrate the feasibility and effectiveness of prosodic parameter manipulation for controlled speech generation, highlighting its potential to significantly improve TTS applications.",
      "authors": [
        "Podakanti Satyajith Chary"
      ],
      "published": "2024-09-10T11:50:35Z",
      "updated": "2024-09-10T11:50:35Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12176v1",
      "landing_url": "https://arxiv.org/abs/2409.12176v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12176"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on manipulating continuous prosodic features (pitch, duration, energy) for TTS enhancement and does not present or analyze any discrete audio token/tokenizer design or quantized codebook, so it fails to meet the discrete-token inclusion criteria and matches the exclusion of continuous-feature-centered work."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on manipulating continuous prosodic features (pitch, duration, energy) for TTS enhancement and does not present or analyze any discrete audio token/tokenizer design or quantized codebook, so it fails to meet the discrete-token inclusion criteria and matches the exclusion of continuous-feature-centered work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
    "abstract": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
    "metadata": {
      "arxiv_id": "2409.12717",
      "title": "NDVQ: Robust Neural Audio Codec with Normal Distribution-Based Vector Quantization",
      "summary": "Built upon vector quantization (VQ), discrete audio codec models have achieved great success in audio compression and auto-regressive audio generation. However, existing models face substantial challenges in perceptual quality and signal distortion, especially when operating in extremely low bandwidth, rooted in the sensitivity of the VQ codebook to noise. This degradation poses significant challenges for several downstream tasks, such as codec-based speech synthesis. To address this issue, we propose a novel VQ method, Normal Distribution-based Vector Quantization (NDVQ), by introducing an explicit margin between the VQ codes via learning a variance. Specifically, our approach involves mapping the waveform to a latent space and quantizing it by selecting the most likely normal distribution, with each codebook entry representing a unique normal distribution defined by its mean and variance. Using these distribution-based VQ codec codes, a decoder reconstructs the input waveform. NDVQ is trained with additional distribution-related losses, alongside reconstruction and discrimination losses. Experiments demonstrate that NDVQ outperforms existing audio compression baselines, such as EnCodec, in terms of audio quality and zero-shot TTS, particularly in very low bandwidth scenarios.",
      "authors": [
        "Zhikang Niu",
        "Sanyuan Chen",
        "Long Zhou",
        "Ziyang Ma",
        "Xie Chen",
        "Shujie Liu"
      ],
      "published": "2024-09-19T12:41:30Z",
      "updated": "2024-09-19T12:41:30Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.12717v1",
      "landing_url": "https://arxiv.org/abs/2409.12717v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.12717"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Reasoning: NDVQ introduces a discrete neural audio codec with normal-distribution-based codebooks, quantization, and reconstruction evaluation, satisfying the token-focused inclusion criteria without hitting any exclusions."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: NDVQ introduces a discrete neural audio codec with normal-distribution-based codebooks, quantization, and reconstruction evaluation, satisfying the token-focused inclusion criteria without hitting any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MuCodec: Ultra Low-Bitrate Music Codec",
    "abstract": "Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to extract both acoustic and semantic features, discretizes them with RVQ, and obtains Mel-VAE features via flow-matching. The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.",
    "metadata": {
      "arxiv_id": "2409.13216",
      "title": "MuCodec: Ultra Low-Bitrate Music Codec",
      "summary": "Music codecs are a vital aspect of audio codec research, and ultra low-bitrate compression holds significant importance for music transmission and generation. Due to the complexity of music backgrounds and the richness of vocals, solely relying on modeling semantic or acoustic information cannot effectively reconstruct music with both vocals and backgrounds. To address this issue, we propose MuCodec, specifically targeting music compression and reconstruction tasks at ultra low bitrates. MuCodec employs MuEncoder to extract both acoustic and semantic features, discretizes them with RVQ, and obtains Mel-VAE features via flow-matching. The music is then reconstructed using a pre-trained MEL-VAE decoder and HiFi-GAN. MuCodec can reconstruct high-fidelity music at ultra low (0.35kbps) or high bitrates (1.35kbps), achieving the best results to date in both subjective and objective metrics. Code and Demo: https://xuyaoxun.github.io/MuCodec_demo/.",
      "authors": [
        "Yaoxun Xu",
        "Hangting Chen",
        "Jianwei Yu",
        "Wei Tan",
        "Rongzhi Gu",
        "Shun Lei",
        "Zhiwei Lin",
        "Zhiyong Wu"
      ],
      "published": "2024-09-20T05:06:49Z",
      "updated": "2025-07-11T13:26:59Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13216v3",
      "landing_url": "https://arxiv.org/abs/2409.13216v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.13216"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "MuCodec explicitly quantizes encoder outputs via RVQ to produce discrete acoustic/semantic codes for reconstruction at ultra-low bitrates, satisfying the discrete token definition with codec-focused evaluation, so it should definitely be included."
    },
    "round-A_JuniorNano_reasoning": "MuCodec explicitly quantizes encoder outputs via RVQ to produce discrete acoustic/semantic codes for reconstruction at ultra-low bitrates, satisfying the discrete token definition with codec-focused evaluation, so it should definitely be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis",
    "abstract": "Automatic detection of synthetic speech is becoming increasingly important as current synthesis methods are both near indistinguishable from human speech and widely accessible to the public. Audio watermarking and other active disclosure methods of are attracting research activity, as they can complement traditional deepfake defenses based on passive detection. In both active and passive detection, robustness is of major interest. Traditional audio watermarks are particularly susceptible to removal attacks by audio codec application. Most generated speech and audio content released into the wild passes through an audio codec purely as a distribution method. We recently proposed collaborative watermarking as method for making generated speech more easily detectable over a noisy but differentiable transmission channel. This paper extends the channel augmentation to work with non-differentiable traditional audio codecs and neural audio codecs and evaluates transferability and effect of codec bitrate over various configurations. The results show that collaborative watermarking can be reliably augmented by black-box audio codecs using a waveform-domain straight-through-estimator for gradient approximation. Furthermore, that results show that channel augmentation with a neural audio codec transfers well to traditional codecs. Listening tests demonstrate collaborative watermarking incurs negligible perceptual degradation with high bitrate codecs or DAC at 8kbps.",
    "metadata": {
      "arxiv_id": "2409.13382",
      "title": "Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis",
      "summary": "Automatic detection of synthetic speech is becoming increasingly important as current synthesis methods are both near indistinguishable from human speech and widely accessible to the public. Audio watermarking and other active disclosure methods of are attracting research activity, as they can complement traditional deepfake defenses based on passive detection. In both active and passive detection, robustness is of major interest. Traditional audio watermarks are particularly susceptible to removal attacks by audio codec application. Most generated speech and audio content released into the wild passes through an audio codec purely as a distribution method. We recently proposed collaborative watermarking as method for making generated speech more easily detectable over a noisy but differentiable transmission channel. This paper extends the channel augmentation to work with non-differentiable traditional audio codecs and neural audio codecs and evaluates transferability and effect of codec bitrate over various configurations. The results show that collaborative watermarking can be reliably augmented by black-box audio codecs using a waveform-domain straight-through-estimator for gradient approximation. Furthermore, that results show that channel augmentation with a neural audio codec transfers well to traditional codecs. Listening tests demonstrate collaborative watermarking incurs negligible perceptual degradation with high bitrate codecs or DAC at 8kbps.",
      "authors": [
        "Lauri Juvela",
        "Xin Wang"
      ],
      "published": "2024-09-20T10:33:17Z",
      "updated": "2024-09-20T10:33:17Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13382v1",
      "landing_url": "https://arxiv.org/abs/2409.13382v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.13382"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper targets robust collaborative watermarking over audio codecs rather than defining or evaluating discrete audio tokens (tokenizer/codebook/quantization specification), so it fails the inclusion focus on discrete token research and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper targets robust collaborative watermarking over audio codecs rather than defining or evaluating discrete audio tokens (tokenizer/codebook/quantization specification), so it fails the inclusion focus on discrete token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder",
    "abstract": "This project, titled \"Machine Translation with Large Language Models: Decoder-only vs. Encoder-Decoder,\" aims to develop a multilingual machine translation (MT) model. Focused on Indian regional languages, especially Telugu, Tamil, and Malayalam, the model seeks to enable accurate and contextually appropriate translations across diverse language pairs. By comparing Decoder-only and Encoder-Decoder architectures, the project aims to optimize translation quality and efficiency, advancing cross-linguistic communication tools.The primary objective is to develop a model capable of delivering high-quality translations that are accurate and contextually appropriate. By leveraging large language models, specifically comparing the effectiveness of Decoder-only and Encoder-Decoder architectures, the project seeks to optimize translation performance and efficiency across multilingual contexts. Through rigorous experimentation and analysis, this project aims to advance the field of machine translation, contributing valuable insights into the effectiveness of different model architectures and paving the way for enhanced cross-linguistic communication tools.",
    "metadata": {
      "arxiv_id": "2409.13747",
      "title": "Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder",
      "summary": "This project, titled \"Machine Translation with Large Language Models: Decoder-only vs. Encoder-Decoder,\" aims to develop a multilingual machine translation (MT) model. Focused on Indian regional languages, especially Telugu, Tamil, and Malayalam, the model seeks to enable accurate and contextually appropriate translations across diverse language pairs. By comparing Decoder-only and Encoder-Decoder architectures, the project aims to optimize translation quality and efficiency, advancing cross-linguistic communication tools.The primary objective is to develop a model capable of delivering high-quality translations that are accurate and contextually appropriate. By leveraging large language models, specifically comparing the effectiveness of Decoder-only and Encoder-Decoder architectures, the project seeks to optimize translation performance and efficiency across multilingual contexts. Through rigorous experimentation and analysis, this project aims to advance the field of machine translation, contributing valuable insights into the effectiveness of different model architectures and paving the way for enhanced cross-linguistic communication tools.",
      "authors": [
        "Abhinav P. M.",
        "SujayKumar Reddy M",
        "Oswald Christopher"
      ],
      "published": "2024-09-12T00:21:05Z",
      "updated": "2024-09-12T00:21:05Z",
      "categories": [
        "cs.CL",
        "cs.ET",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.13747v1",
      "landing_url": "https://arxiv.org/abs/2409.13747v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.13747"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The project focuses on machine translation for Indian languages using decoder vs encoder-decoder LLMs and does not address discrete audio token generation, quantization, or token-level modeling, so it fails every inclusion criterion and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The project focuses on machine translation for Indian languages using decoder vs encoder-decoder LLMs and does not address discrete audio token generation, quantization, or token-level modeling, so it fails every inclusion criterion and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder",
    "abstract": "Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology improves patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a linguistic analysis. We present practical ASR end-to-end training schemes optimized for a fixed number of trainable parameters that are common in industry settings. All code, data, and models are available online: https://github.com/leduckhai/MultiMed/tree/master/MultiMed.",
    "metadata": {
      "arxiv_id": "2409.14074",
      "title": "MultiMed: Multilingual Medical Speech Recognition via Attention Encoder Decoder",
      "summary": "Multilingual automatic speech recognition (ASR) in the medical domain serves as a foundational task for various downstream applications such as speech translation, spoken language understanding, and voice-activated assistants. This technology improves patient care by enabling efficient communication across language barriers, alleviating specialized workforce shortages, and facilitating improved diagnosis and treatment, particularly during pandemics. In this work, we introduce MultiMed, the first multilingual medical ASR dataset, along with the first collection of small-to-large end-to-end medical ASR models, spanning five languages: Vietnamese, English, German, French, and Mandarin Chinese. To our best knowledge, MultiMed stands as the world's largest medical ASR dataset across all major benchmarks: total duration, number of recording conditions, number of accents, and number of speaking roles. Furthermore, we present the first multilinguality study for medical ASR, which includes reproducible empirical baselines, a monolinguality-multilinguality analysis, Attention Encoder Decoder (AED) vs Hybrid comparative study and a linguistic analysis. We present practical ASR end-to-end training schemes optimized for a fixed number of trainable parameters that are common in industry settings. All code, data, and models are available online: https://github.com/leduckhai/MultiMed/tree/master/MultiMed.",
      "authors": [
        "Khai Le-Duc",
        "Phuc Phan",
        "Tan-Hanh Pham",
        "Bach Phan Tat",
        "Minh-Huong Ngo",
        "Chris Ngo",
        "Thanh Nguyen-Tang",
        "Truong-Son Hy"
      ],
      "published": "2024-09-21T09:05:48Z",
      "updated": "2025-05-15T04:35:00Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14074v3",
      "landing_url": "https://arxiv.org/abs/2409.14074v3",
      "doi": "https://doi.org/10.48550/arXiv.2409.14074"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on multilingual medical ASR dataset/models without introducing or evaluating discrete audio tokenization/quantization, so it fails to meet the inclusion criteria for discrete token research and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on multilingual medical ASR dataset/models without introducing or evaluating discrete audio tokenization/quantization, so it fails to meet the inclusion criteria for discrete token research and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
    "abstract": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
    "metadata": {
      "arxiv_id": "2409.14085",
      "title": "Codec-SUPERB @ SLT 2024: A lightweight benchmark for neural audio codec models",
      "summary": "Neural audio codec models are becoming increasingly important as they serve as tokenizers for audio, enabling efficient transmission or facilitating speech language modeling. The ideal neural audio codec should maintain content, paralinguistics, speaker characteristics, and audio information even at low bitrates. Recently, numerous advanced neural codec models have been proposed. However, codec models are often tested under varying experimental conditions. As a result, we introduce the Codec-SUPERB challenge at SLT 2024, designed to facilitate fair and lightweight comparisons among existing codec models and inspire advancements in the field. This challenge brings together representative speech applications and objective metrics, and carefully selects license-free datasets, sampling them into small sets to reduce evaluation computation costs. This paper presents the challenge's rules, datasets, five participant systems, results, and findings.",
      "authors": [
        "Haibin Wu",
        "Xuanjun Chen",
        "Yi-Cheng Lin",
        "Kaiwei Chang",
        "Jiawei Du",
        "Ke-Han Lu",
        "Alexander H. Liu",
        "Ho-Lam Chung",
        "Yuan-Kuei Wu",
        "Dongchao Yang",
        "Songxiang Liu",
        "Yi-Chiao Wu",
        "Xu Tan",
        "James Glass",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-09-21T09:39:36Z",
      "updated": "2024-09-21T09:39:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.14085v1",
      "landing_url": "https://arxiv.org/abs/2409.14085v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.14085"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Challenge introduces standardized evaluation of neural audio codec models—core discrete codec token generation topic with reconstruction metrics—so better to include."
    },
    "round-A_JuniorNano_reasoning": "Challenge introduces standardized evaluation of neural audio codec models—core discrete codec token generation topic with reconstruction metrics—so better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task",
    "abstract": "Recent studies have showcased remarkable capabilities of decoder-only models in many NLP tasks, including translation. Yet, the machine translation field has been largely dominated by encoder-decoder models based on the Transformer architecture. As a consequence, scaling laws of encoder-decoder models for neural machine translation have already been well studied, but decoder-only models have received less attention. This work explores the scaling laws of decoder-only models on the multilingual and multidomain translation task. We trained a collection of six decoder-only models, ranging from 70M to 7B parameters, on a sentence-level, multilingual and multidomain dataset. We conducted a series of experiments showing that the loss of decoder-only models can be estimated using a scaling law similar to the one discovered for large language models, but we also show that this scaling law has difficulties to generalize to too large models or to a different data distribution. We also study different scaling methods and show that scaling the depth and the width of a model lead to similar test loss improvements, but with different impact on the model's efficiency.",
    "metadata": {
      "arxiv_id": "2409.15051",
      "title": "Scaling Laws of Decoder-Only Models on the Multilingual Machine Translation Task",
      "summary": "Recent studies have showcased remarkable capabilities of decoder-only models in many NLP tasks, including translation. Yet, the machine translation field has been largely dominated by encoder-decoder models based on the Transformer architecture. As a consequence, scaling laws of encoder-decoder models for neural machine translation have already been well studied, but decoder-only models have received less attention. This work explores the scaling laws of decoder-only models on the multilingual and multidomain translation task. We trained a collection of six decoder-only models, ranging from 70M to 7B parameters, on a sentence-level, multilingual and multidomain dataset. We conducted a series of experiments showing that the loss of decoder-only models can be estimated using a scaling law similar to the one discovered for large language models, but we also show that this scaling law has difficulties to generalize to too large models or to a different data distribution. We also study different scaling methods and show that scaling the depth and the width of a model lead to similar test loss improvements, but with different impact on the model's efficiency.",
      "authors": [
        "Gaëtan Caillaut",
        "Raheel Qader",
        "Mariam Nakhlé",
        "Jingshu Liu",
        "Jean-Gabriel Barthélemy"
      ],
      "published": "2024-09-23T14:26:01Z",
      "updated": "2024-09-23T14:26:01Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15051v1",
      "landing_url": "https://arxiv.org/abs/2409.15051v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15051"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reason: the paper studies decoder-only multilingual translation scaling rather than any discrete audio token/tokenizer design or evaluation, so it fails the inclusion criteria and meets the exclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Reason: the paper studies decoder-only multilingual translation scaling rather than any discrete audio token/tokenizer design or evaluation, so it fails the inclusion criteria and meets the exclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
    "abstract": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
    "metadata": {
      "arxiv_id": "2409.15741",
      "title": "StyleFusion TTS: Multimodal Style-control and Enhanced Feature Fusion for Zero-shot Text-to-speech Synthesis",
      "summary": "We introduce StyleFusion-TTS, a prompt and/or audio referenced, style and speaker-controllable, zero-shot text-to-speech (TTS) synthesis system designed to enhance the editability and naturalness of current research literature. We propose a general front-end encoder as a compact and effective module to utilize multimodal inputs including text prompts, audio references, and speaker timbre references in a fully zero-shot manner and produce disentangled style and speaker control embeddings. Our novel approach also leverages a hierarchical conformer structure for the fusion of style and speaker control embeddings, aiming to achieve optimal feature fusion within the current advanced TTS architecture. StyleFusion-TTS is evaluated through multiple metrics, both subjectively and objectively. The system shows promising performance across our evaluations, suggesting its potential to contribute to the advancement of the field of zero-shot text-to-speech synthesis.",
      "authors": [
        "Zhiyong Chen",
        "Xinnuo Li",
        "Zhiqi Ai",
        "Shugong Xu"
      ],
      "published": "2024-09-24T04:55:17Z",
      "updated": "2024-09-24T04:55:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.15741v1",
      "landing_url": "https://arxiv.org/abs/2409.15741v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.15741"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes a zero-shot TTS system with multimodal style/speaker control but never mentions discrete audio tokens, quantization, or tokenizer/codebook design, so it fails to satisfy the inclusion focus and matches exclusion of continuous-feature focus."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a zero-shot TTS system with multimodal style/speaker control but never mentions discrete audio tokens, quantization, or tokenizer/codebook design, so it fails to satisfy the inclusion focus and matches exclusion of continuous-feature focus.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates",
    "abstract": "This paper introduces FlowMAC, a novel neural audio codec for high-quality general audio compression at low bit rates based on conditional flow matching (CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder. At inference time the decoder integrates a continuous normalizing flow via an ODE solver to generate a high-quality mel spectrogram. This is the first time that a CFM-based approach is applied to general audio coding, enabling a scalable, simple and memory efficient training. Our subjective evaluations show that FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based and DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC offers a tunable inference pipeline, which permits to trade off complexity and quality. This enables real-time coding on CPU, while maintaining high perceptual quality.",
    "metadata": {
      "arxiv_id": "2409.17635",
      "title": "FlowMAC: Conditional Flow Matching for Audio Coding at Low Bit Rates",
      "summary": "This paper introduces FlowMAC, a novel neural audio codec for high-quality general audio compression at low bit rates based on conditional flow matching (CFM). FlowMAC jointly learns a mel spectrogram encoder, quantizer and decoder. At inference time the decoder integrates a continuous normalizing flow via an ODE solver to generate a high-quality mel spectrogram. This is the first time that a CFM-based approach is applied to general audio coding, enabling a scalable, simple and memory efficient training. Our subjective evaluations show that FlowMAC at 3 kbps achieves similar quality as state-of-the-art GAN-based and DDPM-based neural audio codecs at double the bit rate. Moreover, FlowMAC offers a tunable inference pipeline, which permits to trade off complexity and quality. This enables real-time coding on CPU, while maintaining high perceptual quality.",
      "authors": [
        "Nicola Pia",
        "Martin Strauss",
        "Markus Multrus",
        "Bernd Edler"
      ],
      "published": "2024-09-26T08:32:31Z",
      "updated": "2025-04-06T15:52:53Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.17635v2",
      "landing_url": "https://arxiv.org/abs/2409.17635v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10888898"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "FlowMAC targets neural audio coding but the abstract never defines discrete token vocabularies or token-level modeling, so it fails the discrete audio token focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "FlowMAC targets neural audio coding but the abstract never defines discrete token vocabularies or token-level modeling, so it fails the discrete audio token focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
    "abstract": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
    "metadata": {
      "arxiv_id": "2409.19283",
      "title": "Analyzing and Mitigating Inconsistency in Discrete Audio Tokens for Neural Codec Language Models",
      "summary": "Building upon advancements in Large Language Models (LLMs), the field of audio processing has seen increased interest in training audio generation tasks with discrete audio token sequences. However, directly discretizing audio by neural audio codecs often results in sequences that fundamentally differ from text sequences. Unlike text, where text token sequences are deterministic, discrete audio tokens can exhibit significant variability based on contextual factors, while still producing perceptually identical audio segments. We refer to this phenomenon as \\textbf{Discrete Representation Inconsistency (DRI)}. This inconsistency can lead to a single audio segment being represented by multiple divergent sequences, which creates confusion in neural codec language models and results in omissions and repetitions during speech generation. In this paper, we quantitatively analyze the DRI phenomenon within popular audio tokenizers such as EnCodec. Our approach effectively mitigates the DRI phenomenon of the neural audio codec. Furthermore, extensive experiments on the neural codec language model over LibriTTS and large-scale MLS datases (44,000 hours) demonstrate the effectiveness and generality of our method. The demo of audio samples is available online~\\footnote{\\url{https://consistencyinneuralcodec.github.io}}.",
      "authors": [
        "Wenrui Liu",
        "Zhifang Guo",
        "Jin Xu",
        "Yuanjun Lv",
        "Yunfei Chu",
        "Zhou Zhao",
        "Junyang Lin"
      ],
      "published": "2024-09-28T08:36:44Z",
      "updated": "2024-10-04T22:34:38Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19283v2",
      "landing_url": "https://arxiv.org/abs/2409.19283v2",
      "doi": "https://doi.org/10.48550/arXiv.2409.19283"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract focus on discrete audio tokens from neural codecs, analyze token inconsistency, describe mitigation and evaluations on LibriTTS/MLS, so they clearly center on finite-vocabulary sequences with codec-based tokens and include quality experiments, satisfying the inclusion requirements without hitting exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on discrete audio tokens from neural codecs, analyze token inconsistency, describe mitigation and evaluations on LibriTTS/MLS, so they clearly center on finite-vocabulary sequences with codec-based tokens and include quality experiments, satisfying the inclusion requirements without hitting exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PALM: Few-Shot Prompt Learning for Audio Language Models",
    "abstract": "Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio recognition tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio recognition datasets, encompassing a variety of speech-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Code is available at https://asif-hanif.github.io/palm/",
    "metadata": {
      "arxiv_id": "2409.19806",
      "title": "PALM: Few-Shot Prompt Learning for Audio Language Models",
      "summary": "Audio-Language Models (ALMs) have recently achieved remarkable success in zero-shot audio recognition tasks, which match features of audio waveforms with class-specific text prompt features, inspired by advancements in Vision-Language Models (VLMs). Given the sensitivity of zero-shot performance to the choice of hand-crafted text prompts, many prompt learning techniques have been developed for VLMs. We explore the efficacy of these approaches in ALMs and propose a novel method, Prompt Learning in Audio Language Models (PALM), which optimizes the feature space of the text encoder branch. Unlike existing methods that work in the input space, our approach results in greater training efficiency. We demonstrate the effectiveness of our approach on 11 audio recognition datasets, encompassing a variety of speech-processing tasks, and compare the results with three baselines in a few-shot learning setup. Our method is either on par with or outperforms other approaches while being computationally less demanding. Code is available at https://asif-hanif.github.io/palm/",
      "authors": [
        "Asif Hanif",
        "Maha Tufail Agro",
        "Mohammad Areeb Qazi",
        "Hanan Aldarmaki"
      ],
      "published": "2024-09-29T22:06:07Z",
      "updated": "2024-09-29T22:06:07Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2409.19806v1",
      "landing_url": "https://arxiv.org/abs/2409.19806v1",
      "doi": "https://doi.org/10.48550/arXiv.2409.19806"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on prompt learning for audio-language matching without introducing or analyzing any discrete audio tokens/tokenizers, so it fails the inclusion scope and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on prompt learning for audio-language matching without introducing or analyzing any discrete audio tokens/tokenizers, so it fails the inclusion scope and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
    "abstract": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
    "metadata": {
      "arxiv_id": "2410.00025",
      "title": "Improving Spoken Language Modeling with Phoneme Classification: A Simple Fine-tuning Approach",
      "summary": "Recent progress in Spoken Language Modeling has shown that learning language directly from speech is feasible. Generating speech through a pipeline that operates at the text level typically loses nuances, intonations, and non-verbal vocalizations. Modeling directly from speech opens up the path to more natural and expressive systems. On the other hand, speech-only systems require up to three orders of magnitude more data to catch up to their text-based counterparts in terms of their semantic abilities. We show that fine-tuning speech representation models on phoneme classification leads to more context-invariant representations, and language models trained on these units achieve comparable lexical comprehension to ones trained on hundred times more data.",
      "authors": [
        "Maxime Poli",
        "Emmanuel Chemla",
        "Emmanuel Dupoux"
      ],
      "published": "2024-09-16T10:29:15Z",
      "updated": "2024-10-30T17:46:22Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00025v2",
      "landing_url": "https://arxiv.org/abs/2410.00025v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00025"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The abstract focuses on fine-tuning speech representations via phoneme classification without discussing discrete audio token quantization, vocabularies, or token-level evaluation, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on fine-tuning speech representations via phoneme classification without discussing discrete audio token quantization, vocabularies, or token-level evaluation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Moshi: a speech-text foundation model for real-time dialogue",
    "abstract": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
    "metadata": {
      "arxiv_id": "2410.00037",
      "title": "Moshi: a speech-text foundation model for real-time dialogue",
      "summary": "We introduce Moshi, a speech-text foundation model and full-duplex spoken dialogue framework. Current systems for spoken dialogue rely on pipelines of independent components, namely voice activity detection, speech recognition, textual dialogue and text-to-speech. Such frameworks cannot emulate the experience of real conversations. First, their complexity induces a latency of several seconds between interactions. Second, text being the intermediate modality for dialogue, non-linguistic information that modifies meaning -- such as emotion or non-speech sounds -- is lost in the interaction. Finally, they rely on a segmentation into speaker turns, which does not take into account overlapping speech, interruptions and interjections. Moshi solves these independent issues altogether by casting spoken dialogue as speech-to-speech generation. Starting from a text language model backbone, Moshi generates speech as tokens from the residual quantizer of a neural audio codec, while modeling separately its own speech and that of the user into parallel streams. This allows for the removal of explicit speaker turns, and the modeling of arbitrary conversational dynamics. We moreover extend the hierarchical semantic-to-acoustic token generation of previous work to first predict time-aligned text tokens as a prefix to audio tokens. Not only this \"Inner Monologue\" method significantly improves the linguistic quality of generated speech, but we also illustrate how it can provide streaming speech recognition and text-to-speech. Our resulting model is the first real-time full-duplex spoken large language model, with a theoretical latency of 160ms, 200ms in practice, and is available at https://github.com/kyutai-labs/moshi.",
      "authors": [
        "Alexandre Défossez",
        "Laurent Mazaré",
        "Manu Orsini",
        "Amélie Royer",
        "Patrick Pérez",
        "Hervé Jégou",
        "Edouard Grave",
        "Neil Zeghidour"
      ],
      "published": "2024-09-17T17:55:39Z",
      "updated": "2024-10-02T09:11:45Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.00037v2",
      "landing_url": "https://arxiv.org/abs/2410.00037v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.00037"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Moshi explicitly builds on discrete neural-audio codec tokens and models their generation for full-duplex speech dialogue, matching the discrete audio token focus and evaluation requirements."
    },
    "round-A_JuniorNano_reasoning": "Moshi explicitly builds on discrete neural-audio codec tokens and models their generation for full-duplex speech dialogue, matching the discrete audio token focus and evaluation requirements.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "On The Adaptation of Unlimiformer for Decoder-Only Transformers",
    "abstract": "One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&A and instruction-tuned models.",
    "metadata": {
      "arxiv_id": "2410.01637",
      "title": "On The Adaptation of Unlimiformer for Decoder-Only Transformers",
      "summary": "One of the prominent issues stifling the current generation of large language models is their limited context length. Recent proprietary models such as GPT-4 and Claude 2 have introduced longer context lengths, 8k/32k and 100k, respectively; however, despite the efforts in the community, most common models, such as LLama-2, have a context length of 4k or less. Unlimiformer (Bertsch et al., 2023) is a recently popular vector-retrieval augmentation method that offloads cross-attention computations to a kNN index. However, its main limitation is incompatibility with decoder-only transformers out of the box. In this work, we explore practical considerations of adapting Unlimiformer to decoder-only transformers and introduce a series of modifications to overcome this limitation. Moreover, we expand the original experimental setup on summarization to include a new task (i.e., free-form Q&A) and an instruction-tuned model (i.e., a custom 6.7B GPT model). Our results showcase the effectiveness of these modifications on summarization, performing on par with a model with 2x the context length. Moreover, we discuss limitations and future directions for free-form Q&A and instruction-tuned models.",
      "authors": [
        "Kian Ahrabian",
        "Alon Benhaim",
        "Barun Patra",
        "Jay Pujara",
        "Saksham Singhal",
        "Xia Song"
      ],
      "published": "2024-10-02T15:08:12Z",
      "updated": "2024-10-02T15:08:12Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01637v1",
      "landing_url": "https://arxiv.org/abs/2410.01637v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.01637"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on adapting Unlimiformer for decoder-only Transformers and general NLP tasks, which has no relation to discrete audio token generation, quantization, or evaluation, so it fails to meet the inclusion criteria while matching exclusion points."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adapting Unlimiformer for decoder-only Transformers and general NLP tasks, which has no relation to discrete audio token generation, quantization, or evaluation, so it fails to meet the inclusion criteria while matching exclusion points.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
    "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
    "metadata": {
      "arxiv_id": "2410.01912",
      "title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation",
      "summary": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. Code, datasets and models are open at https://github.com/chenllliang/DnD-Transformer.",
      "authors": [
        "Liang Chen",
        "Sinan Tan",
        "Zefan Cai",
        "Weichu Xie",
        "Haozhe Zhao",
        "Yichi Zhang",
        "Junyang Lin",
        "Jinze Bai",
        "Tianyu Liu",
        "Baobao Chang"
      ],
      "published": "2024-10-02T18:10:05Z",
      "updated": "2024-10-02T18:10:05Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.01912v1",
      "landing_url": "https://arxiv.org/abs/2410.01912v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.01912"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study centers on vision-language image generation with Transformer models rather than any discrete audio token generation/quantization or tokenizer evaluation, so it fails to meet the inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study centers on vision-language image generation with Transformer models rather than any discrete audio token generation/quantization or tokenizer evaluation, so it fails to meet the inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
    "abstract": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
    "metadata": {
      "arxiv_id": "2410.03298",
      "title": "Textless Streaming Speech-to-Speech Translation using Semantic Speech Tokens",
      "summary": "Cascaded speech-to-speech translation systems often suffer from the error accumulation problem and high latency, which is a result of cascaded modules whose inference delays accumulate. In this paper, we propose a transducer-based speech translation model that outputs discrete speech tokens in a low-latency streaming fashion. This approach eliminates the need for generating text output first, followed by machine translation (MT) and text-to-speech (TTS) systems. The produced speech tokens can be directly used to generate a speech signal with low latency by utilizing an acoustic language model (LM) to obtain acoustic tokens and an audio codec model to retrieve the waveform. Experimental results show that the proposed method outperforms other existing approaches and achieves state-of-the-art results for streaming translation in terms of BLEU, average latency, and BLASER 2.0 scores for multiple language pairs using the CVSS-C dataset as a benchmark.",
      "authors": [
        "Jinzheng Zhao",
        "Niko Moritz",
        "Egor Lakomkin",
        "Ruiming Xie",
        "Zhiping Xiu",
        "Katerina Zmolikova",
        "Zeeshan Ahmed",
        "Yashesh Gaur",
        "Duc Le",
        "Christian Fuegen"
      ],
      "published": "2024-10-04T10:21:15Z",
      "updated": "2024-10-04T10:21:15Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03298v1",
      "landing_url": "https://arxiv.org/abs/2410.03298v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03298"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The work proposes streaming speech-to-speech translation using discrete semantic speech tokens and codec-generated acoustic tokens with evaluations, satisfying the discrete-token topic and providing performance metrics, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The work proposes streaming speech-to-speech translation using discrete semantic speech tokens and codec-generated acoustic tokens with evaluations, satisfying the discrete-token topic and providing performance metrics, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generative Semantic Communication for Text-to-Speech Synthesis",
    "abstract": "Semantic communication is a promising technology to improve communication efficiency by transmitting only the semantic information of the source data. However, traditional semantic communication methods primarily focus on data reconstruction tasks, which may not be efficient for emerging generative tasks such as text-to-speech (TTS) synthesis. To address this limitation, this paper develops a novel generative semantic communication framework for TTS synthesis, leveraging generative artificial intelligence technologies. Firstly, we utilize a pre-trained large speech model called WavLM and the residual vector quantization method to construct two semantic knowledge bases (KBs) at the transmitter and receiver, respectively. The KB at the transmitter enables effective semantic extraction, while the KB at the receiver facilitates lifelike speech synthesis. Then, we employ a transformer encoder and a diffusion model to achieve efficient semantic coding without introducing significant communication overhead. Finally, numerical results demonstrate that our framework achieves much higher fidelity for the generated speech than four baselines, in both cases with additive white Gaussian noise channel and Rayleigh fading channel.",
    "metadata": {
      "arxiv_id": "2410.03459",
      "title": "Generative Semantic Communication for Text-to-Speech Synthesis",
      "summary": "Semantic communication is a promising technology to improve communication efficiency by transmitting only the semantic information of the source data. However, traditional semantic communication methods primarily focus on data reconstruction tasks, which may not be efficient for emerging generative tasks such as text-to-speech (TTS) synthesis. To address this limitation, this paper develops a novel generative semantic communication framework for TTS synthesis, leveraging generative artificial intelligence technologies. Firstly, we utilize a pre-trained large speech model called WavLM and the residual vector quantization method to construct two semantic knowledge bases (KBs) at the transmitter and receiver, respectively. The KB at the transmitter enables effective semantic extraction, while the KB at the receiver facilitates lifelike speech synthesis. Then, we employ a transformer encoder and a diffusion model to achieve efficient semantic coding without introducing significant communication overhead. Finally, numerical results demonstrate that our framework achieves much higher fidelity for the generated speech than four baselines, in both cases with additive white Gaussian noise channel and Rayleigh fading channel.",
      "authors": [
        "Jiahao Zheng",
        "Jinke Ren",
        "Peng Xu",
        "Zhihao Yuan",
        "Jie Xu",
        "Fangxin Wang",
        "Gui Gui",
        "Shuguang Cui"
      ],
      "published": "2024-10-04T14:18:31Z",
      "updated": "2024-10-04T14:18:31Z",
      "categories": [
        "cs.SD",
        "cs.IT",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.03459v1",
      "landing_url": "https://arxiv.org/abs/2410.03459v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.03459"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on generative semantic communication for TTS using WavLM and VQ-based semantic knowledge bases rather than describing a discrete audio-token/tokenizer design or vocabularized quantization strategy with token-level evaluations, so it fails to meet the discrete-token criteria and should likely be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on generative semantic communication for TTS using WavLM and VQ-based semantic knowledge bases rather than describing a discrete audio-token/tokenizer design or vocabularized quantization strategy with token-level evaluations, so it fails to meet the discrete-token criteria and should likely be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
    "abstract": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
    "metadata": {
      "arxiv_id": "2410.04029",
      "title": "SyllableLM: Learning Coarse Semantic Units for Speech Language Models",
      "summary": "Language models require tokenized inputs. However, tokenization strategies for continuous data like audio and vision are often based on simple heuristics such as fixed sized convolutions or discrete clustering, which do not necessarily align with the semantic structure of the data. For speech in particular, the high resolution of waveforms (16,000 samples/second or more) presents a significant challenge as speech-based language models have had to use several times more tokens per word than text-based language models. In this work, we introduce a controllable self-supervised technique to merge speech representations into coarser syllable-like units while still preserving semantic information. We do this by 1) extracting noisy boundaries through analyzing correlations in pretrained encoder losses and 2) iteratively improving model representations with a novel distillation technique. Our method produces controllable-rate semantic units at as low as 5Hz and 60bps and achieves SotA in syllabic segmentation and clustering. Using these coarse tokens, we successfully train SyllableLM, a Speech Language Model (SpeechLM) that matches or outperforms current SotA SpeechLMs on a range of spoken language modeling tasks. SyllableLM also achieves significant improvements in efficiency with a 30x reduction in training compute and a 4x wall-clock inference speedup.",
      "authors": [
        "Alan Baade",
        "Puyuan Peng",
        "David Harwath"
      ],
      "published": "2024-10-05T04:29:55Z",
      "updated": "2024-10-05T04:29:55Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04029v1",
      "landing_url": "https://arxiv.org/abs/2410.04029v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04029"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Introduces self-supervised method to merge speech into discrete syllable-like tokens and trains SpeechLM with clear quantized units and evaluations, matching the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Introduces self-supervised method to merge speech into discrete syllable-like tokens and trains SpeechLM with clear quantized units and evaluations, matching the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
    "abstract": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
    "metadata": {
      "arxiv_id": "2410.04380",
      "title": "HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis",
      "summary": "Recently, Text-to-speech (TTS) models based on large language models (LLMs) that translate natural language text into sequences of discrete audio tokens have gained great research attention, with advances in neural audio codec (NAC) models using residual vector quantization (RVQ). However, long-form speech synthesis remains a significant challenge due to the high frame rate, which increases the length of audio tokens and makes it difficult for autoregressive language models to generate audio tokens for even a minute of speech. To address this challenge, this paper introduces two novel post-training approaches: 1) Multi-Resolution Requantization (MReQ) and 2) HALL-E. MReQ is a framework to reduce the frame rate of pre-trained NAC models. Specifically, it incorporates multi-resolution residual vector quantization (MRVQ) module that hierarchically reorganizes discrete audio tokens through teacher-student distillation. HALL-E is an LLM-based TTS model designed to predict hierarchical tokens of MReQ. Specifically, it incorporates the technique of using MRVQ sub-modules and continues training from a pre-trained LLM-based TTS model. Furthermore, to promote TTS research, we create MinutesSpeech, a new benchmark dataset consisting of 40k hours of filtered speech data for training and evaluating speech synthesis ranging from 3s up to 180s. In experiments, we demonstrated the effectiveness of our approaches by applying our post-training framework to VALL-E. We achieved the frame rate down to as low as 8 Hz, enabling the stable minitue-long speech synthesis in a single inference step. Audio samples, dataset, codes and pre-trained models are available at https://yutonishimura-v2.github.io/HALL-E_DEMO/.",
      "authors": [
        "Yuto Nishimura",
        "Takumi Hirose",
        "Masanari Ohi",
        "Hideki Nakayama",
        "Nakamasa Inoue"
      ],
      "published": "2024-10-06T07:20:58Z",
      "updated": "2024-10-06T07:20:58Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.04380v1",
      "landing_url": "https://arxiv.org/abs/2410.04380v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.04380"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "LLM-based TTS leverages RVQ neural codec tokens, proposes MRVQ and hierarchical token modeling for minute-long speech, and presents detailed codec/token mechanisms so it clearly fits the discrete audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "LLM-based TTS leverages RVQ neural codec tokens, proposes MRVQ and hierarchical token modeling for minute-long speech, and presents detailed codec/token mechanisms so it clearly fits the discrete audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
    "abstract": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
    "metadata": {
      "arxiv_id": "2410.06016",
      "title": "Variable Bitrate Residual Vector Quantization for Audio Coding",
      "summary": "Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.",
      "authors": [
        "Yunkee Chae",
        "Woosung Choi",
        "Yuhta Takida",
        "Junghyun Koo",
        "Yukara Ikemiya",
        "Zhi Zhong",
        "Kin Wai Cheuk",
        "Marco A. Martínez-Ramírez",
        "Kyogu Lee",
        "Wei-Hsiang Liao",
        "Yuki Mitsufuji"
      ],
      "published": "2024-10-08T13:18:24Z",
      "updated": "2025-04-27T15:10:16Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06016v3",
      "landing_url": "https://arxiv.org/abs/2410.06016v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.06016"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The work adapts RVQ neural audio codec to generate discrete codebooks with per-frame bitrate adaptation and evaluation, so it clearly meets the discrete token inclusion criteria and merits a score of 5."
    },
    "round-A_JuniorNano_reasoning": "The work adapts RVQ neural audio codec to generate discrete codebooks with per-frame bitrate adaptation and evaluation, so it clearly meets the discrete token inclusion criteria and merits a score of 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Restructuring Vector Quantization with the Rotation Trick",
    "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
    "metadata": {
      "arxiv_id": "2410.06424",
      "title": "Restructuring Vector Quantization with the Rotation Trick",
      "summary": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. They operate by maintaining a set of vectors -- often referred to as the codebook -- and quantizing each encoder output to the nearest vector in the codebook. However, as vector quantization is non-differentiable, the gradient to the encoder flows around the vector quantization layer rather than through it in a straight-through approximation. This approximation may be undesirable as all information from the vector quantization operation is lost. In this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. We smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. As a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder. Across 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error. Our code is available at https://github.com/cfifty/rotation_trick.",
      "authors": [
        "Christopher Fifty",
        "Ronald G. Junkins",
        "Dennis Duan",
        "Aniketh Iyengar",
        "Jerry W. Liu",
        "Ehsan Amid",
        "Sebastian Thrun",
        "Christopher Ré"
      ],
      "published": "2024-10-08T23:39:34Z",
      "updated": "2025-03-16T03:30:10Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.06424v2",
      "landing_url": "https://arxiv.org/abs/2410.06424v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.06424"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on improving gradient flow through VQ-VAE quantization broadly, without explicit treatment of audio codecs or discrete audio token definitions, so it does not meet the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on improving gradient flow through VQ-VAE quantization broadly, without explicit treatment of audio codecs or discrete audio token definitions, so it does not meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
    "abstract": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
    "metadata": {
      "arxiv_id": "2410.07168",
      "title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio",
      "summary": "Syllables are compositional units of spoken language that efficiently structure human speech perception and production. However, current neural speech representations lack such structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised learning (SSL) framework that bootstraps syllabic embeddings by distilling from its own initial unsupervised syllabic segmentation. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) novel phonological units suited for efficient spoken language modeling. Our proposed segmentation method is highly robust and generalizes to out-of-domain data and unseen languages without any tuning. By training token-to-speech generative models, fully intelligible speech can be reconstructed from Sylber tokens with a significantly lower bitrate than baseline SSL tokens. This suggests that our model effectively compresses speech into a compact sequence of tokens with minimal information loss. Lastly, we demonstrate that categorical perception-a linguistic phenomenon in speech perception-emerges naturally in Sylber, making the embedding space more categorical and sparse than previous speech features and thus supporting the high efficiency of our tokenization. Together, we present a novel SSL approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.",
      "authors": [
        "Cheol Jun Cho",
        "Nicholas Lee",
        "Akshat Gupta",
        "Dhruv Agarwal",
        "Ethan Chen",
        "Alan W Black",
        "Gopala K. Anumanchipalli"
      ],
      "published": "2024-10-09T17:59:04Z",
      "updated": "2025-03-02T09:16:05Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.07168v2",
      "landing_url": "https://arxiv.org/abs/2410.07168v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.07168"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Sylber derives syllabic discrete tokens from raw audio via self-supervised segmentation, offers token-based reconstruction, and evaluates compression, so it clearly matches the inclusion requirements with no exclusion triggers, justifying a top score."
    },
    "round-A_JuniorNano_reasoning": "Sylber derives syllabic discrete tokens from raw audio via self-supervised segmentation, offers token-based reconstruction, and evaluates compression, so it clearly matches the inclusion requirements with no exclusion triggers, justifying a top score.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
    "abstract": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
    "metadata": {
      "arxiv_id": "2410.08325",
      "title": "Low Bitrate High-Quality RVQGAN-based Discrete Speech Tokenizer",
      "summary": "Discrete Audio codecs (or audio tokenizers) have recently regained interest due to the ability of Large Language Models (LLMs) to learn their compressed acoustic representations. Various publicly available trainable discrete tokenizers recently demonstrated impressive results for audio tokenization, yet they mostly require high token rates to gain high-quality reconstruction. In this study, we fine-tuned an open-source general audio RVQGAN model using diverse open-source speech data, considering various recording conditions and quality levels. The resulting wideband (24kHz) speech-only model achieves speech reconstruction, which is nearly indistinguishable from PCM (pulse-code modulation) with a rate of 150-300 tokens per second (1500-3000 bps). The evaluation used comprehensive English speech data encompassing different recording conditions, including studio settings. Speech samples are made publicly available in http://ibm.biz/IS24SpeechRVQ . The model is officially released in https://huggingface.co/ibm/DAC.speech.v1.0",
      "authors": [
        "Slava Shechtman",
        "Avihu Dekel"
      ],
      "published": "2024-10-10T19:29:05Z",
      "updated": "2024-10-10T19:29:05Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08325v1",
      "landing_url": "https://arxiv.org/abs/2410.08325v1",
      "doi": "https://doi.org/10.21437/Interspeech.2024-2366"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The study focuses on RVQGAN discrete audio tokens for speech reconstruction, reports token rate, evaluations, and publicly released model/samples, so it satisfies all discrete-token inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on RVQGAN discrete audio tokens for speech reconstruction, reports token rate, evaluations, and publicly released model/samples, so it satisfies all discrete-token inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
    "metadata": {
      "arxiv_id": "2410.08469",
      "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
      "summary": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweighting to build Interpretable text embeddings (SToRI), which incorporates controllability as well. SToRI refines the text encoding process in CLIP by differentially weighting semantic elements based on contextual importance, enabling finer control over emphasis responsive to data-driven insights and user preferences. The efficacy of SToRI is demonstrated through comprehensive experiments on few-shot image classification and image retrieval tailored to user preferences.",
      "authors": [
        "Eunji Kim",
        "Kyuhong Shim",
        "Simyung Chang",
        "Sungroh Yoon"
      ],
      "published": "2024-10-11T02:42:13Z",
      "updated": "2024-10-16T14:09:14Z",
      "categories": [
        "cs.LG",
        "cs.CL",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
      "landing_url": "https://arxiv.org/abs/2410.08469v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.08469"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on text embedding improvements for CLIP (vision-language) with no mention of discrete audio tokenization, so it fails the audio-token inclusion criteria and should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on text embedding improvements for CLIP (vision-language) with no mention of discrete audio tokenization, so it fails the audio-token inclusion criteria and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
    "abstract": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
    "metadata": {
      "arxiv_id": "2410.10180",
      "title": "Gaussian Mixture Vector Quantization with Aggregated Categorical Posterior",
      "summary": "The vector quantization is a widely used method to map continuous representation to discrete space and has important application in tokenization for generative mode, bottlenecking information and many other tasks in machine learning. Vector Quantized Variational Autoencoder (VQ-VAE) is a type of variational autoencoder using discrete embedding as latent. We generalize the technique further, enriching the probabilistic framework with a Gaussian mixture as the underlying generative model. This framework leverages a codebook of latent means and adaptive variances to capture complex data distributions. This principled framework avoids various heuristics and strong assumptions that are needed with the VQ-VAE to address training instability and to improve codebook utilization. This approach integrates the benefits of both discrete and continuous representations within a variational Bayesian framework. Furthermore, by introducing the \\textit{Aggregated Categorical Posterior Evidence Lower Bound} (ALBO), we offer a principled alternative optimization objective that aligns variational distributions with the generative model. Our experiments demonstrate that GM-VQ improves codebook utilization and reduces information loss without relying on handcrafted heuristics.",
      "authors": [
        "Mingyuan Yan",
        "Jiawei Wu",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-10-14T05:58:11Z",
      "updated": "2024-10-14T05:58:11Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.10180v1",
      "landing_url": "https://arxiv.org/abs/2410.10180v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.10180"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses Gaussian mixture VQ for general ML without any mention of discrete audio tokenizers/codecs or audio-specific evaluations, so it fails the audio discrete token inclusion and is best excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses Gaussian mixture VQ for general ML without any mention of discrete audio tokenizers/codecs or audio-specific evaluations, so it fails the audio discrete token inclusion and is best excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
    "abstract": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
    "metadata": {
      "arxiv_id": "2410.11025",
      "title": "Code Drift: Towards Idempotent Neural Audio Codecs",
      "summary": "Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.",
      "authors": [
        "Patrick O'Reilly",
        "Prem Seetharaman",
        "Jiaqi Su",
        "Zeyu Jin",
        "Bryan Pardo"
      ],
      "published": "2024-10-14T19:21:28Z",
      "updated": "2025-04-14T23:07:19Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.11025v2",
      "landing_url": "https://arxiv.org/abs/2410.11025v2",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890096"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Fits focus on discrete neural audio codec tokens with codec-specific fine-tuning and evaluation of reconstruction/modeling impact, so it satisfies all inclusion and no exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Fits focus on discrete neural audio codec tokens with codec-specific fine-tuning and evaluation of reconstruction/modeling impact, so it satisfies all inclusion and no exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
    "abstract": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
    "metadata": {
      "arxiv_id": "2410.12359",
      "title": "ERVQ: Enhanced Residual Vector Quantization with Intra-and-Inter-Codebook Optimization for Neural Audio Codecs",
      "summary": "Current neural audio codecs typically use residual vector quantization (RVQ) to discretize speech signals. However, they often experience codebook collapse, which reduces the effective codebook size and leads to suboptimal performance. To address this problem, we introduce ERVQ, Enhanced Residual Vector Quantization, a novel enhancement strategy for the RVQ framework in neural audio codecs. ERVQ mitigates codebook collapse and boosts codec performance through both intra- and inter-codebook optimization. Intra-codebook optimization incorporates an online clustering strategy and a code balancing loss to ensure balanced and efficient codebook utilization. Inter-codebook optimization improves the diversity of quantized features by minimizing the similarity between successive quantizations. Our experiments show that ERVQ significantly enhances audio codec performance across different models, sampling rates, and bitrates, achieving superior quality and generalization capabilities. It also achieves 100% codebook utilization on one of the most advanced neural audio codecs. Further experiments indicate that audio codecs improved by the ERVQ strategy can improve unified speech-and-text large language models (LLMs). Specifically, there is a notable improvement in the naturalness of generated speech in downstream zero-shot text-to-speech tasks. Audio samples are available here.",
      "authors": [
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-16T08:21:37Z",
      "updated": "2025-06-11T08:43:47Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.12359v2",
      "landing_url": "https://arxiv.org/abs/2410.12359v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.12359"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "ERVQ targets discrete neural audio codec tokens by optimizing codebooks and quantization with measurable codec quality gains, so it fully matches the inclusion criteria without raising any exclusions."
    },
    "round-A_JuniorNano_reasoning": "ERVQ targets discrete neural audio codec tokens by optimizing codebooks and quantization with measurable codec quality gains, so it fully matches the inclusion criteria without raising any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Learning Graph Quantized Tokenizers",
    "abstract": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
    "metadata": {
      "arxiv_id": "2410.13798",
      "title": "Learning Graph Quantized Tokenizers",
      "summary": "Transformers serve as the backbone architectures of Foundational Models, where domain-specific tokenizers allow them to adapt to various domains. Graph Transformers (GTs) have recently emerged as leading models in geometric deep learning, outperforming Graph Neural Networks (GNNs) in various graph learning tasks. However, the development of tokenizers for graphs has lagged behind other modalities. To address this, we introduce GQT (\\textbf{G}raph \\textbf{Q}uantized \\textbf{T}okenizer), which decouples tokenizer training from Transformer training by leveraging multi-task graph self-supervised learning, yielding robust and generalizable graph tokens. Furthermore, the GQT utilizes Residual Vector Quantization (RVQ) to learn hierarchical discrete tokens, resulting in significantly reduced memory requirements and improved generalization capabilities. By combining the GQT with token modulation, a Transformer encoder achieves state-of-the-art performance on 20 out of 22 benchmarks, including large-scale homophilic and heterophilic datasets.",
      "authors": [
        "Limei Wang",
        "Kaveh Hassani",
        "Si Zhang",
        "Dongqi Fu",
        "Baichuan Yuan",
        "Weilin Cong",
        "Zhigang Hua",
        "Hao Wu",
        "Ning Yao",
        "Bo Long"
      ],
      "published": "2024-10-17T17:38:24Z",
      "updated": "2025-04-02T03:04:44Z",
      "categories": [
        "cs.NE",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.13798v2",
      "landing_url": "https://arxiv.org/abs/2410.13798v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.13798"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on graph tokenization for transformers, not on discrete audio tokens or codecs, so it fails the inclusion criteria and hits the exclusion rule about non-audio tokens."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on graph tokenization for transformers, not on discrete audio tokens or codecs, so it fails the inclusion criteria and hits the exclusion rule about non-audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SNAC: Multi-Scale Neural Audio Codec",
    "abstract": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
    "metadata": {
      "arxiv_id": "2410.14411",
      "title": "SNAC: Multi-Scale Neural Audio Codec",
      "summary": "Neural audio codecs have recently gained popularity because they can represent audio signals with high fidelity at very low bitrates, making it feasible to use language modeling approaches for audio generation and understanding. Residual Vector Quantization (RVQ) has become the standard technique for neural audio compression using a cascade of VQ codebooks. This paper proposes the Multi-Scale Neural Audio Codec, a simple extension of RVQ where the quantizers can operate at different temporal resolutions. By applying a hierarchy of quantizers at variable frame rates, the codec adapts to the audio structure across multiple timescales. This leads to more efficient compression, as demonstrated by extensive objective and subjective evaluations. The code and model weights are open-sourced at https://github.com/hubertsiuzdak/snac.",
      "authors": [
        "Hubert Siuzdak",
        "Florian Grötschla",
        "Luca A. Lanzendörfer"
      ],
      "published": "2024-10-18T12:24:05Z",
      "updated": "2024-10-18T12:24:05Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14411v1",
      "landing_url": "https://arxiv.org/abs/2410.14411v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.14411"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper presents a multi-scale RVQ neural audio codec that explicitly generates discrete codebook tokens and evaluates their reconstruction efficiency, so it satisfies the discrete-token inclusion criteria without any exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "The paper presents a multi-scale RVQ neural audio codec that explicitly generates discrete codebook tokens and evaluates their reconstruction efficiency, so it satisfies the discrete-token inclusion criteria without any exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step",
    "abstract": "Score identity Distillation (SiD) is a data-free method that has achieved SOTA performance in image generation by leveraging only a pretrained diffusion model, without requiring any training data. However, its ultimate performance is constrained by how accurate the pretrained model captures the true data scores at different stages of the diffusion process. In this paper, we introduce SiDA (SiD with Adversarial Loss), which not only enhances generation quality but also improves distillation efficiency by incorporating real images and adversarial loss. SiDA utilizes the encoder from the generator's score network as a discriminator, allowing it to distinguish between real images and those generated by SiD. The adversarial loss is batch-normalized within each GPU and then combined with the original SiD loss. This integration effectively incorporates the average \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA to distill a single-step generator. SiDA converges significantly faster than its predecessor when distilled from scratch, and swiftly improves upon the original model's performance during fine-tuning from a pre-distilled SiD generator. This one-step adversarial distillation method establishes new benchmarks in generation performance when distilling EDM diffusion models, achieving FID scores of 1.110 on ImageNet 64x64. When distilling EDM2 models trained on ImageNet 512x512, our SiDA method surpasses even the largest teacher model, EDM2-XXL, which achieved an FID of 1.81 using classifier-free guidance (CFG) and 63 generation steps. In contrast, SiDA achieves FID scores of 2.156 for size XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and 1.366 for XXL, all without CFG and in a single generation step. These results highlight substantial improvements across all model sizes. Our code is available at https://github.com/mingyuanzhou/SiD/tree/sida.",
    "metadata": {
      "arxiv_id": "2410.14919",
      "title": "Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step",
      "summary": "Score identity Distillation (SiD) is a data-free method that has achieved SOTA performance in image generation by leveraging only a pretrained diffusion model, without requiring any training data. However, its ultimate performance is constrained by how accurate the pretrained model captures the true data scores at different stages of the diffusion process. In this paper, we introduce SiDA (SiD with Adversarial Loss), which not only enhances generation quality but also improves distillation efficiency by incorporating real images and adversarial loss. SiDA utilizes the encoder from the generator's score network as a discriminator, allowing it to distinguish between real images and those generated by SiD. The adversarial loss is batch-normalized within each GPU and then combined with the original SiD loss. This integration effectively incorporates the average \"fakeness\" per GPU batch into the pixel-based SiD loss, enabling SiDA to distill a single-step generator. SiDA converges significantly faster than its predecessor when distilled from scratch, and swiftly improves upon the original model's performance during fine-tuning from a pre-distilled SiD generator. This one-step adversarial distillation method establishes new benchmarks in generation performance when distilling EDM diffusion models, achieving FID scores of 1.110 on ImageNet 64x64. When distilling EDM2 models trained on ImageNet 512x512, our SiDA method surpasses even the largest teacher model, EDM2-XXL, which achieved an FID of 1.81 using classifier-free guidance (CFG) and 63 generation steps. In contrast, SiDA achieves FID scores of 2.156 for size XS, 1.669 for S, 1.488 for M, 1.413 for L, 1.379 for XL, and 1.366 for XXL, all without CFG and in a single generation step. These results highlight substantial improvements across all model sizes. Our code is available at https://github.com/mingyuanzhou/SiD/tree/sida.",
      "authors": [
        "Mingyuan Zhou",
        "Huangjie Zheng",
        "Yi Gu",
        "Zhendong Wang",
        "Hai Huang"
      ],
      "published": "2024-10-19T00:33:51Z",
      "updated": "2024-12-24T05:06:20Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.14919v4",
      "landing_url": "https://arxiv.org/abs/2410.14919v4",
      "doi": "https://doi.org/10.48550/arXiv.2410.14919"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on image diffusion models and adversarial distillation without any discrete audio token encoder/decoder or quantization discussion, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on image diffusion models and adversarial distillation without any discrete audio token encoder/decoder or quantization discussion, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "abstract": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
    "metadata": {
      "arxiv_id": "2410.15017",
      "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
      "summary": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. Code, samples, and checkpoints are available at https://github.com/mubtasimahasan/DM-Codec.",
      "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
      ],
      "published": "2024-10-19T07:14:14Z",
      "updated": "2025-09-29T08:08:40Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15017v2",
      "landing_url": "https://arxiv.org/abs/2410.15017v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.15017"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "DM-Codec explicitly learns discrete speech tokens via RVQ-based codec plus semantic/contextual distillation and reports token-level improvements (WER/WIL) with quality metrics, so it satisfies all inclusion and no exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "DM-Codec explicitly learns discrete speech tokens via RVQ-based codec plus semantic/contextual distillation and reports token-level improvements (WER/WIL) with quality metrics, so it satisfies all inclusion and no exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Residual vector quantization for KV cache compression in large language model",
    "abstract": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
    "metadata": {
      "arxiv_id": "2410.15704",
      "title": "Residual vector quantization for KV cache compression in large language model",
      "summary": "KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.",
      "authors": [
        "Ankur Kumar"
      ],
      "published": "2024-10-21T07:20:41Z",
      "updated": "2024-10-21T07:20:41Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15704v1",
      "landing_url": "https://arxiv.org/abs/2410.15704v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15704"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on compressing KV cache in large language models via residual vector quantization, with no mention of discrete audio tokens, tokenizers/codecs, or audio reconstruction, so it fails the project’s audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on compressing KV cache in large language models via residual vector quantization, with no mention of discrete audio tokens, tokenizers/codecs, or audio reconstruction, so it fails the project’s audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimizing Neural Speech Codec for Low-Bitrate Compression via Multi-Scale Encoding",
    "abstract": "Neural speech codecs have demonstrated their ability to compress high-quality speech and audio by converting them into discrete token representations. Most existing methods utilize Residual Vector Quantization (RVQ) to encode speech into multiple layers of discrete codes with uniform time scales. However, this strategy overlooks the differences in information density across various speech features, leading to redundant encoding of sparse information, which limits the performance of these methods at low bitrate. This paper proposes MsCodec, a novel multi-scale neural speech codec that encodes speech into multiple layers of discrete codes, each corresponding to a different time scale. This encourages the model to decouple speech features according to their diverse information densities, consequently enhancing the performance of speech compression. Furthermore, we incorporate mutual information loss to augment the diversity among speech codes across different layers. Experimental results indicate that our proposed method significantly improves codec performance at low bitrate.",
    "metadata": {
      "arxiv_id": "2410.15749",
      "title": "Optimizing Neural Speech Codec for Low-Bitrate Compression via Multi-Scale Encoding",
      "summary": "Neural speech codecs have demonstrated their ability to compress high-quality speech and audio by converting them into discrete token representations. Most existing methods utilize Residual Vector Quantization (RVQ) to encode speech into multiple layers of discrete codes with uniform time scales. However, this strategy overlooks the differences in information density across various speech features, leading to redundant encoding of sparse information, which limits the performance of these methods at low bitrate. This paper proposes MsCodec, a novel multi-scale neural speech codec that encodes speech into multiple layers of discrete codes, each corresponding to a different time scale. This encourages the model to decouple speech features according to their diverse information densities, consequently enhancing the performance of speech compression. Furthermore, we incorporate mutual information loss to augment the diversity among speech codes across different layers. Experimental results indicate that our proposed method significantly improves codec performance at low bitrate.",
      "authors": [
        "Peiji Yang",
        "Fengping Wang",
        "Yicheng Zhong",
        "Huawei Wei",
        "Zhisheng Wang"
      ],
      "published": "2024-10-21T08:04:36Z",
      "updated": "2024-10-21T08:04:36Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15749v1",
      "landing_url": "https://arxiv.org/abs/2410.15749v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.15749"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "MsCodec clearly focuses on neural codec-based discrete audio tokens via multi-scale RVQ-like quantization and evaluates low-bitrate compression, so it meets the inclusion criteria and lacks any exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "MsCodec clearly focuses on neural codec-based discrete audio tokens via multi-scale RVQ-like quantization and evaluates low-bitrate compression, so it meets the inclusion criteria and lacks any exclusion conditions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
    "abstract": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
    "metadata": {
      "arxiv_id": "2410.15764",
      "title": "LSCodec: Low-Bitrate and Speaker-Decoupled Discrete Speech Codec",
      "summary": "Although discrete speech tokens have exhibited strong potential for language model-based speech generation, their high bitrates and redundant timbre information restrict the development of such models. In this work, we propose LSCodec, a discrete speech codec that has both low bitrate and speaker decoupling ability. LSCodec adopts a multi-stage unsupervised training framework with a speaker perturbation technique. A continuous information bottleneck is first established, followed by vector quantization that produces a discrete speaker-decoupled space. A discrete token vocoder finally refines acoustic details from LSCodec. By reconstruction evaluations, LSCodec demonstrates superior intelligibility and audio quality with only a single codebook and smaller vocabulary size than baselines. Voice conversion and speaker probing experiments prove the excellent speaker disentanglement of LSCodec, and ablation study verifies the effectiveness of the proposed training framework.",
      "authors": [
        "Yiwei Guo",
        "Zhihan Li",
        "Chenpeng Du",
        "Hankun Wang",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2024-10-21T08:23:31Z",
      "updated": "2025-05-21T16:46:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.15764v3",
      "landing_url": "https://arxiv.org/abs/2410.15764v3",
      "doi": "https://doi.org/10.48550/arXiv.2410.15764"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study clearly proposes discrete speech codec tokens with quantization, vocoder reconstruction, and speaker disentanglement evaluations that satisfy the inclusion criteria and avoid the exclusions, so include it (score 5)."
    },
    "round-A_JuniorNano_reasoning": "Study clearly proposes discrete speech codec tokens with quantization, vocoder reconstruction, and speaker disentanglement evaluations that satisfy the inclusion criteria and avoid the exclusions, so include it (score 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning",
    "abstract": "Recent advancements in large audio-language models (LALMs) have shown impressive capabilities in understanding and reasoning about audio and speech information. However, these models still face challenges, including hallucinating non-existent sound events, misidentifying the order of sound events, and incorrectly attributing sound sources, which undermine their reliability and real-world application. To systematically evaluate these issues, we propose three distinct tasks: object existence, temporal order, and object attribute within audio. These tasks assess the models' comprehension of critical audio information aspects. Our experimental results reveal limitations in these fundamental tasks, underscoring the need for better models in recognizing specific sound events, determining event sequences, and identifying sound sources. To improve performance in these areas, we introduce a multi-turn chain-of-thought approach, which demonstrates significantly improved model performance across the proposed tasks.",
    "metadata": {
      "arxiv_id": "2410.16130",
      "title": "Can Large Audio-Language Models Truly Hear? Tackling Hallucinations with Multi-Task Assessment and Stepwise Audio Reasoning",
      "summary": "Recent advancements in large audio-language models (LALMs) have shown impressive capabilities in understanding and reasoning about audio and speech information. However, these models still face challenges, including hallucinating non-existent sound events, misidentifying the order of sound events, and incorrectly attributing sound sources, which undermine their reliability and real-world application. To systematically evaluate these issues, we propose three distinct tasks: object existence, temporal order, and object attribute within audio. These tasks assess the models' comprehension of critical audio information aspects. Our experimental results reveal limitations in these fundamental tasks, underscoring the need for better models in recognizing specific sound events, determining event sequences, and identifying sound sources. To improve performance in these areas, we introduce a multi-turn chain-of-thought approach, which demonstrates significantly improved model performance across the proposed tasks.",
      "authors": [
        "Chun-Yi Kuan",
        "Hung-yi Lee"
      ],
      "published": "2024-10-21T15:55:27Z",
      "updated": "2024-12-31T09:35:31Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16130v2",
      "landing_url": "https://arxiv.org/abs/2410.16130v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16130"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study evaluates hallucinations in audio-language models through reasoning tasks without describing any discrete audio token generation, quantization, tokenizer, or codec details, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The study evaluates hallucinations in audio-language models through reasoning tasks without describing any discrete audio token generation, quantization, tokenizer, or codec details, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Do Audio-Language Models Understand Linguistic Variations?",
    "abstract": "Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are treated as different views of the same audio scene and use this for training. Our proposed approach improves the text-to-audio retrieval performance of CLAP by 0.8%-13% across benchmarks and enhances robustness to linguistic variation.",
    "metadata": {
      "arxiv_id": "2410.16505",
      "title": "Do Audio-Language Models Understand Linguistic Variations?",
      "summary": "Open-vocabulary audio language models (ALMs), like Contrastive Language Audio Pretraining (CLAP), represent a promising new paradigm for audio-text retrieval using natural language queries. In this paper, for the first time, we perform controlled experiments on various benchmarks to show that existing ALMs struggle to generalize to linguistic variations in textual queries. To address this issue, we propose RobustCLAP, a novel and compute-efficient technique to learn audio-language representations agnostic to linguistic variations. Specifically, we reformulate the contrastive loss used in CLAP architectures by introducing a multi-view contrastive learning objective, where paraphrases are treated as different views of the same audio scene and use this for training. Our proposed approach improves the text-to-audio retrieval performance of CLAP by 0.8%-13% across benchmarks and enhances robustness to linguistic variation.",
      "authors": [
        "Ramaneswaran Selvakumar",
        "Sonal Kumar",
        "Hemant Kumar Giri",
        "Nishit Anand",
        "Ashish Seth",
        "Sreyan Ghosh",
        "Dinesh Manocha"
      ],
      "published": "2024-10-21T20:55:33Z",
      "updated": "2025-02-20T00:51:26Z",
      "categories": [
        "cs.SD",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16505v2",
      "landing_url": "https://arxiv.org/abs/2410.16505v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16505"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on audio-language retrieval and robustness to linguistic variation without discussing discrete audio tokens, codec/quantization, or tokenization specifications, so it fails to meet inclusion requirements and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on audio-language retrieval and robustness to linguistic variation without discussing discrete audio tokens, codec/quantization, or tokenization specifications, so it fails to meet inclusion requirements and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Conflict-Aware Adversarial Training",
    "abstract": "Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for the standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between the gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named \\textbf{Conflict-Aware Adversarial Training~(CA-AT)}. Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient finetuning.",
    "metadata": {
      "arxiv_id": "2410.16579",
      "title": "Conflict-Aware Adversarial Training",
      "summary": "Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for the standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between the gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named \\textbf{Conflict-Aware Adversarial Training~(CA-AT)}. Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient finetuning.",
      "authors": [
        "Zhiyu Xue",
        "Haohan Wang",
        "Yao Qin",
        "Ramtin Pedarsani"
      ],
      "published": "2024-10-21T23:44:03Z",
      "updated": "2024-10-21T23:44:03Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16579v1",
      "landing_url": "https://arxiv.org/abs/2410.16579v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.16579"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on adversarial training for vision/standard models and does not discuss discrete audio tokenization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on adversarial training for vision/standard models and does not discuss discrete audio tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Pyramid Vector Quantization for LLMs",
    "abstract": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
    "metadata": {
      "arxiv_id": "2410.16926",
      "title": "Pyramid Vector Quantization for LLMs",
      "summary": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\\% accuracy on downstream tasks.",
      "authors": [
        "Tycho F. A. van der Ouderaa",
        "Maximilian L. Croci",
        "Agrin Hilmkil",
        "James Hensman"
      ],
      "published": "2024-10-22T11:57:32Z",
      "updated": "2024-12-04T10:52:04Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.16926v2",
      "landing_url": "https://arxiv.org/abs/2410.16926v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.16926"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on LLM weight quantization and does not address discrete audio tokens or any codec/tokenizer design for audio, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on LLM weight quantization and does not address discrete audio tokens or any codec/tokenizer design for audio, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokenizer in Text To Speech",
    "abstract": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
    "metadata": {
      "arxiv_id": "2410.17081",
      "title": "Continuous Speech Tokenizer in Text To Speech",
      "summary": "The fusion of speech and language in the era of large language models has garnered significant attention. Discrete speech token is often utilized in text-to-speech tasks for speech compression and portability, which is convenient for joint training with text and have good compression efficiency. However, we found that the discrete speech tokenizer still suffers from information loss. Therefore, we propose a simple yet effective continuous speech tokenizer named Cont-SPT, and a text-to-speech model based on continuous speech tokens. Our results show that the speech language model based on the continuous speech tokenizer has better continuity and higher estimated Mean Opinion Scores (MoS). This enhancement is attributed to better information preservation rate of the continuous speech tokenizer across both low and high frequencies in the frequency domain. The code and resources for Cont-SPT can be found in https://github.com/Yixing-Li/Continuous-Speech-Tokenizer",
      "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Xingwu Sun",
        "Yu Cheng",
        "Zhanhui Kang"
      ],
      "published": "2024-10-22T15:02:37Z",
      "updated": "2025-03-31T13:57:49Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.17081v2",
      "landing_url": "https://arxiv.org/abs/2410.17081v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.17081"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper centers on continuous speech tokens rather than the discrete audio-token representations defined in the inclusion criteria, so it fails the requirement to focus on finite-vocabulary discrete tokenization research."
    },
    "round-A_JuniorNano_reasoning": "Paper centers on continuous speech tokens rather than the discrete audio-token representations defined in the inclusion criteria, so it fails the requirement to focus on finite-vocabulary discrete tokenization research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
    "abstract": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
    "metadata": {
      "arxiv_id": "2410.19199",
      "title": "Making Social Platforms Accessible: Emotion-Aware Speech Generation with Integrated Text Analysis",
      "summary": "Recent studies have outlined the accessibility challenges faced by blind or visually impaired, and less-literate people, in interacting with social networks, in-spite of facilitating technologies such as monotone text-to-speech (TTS) screen readers and audio narration of visual elements such as emojis. Emotional speech generation traditionally relies on human input of the expected emotion together with the text to synthesise, with additional challenges around data simplification (causing information loss) and duration inaccuracy, leading to lack of expressive emotional rendering. In real-life communications, the duration of phonemes can vary since the same sentence might be spoken in a variety of ways depending on the speakers' emotional states or accents (referred to as the one-to-many problem of text to speech generation). As a result, an advanced voice synthesis system is required to account for this unpredictability. We propose an end-to-end context-aware Text-to-Speech (TTS) synthesis system that derives the conveyed emotion from text input and synthesises audio that focuses on emotions and speaker features for natural and expressive speech, integrating advanced natural language processing (NLP) and speech synthesis techniques for real-time applications. Our system also showcases competitive inference time performance when benchmarked against the state-of-the-art TTS models, making it suitable for real-time accessibility applications.",
      "authors": [
        "Suparna De",
        "Ionut Bostan",
        "Nishanth Sastry"
      ],
      "published": "2024-10-24T23:18:02Z",
      "updated": "2024-10-24T23:18:02Z",
      "categories": [
        "cs.SI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.19199v1",
      "landing_url": "https://arxiv.org/abs/2410.19199v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.19199"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes an emotion-aware TTS system without mentioning discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion focus on discrete audio tokens and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper describes an emotion-aware TTS system without mentioning discrete audio tokenization, quantization, or token-level evaluations, so it fails to meet the inclusion focus on discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic layer selection in decoder-only transformers",
    "abstract": "The vast size of Large Language Models (LLMs) has prompted a search to optimize inference. One effective approach is dynamic inference, which adapts the architecture to the sample-at-hand to reduce the overall computational cost. We empirically examine two common dynamic inference methods for natural language generation (NLG): layer skipping and early exiting. We find that a pre-trained decoder-only model is significantly more robust to layer removal via layer skipping, as opposed to early exit. We demonstrate the difficulty of using hidden state information to adapt computation on a per-token basis for layer skipping. Finally, we show that dynamic computation allocation on a per-sequence basis holds promise for significant efficiency gains by constructing an oracle controller. Remarkably, we find that there exists an allocation which achieves equal performance to the full model using only 23.3% of its layers on average.",
    "metadata": {
      "arxiv_id": "2410.20022",
      "title": "Dynamic layer selection in decoder-only transformers",
      "summary": "The vast size of Large Language Models (LLMs) has prompted a search to optimize inference. One effective approach is dynamic inference, which adapts the architecture to the sample-at-hand to reduce the overall computational cost. We empirically examine two common dynamic inference methods for natural language generation (NLG): layer skipping and early exiting. We find that a pre-trained decoder-only model is significantly more robust to layer removal via layer skipping, as opposed to early exit. We demonstrate the difficulty of using hidden state information to adapt computation on a per-token basis for layer skipping. Finally, we show that dynamic computation allocation on a per-sequence basis holds promise for significant efficiency gains by constructing an oracle controller. Remarkably, we find that there exists an allocation which achieves equal performance to the full model using only 23.3% of its layers on average.",
      "authors": [
        "Theodore Glavas",
        "Joud Chataoui",
        "Florence Regol",
        "Wassim Jabbour",
        "Antonios Valkanas",
        "Boris N. Oreshkin",
        "Mark Coates"
      ],
      "published": "2024-10-26T00:44:11Z",
      "updated": "2024-10-26T00:44:11Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20022v1",
      "landing_url": "https://arxiv.org/abs/2410.20022v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.20022"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on dynamic inference for decoder-only transformers with no mention of discrete audio tokenization, codec learning, or related audio token designs, so it fails to meet any inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on dynamic inference for decoder-only transformers with no mention of discrete audio tokenization, codec learning, or related audio token designs, so it fails to meet any inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
    "metadata": {
      "arxiv_id": "2410.20336",
      "title": "Get Large Language Models Ready to Speak: A Late-fusion Approach for Speech Generation",
      "summary": "Large language models (LLMs) have revolutionized natural language processing (NLP) with impressive performance across various text-based tasks. However, the extension of text-dominant LLMs to with speech generation tasks remains under-explored. In this work, we introduce a text-to-speech (TTS) system powered by a fine-tuned Llama model, named TTS-Llama, that achieves state-of-the-art speech synthesis performance. Building on TTS-Llama, we further propose MoLE-Llama, a text-and-speech multimodal LLM developed through purely late-fusion parameter-efficient fine-tuning (PEFT) and a mixture-of-expert architecture. Extensive empirical results demonstrate MoLE-Llama's competitive performance on both text-only question-answering (QA) and TTS tasks, mitigating catastrophic forgetting issue in either modality. Finally, we further explore MoLE-Llama in text-in-speech-out QA tasks, demonstrating its great potential as a multimodal dialog system capable of speech generation.",
      "authors": [
        "Maohao Shen",
        "Shun Zhang",
        "Jilong Wu",
        "Zhiping Xiu",
        "Ehab AlBadawy",
        "Yiting Lu",
        "Mike Seltzer",
        "Qing He"
      ],
      "published": "2024-10-27T04:28:57Z",
      "updated": "2024-10-27T04:28:57Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20336v1",
      "landing_url": "https://arxiv.org/abs/2410.20336v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.20336"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on fine-tuning Llama for TTS and multimodal QA without describing any discrete audio-token/codec/tokenizer design, quantization, vocabularies, or token-level evaluation, so it fails the core inclusion criterion centered on discrete audio tokens."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on fine-tuning Llama for TTS and multimodal QA without describing any discrete audio-token/codec/tokenizer design, quantization, vocabularies, or token-level evaluation, so it fails the core inclusion criterion centered on discrete audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
    "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
    "metadata": {
      "arxiv_id": "2410.20573",
      "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
      "summary": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions, which requires exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space, making it interpretable. We apply this technique to model the latent space of pre-trained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space such that it determines which parts of the latent space correspond to specific generative factors. Furthermore, we demonstrate that each line of the SFVQ curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also demonstrate that the points located on an SFVQ line can be used for controllable data augmentation.",
      "authors": [
        "Mohammad Hassan Vali",
        "Tom Bäckström"
      ],
      "published": "2024-10-27T19:56:02Z",
      "updated": "2025-07-02T10:27:13Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.20573v2",
      "landing_url": "https://arxiv.org/abs/2410.20573v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.20573"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on interpreting GAN latent spaces for image generation without any discussion of discrete audio tokens, their quantization, vocabularies, or audio-specific evaluation, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on interpreting GAN latent spaces for image generation without any discussion of discrete audio tokens, their quantization, vocabularies, or audio-specific evaluation, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Quality Analysis of the Coding Bitrate Tradeoff Between Geometry and Attributes for Colored Point Clouds",
    "abstract": "Typically, point cloud encoders allocate a similar bitrate for geometry and attributes (usually RGB color components) information coding. This paper reports a quality study considering different coding bitrate tradeoff between geometry and attributes. A set of five point clouds, representing different characteristics and types of content was encoded with the MPEG standard Geometry Point Cloud Compression (G-PCC), using octree to encode geometry information, and both the Region Adaptive Hierarchical Transform and the Prediction Lifting transform for attributes. Furthermore, the JPEG Pleno Point Cloud Verification Model was also tested. Five different attributes/geometry bitrate tradeoffs were considered, notably 70%/30%, 60%/40%, 50%/50%, 40%/60%, 30%/70%. Three point cloud objective metrics were selected to assess the quality of the reconstructed point clouds, notably the PSNR YUV, the Point Cloud Quality Metric, and GraphSIM. Furthermore, for each encoder, the Bjonteegaard Deltas were computed for each tradeoff, using the 50%/50% tradeoff as a reference. The reported results indicate that using a higher bitrate allocation for attribute encoding usually yields slightly better results.",
    "metadata": {
      "arxiv_id": "2410.21613",
      "title": "Quality Analysis of the Coding Bitrate Tradeoff Between Geometry and Attributes for Colored Point Clouds",
      "summary": "Typically, point cloud encoders allocate a similar bitrate for geometry and attributes (usually RGB color components) information coding. This paper reports a quality study considering different coding bitrate tradeoff between geometry and attributes. A set of five point clouds, representing different characteristics and types of content was encoded with the MPEG standard Geometry Point Cloud Compression (G-PCC), using octree to encode geometry information, and both the Region Adaptive Hierarchical Transform and the Prediction Lifting transform for attributes. Furthermore, the JPEG Pleno Point Cloud Verification Model was also tested. Five different attributes/geometry bitrate tradeoffs were considered, notably 70%/30%, 60%/40%, 50%/50%, 40%/60%, 30%/70%. Three point cloud objective metrics were selected to assess the quality of the reconstructed point clouds, notably the PSNR YUV, the Point Cloud Quality Metric, and GraphSIM. Furthermore, for each encoder, the Bjonteegaard Deltas were computed for each tradeoff, using the 50%/50% tradeoff as a reference. The reported results indicate that using a higher bitrate allocation for attribute encoding usually yields slightly better results.",
      "authors": [
        "Joao Prazeres",
        "Rafael Rodrigues",
        "Manuela Pereira",
        "Antonio M. G. Pinheiro"
      ],
      "published": "2024-10-28T23:34:09Z",
      "updated": "2024-10-28T23:34:09Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.21613v1",
      "landing_url": "https://arxiv.org/abs/2410.21613v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.21613"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on bitrate tradeoffs for colored point cloud encoding, which is unrelated to discrete audio codec/token design and thus fails all inclusion criteria while meeting an exclusion condition."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on bitrate tradeoffs for colored point cloud encoding, which is unrelated to discrete audio codec/token design and thus fails all inclusion criteria while meeting an exclusion condition.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Are Decoder-Only Large Language Models the Silver Bullet for Code Search?",
    "abstract": "Code search is essential for code reuse, allowing developers to efficiently locate relevant code snippets. The advent of powerful decoder-only Large Language Models (LLMs) has revolutionized many code intelligence tasks. However, their effectiveness for the retrieval-based task of code search, particularly compared to established encoder-based models, remains underexplored. This paper addresses this gap by presenting a large-scale systematic evaluation of eleven decoder-only LLMs, analyzing their performance across zero-shot and fine-tuned settings. Our results show that fine-tuned decoder-only models, particularly CodeGemma, significantly outperform encoder-only models like UniXcoder, achieving a 40.4% higher Mean Average Precision (MAP) on the CoSQA$^+$ benchmark. Our analysis further reveals two crucial nuances for practitioners: first, the relationship between model size and performance is non-monotonic, with mid-sized models often outperforming larger variants; second, the composition of the training data is critical, as a multilingual dataset enhances generalization while a small amount of data from a specific language can act as noise and interfere with model effectiveness. These findings offer a comprehensive guide to selecting and optimizing modern LLMs for code search.",
    "metadata": {
      "arxiv_id": "2410.22240",
      "title": "Are Decoder-Only Large Language Models the Silver Bullet for Code Search?",
      "summary": "Code search is essential for code reuse, allowing developers to efficiently locate relevant code snippets. The advent of powerful decoder-only Large Language Models (LLMs) has revolutionized many code intelligence tasks. However, their effectiveness for the retrieval-based task of code search, particularly compared to established encoder-based models, remains underexplored. This paper addresses this gap by presenting a large-scale systematic evaluation of eleven decoder-only LLMs, analyzing their performance across zero-shot and fine-tuned settings.\n  Our results show that fine-tuned decoder-only models, particularly CodeGemma, significantly outperform encoder-only models like UniXcoder, achieving a 40.4% higher Mean Average Precision (MAP) on the CoSQA$^+$ benchmark. Our analysis further reveals two crucial nuances for practitioners: first, the relationship between model size and performance is non-monotonic, with mid-sized models often outperforming larger variants; second, the composition of the training data is critical, as a multilingual dataset enhances generalization while a small amount of data from a specific language can act as noise and interfere with model effectiveness. These findings offer a comprehensive guide to selecting and optimizing modern LLMs for code search.",
      "authors": [
        "Yuxuan Chen",
        "Mingwei Liu",
        "Guangsheng Ou",
        "Anji Li",
        "Dekun Dai",
        "Yanlin Wang",
        "Zibin Zheng"
      ],
      "published": "2024-10-29T17:05:25Z",
      "updated": "2025-08-30T16:18:40Z",
      "categories": [
        "cs.SE"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22240v2",
      "landing_url": "https://arxiv.org/abs/2410.22240v2",
      "doi": "https://doi.org/10.48550/arXiv.2410.22240"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract discuss decoder-only LLMs for code search with no mention of discrete audio tokenization, so it fails to meet the audio token inclusion criteria and matches the exclusion of non-audio discrete tokens."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract discuss decoder-only LLMs for code search with no mention of discrete audio tokenization, so it fails to meet the audio token inclusion criteria and matches the exclusion of non-audio discrete tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
    "abstract": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
    "metadata": {
      "arxiv_id": "2410.22448",
      "title": "A Closer Look at Neural Codec Resynthesis: Bridging the Gap between Codec and Waveform Generation",
      "summary": "Neural Audio Codecs, initially designed as a compression technique, have gained more attention recently for speech generation. Codec models represent each audio frame as a sequence of tokens, i.e., discrete embeddings. The discrete and low-frequency nature of neural codecs introduced a new way to generate speech with token-based models. As these tokens encode information at various levels of granularity, from coarse to fine, most existing works focus on how to better generate the coarse tokens. In this paper, we focus on an equally important but often overlooked question: How can we better resynthesize the waveform from coarse tokens? We point out that both the choice of learning target and resynthesis approach have a dramatic impact on the generated audio quality. Specifically, we study two different strategies based on token prediction and regression, and introduce a new method based on Schrödinger Bridge. We examine how different design choices affect machine and human perception.",
      "authors": [
        "Alexander H. Liu",
        "Qirui Wang",
        "Yuan Gong",
        "James Glass"
      ],
      "published": "2024-10-29T18:29:39Z",
      "updated": "2024-10-29T18:29:39Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22448v1",
      "landing_url": "https://arxiv.org/abs/2410.22448v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22448"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper studies neural audio codec tokens and their resynthesis, clearly operating on discrete codec token sequences with quality comparisons, so it satisfies all inclusion requirements and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "The paper studies neural audio codec tokens and their resynthesis, clearly operating on discrete codec token sequences with quality comparisons, so it satisfies all inclusion requirements and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
    "abstract": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
    "metadata": {
      "arxiv_id": "2410.22807",
      "title": "APCodec+: A Spectrum-Coding-Based High-Fidelity and High-Compression-Rate Neural Audio Codec with Staged Training Paradigm",
      "summary": "This paper proposes a novel neural audio codec, named APCodec+, which is an improved version of APCodec. The APCodec+ takes the audio amplitude and phase spectra as the coding object, and employs an adversarial training strategy. Innovatively, we propose a two-stage joint-individual training paradigm for APCodec+. In the joint training stage, the encoder, quantizer, decoder and discriminator are jointly trained with complete spectral loss, quantization loss, and adversarial loss. In the individual training stage, the encoder and quantizer fix their parameters and provide high-quality training data for the decoder and discriminator. The decoder and discriminator are individually trained from scratch without the quantization loss. The purpose of introducing individual training is to reduce the learning difficulty of the decoder, thereby further improving the fidelity of the decoded audio. Experimental results confirm that our proposed APCodec+ at low bitrates achieves comparable performance with baseline codecs at higher bitrates, thanks to the proposed staged training paradigm.",
      "authors": [
        "Hui-Peng Du",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2024-10-30T08:36:17Z",
      "updated": "2024-10-30T08:36:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.22807v1",
      "landing_url": "https://arxiv.org/abs/2410.22807v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.22807"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The proposed APCodec+ clearly targets encoder–quantizer–decoder discrete coding with evaluations at low bitrates, so it satisfies the inclusion criteria and deserves inclusion (score 5)."
    },
    "round-A_JuniorNano_reasoning": "The proposed APCodec+ clearly targets encoder–quantizer–decoder discrete coding with evaluations at low bitrates, so it satisfies the inclusion criteria and deserves inclusion (score 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
    "abstract": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
    "metadata": {
      "arxiv_id": "2410.24177",
      "title": "DC-Spin: A Speaker-invariant Speech Tokenizer for Spoken Language Models",
      "summary": "Spoken language models (SLMs) have gained increasing attention with advancements in text-based, decoder-only language models. SLMs process text and speech, enabling simultaneous speech understanding and generation. This paper presents Double-Codebook Speaker-invariant Clustering (DC-Spin), which aims to improve speech tokenization by bridging audio signals and SLM tokens. DC-Spin extracts speaker-invariant tokens rich in phonetic information and resilient to input variations, enhancing zero-shot SLM tasks and speech resynthesis. We propose a chunk-wise approach to enable streamable DC-Spin without retraining and degradation. Comparisons of tokenization methods (self-supervised and neural audio codecs), model scalability, and downstream task proxies show that tokens easily modeled by an n-gram LM or aligned with phonemes offer strong performance, providing insights for designing speech tokenizers for SLMs.",
      "authors": [
        "Heng-Jui Chang",
        "Hongyu Gong",
        "Changhan Wang",
        "James Glass",
        "Yu-An Chung"
      ],
      "published": "2024-10-31T17:43:13Z",
      "updated": "2024-10-31T17:43:13Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2410.24177v1",
      "landing_url": "https://arxiv.org/abs/2410.24177v1",
      "doi": "https://doi.org/10.48550/arXiv.2410.24177"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "DC-Spin clearly proposes a discrete tokenization method (Double-Codebook speaker-invariant clustering) for spoken language models with evaluations comparing tokenization methods and downstream proxies, satisfying the inclusion criteria for discrete audio token research; score reflects strong include."
    },
    "round-A_JuniorNano_reasoning": "DC-Spin clearly proposes a discrete tokenization method (Double-Codebook speaker-invariant clustering) for spoken language models with evaluations comparing tokenization methods and downstream proxies, satisfying the inclusion criteria for discrete audio token research; score reflects strong include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios",
    "abstract": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.",
    "metadata": {
      "arxiv_id": "2411.00464",
      "title": "MDCTCodec: A Lightweight MDCT-based Neural Audio Codec towards High Sampling Rate and Low Bitrate Scenarios",
      "summary": "In this paper, we propose MDCTCodec, an efficient lightweight end-to-end neural audio codec based on the modified discrete cosine transform (MDCT). The encoder takes the MDCT spectrum of audio as input, encoding it into a continuous latent code which is then discretized by a residual vector quantizer (RVQ). Subsequently, the decoder decodes the MDCT spectrum from the quantized latent code and reconstructs audio via inverse MDCT. During the training phase, a novel multi-resolution MDCT-based discriminator (MR-MDCTD) is adopted to discriminate the natural or decoded MDCT spectrum for adversarial training. Experimental results confirm that, in scenarios with high sampling rates and low bitrates, the MDCTCodec exhibited high decoded audio quality, improved training and generation efficiency, and compact model size compared to baseline codecs. Specifically, the MDCTCodec achieved a ViSQOL score of 4.18 at a sampling rate of 48 kHz and a bitrate of 6 kbps on the public VCTK corpus.",
      "authors": [
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Hui-Peng Du",
        "Ye-Xin Lu",
        "Zhen-Hua Ling"
      ],
      "published": "2024-11-01T09:24:28Z",
      "updated": "2024-11-01T09:24:28Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.00464v1",
      "landing_url": "https://arxiv.org/abs/2411.00464v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.00464"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper proposes a codec that quantizes MDCT spectra into discrete latent codes via RVQ and evaluates reconstruction quality (e.g., ViSQOL at 48 kHz/6 kbps) so it clearly targets discrete audio token generation and meets the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes a codec that quantizes MDCT spectra into discrete latent codes via RVQ and evaluates reconstruction quality (e.g., ViSQOL at 48 kHz/6 kbps) so it clearly targets discrete audio token generation and meets the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
    "abstract": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
    "metadata": {
      "arxiv_id": "2411.01834",
      "title": "Align-SLM: Textless Spoken Language Models with Reinforcement Learning from AI Feedback",
      "summary": "While textless Spoken Language Models (SLMs) have shown potential in end-to-end speech-to-speech modeling, they still lag behind text-based Large Language Models (LLMs) in terms of semantic coherence and relevance. This work introduces the Align-SLM framework, which leverages preference optimization inspired by Reinforcement Learning with AI Feedback (RLAIF) to enhance the semantic understanding of SLMs. Our approach generates multiple speech continuations from a given prompt and uses semantic metrics to create preference data for Direct Preference Optimization (DPO). We evaluate the framework using ZeroSpeech 2021 benchmarks for lexical and syntactic modeling, the spoken version of the StoryCloze dataset for semantic coherence, and other speech generation metrics, including the GPT4-o score and human evaluation. Experimental results show that our method achieves state-of-the-art performance for SLMs on most benchmarks, highlighting the importance of preference optimization to improve the semantics of SLMs.",
      "authors": [
        "Guan-Ting Lin",
        "Prashanth Gurunath Shivakumar",
        "Aditya Gourav",
        "Yile Gu",
        "Ankur Gandhe",
        "Hung-yi Lee",
        "Ivan Bulyko"
      ],
      "published": "2024-11-04T06:07:53Z",
      "updated": "2025-05-27T16:17:52Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.01834v2",
      "landing_url": "https://arxiv.org/abs/2411.01834v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.01834"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "The abstract describes improving textless spoken language models via preference optimization, but it never clarifies how discrete audio tokens (tokenizer/codebook quantization) are generated, defined, or evaluated, so I can’t confidently determine whether it meets the discrete-token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "The abstract describes improving textless spoken language models via preference optimization, but it never clarifies how discrete audio tokens (tokenizer/codebook quantization) are generated, defined, or evaluated, so I can’t confidently determine whether it meets the discrete-token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
    "abstract": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
    "metadata": {
      "arxiv_id": "2411.04530",
      "title": "Tomato, Tomahto, Tomate: Do Multilingual Language Models Understand Based on Subword-Level Semantic Concepts?",
      "summary": "Human understanding of text depends on general semantic concepts of words rather than their superficial forms. To what extent does our human intuition transfer to language models? In this work, we study the degree to which current multilingual language models (mLMs) understand based on subword-level semantic concepts. To this end, we form \"semantic tokens\" by merging the semantically similar subwords and their embeddings, and evaluate the updated mLMs on five heterogeneous multilingual downstream tasks. Results show that the general shared semantics could get the models a long way in making the predictions on mLMs with different tokenizers and model sizes. Inspections of the grouped subwords show that they exhibit a wide range of semantic similarities, including synonyms and translations across many languages and scripts. Lastly, we find that the zero-shot results with semantic tokens are on par with or even better than the original models on certain classification tasks, suggesting that the shared subword-level semantics may serve as the anchors for cross-lingual transfer.",
      "authors": [
        "Crystina Zhang",
        "Jing Lu",
        "Vinh Q. Tran",
        "Tal Schuster",
        "Donald Metzler",
        "Jimmy Lin"
      ],
      "published": "2024-11-07T08:38:32Z",
      "updated": "2025-11-19T00:30:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.04530v2",
      "landing_url": "https://arxiv.org/abs/2411.04530v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.04530"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses subword semantic tokens for multilingual language models (textual domain) rather than discrete audio tokens or codecs, so it fails the audio-focused inclusion criteria and matches the exclusion for non-audio token studies."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses subword semantic tokens for multilingual language models (textual domain) rather than discrete audio tokens or codecs, so it fails the audio-focused inclusion criteria and matches the exclusion for non-audio token studies.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
    "abstract": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
    "metadata": {
      "arxiv_id": "2411.05361",
      "title": "Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks",
      "summary": "Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.",
      "authors": [
        "Chien-yu Huang",
        "Wei-Chih Chen",
        "Shu-wen Yang",
        "Andy T. Liu",
        "Chen-An Li",
        "Yu-Xiang Lin",
        "Wei-Cheng Tseng",
        "Anuj Diwan",
        "Yi-Jen Shih",
        "Jiatong Shi",
        "William Chen",
        "Chih-Kai Yang",
        "Wenze Ren",
        "Xuanjun Chen",
        "Chi-Yuan Hsiao",
        "Puyuan Peng",
        "Shih-Heng Wang",
        "Chun-Yi Kuan",
        "Ke-Han Lu",
        "Kai-Wei Chang",
        "Fabian Ritter-Gutierrez",
        "Kuan-Po Huang",
        "Siddhant Arora",
        "You-Kuan Lin",
        "Ming To Chuang",
        "Eunjung Yeo",
        "Kalvin Chang",
        "Chung-Ming Chien",
        "Kwanghee Choi",
        "Jun-You Wang",
        "Cheng-Hsiu Hsieh",
        "Yi-Cheng Lin",
        "Chee-En Yu",
        "I-Hsiang Chiu",
        "Heitor R. Guimarães",
        "Jionghao Han",
        "Tzu-Quan Lin",
        "Tzu-Yuan Lin",
        "Homu Chang",
        "Ting-Wu Chang",
        "Chun Wei Chen",
        "Shou-Jen Chen",
        "Yu-Hua Chen",
        "Hsi-Chun Cheng",
        "Kunal Dhawan",
        "Jia-Lin Fang",
        "Shi-Xin Fang",
        "Kuan-Yu Fang Chiang",
        "Chi An Fu",
        "Hsien-Fu Hsiao",
        "Ching Yu Hsu",
        "Shao-Syuan Huang",
        "Lee Chen Wei",
        "Hsi-Che Lin",
        "Hsuan-Hao Lin",
        "Hsuan-Ting Lin",
        "Jian-Ren Lin",
        "Ting-Chun Liu",
        "Li-Chun Lu",
        "Tsung-Min Pai",
        "Ankita Pasad",
        "Shih-Yun Shan Kuan",
        "Suwon Shon",
        "Yuxun Tang",
        "Yun-Shao Tsai",
        "Jui-Chiang Wei",
        "Tzu-Chieh Wei",
        "Chengxi Wu",
        "Dien-Ruei Wu",
        "Chao-Han Huck Yang",
        "Chieh-Chi Yang",
        "Jia Qi Yip",
        "Shao-Xiang Yuan",
        "Vahid Noroozi",
        "Zhehuai Chen",
        "Haibin Wu",
        "Karen Livescu",
        "David Harwath",
        "Shinji Watanabe",
        "Hung-yi Lee"
      ],
      "published": "2024-11-08T06:33:22Z",
      "updated": "2025-06-09T16:36:12Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.05361v2",
      "landing_url": "https://arxiv.org/abs/2411.05361v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.05361"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper describes a benchmark covering many speech/audio tasks without discussing discrete audio tokenization, quantization, or any tokenizer/codebook specifications, so it fails to meet the inclusion scope and is better excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a benchmark covering many speech/audio tasks without discussing discrete audio tokenization, quantization, or any tokenizer/codebook specifications, so it fails to meet the inclusion scope and is better excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
    "abstract": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
    "metadata": {
      "arxiv_id": "2411.06968",
      "title": "Mamba-based Decoder-Only Approach with Bidirectional Speech Modeling for Speech Recognition",
      "summary": "Selective state space models (SSMs) represented by Mamba have demonstrated their computational efficiency and promising outcomes in various tasks, including automatic speech recognition (ASR). Mamba has been applied to ASR task with the attention-based encoder-decoder framework, where the cross-attention mechanism between encoder and decoder remains. This paper explores the capability of Mamba as the decoder-only architecture in ASR task. Our MAmba-based DEcoder-ONly approach (MADEON) consists of a single decoder that takes speech tokens as a condition and predicts text tokens in an autoregressive manner. To enhance MADEON, we further propose speech prefixing that performs bidirectional processing on speech tokens, which enriches the contextual information in the hidden states. Our experiments show that MADEON significantly outperforms a non-selective SSM. The combination of speech prefixing and the recently proposed Mamba-2 yields comparable performance to Transformer-based models on large datasets.",
      "authors": [
        "Yoshiki Masuyama",
        "Koichi Miyazaki",
        "Masato Murata"
      ],
      "published": "2024-11-11T13:17:24Z",
      "updated": "2024-11-11T13:17:24Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.06968v1",
      "landing_url": "https://arxiv.org/abs/2411.06968v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.06968"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on ASR modeling using Mamba decoder-only architecture and speech prefixing without detailing discrete audio token quantization/codebook design or treating the finite vocabulary tokens as the core research object, so it fails the discrete-token inclusion requirements and is better excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on ASR modeling using Mamba decoder-only architecture and speech prefixing without detailing discrete audio token quantization/codebook design or treating the finite vocabulary tokens as the core research object, so it fails the discrete-token inclusion requirements and is better excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
    "abstract": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
    "metadata": {
      "arxiv_id": "2411.07111",
      "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
      "summary": "This technical report presents our initial attempt to build a spoken large language model (LLM) for Taiwanese Mandarin, specifically tailored to enable real-time, speech-to-speech interaction in multi-turn conversations. Our end-to-end model incorporates a decoder-only transformer architecture and aims to achieve seamless interaction while preserving the conversational flow, including full-duplex capabilities allowing simultaneous speaking and listening. The paper also details the training process, including data preparation with synthesized dialogues and adjustments for real-time interaction. We also developed a platform to evaluate conversational fluency and response coherence in multi-turn dialogues. We hope the release of the report can contribute to the future development of spoken LLMs in Taiwanese Mandarin.",
      "authors": [
        "Chih-Kai Yang",
        "Yu-Kuan Fu",
        "Chen-An Li",
        "Yi-Cheng Lin",
        "Yu-Xiang Lin",
        "Wei-Chih Chen",
        "Ho Lam Chung",
        "Chun-Yi Kuan",
        "Wei-Ping Huang",
        "Ke-Han Lu",
        "Tzu-Quan Lin",
        "Hsiu-Hsuan Wang",
        "En-Pei Hu",
        "Chan-Jan Hsu",
        "Liang-Hsuan Tseng",
        "I-Hsiang Chiu",
        "Ulin Sanga",
        "Xuanjun Chen",
        "Po-chun Hsu",
        "Shu-wen Yang",
        "Hung-yi Lee"
      ],
      "published": "2024-11-11T16:37:40Z",
      "updated": "2024-12-27T07:29:19Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07111v2",
      "landing_url": "https://arxiv.org/abs/2411.07111v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.07111"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes building spoken Taiwanese Mandarin LLM for speech-to-speech interactions but does not mention discrete audio token/codec/tokenizer design, quantization, vocabulary, or evaluations tied to discrete tokens, so it fails all inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes building spoken Taiwanese Mandarin LLM for speech-to-speech interactions but does not mention discrete audio token/codec/tokenizer design, quantization, vocabulary, or evaluations tied to discrete tokens, so it fails all inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only ASR",
    "abstract": "CTC compressor can be an effective approach to integrate audio encoders to decoder-only models, which has gained growing interest for different speech applications. In this work, we propose a novel CTC compressor based joint speech and text training (CJST) framework for decoder-only ASR. CJST matches speech and text modalities from both directions by exploring a simple modality adaptor and several features of the CTC compressor, including sequence compression, on-the-fly forced peaky alignment and CTC class embeddings. Experimental results on the Librispeech and TED-LIUM2 corpora show that the proposed CJST achieves an effective text injection without the need of duration handling, leading to the best performance for both in-domain and cross-domain scenarios. We also provide a comprehensive study on CTC compressor, covering various compression modes, edge case handling and behavior under both clean and noisy data conditions, which reveals the most robust setting to use CTC compressor for decoder-only models.",
    "metadata": {
      "arxiv_id": "2411.07607",
      "title": "CJST: CTC Compressor based Joint Speech and Text Training for Decoder-Only ASR",
      "summary": "CTC compressor can be an effective approach to integrate audio encoders to decoder-only models, which has gained growing interest for different speech applications. In this work, we propose a novel CTC compressor based joint speech and text training (CJST) framework for decoder-only ASR. CJST matches speech and text modalities from both directions by exploring a simple modality adaptor and several features of the CTC compressor, including sequence compression, on-the-fly forced peaky alignment and CTC class embeddings. Experimental results on the Librispeech and TED-LIUM2 corpora show that the proposed CJST achieves an effective text injection without the need of duration handling, leading to the best performance for both in-domain and cross-domain scenarios. We also provide a comprehensive study on CTC compressor, covering various compression modes, edge case handling and behavior under both clean and noisy data conditions, which reveals the most robust setting to use CTC compressor for decoder-only models.",
      "authors": [
        "Wei Zhou",
        "Junteng Jia",
        "Leda Sari",
        "Jay Mahadeokar",
        "Ozlem Kalinli"
      ],
      "published": "2024-11-12T07:30:29Z",
      "updated": "2024-12-31T21:11:07Z",
      "categories": [
        "eess.AS",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.07607v2",
      "landing_url": "https://arxiv.org/abs/2411.07607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.07607"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discrete audio token generation/quantization or token-focused design is described, so it fails to meet the inclusion criteria centered on discrete token research."
    },
    "round-A_JuniorNano_reasoning": "No discrete audio token generation/quantization or token-focused design is described, so it fails to meet the inclusion criteria centered on discrete token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
    "abstract": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
    "metadata": {
      "arxiv_id": "2411.08742",
      "title": "A Comparative Study of Discrete Speech Tokens for Semantic-Related Tasks with Large Language Models",
      "summary": "With the rise of Speech Large Language Models (Speech LLMs), there has been growing interest in discrete speech tokens for their ability to integrate with text-based tokens seamlessly. Compared to most studies that focus on continuous speech features, although discrete-token based LLMs have shown promising results on certain tasks, the performance gap between these two paradigms is rarely explored. In this paper, we present a fair and thorough comparison between discrete and continuous features across a variety of semantic-related tasks using a light-weight LLM (Qwen1.5-0.5B). Our findings reveal that continuous features generally outperform discrete tokens, particularly in tasks requiring fine-grained semantic understanding. Moreover, this study goes beyond surface-level comparison by identifying key factors behind the under-performance of discrete tokens, such as limited token granularity and inefficient information retention. To enhance the performance of discrete tokens, we explore potential aspects based on our analysis. We hope our results can offer new insights into the opportunities for advancing discrete speech tokens in Speech LLMs.",
      "authors": [
        "Dingdong Wang",
        "Mingyu Cui",
        "Dongchao Yang",
        "Xueyuan Chen",
        "Helen Meng"
      ],
      "published": "2024-11-13T16:20:20Z",
      "updated": "2024-11-13T16:20:20Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.08742v1",
      "landing_url": "https://arxiv.org/abs/2411.08742v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.08742"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The study explicitly compares discrete speech tokens (with tokenization methods and semantic downstream evaluation) against continuous features in Speech LLMs, so it satisfies the inclusion criteria and avoids exclusions, warranting a score of 5."
    },
    "round-A_JuniorNano_reasoning": "The study explicitly compares discrete speech tokens (with tokenization methods and semantic downstream evaluation) against continuous features in Speech LLMs, so it satisfies the inclusion criteria and avoids exclusions, warranting a score of 5.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention",
    "abstract": "To address the sycophancy problem caused by reinforcement learning from human feedback in large language models, this research applies synthetic data intervention technology to the decoder-only transformer architecture. Based on the research gaps in the existing literature, the researcher designed an experimental process to reduce the tendency of models to cater by generating diversified data, and used GPT4o as an experimental tool for verification. The experiment used 100 true and false questions, and compared the performance of the model trained with synthetic data intervention and the original untrained model on multiple indicators. The results show that the SDI training model supports the technology in terms of accuracy rate and sycophancy rate and has significant effectiveness in reducing sycophancy phenomena.",
    "metadata": {
      "arxiv_id": "2411.10156",
      "title": "Mitigating Sycophancy in Decoder-Only Transformer Architectures: Synthetic Data Intervention",
      "summary": "To address the sycophancy problem caused by reinforcement learning from human feedback in large language models, this research applies synthetic data intervention technology to the decoder-only transformer architecture. Based on the research gaps in the existing literature, the researcher designed an experimental process to reduce the tendency of models to cater by generating diversified data, and used GPT4o as an experimental tool for verification. The experiment used 100 true and false questions, and compared the performance of the model trained with synthetic data intervention and the original untrained model on multiple indicators. The results show that the SDI training model supports the technology in terms of accuracy rate and sycophancy rate and has significant effectiveness in reducing sycophancy phenomena.",
      "authors": [
        "Libo Wang"
      ],
      "published": "2024-11-15T12:59:46Z",
      "updated": "2025-03-20T13:29:49Z",
      "categories": [
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10156v5",
      "landing_url": "https://arxiv.org/abs/2411.10156v5",
      "doi": "https://doi.org/10.48550/arXiv.2411.10156"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on mitigating sycophancy in decoder-only text LMs via synthetic data, without any mention of discrete audio tokens, tokenization/codec design, or audio evaluations, so it fails inclusion and matches exclusion (non-audio discrete tokens)."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on mitigating sycophancy in decoder-only text LMs via synthetic data, without any mention of discrete audio tokens, tokenization/codec design, or audio evaluations, so it fails inclusion and matches exclusion (non-audio discrete tokens).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
    "abstract": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
    "metadata": {
      "arxiv_id": "2411.10293",
      "title": "RETR: Multi-View Radar Detection Transformer for Indoor Perception",
      "summary": "Indoor radar perception has seen rising interest due to affordable costs driven by emerging automotive imaging radar developments and the benefits of reduced privacy concerns and reliability under hazardous conditions (e.g., fire and smoke). However, existing radar perception pipelines fail to account for distinctive characteristics of the multi-view radar setting. In this paper, we propose Radar dEtection TRansformer (RETR), an extension of the popular DETR architecture, tailored for multi-view radar perception. RETR inherits the advantages of DETR, eliminating the need for hand-crafted components for object detection and segmentation in the image plane. More importantly, RETR incorporates carefully designed modifications such as 1) depth-prioritized feature similarity via a tunable positional encoding (TPE); 2) a tri-plane loss from both radar and camera coordinates; and 3) a learnable radar-to-camera transformation via reparameterization, to account for the unique multi-view radar setting. Evaluated on two indoor radar perception datasets, our approach outperforms existing state-of-the-art methods by a margin of 15.38+ AP for object detection and 11.91+ IoU for instance segmentation, respectively. Our implementation is available at https://github.com/merlresearch/radar-detection-transformer.",
      "authors": [
        "Ryoma Yataka",
        "Adriano Cardace",
        "Pu Perry Wang",
        "Petros Boufounos",
        "Ryuhei Takahashi"
      ],
      "published": "2024-11-15T15:51:25Z",
      "updated": "2025-01-17T19:06:26Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "math.DG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.10293v3",
      "landing_url": "https://arxiv.org/abs/2411.10293v3",
      "doi": "https://doi.org/10.48550/arXiv.2411.10293"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on indoor multi-view radar perception and a transformer-based detector, which is unrelated to discrete audio tokenization or codec/token evaluations required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on indoor multi-view radar perception and a transformer-based detector, which is unrelated to discrete audio tokenization or codec/token evaluations required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers",
    "abstract": "Generating synthetic Electronic Health Records (EHRs) offers significant potential for data augmentation, privacy-preserving data sharing, and improving machine learning model training. We propose a novel tokenization strategy tailored for structured EHR data, which encompasses diverse data types such as covariates, ICD codes, and irregularly sampled time series. Using a GPT-like decoder-only transformer model, we demonstrate the generation of high-quality synthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we benchmark the fidelity, utility, and privacy of the generated data against state-of-the-art models.",
    "metadata": {
      "arxiv_id": "2411.13428",
      "title": "SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers",
      "summary": "Generating synthetic Electronic Health Records (EHRs) offers significant potential for data augmentation, privacy-preserving data sharing, and improving machine learning model training. We propose a novel tokenization strategy tailored for structured EHR data, which encompasses diverse data types such as covariates, ICD codes, and irregularly sampled time series. Using a GPT-like decoder-only transformer model, we demonstrate the generation of high-quality synthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we benchmark the fidelity, utility, and privacy of the generated data against state-of-the-art models.",
      "authors": [
        "Hojjat Karami",
        "David Atienza",
        "Anisoara Ionescu"
      ],
      "published": "2024-11-20T16:11:20Z",
      "updated": "2024-11-20T16:11:20Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.13428v1",
      "landing_url": "https://arxiv.org/abs/2411.13428v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.13428"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on generating synthetic structured EHR data with a GPT-style transformer and does not address discrete audio tokenization or codec/semantic token design, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on generating synthetic structured EHR data with a GPT-style transformer and does not address discrete audio tokenization or codec/semantic token design, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
    "abstract": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
    "metadata": {
      "arxiv_id": "2411.14100",
      "title": "BEST-STD: Bidirectional Mamba-Enhanced Speech Tokenization for Spoken Term Detection",
      "summary": "Spoken term detection (STD) is often hindered by reliance on frame-level features and the computationally intensive DTW-based template matching, limiting its practicality. To address these challenges, we propose a novel approach that encodes speech into discrete, speaker-agnostic semantic tokens. This facilitates fast retrieval using text-based search algorithms and effectively handles out-of-vocabulary terms. Our approach focuses on generating consistent token sequences across varying utterances of the same term. We also propose a bidirectional state space modeling within the Mamba encoder, trained in a self-supervised learning framework, to learn contextual frame-level features that are further encoded into discrete tokens. Our analysis shows that our speech tokens exhibit greater speaker invariance than those from existing tokenizers, making them more suitable for STD tasks. Empirical evaluation on LibriSpeech and TIMIT databases indicates that our method outperforms existing STD baselines while being more efficient.",
      "authors": [
        "Anup Singh",
        "Kris Demuynck",
        "Vipul Arora"
      ],
      "published": "2024-11-21T13:05:18Z",
      "updated": "2024-12-21T19:15:27Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14100v2",
      "landing_url": "https://arxiv.org/abs/2411.14100v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14100"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Proposes a discrete, self-supervised speech tokenizer that maps audio to speaker-agnostic tokens, evaluates invariance and STD performance, and thus fully matches the discrete-audio-token inclusion criteria without hitting any exclusion rules."
    },
    "round-A_JuniorNano_reasoning": "Proposes a discrete, self-supervised speech tokenizer that maps audio to speaker-agnostic tokens, evaluates invariance and STD performance, and thus fully matches the discrete-audio-token inclusion criteria without hitting any exclusion rules.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models",
    "abstract": "Adversarial audio attacks pose a significant threat to the growing use of large audio-language models (LALMs) in voice-based human-machine interactions. While existing research focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the vulnerabilities of LALMs to these audio attacks in conversational scenarios. To evaluate the robustness of LALMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-4o-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LALMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-4o exhibits the highest level of resilience. Our data can be accessed via the following link: \\href{https://github.com/crystraldo/CAA}{CAA}.",
    "metadata": {
      "arxiv_id": "2411.14842",
      "title": "Who Can Withstand Chat-Audio Attacks? An Evaluation Benchmark for Large Audio-Language Models",
      "summary": "Adversarial audio attacks pose a significant threat to the growing use of large audio-language models (LALMs) in voice-based human-machine interactions. While existing research focused on model-specific adversarial methods, real-world applications demand a more generalizable and universal approach to audio adversarial attacks. In this paper, we introduce the Chat-Audio Attacks (CAA) benchmark including four distinct types of audio attacks, which aims to explore the vulnerabilities of LALMs to these audio attacks in conversational scenarios. To evaluate the robustness of LALMs, we propose three evaluation strategies: Standard Evaluation, utilizing traditional metrics to quantify model performance under attacks; GPT-4o-Based Evaluation, which simulates real-world conversational complexities; and Human Evaluation, offering insights into user perception and trust. We evaluate six state-of-the-art LALMs with voice interaction capabilities, including Gemini-1.5-Pro, GPT-4o, and others, using three distinct evaluation methods on the CAA benchmark. Our comprehensive analysis reveals the impact of four types of audio attacks on the performance of these models, demonstrating that GPT-4o exhibits the highest level of resilience. Our data can be accessed via the following link: \\href{https://github.com/crystraldo/CAA}{CAA}.",
      "authors": [
        "Wanqi Yang",
        "Yanda Li",
        "Meng Fang",
        "Yunchao Wei",
        "Ling Chen"
      ],
      "published": "2024-11-22T10:30:48Z",
      "updated": "2025-06-06T07:43:02Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.14842v2",
      "landing_url": "https://arxiv.org/abs/2411.14842v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.14842"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Does not discuss discrete audio tokens/tokenizers or quantized vocabularies; it focuses on adversarial robustness of LALMs under audio attacks, so it fails all inclusion criteria and hits exclusion."
    },
    "round-A_JuniorNano_reasoning": "Does not discuss discrete audio tokens/tokenizers or quantized vocabularies; it focuses on adversarial robustness of LALMs under audio attacks, so it fails all inclusion criteria and hits exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "State-Space Large Audio Language Models",
    "abstract": "Large Audio Language Models (LALM) combine the audio perception models and the Large Language Models (LLM) and show a remarkable ability to reason about the input audio, infer the meaning, and understand the intent. However, these systems rely on Transformers which scale quadratically with the input sequence lengths which poses computational challenges in deploying these systems in memory and time-constrained scenarios. Recently, the state-space models (SSMs) have emerged as an alternative to transformer networks. While there have been successful attempts to replace transformer-based audio perception models with state-space ones, state-space-based LALMs remain unexplored. First, we begin by replacing the transformer-based audio perception module and then replace the transformer-based LLM and propose the first state-space-based LALM. Experimental results demonstrate that space-based LALM despite having a significantly lower number of parameters performs competitively with transformer-based LALMs on close-ended tasks on a variety of datasets.",
    "metadata": {
      "arxiv_id": "2411.15685",
      "title": "State-Space Large Audio Language Models",
      "summary": "Large Audio Language Models (LALM) combine the audio perception models and the Large Language Models (LLM) and show a remarkable ability to reason about the input audio, infer the meaning, and understand the intent. However, these systems rely on Transformers which scale quadratically with the input sequence lengths which poses computational challenges in deploying these systems in memory and time-constrained scenarios. Recently, the state-space models (SSMs) have emerged as an alternative to transformer networks.\n  While there have been successful attempts to replace transformer-based audio perception models with state-space ones, state-space-based LALMs remain unexplored. First, we begin by replacing the transformer-based audio perception module and then replace the transformer-based LLM and propose the first state-space-based LALM. Experimental results demonstrate that space-based LALM despite having a significantly lower number of parameters performs competitively with transformer-based LALMs on close-ended tasks on a variety of datasets.",
      "authors": [
        "Saurabhchand Bhati",
        "Yuan Gong",
        "Leonid Karlinsky",
        "Hilde Kuehne",
        "Rogerio Feris",
        "James Glass"
      ],
      "published": "2024-11-24T02:21:28Z",
      "updated": "2024-11-24T02:21:28Z",
      "categories": [
        "eess.AS",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.15685v1",
      "landing_url": "https://arxiv.org/abs/2411.15685v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.15685"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on replacing transformers with state-space models in end-to-end audio language models without discussing any discrete audio tokenization/quantization pipeline or token-level evaluation, so it does not meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on replacing transformers with state-space models in end-to-end audio language models without discussing any discrete audio tokenization/quantization pipeline or token-level evaluation, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
    "abstract": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
    "metadata": {
      "arxiv_id": "2411.16119",
      "title": "Learning Optimal Lattice Vector Quantizers for End-to-end Neural Image Compression",
      "summary": "It is customary to deploy uniform scalar quantization in the end-to-end optimized Neural image compression methods, instead of more powerful vector quantization, due to the high complexity of the latter. Lattice vector quantization (LVQ), on the other hand, presents a compelling alternative, which can exploit inter-feature dependencies more effectively while keeping computational efficiency almost the same as scalar quantization. However, traditional LVQ structures are designed/optimized for uniform source distributions, hence nonadaptive and suboptimal for real source distributions of latent code space for Neural image compression tasks. In this paper, we propose a novel learning method to overcome this weakness by designing the rate-distortion optimal lattice vector quantization (OLVQ) codebooks with respect to the sample statistics of the latent features to be compressed. By being able to better fit the LVQ structures to any given latent sample distribution, the proposed OLVQ method improves the rate-distortion performances of the existing quantization schemes in neural image compression significantly, while retaining the amenability of uniform scalar quantization.",
      "authors": [
        "Xi Zhang",
        "Xiaolin Wu"
      ],
      "published": "2024-11-25T06:05:08Z",
      "updated": "2024-11-25T06:05:08Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16119v1",
      "landing_url": "https://arxiv.org/abs/2411.16119v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16119"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on lattice vector quantization for neural image compression and does not address discrete audio tokens, tokenizers, or audio codec semantics, so it clearly fails the topic-specific inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on lattice vector quantization for neural image compression and does not address discrete audio tokens, tokenizers, or audio codec semantics, so it clearly fails the topic-specific inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Representation Collapsing Problems in Vector Quantization",
    "abstract": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
    "metadata": {
      "arxiv_id": "2411.16550",
      "title": "Representation Collapsing Problems in Vector Quantization",
      "summary": "Vector quantization is a technique in machine learning that discretizes continuous representations into a set of discrete vectors. It is widely employed in tokenizing data representations for large language models, diffusion models, and other generative models. Despite its prevalence, the characteristics and behaviors of vector quantization in generative models remain largely underexplored. In this study, we investigate representation collapse in vector quantization - a critical degradation where codebook tokens or latent embeddings lose their discriminative power by converging to a limited subset of values. This collapse fundamentally compromises the model's ability to capture diverse data patterns. By leveraging both synthetic and real datasets, we identify the severity of each type of collapses and triggering conditions. Our analysis reveals that restricted initialization and limited encoder capacity result in tokens collapse and embeddings collapse. Building on these findings, we propose potential solutions aimed at mitigating each collapse. To the best of our knowledge, this is the first comprehensive study examining representation collapsing problems in vector quantization.",
      "authors": [
        "Wenhao Zhao",
        "Qiran Zou",
        "Rushi Shah",
        "Dianbo Liu"
      ],
      "published": "2024-11-25T16:32:29Z",
      "updated": "2024-11-25T16:32:29Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.16550v1",
      "landing_url": "https://arxiv.org/abs/2411.16550v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.16550"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on general vector quantization collapse in generative models, not on discrete audio-token/tokenizer design, quantized audio representations, or related evaluations, so it fails the inclusion and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Focuses on general vector quantization collapse in generative models, not on discrete audio-token/tokenizer design, quantized audio representations, or related evaluations, so it fails the inclusion and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
    "abstract": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
    "metadata": {
      "arxiv_id": "2411.17607",
      "title": "Scaling Speech-Text Pre-training with Synthetic Interleaved Data",
      "summary": "Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.",
      "authors": [
        "Aohan Zeng",
        "Zhengxiao Du",
        "Mingdao Liu",
        "Lei Zhang",
        "Shengmin Jiang",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "published": "2024-11-26T17:19:09Z",
      "updated": "2024-12-02T16:13:24Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17607v2",
      "landing_url": "https://arxiv.org/abs/2411.17607v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17607"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Includes a learned supervised speech tokenizer with a vector-quantized bottleneck producing discrete speech tokens used directly in downstream speech language modeling, thus meeting all inclusion criteria and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "Includes a learned supervised speech tokenizer with a vector-quantized bottleneck producing discrete speech tokens used directly in downstream speech language modeling, thus meeting all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
    "abstract": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
    "metadata": {
      "arxiv_id": "2411.17666",
      "title": "How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations",
      "summary": "Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.",
      "authors": [
        "Hyunji Lee",
        "Danni Liu",
        "Supriti Sinhamahapatra",
        "Jan Niehues"
      ],
      "published": "2024-11-26T18:29:11Z",
      "updated": "2025-02-20T18:04:45Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17666v2",
      "landing_url": "https://arxiv.org/abs/2411.17666v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17666"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study analyzes cross-modal representation shifts in foundation models without focusing on discrete audio tokens, tokenizers, quantization, or vocabulary-level design, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study analyzes cross-modal representation shifts in foundation models without focusing on discrete audio tokens, tokenizers, quantization, or vocabulary-level design, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis",
    "abstract": "The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the model's ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: https://apple.github.io/visatronic-demo/",
    "metadata": {
      "arxiv_id": "2411.17690",
      "title": "Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis",
      "summary": "The rapid progress of foundation models and large language models (LLMs) has fueled significantly improvement in the capabilities of machine learning systems that benefit from mutlimodal input data. However, existing multimodal models are predominantly built on top of pre-trained LLMs, which can limit accurate modeling of temporal dependencies across other modalities and thus limit the model's ability to jointly process and leverage multimodal inputs. To specifically investigate the alignment of text, video, and speech modalities in LLM-style (decoder-only) models, we consider a simplified multimodal generation task, Video-Text to Speech (VTTS): speech generation conditioned on both its corresponding text and video of talking people. The ultimate goal is to generate speech that not only follows the text but also aligns temporally with the video and is consistent with the facial expressions. In this paper, we first introduce Visatronic, a unified multimodal decoder-only transformer model that adopts an LLM-style architecture to embed visual, textual, and speech inputs into a shared subspace, treating all modalities as temporally aligned token streams. Next, we carefully explore different token mixing strategies to understand the best way to propagate information from the steps where video and text conditioning is input to the steps where the audio is generated. We extensively evaluate Visatronic on the challenging VoxCeleb2 dataset and demonstrate zero-shot generalization to LRS3, where Visatronic, trained on VoxCeleb2, achieves a 4.5% WER, outperforming prior SOTA methods trained only on LRS3, which report a 21.4% WER. Additionally, we propose a new objective metric, TimeSync, specifically designed to measure phoneme-level temporal alignment between generated and reference speech, further ensuring synchronization quality. Demo: https://apple.github.io/visatronic-demo/",
      "authors": [
        "Akshita Gupta",
        "Tatiana Likhomanenko",
        "Karren Dai Yang",
        "Richard He Bai",
        "Zakaria Aldeneh",
        "Navdeep Jaitly"
      ],
      "published": "2024-11-26T18:57:29Z",
      "updated": "2025-05-29T17:58:02Z",
      "categories": [
        "cs.MM",
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17690v2",
      "landing_url": "https://arxiv.org/abs/2411.17690v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.17690"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discrete audio tokenization, quantization, or token-level modeling is described in this VTTS decoder-only work, so it fails the discrete-token-focused inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "No discrete audio tokenization, quantization, or token-level modeling is described in this VTTS decoder-only work, so it fails the discrete-token-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
    "abstract": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
    "metadata": {
      "arxiv_id": "2411.17998",
      "title": "Speech Separation using Neural Audio Codecs with Embedding Loss",
      "summary": "Neural audio codecs have revolutionized audio processing by enabling speech tasks to be performed on highly compressed representations. Recent work has shown that speech separation can be achieved within these compressed domains, offering faster training and reduced inference costs. However, current approaches still rely on waveform-based loss functions, necessitating unnecessary decoding steps during training. We propose a novel embedding loss for neural audio codec-based speech separation that operates directly on compressed audio representations, eliminating the need for decoding during training. To validate our approach, we conduct comprehensive evaluations using both objective metrics and perceptual assessment techniques, including intrusive and non-intrusive methods. Our results demonstrate that embedding loss can be used to train codec-based speech separation models with a 2x improvement in training speed and computational cost while achieving better DNSMOS and STOI performance on the WSJ0-2mix dataset across 3 different pre-trained codecs.",
      "authors": [
        "Jia Qi Yip",
        "Chin Yuen Kwok",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2024-11-27T02:31:52Z",
      "updated": "2024-11-27T02:31:52Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.17998v1",
      "landing_url": "https://arxiv.org/abs/2411.17998v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.17998"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Paper trains speech separation directly on neural audio codec compressed representations with codec-specific evaluation metrics, aligning with discrete audio token and codec-focused inclusion criteria, so it’s stronger to include."
    },
    "round-A_JuniorNano_reasoning": "Paper trains speech separation directly on neural audio codec compressed representations with codec-specific evaluation metrics, aligning with discrete audio token and codec-focused inclusion criteria, so it’s stronger to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec",
    "abstract": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.",
    "metadata": {
      "arxiv_id": "2411.18803",
      "title": "TS3-Codec: Transformer-Based Simple Streaming Single Codec",
      "summary": "Neural audio codecs (NACs) have garnered significant attention as key technologies for audio compression as well as audio representation for speech language models. While mainstream NAC models are predominantly convolution-based, the performance of NACs with a purely transformer-based, and convolution-free architecture remains unexplored. This paper introduces TS3-Codec, a Transformer-Based Simple Streaming Single Codec. TS3-Codec consists of only a stack of transformer layers with a few linear layers, offering greater simplicity and expressiveness by fully eliminating convolution layers that require careful hyperparameter tuning and large computations. Under the streaming setup, the proposed TS3-Codec achieves comparable or superior performance compared to the codec with state-of-the-art convolution-based architecture while requiring only 12% of the computation and 77% of bitrate. Furthermore, it significantly outperforms the convolution-based codec when using similar computational resources.",
      "authors": [
        "Haibin Wu",
        "Naoyuki Kanda",
        "Sefik Emre Eskimez",
        "Jinyu Li"
      ],
      "published": "2024-11-27T23:07:52Z",
      "updated": "2025-04-27T16:37:28Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.18803v2",
      "landing_url": "https://arxiv.org/abs/2411.18803v2",
      "doi": "https://doi.org/10.48550/arXiv.2411.18803"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Transformer-only codec that outputs quantized discrete tokens for efficient streaming audio compression with competitive evaluation aligns well with the discrete audio token inclusion criteria and contradicts no exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Transformer-only codec that outputs quantized discrete tokens for efficient streaming audio compression with competitive evaluation aligns well with the discrete audio token inclusion criteria and contradicts no exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MCUCoder: Adaptive Bitrate Learned Video Compression for IoT Devices",
    "abstract": "The rapid growth of camera-based IoT devices demands the need for efficient video compression, particularly for edge applications where devices face hardware constraints, often with only 1 or 2 MB of RAM and unstable internet connections. Traditional and deep video compression methods are designed for high-end hardware, exceeding the capabilities of these constrained devices. Consequently, video compression in these scenarios is often limited to M-JPEG due to its high hardware efficiency and low complexity. This paper introduces , an open-source adaptive bitrate video compression model tailored for resource-limited IoT settings. MCUCoder features an ultra-lightweight encoder with only 10.5K parameters and a minimal 350KB memory footprint, making it well-suited for edge devices and MCUs. While MCUCoder uses a similar amount of energy as M-JPEG, it reduces bitrate by 55.65% on the MCL-JCV dataset and 55.59% on the UVG dataset, measured in MS-SSIM. Moreover, MCUCoder supports adaptive bitrate streaming by generating a latent representation that is sorted by importance, allowing transmission based on available bandwidth. This ensures smooth real-time video transmission even under fluctuating network conditions on low-resource devices. Source code available at https://github.com/ds-kiel/MCUCoder.",
    "metadata": {
      "arxiv_id": "2411.19442",
      "title": "MCUCoder: Adaptive Bitrate Learned Video Compression for IoT Devices",
      "summary": "The rapid growth of camera-based IoT devices demands the need for efficient video compression, particularly for edge applications where devices face hardware constraints, often with only 1 or 2 MB of RAM and unstable internet connections. Traditional and deep video compression methods are designed for high-end hardware, exceeding the capabilities of these constrained devices. Consequently, video compression in these scenarios is often limited to M-JPEG due to its high hardware efficiency and low complexity. This paper introduces , an open-source adaptive bitrate video compression model tailored for resource-limited IoT settings. MCUCoder features an ultra-lightweight encoder with only 10.5K parameters and a minimal 350KB memory footprint, making it well-suited for edge devices and MCUs. While MCUCoder uses a similar amount of energy as M-JPEG, it reduces bitrate by 55.65% on the MCL-JCV dataset and 55.59% on the UVG dataset, measured in MS-SSIM. Moreover, MCUCoder supports adaptive bitrate streaming by generating a latent representation that is sorted by importance, allowing transmission based on available bandwidth. This ensures smooth real-time video transmission even under fluctuating network conditions on low-resource devices. Source code available at https://github.com/ds-kiel/MCUCoder.",
      "authors": [
        "Ali Hojjat",
        "Janek Haberer",
        "Olaf Landsiedel"
      ],
      "published": "2024-11-29T03:00:21Z",
      "updated": "2024-11-29T03:00:21Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19442v1",
      "landing_url": "https://arxiv.org/abs/2411.19442v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.19442"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on lightweight video compression for IoT rather than learning or using discrete audio tokens, so it fails all inclusion criteria and fits the exclusion rules (non-audio discrete sequences)."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on lightweight video compression for IoT rather than learning or using discrete audio tokens, so it fails all inclusion criteria and fits the exclusion rules (non-audio discrete sequences).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding",
    "abstract": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.",
    "metadata": {
      "arxiv_id": "2411.19842",
      "title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding",
      "summary": "The tokenization of speech with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by scaling a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.",
      "authors": [
        "Julian D Parker",
        "Anton Smirnov",
        "Jordi Pons",
        "CJ Carr",
        "Zack Zukowski",
        "Zach Evans",
        "Xubo Liu"
      ],
      "published": "2024-11-29T16:58:02Z",
      "updated": "2024-11-29T16:58:02Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2411.19842v1",
      "landing_url": "https://arxiv.org/abs/2411.19842v1",
      "doi": "https://doi.org/10.48550/arXiv.2411.19842"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Scaling a transformer codec to produce discrete neural audio tokens at very low bitrates with quantized bottlenecking and objective/subjective evaluations matches the discrete token codec criteria, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "Scaling a transformer codec to produce discrete neural audio tokens at very low bitrates with quantized bottlenecking and objective/subjective evaluations matches the discrete token codec criteria, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "FreeCodec: A disentangled neural speech codec with fewer tokens",
    "abstract": "Neural speech codecs have gained great attention for their outstanding reconstruction with discrete token representations. It is a crucial component in generative tasks such as speech coding and large language models (LLM). However, most works based on residual vector quantization perform worse with fewer tokens due to low coding efficiency for modeling complex coupled information. In this paper, we propose a neural speech codec named FreeCodec which employs a more effective encoding framework by decomposing intrinsic properties of speech into different components: 1) a global vector is extracted as the timbre information, 2) a prosody encoder with a long stride level is used to model the prosody information, 3) the content information is from a content encoder. Using different training strategies, FreeCodec achieves state-of-the-art performance in reconstruction and disentanglement scenarios. Results from subjective and objective experiments demonstrate that our framework outperforms existing methods.",
    "metadata": {
      "arxiv_id": "2412.01053",
      "title": "FreeCodec: A disentangled neural speech codec with fewer tokens",
      "summary": "Neural speech codecs have gained great attention for their outstanding reconstruction with discrete token representations.\n  It is a crucial component in generative tasks such as speech coding and large language models (LLM).\n  However, most works based on residual vector quantization perform worse with fewer tokens due to low coding efficiency for modeling complex coupled information.\n  In this paper, we propose a neural speech codec named FreeCodec which employs a more effective encoding framework by decomposing intrinsic properties of speech into different components:\n  1) a global vector is extracted as the timbre information,\n  2) a prosody encoder with a long stride level is used to model the prosody information,\n  3) the content information is from a content encoder.\n  Using different training strategies, FreeCodec achieves state-of-the-art performance in reconstruction and disentanglement scenarios.\n  Results from subjective and objective experiments demonstrate that our framework outperforms existing methods.",
      "authors": [
        "Youqiang Zheng",
        "Weiping Tu",
        "Yueteng Kang",
        "Jie Chen",
        "Yike Zhang",
        "Li Xiao",
        "Yuhong Yang",
        "Long Ma"
      ],
      "published": "2024-12-02T02:29:22Z",
      "updated": "2025-06-28T09:45:02Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01053v3",
      "landing_url": "https://arxiv.org/abs/2412.01053v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.01053"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Title/abstract describe a neural speech codec producing discrete tokens for reconstruction and disentanglement, with benchmarks, so meets the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe a neural speech codec producing discrete tokens for reconstruction and disentanglement, with benchmarks, so meets the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
    "abstract": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.",
    "metadata": {
      "arxiv_id": "2412.01827",
      "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
      "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a \"position instruction token\" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.",
      "authors": [
        "Ziqi Pang",
        "Tianyuan Zhang",
        "Fujun Luan",
        "Yunze Man",
        "Hao Tan",
        "Kai Zhang",
        "William T. Freeman",
        "Yu-Xiong Wang"
      ],
      "published": "2024-12-02T18:59:53Z",
      "updated": "2025-07-08T00:51:16Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.01827v2",
      "landing_url": "https://arxiv.org/abs/2412.01827v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.01827"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper targets decoder-only visual generation with image tokens and no discussion of discrete audio tokens or codecs, so it fails the inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper targets decoder-only visual generation with image tokens and no discussion of discrete audio tokens or codecs, so it fails the inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Language Transfer Capability of Decoder-only Architecture in Multilingual Neural Machine Translation",
    "abstract": "Existing multilingual neural machine translation (MNMT) approaches mainly focus on improving models with the encoder-decoder architecture to translate multiple languages. However, decoder-only architecture has been explored less in MNMT due to its underperformance when trained on parallel data solely. In this work, we attribute the issue of the decoder-only architecture to its lack of language transfer capability. Specifically, the decoder-only architecture is insufficient in encoding source tokens with the target language features. We propose dividing the decoding process into two stages so that target tokens are explicitly excluded in the first stage to implicitly boost the transfer capability across languages. Additionally, we impose contrastive learning on translation instructions, resulting in improved performance in zero-shot translation. We conduct experiments on TED-19 and OPUS-100 datasets, considering both training from scratch and fine-tuning scenarios. Experimental results show that, compared to the encoder-decoder architecture, our methods not only perform competitively in supervised translations but also achieve improvements of up to 3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, and 4.81 COMET in zero-shot translations.",
    "metadata": {
      "arxiv_id": "2412.02101",
      "title": "Improving Language Transfer Capability of Decoder-only Architecture in Multilingual Neural Machine Translation",
      "summary": "Existing multilingual neural machine translation (MNMT) approaches mainly focus on improving models with the encoder-decoder architecture to translate multiple languages. However, decoder-only architecture has been explored less in MNMT due to its underperformance when trained on parallel data solely. In this work, we attribute the issue of the decoder-only architecture to its lack of language transfer capability. Specifically, the decoder-only architecture is insufficient in encoding source tokens with the target language features. We propose dividing the decoding process into two stages so that target tokens are explicitly excluded in the first stage to implicitly boost the transfer capability across languages. Additionally, we impose contrastive learning on translation instructions, resulting in improved performance in zero-shot translation. We conduct experiments on TED-19 and OPUS-100 datasets, considering both training from scratch and fine-tuning scenarios. Experimental results show that, compared to the encoder-decoder architecture, our methods not only perform competitively in supervised translations but also achieve improvements of up to 3.39 BLEU, 6.99 chrF++, 3.22 BERTScore, and 4.81 COMET in zero-shot translations.",
      "authors": [
        "Zhi Qu",
        "Yiran Wang",
        "Chenchen Ding",
        "Hideki Tanaka",
        "Masao Utiyama",
        "Taro Watanabe"
      ],
      "published": "2024-12-03T02:52:14Z",
      "updated": "2024-12-03T02:52:14Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02101v1",
      "landing_url": "https://arxiv.org/abs/2412.02101v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02101"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies decoder-only multilingual text translation without any discrete audio token modeling, so it clearly fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper studies decoder-only multilingual text translation without any discrete audio token modeling, so it clearly fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Semantic Tokens in Retrieval Augmented Generation",
    "abstract": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
    "metadata": {
      "arxiv_id": "2412.02563",
      "title": "Semantic Tokens in Retrieval Augmented Generation",
      "summary": "Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.",
      "authors": [
        "Joel Suro"
      ],
      "published": "2024-12-03T16:52:06Z",
      "updated": "2024-12-03T16:52:06Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.02563v1",
      "landing_url": "https://arxiv.org/abs/2412.02563v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.02563"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on retrieval-augmented generation for text answers and does not address discrete audio tokens or any codec/tokenization of audio, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on retrieval-augmented generation for text answers and does not address discrete audio tokens or any codec/tokenization of audio, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
    "abstract": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at https://github.com/FoundationVision/Liquid.",
    "metadata": {
      "arxiv_id": "2412.04332",
      "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators",
      "summary": "We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at https://github.com/FoundationVision/Liquid.",
      "authors": [
        "Junfeng Wu",
        "Yi Jiang",
        "Chuofan Ma",
        "Yuliang Liu",
        "Hengshuang Zhao",
        "Zehuan Yuan",
        "Song Bai",
        "Xiang Bai"
      ],
      "published": "2024-12-05T16:48:16Z",
      "updated": "2025-04-10T18:28:11Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04332v4",
      "landing_url": "https://arxiv.org/abs/2412.04332v4",
      "doi": "https://doi.org/10.48550/arXiv.2412.04332"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Liquid focuses on multimodal visual-language generation without any discrete audio tokenization or codec-centric methods, so it fails the audio-token inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Liquid focuses on multimodal visual-language generation without any discrete audio tokenization or codec-centric methods, so it fails the audio-token inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
    "abstract": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
    "metadata": {
      "arxiv_id": "2412.04917",
      "title": "Continuous Speech Tokens Makes LLMs Robust Multi-Modality Learners",
      "summary": "Recent advances in GPT-4o like multi-modality models have demonstrated remarkable progress for direct speech-to-speech conversation, with real-time speech interaction experience and strong speech understanding ability. However, current research focuses on discrete speech tokens to align with discrete text tokens for language modelling, which depends on an audio codec with residual connections or independent group tokens, such a codec usually leverages large scale and diverse datasets training to ensure that the discrete speech codes have good representation for varied domain, noise, style data reconstruction as well as a well-designed codec quantizer and encoder-decoder architecture for discrete token language modelling. This paper introduces Flow-Omni, a continuous speech token based GPT-4o like model, capable of real-time speech interaction and low streaming latency. Specifically, first, instead of cross-entropy loss only, we combine flow matching loss with a pretrained autoregressive LLM and a small MLP network to predict the probability distribution of the continuous-valued speech tokens from speech prompt. second, we incorporated the continuous speech tokens to Flow-Omni multi-modality training, thereby achieving robust speech-to-speech performance with discrete text tokens and continuous speech tokens together. Experiments demonstrate that, compared to discrete text and speech multi-modality training and its variants, the continuous speech tokens mitigate robustness issues by avoiding the inherent flaws of discrete speech code's representation loss for LLM.",
      "authors": [
        "Ze Yuan",
        "Yanqing Liu",
        "Shujie Liu",
        "Sheng Zhao"
      ],
      "published": "2024-12-06T10:16:04Z",
      "updated": "2024-12-06T10:16:04Z",
      "categories": [
        "cs.SD",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.04917v1",
      "landing_url": "https://arxiv.org/abs/2412.04917v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.04917"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on continuous speech tokens rather than discrete audio tokens and lacks the required finite vocab/codebook formulation, so it fails the inclusion criteria and matches the exclusion rule about continuous representations."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on continuous speech tokens rather than discrete audio tokens and lacks the required finite vocab/codebook formulation, so it fails the inclusion criteria and matches the exclusion rule about continuous representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models",
    "abstract": "Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at https://adu-bench.github.io/.",
    "metadata": {
      "arxiv_id": "2412.05167",
      "title": "Benchmarking Open-ended Audio Dialogue Understanding for Large Audio-Language Models",
      "summary": "Large Audio-Language Models (LALMs), such as GPT-4o, have recently unlocked audio dialogue capabilities, enabling direct spoken exchanges with humans. The potential of LALMs broadens their applicability across a wide range of practical scenarios supported by audio dialogues. However, given these advancements, a comprehensive benchmark to evaluate the performance of LALMs in the open-ended audio dialogue understanding remains absent currently. To address this gap, we propose an Audio Dialogue Understanding Benchmark (ADU-Bench), which consists of 4 benchmark datasets. They assess the open-ended audio dialogue ability for LALMs in 3 general scenarios, 12 skills, 9 multilingual languages, and 4 categories of ambiguity handling. Notably, we firstly propose the evaluation of ambiguity handling in audio dialogues that expresses different intentions beyond the same literal meaning of sentences, e.g., \"Really!?\" with different intonations. In summary, ADU-Bench includes over 20,000 open-ended audio dialogues for the assessment of LALMs. Through extensive experiments on 16 LALMs, our analysis reveals that existing LALMs struggle with mathematical symbols and formulas, understanding human behavior such as roleplay, comprehending multiple languages, and handling audio dialogue ambiguities from different phonetic elements, such as intonations, pause positions, and homophones. The benchmark is available at https://adu-bench.github.io/.",
      "authors": [
        "Kuofeng Gao",
        "Shu-Tao Xia",
        "Ke Xu",
        "Philip Torr",
        "Jindong Gu"
      ],
      "published": "2024-12-06T16:34:15Z",
      "updated": "2025-07-28T15:07:08Z",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.05167v2",
      "landing_url": "https://arxiv.org/abs/2412.05167v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.05167"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes benchmarking audio dialogue understanding for LALMs but lacks any mention of discrete audio token/tokenizer design, quantization, or vocab specification, so it does not meet the inclusion focus on discrete token generation or analysis."
    },
    "round-A_JuniorNano_reasoning": "The paper describes benchmarking audio dialogue understanding for LALMs but lacks any mention of discrete audio token/tokenizer design, quantization, or vocab specification, so it does not meet the inclusion focus on discrete token generation or analysis.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "User Authentication and Vital Signs Extraction from Low-Frame-Rate and Monochrome No-contact Fingerprint Captures",
    "abstract": "We present our work on leveraging low-frame-rate monochrome (blue light) videos of fingertips, captured with an off-the-shelf fingerprint capture device, to extract vital signs and identify users. These videos utilize photoplethysmography (PPG), commonly used to measure vital signs like heart rate. While prior research predominantly utilizes high-frame-rate, multi-wavelength PPG sensors (e.g., infrared, red, or RGB), our preliminary findings demonstrate that both user identification and vital sign extraction are achievable with the low-frame-rate data we collected. Preliminary results are promising, with low error rates for both heart rate estimation and user authentication. These results indicate promise for effective biometric systems. We anticipate further optimization will enhance accuracy and advance healthcare and security.",
    "metadata": {
      "arxiv_id": "2412.07082",
      "title": "User Authentication and Vital Signs Extraction from Low-Frame-Rate and Monochrome No-contact Fingerprint Captures",
      "summary": "We present our work on leveraging low-frame-rate monochrome (blue light) videos of fingertips, captured with an off-the-shelf fingerprint capture device, to extract vital signs and identify users. These videos utilize photoplethysmography (PPG), commonly used to measure vital signs like heart rate. While prior research predominantly utilizes high-frame-rate, multi-wavelength PPG sensors (e.g., infrared, red, or RGB), our preliminary findings demonstrate that both user identification and vital sign extraction are achievable with the low-frame-rate data we collected. Preliminary results are promising, with low error rates for both heart rate estimation and user authentication. These results indicate promise for effective biometric systems. We anticipate further optimization will enhance accuracy and advance healthcare and security.",
      "authors": [
        "Olaoluwayimika Olugbenle",
        "Logan Drake",
        "Naveenkumar G. Venkataswamy",
        "Arfina Rahman",
        "Yemi Afolayanka",
        "Masudul Imtiaz",
        "Mahesh K. Banavar"
      ],
      "published": "2024-12-10T00:47:36Z",
      "updated": "2024-12-10T00:47:36Z",
      "categories": [
        "eess.IV",
        "cs.CR",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.07082v1",
      "landing_url": "https://arxiv.org/abs/2412.07082v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.07082"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on biometric signals from fingertip video rather than discrete audio tokens/tokenizers, so it fails all inclusion criteria and matches the exclusion of non-audio token research."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on biometric signals from fingertip video rather than discrete audio tokens/tokenizers, so it fails all inclusion criteria and matches the exclusion of non-audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Traffic Wastage in Video Streaming via Bandwidth-Efficient Bitrate Adaptation",
    "abstract": "Bitrate adaptation (also known as ABR) is a crucial technique to improve the quality of experience (QoE) for video streaming applications. However, existing ABR algorithms suffer from severe traffic wastage, which refers to the traffic cost of downloading the video segments that users do not finally consume, for example, due to early departure or video skipping. In this paper, we carefully formulate the dynamics of buffered data volume (BDV), a strongly correlated indicator of traffic wastage, which, to the best of our knowledge, is the first time to rigorously clarify the effect of downloading plans on potential wastage. To reduce wastage while keeping a high QoE, we present a bandwidth-efficient bitrate adaptation algorithm (named BE-ABR), achieving consistently low BDV without distinct QoE losses. Specifically, we design a precise, time-aware transmission delay prediction model over the Transformer architecture, and develop a fine-grained buffer control scheme. Through extensive experiments conducted on emulated and real network environments including WiFi, 4G, and 5G, we demonstrate that BE-ABR performs well in both QoE and bandwidth savings, enabling a 60.87\\% wastage reduction and a comparable, or even better, QoE, compared to the state-of-the-art methods.",
    "metadata": {
      "arxiv_id": "2412.07270",
      "title": "Reducing Traffic Wastage in Video Streaming via Bandwidth-Efficient Bitrate Adaptation",
      "summary": "Bitrate adaptation (also known as ABR) is a crucial technique to improve the quality of experience (QoE) for video streaming applications. However, existing ABR algorithms suffer from severe traffic wastage, which refers to the traffic cost of downloading the video segments that users do not finally consume, for example, due to early departure or video skipping. In this paper, we carefully formulate the dynamics of buffered data volume (BDV), a strongly correlated indicator of traffic wastage, which, to the best of our knowledge, is the first time to rigorously clarify the effect of downloading plans on potential wastage. To reduce wastage while keeping a high QoE, we present a bandwidth-efficient bitrate adaptation algorithm (named BE-ABR), achieving consistently low BDV without distinct QoE losses. Specifically, we design a precise, time-aware transmission delay prediction model over the Transformer architecture, and develop a fine-grained buffer control scheme. Through extensive experiments conducted on emulated and real network environments including WiFi, 4G, and 5G, we demonstrate that BE-ABR performs well in both QoE and bandwidth savings, enabling a 60.87\\% wastage reduction and a comparable, or even better, QoE, compared to the state-of-the-art methods.",
      "authors": [
        "Hairong Su",
        "Shibo Wang",
        "Shusen Yang",
        "Tianchi Huang",
        "Xuebin Ren"
      ],
      "published": "2024-12-10T07:52:23Z",
      "updated": "2024-12-10T07:52:23Z",
      "categories": [
        "cs.NI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.07270v1",
      "landing_url": "https://arxiv.org/abs/2412.07270v1",
      "doi": "https://doi.org/10.1109/TMC.2024.3373498"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on bitrate adaptation for video streaming and does not address discrete audio token generation, quantization, or evaluation, so it fails to meet the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on bitrate adaptation for video streaming and does not address discrete audio token generation, quantization, or evaluation, so it fails to meet the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
    "abstract": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
    "metadata": {
      "arxiv_id": "2412.07783",
      "title": "Swin fMRI Transformer Predicts Early Neurodevelopmental Outcomes from Neonatal fMRI",
      "summary": "Brain development in the first few months of human life is a critical phase characterized by rapid structural growth and functional organization. Accurately predicting developmental outcomes during this time is crucial for identifying delays and enabling timely interventions. This study introduces the SwiFT (Swin 4D fMRI Transformer) model, designed to predict Bayley-III composite scores using neonatal fMRI from the Developing Human Connectome Project (dHCP). To enhance predictive accuracy, we apply dimensionality reduction via group independent component analysis (ICA) and pretrain SwiFT on large adult fMRI datasets to address the challenges of limited neonatal data. Our analysis shows that SwiFT significantly outperforms baseline models in predicting cognitive, motor, and language outcomes, leveraging both single-label and multi-label prediction strategies. The model's attention-based architecture processes spatiotemporal data end-to-end, delivering superior predictive performance. Additionally, we use Integrated Gradients with Smoothgrad sQuare (IG-SQ) to interpret predictions, identifying neural spatial representations linked to early cognitive and behavioral development. These findings underscore the potential of Transformer models to advance neurodevelopmental research and clinical practice.",
      "authors": [
        "Patrick Styll",
        "Dowon Kim",
        "Jiook Cha"
      ],
      "published": "2024-11-25T12:20:07Z",
      "updated": "2025-01-30T10:33:33Z",
      "categories": [
        "q-bio.NC",
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.07783v3",
      "landing_url": "https://arxiv.org/abs/2412.07783v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.07783"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on neonatal fMRI-based outcome prediction with transformer models and contains no work on discrete audio tokens, tokenizers, quantization, or vocabularies, so it fails the inclusion scope and hits the exclusion on non-audio sequences."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on neonatal fMRI-based outcome prediction with transformer models and contains no work on discrete audio tokens, tokenizers, quantization, or vocabularies, so it fails the inclusion scope and hits the exclusion on non-audio sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
    "abstract": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
    "metadata": {
      "arxiv_id": "2412.08117",
      "title": "LatentSpeech: Latent Diffusion for Text-To-Speech Generation",
      "summary": "Diffusion-based Generative AI gains significant attention for its superior performance over other generative techniques like Generative Adversarial Networks and Variational Autoencoders. While it has achieved notable advancements in fields such as computer vision and natural language processing, their application in speech generation remains under-explored. Mainstream Text-to-Speech systems primarily map outputs to Mel-Spectrograms in the spectral space, leading to high computational loads due to the sparsity of MelSpecs. To address these limitations, we propose LatentSpeech, a novel TTS generation approach utilizing latent diffusion models. By using latent embeddings as the intermediate representation, LatentSpeech reduces the target dimension to 5% of what is required for MelSpecs, simplifying the processing for the TTS encoder and vocoder and enabling efficient high-quality speech generation. This study marks the first integration of latent diffusion models in TTS, enhancing the accuracy and naturalness of generated speech. Experimental results on benchmark datasets demonstrate that LatentSpeech achieves a 25% improvement in Word Error Rate and a 24% improvement in Mel Cepstral Distortion compared to existing models, with further improvements rising to 49.5% and 26%, respectively, with additional training data. These findings highlight the potential of LatentSpeech to advance the state-of-the-art in TTS technology",
      "authors": [
        "Haowei Lou",
        "Helen Paik",
        "Pari Delir Haghighi",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2024-12-11T05:55:06Z",
      "updated": "2024-12-11T05:55:06Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08117v1",
      "landing_url": "https://arxiv.org/abs/2412.08117v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08117"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "LatentSpeech focuses on continuous latent embeddings for diffusion-based TTS rather than defining or evaluating discrete audio tokens/quantized vocabularies, so it fails the inclusion criterion and meets the exclusion condition."
    },
    "round-A_JuniorNano_reasoning": "LatentSpeech focuses on continuous latent embeddings for diffusion-based TTS rather than defining or evaluating discrete audio tokens/quantized vocabularies, so it fails the inclusion criterion and meets the exclusion condition.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models",
    "abstract": "Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate.",
    "metadata": {
      "arxiv_id": "2412.08608",
      "title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models",
      "summary": "Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate.",
      "authors": [
        "Mintong Kang",
        "Chejian Xu",
        "Bo Li"
      ],
      "published": "2024-12-11T18:30:57Z",
      "updated": "2024-12-11T18:30:57Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CR",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.08608v1",
      "landing_url": "https://arxiv.org/abs/2412.08608v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.08608"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on adversarial jailbreak attacks against audio-language models rather than discrete tokenization/codec design, so it fails to meet the inclusion focus on discrete audio tokens and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on adversarial jailbreak attacks against audio-language models rather than discrete tokenization/codec design, so it fails to meet the inclusion focus on discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
    "abstract": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
    "metadata": {
      "arxiv_id": "2412.10117",
      "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models",
      "summary": "In our previous work, we introduced CosyVoice, a multilingual speech synthesis model based on supervised discrete speech tokens. By employing progressive semantic decoding with two popular generative models, language models (LMs) and Flow Matching, CosyVoice demonstrated high prosody naturalness, content consistency, and speaker similarity in speech in-context learning. Recently, significant progress has been made in multi-modal large language models (LLMs), where the response latency and real-time factor of speech synthesis play a crucial role in the interactive experience. Therefore, in this report, we present an improved streaming speech synthesis model, CosyVoice 2, which incorporates comprehensive and systematic optimizations. Specifically, we introduce finite-scalar quantization to improve the codebook utilization of speech tokens. For the text-speech LM, we streamline the model architecture to allow direct use of a pre-trained LLM as the backbone. In addition, we develop a chunk-aware causal flow matching model to support various synthesis scenarios, enabling both streaming and non-streaming synthesis within a single model. By training on a large-scale multilingual dataset, CosyVoice 2 achieves human-parity naturalness, minimal response latency, and virtually lossless synthesis quality in the streaming mode. We invite readers to listen to the demos at https://funaudiollm.github.io/cosyvoice2.",
      "authors": [
        "Zhihao Du",
        "Yuxuan Wang",
        "Qian Chen",
        "Xian Shi",
        "Xiang Lv",
        "Tianyu Zhao",
        "Zhifu Gao",
        "Yexin Yang",
        "Changfeng Gao",
        "Hui Wang",
        "Fan Yu",
        "Huadai Liu",
        "Zhengyan Sheng",
        "Yue Gu",
        "Chong Deng",
        "Wen Wang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Jingren Zhou"
      ],
      "published": "2024-12-13T12:59:39Z",
      "updated": "2024-12-25T11:54:03Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10117v3",
      "landing_url": "https://arxiv.org/abs/2412.10117v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10117"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "CosyVoice 2 centers on discrete speech tokens learned via quantized codec/LM modeling, addresses token/codebook utilization, and evaluates streaming synthesis, so it satisfies all inclusion criteria with no exclusions."
    },
    "round-A_JuniorNano_reasoning": "CosyVoice 2 centers on discrete speech tokens learned via quantized codec/LM modeling, addresses token/codebook utilization, and evaluates streaming synthesis, so it satisfies all inclusion criteria with no exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens",
    "abstract": "We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models.",
    "metadata": {
      "arxiv_id": "2412.10208",
      "title": "Efficient Generative Modeling with Residual Vector Quantization-Based Tokens",
      "summary": "We introduce ResGen, an efficient Residual Vector Quantization (RVQ)-based generative model for high-fidelity generation with fast sampling. RVQ improves data fidelity by increasing the number of quantization steps, referred to as depth, but deeper quantization typically increases inference steps in generative models. To address this, ResGen directly predicts the vector embedding of collective tokens rather than individual ones, ensuring that inference steps remain independent of RVQ depth. Additionally, we formulate token masking and multi-token prediction within a probabilistic framework using discrete diffusion and variational inference. We validate the efficacy and generalizability of the proposed method on two challenging tasks across different modalities: conditional image generation on ImageNet 256x256 and zero-shot text-to-speech synthesis. Experimental results demonstrate that ResGen outperforms autoregressive counterparts in both tasks, delivering superior performance without compromising sampling speed. Furthermore, as we scale the depth of RVQ, our generative models exhibit enhanced generation fidelity or faster sampling speeds compared to similarly sized baseline models.",
      "authors": [
        "Jaehyeon Kim",
        "Taehong Moon",
        "Keon Lee",
        "Jaewoong Cho"
      ],
      "published": "2024-12-13T15:31:17Z",
      "updated": "2025-06-02T11:03:20Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10208v3",
      "landing_url": "https://arxiv.org/abs/2412.10208v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.10208"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "Paper mentions RVQ-based tokens and zero-shot TTS, but the abstract focuses on general generative modeling (images and TTS) without clear token/codec design or evaluation details for discrete audio tokens, so inclusion is uncertain."
    },
    "round-A_JuniorNano_reasoning": "Paper mentions RVQ-based tokens and zero-shot TTS, but the abstract focuses on general generative modeling (images and TTS) without clear token/codec design or evaluation details for discrete audio tokens, so inclusion is uncertain.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
    "abstract": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
    "metadata": {
      "arxiv_id": "2412.10261",
      "title": "MVQ:Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
      "summary": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
      "authors": [
        "Shuaiting Li",
        "Chengxuan Wang",
        "Juncan Deng",
        "Zeyu Wang",
        "Zewen Ye",
        "Zongsheng Wang",
        "Haibin Shen",
        "Kejie Huang"
      ],
      "published": "2024-12-13T16:30:35Z",
      "updated": "2024-12-16T08:54:43Z",
      "categories": [
        "cs.CV",
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10261v2",
      "landing_url": "https://arxiv.org/abs/2412.10261v2",
      "doi": "https://doi.org/10.1145/3669940.3707268"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on DNN compression/accelerator via masked vector quantization for general weights, not on discrete audio token generation or evaluation, so it fails inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on DNN compression/accelerator via masked vector quantization for general weights, not on discrete audio token generation or evaluation, so it fails inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Optimizing Vision-Language Interactions Through Decoder-Only Models",
    "abstract": "Vision-Language Models (VLMs) have emerged as key enablers for multimodal tasks, but their reliance on separate visual encoders introduces challenges in efficiency, scalability, and modality alignment. To address these limitations, we propose MUDAIF (Multimodal Unified Decoder with Adaptive Input Fusion), a decoder-only vision-language model that seamlessly integrates visual and textual inputs through a novel Vision-Token Adapter (VTA) and adaptive co-attention mechanism. By eliminating the need for a visual encoder, MUDAIF achieves enhanced efficiency, flexibility, and cross-modal understanding. Trained on a large-scale dataset of 45M image-text pairs, MUDAIF consistently outperforms state-of-the-art methods across multiple benchmarks, including VQA, image captioning, and multimodal reasoning tasks. Extensive analyses and human evaluations demonstrate MUDAIF's robustness, generalization capabilities, and practical usability, establishing it as a new standard in encoder-free vision-language models.",
    "metadata": {
      "arxiv_id": "2412.10758",
      "title": "Optimizing Vision-Language Interactions Through Decoder-Only Models",
      "summary": "Vision-Language Models (VLMs) have emerged as key enablers for multimodal tasks, but their reliance on separate visual encoders introduces challenges in efficiency, scalability, and modality alignment. To address these limitations, we propose MUDAIF (Multimodal Unified Decoder with Adaptive Input Fusion), a decoder-only vision-language model that seamlessly integrates visual and textual inputs through a novel Vision-Token Adapter (VTA) and adaptive co-attention mechanism. By eliminating the need for a visual encoder, MUDAIF achieves enhanced efficiency, flexibility, and cross-modal understanding. Trained on a large-scale dataset of 45M image-text pairs, MUDAIF consistently outperforms state-of-the-art methods across multiple benchmarks, including VQA, image captioning, and multimodal reasoning tasks. Extensive analyses and human evaluations demonstrate MUDAIF's robustness, generalization capabilities, and practical usability, establishing it as a new standard in encoder-free vision-language models.",
      "authors": [
        "Kaito Tanaka",
        "Benjamin Tan",
        "Brian Wong"
      ],
      "published": "2024-12-14T09:04:32Z",
      "updated": "2024-12-14T09:04:32Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.10758v1",
      "landing_url": "https://arxiv.org/abs/2412.10758v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.10758"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on decoder-only vision-language modeling without any notion of discrete audio tokenization, so it fails the inclusion criteria centered on audio-codec/token learning and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on decoder-only vision-language modeling without any notion of discrete audio tokenization, so it fails the inclusion criteria centered on audio-codec/token learning and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
    "abstract": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
    "metadata": {
      "arxiv_id": "2412.11102",
      "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
      "summary": "The unprecedented advancements in Large Language Models (LLMs) have profoundly impacted natural language processing but have yet to fully embrace the realm of scalable vector graphics (SVG) generation. While LLMs encode partial knowledge of SVG data from web pages during training, recent findings suggest that semantically ambiguous and tokenized representations within LLMs may result in hallucinations in vector primitive predictions. Additionally, LLM training typically lacks modeling and understanding of the rendering sequence of vector paths, which can lead to occlusion between output vector primitives. In this paper, we present LLM4SVG, an initial yet substantial step toward bridging this gap by enabling LLMs to better understand and generate vector graphics. LLM4SVG facilitates a deeper understanding of SVG components through learnable semantic tokens, which precisely encode these tokens and their corresponding properties to generate semantically aligned SVG outputs. Using a series of learnable semantic tokens, a structured dataset for instruction following is developed to support comprehension and generation across two primary tasks. Our method introduces a modular architecture to existing large language models, integrating semantic tags, vector instruction encoders, fine-tuned commands, and powerful LLMs to tightly combine geometric, appearance, and language information. To overcome the scarcity of SVG-text instruction data, we developed an automated data generation pipeline that collected our SVGX-SFT Dataset, consisting of high-quality human-designed SVGs and 580k SVG instruction following data specifically crafted for LLM training, which facilitated the adoption of the supervised fine-tuning strategy popular in LLM development.",
      "authors": [
        "Ximing Xing",
        "Juncheng Hu",
        "Guotao Liang",
        "Jing Zhang",
        "Dong Xu",
        "Qian Yu"
      ],
      "published": "2024-12-15T07:49:31Z",
      "updated": "2025-03-25T15:35:29Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11102v3",
      "landing_url": "https://arxiv.org/abs/2412.11102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.11102"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "LLM4SVG targets generating vector graphics rather than describing or quantizing discrete audio tokens, so it fails the inclusion requirements and matches the exclusion of non-audio-token research."
    },
    "round-A_JuniorNano_reasoning": "LLM4SVG targets generating vector graphics rather than describing or quantizing discrete audio tokens, so it fails the inclusion requirements and matches the exclusion of non-audio-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
    "abstract": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
    "metadata": {
      "arxiv_id": "2412.11449",
      "title": "Whisper-GPT: A Hybrid Representation Audio Large Language Model",
      "summary": "We propose WHISPER-GPT: A generative large language model (LLM) for speech and music that allows us to work with continuous audio representations and discrete tokens simultaneously as part of a single architecture. There has been a huge surge in generative audio, speech, and music models that utilize discrete audio tokens derived from neural compression algorithms, e.g. ENCODEC. However, one of the major drawbacks of this approach is handling the context length. It blows up for high-fidelity generative architecture if one has to account for all the audio contents at various frequencies for the next token prediction. By combining continuous audio representation like the spectrogram and discrete acoustic tokens, we retain the best of both worlds: Have all the information needed from the audio at a specific time instance in a single token, yet allow LLM to predict the future token to allow for sampling and other benefits discrete space provides. We show how our architecture improves the perplexity and negative log-likelihood scores for the next token prediction compared to a token-based LLM for speech and music.",
      "authors": [
        "Prateek Verma"
      ],
      "published": "2024-12-16T05:03:48Z",
      "updated": "2024-12-16T05:03:48Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.11449v1",
      "landing_url": "https://arxiv.org/abs/2412.11449v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.11449"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "This hybrid LLM paper explicitly uses discrete codec-derived tokens alongside continuous representations and reports next-token modeling metrics for speech/music, satisfying the discrete-token modeling/f evaluation criteria and no exclusion applies, so include it."
    },
    "round-A_JuniorNano_reasoning": "This hybrid LLM paper explicitly uses discrete codec-derived tokens alongside continuous representations and reports next-token modeling metrics for speech/music, satisfying the discrete-token modeling/f evaluation criteria and no exclusion applies, so include it.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
    "abstract": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
    "metadata": {
      "arxiv_id": "2412.12276",
      "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective",
      "summary": "Autoregressive transformers exhibit adaptive learning through in-context learning (ICL), which begs the question of how. Prior works have shown that transformers represent the ICL tasks as vectors in their representations. In this paper, we leverage the encoding-decoding framework to study how transformers form task vectors during pretraining and how their task encoding quality predicts ICL task performance. On synthetic ICL tasks, we analyze the training dynamics of a small transformer and report the coupled emergence of task encoding and decoding. As the model learns to encode different latent tasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it concurrently builds conditional decoding algorithms and improves its ICL performance. We validate this phenomenon across pretrained models of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the course of pretraining in OLMo-7B. Further, we demonstrate that the quality of task encoding inferred from representations predicts ICL performance, and that, surprisingly, finetuning the earlier layers can improve the task encoding and performance more than finetuning the latter layers. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.",
      "authors": [
        "Seungwook Han",
        "Jinyeop Song",
        "Jeff Gore",
        "Pulkit Agrawal"
      ],
      "published": "2024-12-16T19:00:18Z",
      "updated": "2025-06-02T12:55:12Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.12276v3",
      "landing_url": "https://arxiv.org/abs/2412.12276v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.12276"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper studies transformers and task vectors in in-context learning with text models without any focus on discrete audio tokens or audio tokenization, so it fails to meet the audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper studies transformers and task vectors in in-context learning with text models without any focus on discrete audio tokens or audio tokenization, so it fails to meet the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Autoregressive Video Generation without Vector Quantization",
    "abstract": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
    "metadata": {
      "arxiv_id": "2412.14169",
      "title": "Autoregressive Video Generation without Vector Quantization",
      "summary": "This paper presents a novel approach that enables autoregressive video generation with high efficiency. We propose to reformulate the video generation problem as a non-quantized autoregressive modeling of temporal frame-by-frame prediction and spatial set-by-set prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. With the proposed approach, we train a novel video autoregressive model without vector quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, i.e., 0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models in text-to-image generation tasks, with a significantly lower training cost. Additionally, NOVA generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Code and models are publicly available at https://github.com/baaivision/NOVA.",
      "authors": [
        "Haoge Deng",
        "Ting Pan",
        "Haiwen Diao",
        "Zhengxiong Luo",
        "Yufeng Cui",
        "Huchuan Lu",
        "Shiguang Shan",
        "Yonggang Qi",
        "Xinlong Wang"
      ],
      "published": "2024-12-18T18:59:53Z",
      "updated": "2025-03-02T08:09:39Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.14169v2",
      "landing_url": "https://arxiv.org/abs/2412.14169v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.14169"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract focus on video generation using autoregressive modeling with no mention of discrete audio tokens, tokenizers, quantization for audio, or any audio-related evaluations, so it fails to meet the inclusion topic and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract focus on video generation using autoregressive modeling with no mention of discrete audio tokens, tokenizers, quantization for audio, or any audio-related evaluations, so it fails to meet the inclusion topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
    "abstract": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
    "metadata": {
      "arxiv_id": "2412.15195",
      "title": "Preventing Local Pitfalls in Vector Quantization via Optimal Transport",
      "summary": "Vector-quantized networks (VQNs) have exhibited remarkable performance across various tasks, yet they are prone to training instability, which complicates the training process due to the necessity for techniques such as subtle initialization and model distillation. In this study, we identify the local minima issue as the primary cause of this instability. To address this, we integrate an optimal transport method in place of the nearest neighbor search to achieve a more globally informed assignment. We introduce OptVQ, a novel vector quantization method that employs the Sinkhorn algorithm to optimize the optimal transport problem, thereby enhancing the stability and efficiency of the training process. To mitigate the influence of diverse data distributions on the Sinkhorn algorithm, we implement a straightforward yet effective normalization strategy. Our comprehensive experiments on image reconstruction tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses current state-of-the-art VQNs in reconstruction quality.",
      "authors": [
        "Borui Zhang",
        "Wenzhao Zheng",
        "Jie Zhou",
        "Jiwen Lu"
      ],
      "published": "2024-12-19T18:58:14Z",
      "updated": "2024-12-19T18:58:14Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15195v1",
      "landing_url": "https://arxiv.org/abs/2412.15195v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15195"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract is about general vector quantization for image reconstruction with no mention of audio tokens or discrete audio codec/tokenizer design, so it fails the audio-token inclusion scope entirely."
    },
    "round-A_JuniorNano_reasoning": "Abstract is about general vector quantization for image reconstruction with no mention of audio tokens or discrete audio codec/tokenizer design, so it fails the audio-token inclusion scope entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
    "abstract": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
    "metadata": {
      "arxiv_id": "2412.15649",
      "title": "SLAM-Omni: Timbre-Controllable Voice Interaction System with Single-Stage Training",
      "summary": "Recent advancements highlight the potential of end-to-end real-time spoken dialogue systems, showcasing their low latency and high quality. In this paper, we introduce SLAM-Omni, a timbre-controllable, end-to-end voice interaction system with single-stage training. SLAM-Omni achieves zero-shot timbre control by modeling spoken language with semantic tokens and decoupling speaker information to a vocoder. By predicting grouped speech semantic tokens at each step, our method significantly reduces the sequence length of audio tokens, accelerating both training and inference. Additionally, we propose historical text prompting to compress dialogue history, facilitating efficient multi-round interactions. Comprehensive evaluations reveal that SLAM-Omni outperforms prior models of similar scale, requiring only 15 hours of training on 4 GPUs with limited data. Notably, it is the first spoken dialogue system to achieve competitive performance with a single-stage training approach, eliminating the need for pre-training on TTS or ASR tasks. Further experiments validate its multilingual and multi-turn dialogue capabilities on larger datasets.",
      "authors": [
        "Wenxi Chen",
        "Ziyang Ma",
        "Ruiqi Yan",
        "Yuzhe Liang",
        "Xiquan Li",
        "Ruiyang Xu",
        "Zhikang Niu",
        "Yanqiao Zhu",
        "Yifan Yang",
        "Zhanxun Liu",
        "Kai Yu",
        "Yuxuan Hu",
        "Jinyu Li",
        "Yan Lu",
        "Shujie Liu",
        "Xie Chen"
      ],
      "published": "2024-12-20T08:05:55Z",
      "updated": "2024-12-20T08:05:55Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.15649v1",
      "landing_url": "https://arxiv.org/abs/2412.15649v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.15649"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Proposes SLAM-Omni, which models spoken language with semantic tokens, predicts grouped audio tokens for efficiency, and provides concrete evaluations on reconstruction/interaction tasks, matching the discrete audio token focus and reporting performance details."
    },
    "round-A_JuniorNano_reasoning": "Proposes SLAM-Omni, which models spoken language with semantic tokens, predicts grouped audio tokens for efficiency, and provides concrete evaluations on reconstruction/interaction tasks, matching the discrete audio token focus and reporting performance details.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
    "abstract": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
    "metadata": {
      "arxiv_id": "2412.16102",
      "title": "Interleaved Speech-Text Language Models for Simple Streaming Text-to-Speech Synthesis",
      "summary": "This paper introduces Interleaved Speech-Text Language Model (IST-LM) for zero-shot streaming Text-to-Speech (TTS). Unlike many previous approaches, IST-LM is directly trained on interleaved sequences of text and speech tokens with a fixed ratio, eliminating the need for additional efforts like forced alignment or complex designs. The ratio of text chunk size to speech chunk size is crucial for the performance of IST-LM. To explore this, we conducted a comprehensive series of statistical analyses on the training data and performed correlation analysis with the final performance, uncovering several key factors: 1) the distance between speech tokens and their corresponding text tokens, 2) the number of future text tokens accessible to each speech token, and 3) the frequency of speech tokens precedes their corresponding text tokens. Experimental results demonstrate how to achieve an optimal streaming TTS system with a limited performance gap compared to its non-streaming counterpart. IST-LM is conceptually simple and empirically powerful, enabling streaming TTS with minimal overhead while largely preserving performance, and offering broad potential for integration with real-time text streams from large language models.",
      "authors": [
        "Yifan Yang",
        "Shujie Liu",
        "Jinyu Li",
        "Hui Wang",
        "Lingwei Meng",
        "Haiyang Sun",
        "Yuzhe Liang",
        "Ziyang Ma",
        "Yuxuan Hu",
        "Rui Zhao",
        "Jianwei Yu",
        "Yan Lu",
        "Xie Chen"
      ],
      "published": "2024-12-20T17:43:50Z",
      "updated": "2025-08-09T10:01:51Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16102v3",
      "landing_url": "https://arxiv.org/abs/2412.16102v3",
      "doi": "https://doi.org/10.48550/arXiv.2412.16102"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The IST-LM paper explicitly models interleaved sequences of text and speech tokens, analyzes token distance/frequency effects on streaming TTS performance, and thus centers on discrete audio tokens in modeling/benchmarking, so it better to include (score 4)."
    },
    "round-A_JuniorNano_reasoning": "The IST-LM paper explicitly models interleaved sequences of text and speech tokens, analyzes token distance/frequency effects on streaming TTS performance, and thus centers on discrete audio tokens in modeling/benchmarking, so it better to include (score 4).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
    "abstract": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
    "metadata": {
      "arxiv_id": "2412.16846",
      "title": "KALL-E:Autoregressive Speech Synthesis with Next-Distribution Prediction",
      "summary": "We introduce KALL-E, a novel autoregressive (AR) language model for text-to-speech (TTS) synthesis that operates by predicting the next distribution of continuous speech frames. Unlike existing methods, KALL-E directly models the continuous speech distribution conditioned on text, eliminating the need for any diffusion-based components. Specifically, we utilize a Flow-VAE to extract a continuous latent speech representation from waveforms, instead of relying on discrete speech tokens. A single AR Transformer is then trained to predict these continuous speech distributions from text, optimizing a Kullback-Leibler divergence loss as its objective. Experimental results demonstrate that KALL-E achieves superior speech synthesis quality and can even adapt to a target speaker from just a single sample. Importantly, KALL-E provides a more direct and effective approach for utilizing continuous speech representations in TTS.",
      "authors": [
        "Kangxiang Xia",
        "Xinfa Zhu",
        "Jixun Yao",
        "Wenjie Tian",
        "Wenhao Li",
        "Lei Xie"
      ],
      "published": "2024-12-22T04:03:24Z",
      "updated": "2025-09-17T16:01:26Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.16846v2",
      "landing_url": "https://arxiv.org/abs/2412.16846v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.16846"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "KALL-E models continuous speech distributions via Flow-VAE latents and autoregressive prediction without quantization into any finite vocabulary tokens, so it fails the discrete-token inclusion requirements and fits the continuous-feature exclusion criterion."
    },
    "round-A_JuniorNano_reasoning": "KALL-E models continuous speech distributions via Flow-VAE latents and autoregressive prediction without quantization into any finite vocabulary tokens, so it fails the discrete-token inclusion requirements and fits the continuous-feature exclusion criterion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multiple Consistency-guided Test-Time Adaptation for Contrastive Audio-Language Models with Unlabeled Audio",
    "abstract": "One fascinating aspect of pre-trained Audio-Language Models (ALMs) learning is their impressive zero-shot generalization capability and test-time adaptation (TTA) methods aiming to improve domain performance without annotations. However, previous test time adaptation (TTA) methods for ALMs in zero-shot classification tend to be stuck in incorrect model predictions. In order to further boost the performance, we propose multiple guidance on prompt learning without annotated labels. First, guidance of consistency on both context tokens and domain tokens of ALMs is set. Second, guidance of both consistency across multiple augmented views of each single test sample and contrastive learning across different test samples is set. Third, we propose a corresponding end-end learning framework for the proposed test-time adaptation method without annotated labels. We extensively evaluate our approach on 12 downstream tasks across domains, our proposed adaptation method leads to 4.41% (max 7.50%) average zero-shot performance improvement in comparison with the state-of-the-art models.",
    "metadata": {
      "arxiv_id": "2412.17306",
      "title": "Multiple Consistency-guided Test-Time Adaptation for Contrastive Audio-Language Models with Unlabeled Audio",
      "summary": "One fascinating aspect of pre-trained Audio-Language Models (ALMs) learning is their impressive zero-shot generalization capability and test-time adaptation (TTA) methods aiming to improve domain performance without annotations. However, previous test time adaptation (TTA) methods for ALMs in zero-shot classification tend to be stuck in incorrect model predictions. In order to further boost the performance, we propose multiple guidance on prompt learning without annotated labels. First, guidance of consistency on both context tokens and domain tokens of ALMs is set. Second, guidance of both consistency across multiple augmented views of each single test sample and contrastive learning across different test samples is set. Third, we propose a corresponding end-end learning framework for the proposed test-time adaptation method without annotated labels. We extensively evaluate our approach on 12 downstream tasks across domains, our proposed adaptation method leads to 4.41% (max 7.50%) average zero-shot performance improvement in comparison with the state-of-the-art models.",
      "authors": [
        "Gongyu Chen",
        "Haomin Zhang",
        "Chaofan Ding",
        "Zihao Chen",
        "Xinhan Di"
      ],
      "published": "2024-12-23T05:53:52Z",
      "updated": "2024-12-23T05:53:52Z",
      "categories": [
        "cs.SD",
        "cs.CV",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17306v1",
      "landing_url": "https://arxiv.org/abs/2412.17306v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.17306"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on test-time adaptation for contrastive audio-language models without discussing discrete audio tokens, tokenizers, or quantization, so it fails to meet the inclusion topic and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Focuses on test-time adaptation for contrastive audio-language models without discussing discrete audio tokens, tokenizers, or quantization, so it fails to meet the inclusion topic and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
    "abstract": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
    "metadata": {
      "arxiv_id": "2412.17640",
      "title": "Hierarchical Vector Quantization for Unsupervised Action Segmentation",
      "summary": "In this work, we address unsupervised temporal action segmentation, which segments a set of long, untrimmed videos into semantically meaningful segments that are consistent across videos. While recent approaches combine representation learning and clustering in a single step for this task, they do not cope with large variations within temporal segments of the same class. To address this limitation, we propose a novel method, termed Hierarchical Vector Quantization (HVQ), that consists of two subsequent vector quantization modules. This results in a hierarchical clustering where the additional subclusters cover the variations within a cluster. We demonstrate that our approach captures the distribution of segment lengths much better than the state of the art. To this end, we introduce a new metric based on the Jensen-Shannon Distance (JSD) for unsupervised temporal action segmentation. We evaluate our approach on three public datasets, namely Breakfast, YouTube Instructional and IKEA ASM. Our approach outperforms the state of the art in terms of F1 score, recall and JSD.",
      "authors": [
        "Federico Spurio",
        "Emad Bahrami",
        "Gianpiero Francesca",
        "Juergen Gall"
      ],
      "published": "2024-12-23T15:18:24Z",
      "updated": "2025-01-24T17:43:56Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.17640v2",
      "landing_url": "https://arxiv.org/abs/2412.17640v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.17640"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on hierarchical vector quantization for unsupervised action segmentation in video, without any discrete audio token generation or codec quantization focus, so it fails to meet the audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on hierarchical vector quantization for unsupervised action segmentation in video, without any discrete audio token generation or codec quantization focus, so it fails to meet the audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Long-Form Speech Generation with Spoken Language Models",
    "abstract": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
    "metadata": {
      "arxiv_id": "2412.18603",
      "title": "Long-Form Speech Generation with Spoken Language Models",
      "summary": "We consider the generative modeling of speech over multiple minutes, a requirement for long-form multimedia generation and audio-native voice assistants. However, textless spoken language models struggle to generate plausible speech past tens of seconds, due to high temporal resolution of speech tokens causing loss of coherence, architectural issues with long-sequence training or extrapolation, and memory costs at inference time. From these considerations we derive SpeechSSM, the first speech language model family to learn from and sample long-form spoken audio (e.g., 16 minutes of read or extemporaneous speech) in a single decoding session without text intermediates. SpeechSSMs leverage recent advances in linear-time sequence modeling to greatly surpass current Transformer spoken LMs in coherence and efficiency on multi-minute generations while still matching them at the utterance level. As we found current spoken language evaluations uninformative, especially in this new long-form setting, we also introduce: LibriSpeech-Long, a benchmark for long-form speech evaluation; new embedding-based and LLM-judged metrics; and quality measurements over length and time. Speech samples, the LibriSpeech-Long dataset, and any future code or model releases can be found at https://google.github.io/tacotron/publications/speechssm/.",
      "authors": [
        "Se Jin Park",
        "Julian Salazar",
        "Aren Jansen",
        "Keisuke Kinoshita",
        "Yong Man Ro",
        "RJ Skerry-Ryan"
      ],
      "published": "2024-12-24T18:56:46Z",
      "updated": "2025-07-10T17:52:43Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.18603v2",
      "landing_url": "https://arxiv.org/abs/2412.18603v2",
      "doi": "https://doi.org/10.48550/arXiv.2412.18603"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 3,
      "reasoning": "The abstract mentions textless spoken language models and long-form speech generation but gives no details on discrete audio tokenization or codec/tokenizer design, so I can’t be confident it satisfies the discrete-token-focused inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The abstract mentions textless spoken language models and long-form speech generation but gives no details on discrete audio tokenization or codec/tokenizer design, so I can’t be confident it satisfies the discrete-token-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 3,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "maybe (junior:3)",
    "review_skipped": false,
    "discard_reason": "review_needs_followup"
  },
  {
    "title": "Semantic Residual for Multimodal Unified Discrete Representation",
    "abstract": "Recent research in the domain of multimodal unified representations predominantly employs codebook as representation forms, utilizing Vector Quantization(VQ) for quantization, yet there has been insufficient exploration of other quantization representation forms. Our work explores more precise quantization methods and introduces a new framework, Semantic Residual Cross-modal Information Disentanglement (SRCID), inspired by the numerical residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs semantic residual-based information disentanglement for multimodal data to better handle the inherent discrepancies between different modalities. Our method enhances the capabilities of unified multimodal representations and demonstrates exceptional performance in cross-modal generalization and cross-modal zero-shot retrieval. Its average results significantly surpass existing state-of-the-art models, as well as previous attempts with RVQ and Finite Scalar Quantization (FSQ) based on these modals.",
    "metadata": {
      "arxiv_id": "2412.19128",
      "title": "Semantic Residual for Multimodal Unified Discrete Representation",
      "summary": "Recent research in the domain of multimodal unified representations predominantly employs codebook as representation forms, utilizing Vector Quantization(VQ) for quantization, yet there has been insufficient exploration of other quantization representation forms. Our work explores more precise quantization methods and introduces a new framework, Semantic Residual Cross-modal Information Disentanglement (SRCID), inspired by the numerical residual concept inherent to Residual Vector Quantization (RVQ). SRCID employs semantic residual-based information disentanglement for multimodal data to better handle the inherent discrepancies between different modalities. Our method enhances the capabilities of unified multimodal representations and demonstrates exceptional performance in cross-modal generalization and cross-modal zero-shot retrieval. Its average results significantly surpass existing state-of-the-art models, as well as previous attempts with RVQ and Finite Scalar Quantization (FSQ) based on these modals.",
      "authors": [
        "Hai Huang",
        "Shulei Wang",
        "Yan Xia"
      ],
      "published": "2024-12-26T09:08:52Z",
      "updated": "2024-12-26T09:08:52Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19128v1",
      "landing_url": "https://arxiv.org/abs/2412.19128v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19128"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on multimodal quantization/residual representation without mentioning discrete audio tokens or codec/tokenizer design, so it fails to meet the audio-specific inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on multimodal quantization/residual representation without mentioning discrete audio tokens or codec/tokenizer design, so it fails to meet the audio-specific inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
    "abstract": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
    "metadata": {
      "arxiv_id": "2412.19248",
      "title": "Causal Speech Enhancement with Predicting Semantics based on Quantized Self-supervised Learning Features",
      "summary": "Real-time speech enhancement (SE) is essential to online speech communication. Causal SE models use only the previous context while predicting future information, such as phoneme continuation, may help performing causal SE. The phonetic information is often represented by quantizing latent features of self-supervised learning (SSL) models. This work is the first to incorporate SSL features with causality into an SE model. The causal SSL features are encoded and combined with spectrogram features using feature-wise linear modulation to estimate a mask for enhancing the noisy input speech. Simultaneously, we quantize the causal SSL features using vector quantization to represent phonetic characteristics as semantic tokens. The model not only encodes SSL features but also predicts the future semantic tokens in multi-task learning (MTL). The experimental results using VoiceBank + DEMAND dataset show that our proposed method achieves 2.88 in PESQ, especially with semantic prediction MTL, in which we confirm that the semantic prediction played an important role in causal SE.",
      "authors": [
        "Emiru Tsunoo",
        "Yuki Saito",
        "Wataru Nakata",
        "Hiroshi Saruwatari"
      ],
      "published": "2024-12-26T15:08:36Z",
      "updated": "2024-12-26T15:08:36Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.19248v1",
      "landing_url": "https://arxiv.org/abs/2412.19248v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.19248"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper quantizes self-supervised latent features into semantic tokens and models them explicitly (including token prediction) to improve causal speech enhancement, so it clearly targets discrete audio token design and evaluation as required."
    },
    "round-A_JuniorNano_reasoning": "Paper quantizes self-supervised latent features into semantic tokens and models them explicitly (including token prediction) to improve causal speech enhancement, so it clearly targets discrete audio token design and evaluation as required.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Considering experimental frame rates and robust segmentation analysis of piecewise-linear microparticle trajectories",
    "abstract": "The movement of intracellular cargo transported by molecular motors is commonly marked by switches between directed motion and stationary pauses. The predominant measure for assessing movement is effective diffusivity, which predicts the mean-squared displacement of particles over long time scales. In this work, we consider an alternative analysis regime that focuses on shorter time scales and relies on automated segmentation of paths. Due to intrinsic uncertainty in changepoint analysis, we highlight the importance of statistical summaries that are robust with respect to the performance of segmentation algorithms. In contrast to effective diffusivity, which averages over multiple behaviors, we emphasize tools that highlight the different motor-cargo states, with an eye toward identifying biophysical mechanisms that determine emergent whole-cell transport properties. By developing a Markov chain model for noisy, continuous, piecewise-linear microparticle movement, and associated mathematical analysis, we provide insight into a common question posed by experimentalists: how does the choice of observational frame rate affect what is inferred about transport properties?",
    "metadata": {
      "arxiv_id": "2412.21025",
      "title": "Considering experimental frame rates and robust segmentation analysis of piecewise-linear microparticle trajectories",
      "summary": "The movement of intracellular cargo transported by molecular motors is commonly marked by switches between directed motion and stationary pauses. The predominant measure for assessing movement is effective diffusivity, which predicts the mean-squared displacement of particles over long time scales. In this work, we consider an alternative analysis regime that focuses on shorter time scales and relies on automated segmentation of paths. Due to intrinsic uncertainty in changepoint analysis, we highlight the importance of statistical summaries that are robust with respect to the performance of segmentation algorithms. In contrast to effective diffusivity, which averages over multiple behaviors, we emphasize tools that highlight the different motor-cargo states, with an eye toward identifying biophysical mechanisms that determine emergent whole-cell transport properties. By developing a Markov chain model for noisy, continuous, piecewise-linear microparticle movement, and associated mathematical analysis, we provide insight into a common question posed by experimentalists: how does the choice of observational frame rate affect what is inferred about transport properties?",
      "authors": [
        "Keisha J. Cook",
        "Nathan Rayens",
        "Linh Do",
        "Christine K. Payne",
        "Scott A. McKinley"
      ],
      "published": "2024-12-30T15:50:27Z",
      "updated": "2024-12-30T15:50:27Z",
      "categories": [
        "q-bio.QM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2412.21025v1",
      "landing_url": "https://arxiv.org/abs/2412.21025v1",
      "doi": "https://doi.org/10.48550/arXiv.2412.21025"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on biophysical analysis of particle trajectories and frame rate effects, with no discrete audio token generation or codec-related work, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on biophysical analysis of particle trajectories and frame rate effects, with no discrete audio token generation or codec-related work, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
    "abstract": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
    "metadata": {
      "arxiv_id": "2501.01108",
      "title": "MuQ: Self-Supervised Music Representation Learning with Mel Residual Vector Quantization",
      "summary": "Recent years have witnessed the success of foundation models pre-trained with self-supervised learning (SSL) in various music informatics understanding tasks, including music tagging, instrument classification, key detection, and more. In this paper, we propose a self-supervised music representation learning model for music understanding. Distinguished from previous studies adopting random projection or existing neural codec, the proposed model, named MuQ, is trained to predict tokens generated by Mel Residual Vector Quantization (Mel-RVQ). Our Mel-RVQ utilizes residual linear projection structure for Mel spectrum quantization to enhance the stability and efficiency of target extraction and lead to better performance. Experiments in a large variety of downstream tasks demonstrate that MuQ outperforms previous self-supervised music representation models with only 0.9K hours of open-source pre-training data. Scaling up the data to over 160K hours and adopting iterative training consistently improve the model performance. To further validate the strength of our model, we present MuQ-MuLan, a joint music-text embedding model based on contrastive learning, which achieves state-of-the-art performance in the zero-shot music tagging task on the MagnaTagATune dataset. Code and checkpoints are open source in https://github.com/tencent-ailab/MuQ.",
      "authors": [
        "Haina Zhu",
        "Yizhi Zhou",
        "Hangting Chen",
        "Jianwei Yu",
        "Ziyang Ma",
        "Rongzhi Gu",
        "Yi Luo",
        "Wei Tan",
        "Xie Chen"
      ],
      "published": "2025-01-02T07:08:29Z",
      "updated": "2025-01-03T08:35:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.01108v2",
      "landing_url": "https://arxiv.org/abs/2501.01108v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.01108"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper proposes MuQ which predicts discrete tokens from Mel Residual Vector Quantization and evaluates them across SSL music tasks including zero-shot tagging, satisfying the token/codec design and downstream evaluation criteria, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The paper proposes MuQ which predicts discrete tokens from Mel Residual Vector Quantization and evaluates them across SSL music tasks including zero-shot tagging, satisfying the token/codec design and downstream evaluation criteria, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models",
    "abstract": "In the realm of fractal geometry, intricate structures emerge from simple iterative processes that partition parameter spaces into regions of stability and instability. Likewise, training large language models involves iteratively applying update functions, such as Adam, where even slight hyperparameter adjustments can shift the training process from convergence to divergence. Recent evidence from miniature neural networks suggests that the boundary separating these outcomes displays fractal characteristics. Building on these insights, this study extends them to medium-sized, decoder-only transformer architectures by employing a more consistent convergence measure and examining the learning rate hyperparameter landscape for attention and fully connected layers. The results show that the trainability frontier is not a simple threshold; rather, it forms a self-similar yet seemingly random structure at multiple scales, with statistically consistent and repeating patterns. Within this landscape, a region of stable convergence is surrounded by a complex chaotic border, illustrating the sensitive nature of the underlying training dynamics.",
    "metadata": {
      "arxiv_id": "2501.04286",
      "title": "Mapping the Edge of Chaos: Fractal-Like Boundaries in The Trainability of Decoder-Only Transformer Models",
      "summary": "In the realm of fractal geometry, intricate structures emerge from simple iterative processes that partition parameter spaces into regions of stability and instability. Likewise, training large language models involves iteratively applying update functions, such as Adam, where even slight hyperparameter adjustments can shift the training process from convergence to divergence. Recent evidence from miniature neural networks suggests that the boundary separating these outcomes displays fractal characteristics. Building on these insights, this study extends them to medium-sized, decoder-only transformer architectures by employing a more consistent convergence measure and examining the learning rate hyperparameter landscape for attention and fully connected layers. The results show that the trainability frontier is not a simple threshold; rather, it forms a self-similar yet seemingly random structure at multiple scales, with statistically consistent and repeating patterns. Within this landscape, a region of stable convergence is surrounded by a complex chaotic border, illustrating the sensitive nature of the underlying training dynamics.",
      "authors": [
        "Bahman Torkamandi"
      ],
      "published": "2025-01-08T05:24:11Z",
      "updated": "2025-02-15T01:26:37Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04286v2",
      "landing_url": "https://arxiv.org/abs/2501.04286v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.04286"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on trainability of decoder-only transformers with no mention of discrete audio tokenization, so it fails the domain-specific inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on trainability of decoder-only transformers with no mention of discrete audio tokenization, so it fails the domain-specific inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
    "abstract": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
    "metadata": {
      "arxiv_id": "2501.04644",
      "title": "FleSpeech: Flexibly Controllable Speech Generation with Various Prompts",
      "summary": "Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/",
      "authors": [
        "Hanzhao Li",
        "Yuke Li",
        "Xinsheng Wang",
        "Jingbin Hu",
        "Qicong Xie",
        "Shan Yang",
        "Lei Xie"
      ],
      "published": "2025-01-08T17:52:35Z",
      "updated": "2025-04-30T09:30:49Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04644v2",
      "landing_url": "https://arxiv.org/abs/2501.04644v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.04644"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The abstract only discusses prompt control for speech generation without any mention of discrete audio token vocabularies, quantization, or codec/semantic unit design, so it clearly fails the inclusion criteria and warrants exclusion (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The abstract only discusses prompt control for speech generation without any mention of discrete audio token vocabularies, quantization, or codec/semantic unit design, so it clearly fails the inclusion criteria and warrants exclusion (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
    "abstract": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
    "metadata": {
      "arxiv_id": "2501.04962",
      "title": "VoxEval: Benchmarking the Knowledge Understanding Capabilities of End-to-End Spoken Language Models",
      "summary": "With the rising need for speech-based interaction models, end-to-end Spoken Language Models (SLMs) have emerged as a promising solution. While these models require comprehensive world knowledge for meaningful and reliable human interactions, existing question-answering (QA) benchmarks fall short in evaluating SLMs' knowledge understanding due to their inability to support end-to-end speech evaluation and account for varied input audio conditions. To address these limitations, we present VoxEval, a novel SpeechQA benchmark that assesses SLMs' knowledge understanding through pure speech interactions. Our benchmark 1) uniquely maintains speech format for both inputs and outputs, 2) evaluates model robustness across diverse input audio conditions, and 3) pioneers the assessment of complex tasks like mathematical reasoning in spoken format. Systematic evaluation demonstrates that VoxEval presents significant challenges to current SLMs, revealing their sensitivity to varying audio conditions and highlighting the need to enhance reasoning capabilities in future development. We hope this benchmark could guide the advancement of more sophisticated and reliable SLMs. VoxEval dataset is available at: https://github.com/dreamtheater123/VoxEval",
      "authors": [
        "Wenqian Cui",
        "Xiaoqi Jiao",
        "Ziqiao Meng",
        "Irwin King"
      ],
      "published": "2025-01-09T04:30:12Z",
      "updated": "2025-05-27T16:14:30Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.04962v4",
      "landing_url": "https://arxiv.org/abs/2501.04962v4",
      "doi": "https://doi.org/10.48550/arXiv.2501.04962"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on speech-based QA for end-to-end spoken language models and does not propose or evaluate any discrete audio tokenization/quantization methods, so it fails to meet the inclusion criteria and matches exclusion rule of lacking discrete token scope."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on speech-based QA for end-to-end spoken language models and does not propose or evaluate any discrete audio tokenization/quantization methods, so it fails to meet the inclusion criteria and matches exclusion rule of lacking discrete token scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
    "abstract": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
    "metadata": {
      "arxiv_id": "2501.05787",
      "title": "MARS6: A Small and Robust Hierarchical-Codec Text-to-Speech Model",
      "summary": "Codec-based text-to-speech (TTS) models have shown impressive quality with zero-shot voice cloning abilities. However, they often struggle with more expressive references or complex text inputs. We present MARS6, a robust encoder-decoder transformer for rapid, expressive TTS. MARS6 is built on recent improvements in spoken language modelling. Utilizing a hierarchical setup for its decoder, new speech tokens are processed at a rate of only 12 Hz, enabling efficient modelling of long-form text while retaining reconstruction quality. We combine several recent training and inference techniques to reduce repetitive generation and improve output stability and quality. This enables the 70M-parameter MARS6 to achieve similar performance to models many times larger. We show this in objective and subjective evaluations, comparing TTS output quality and reference speaker cloning ability. Project page: https://camb-ai.github.io/mars6-turbo/",
      "authors": [
        "Matthew Baas",
        "Pieter Scholtz",
        "Arnav Mehta",
        "Elliott Dyson",
        "Akshat Prakash",
        "Herman Kamper"
      ],
      "published": "2025-01-10T08:41:42Z",
      "updated": "2025-01-10T08:41:42Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.05787v1",
      "landing_url": "https://arxiv.org/abs/2501.05787v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.05787"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "MARS6 centers on a codec-based TTS that models speech via discrete tokens (hierarchical decoder processing new speech tokens) and evaluates reconstruction quality, matching the inclusion focus on discrete audio code/token design and assessment, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "MARS6 centers on a codec-based TTS that models speech via discrete tokens (hierarchical decoder processing new speech tokens) and evaluates reconstruction quality, matching the inclusion focus on discrete audio code/token design and assessment, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer",
    "abstract": "This work introduces TTS-Transducer - a novel architecture for text-to-speech, leveraging the strengths of audio codec models and neural transducers. Transducers, renowned for their superior quality and robustness in speech recognition, are employed to learn monotonic alignments and allow for avoiding using explicit duration predictors. Neural audio codecs efficiently compress audio into discrete codes, revealing the possibility of applying text modeling approaches to speech generation. However, the complexity of predicting multiple tokens per frame from several codebooks, as necessitated by audio codec models with residual quantizers, poses a significant challenge. The proposed system first uses a transducer architecture to learn monotonic alignments between tokenized text and speech codec tokens for the first codebook. Next, a non-autoregressive Transformer predicts the remaining codes using the alignment extracted from transducer loss. The proposed system is trained end-to-end. We show that TTS-Transducer is a competitive and robust alternative to contemporary TTS systems.",
    "metadata": {
      "arxiv_id": "2501.06320",
      "title": "TTS-Transducer: End-to-End Speech Synthesis with Neural Transducer",
      "summary": "This work introduces TTS-Transducer - a novel architecture for text-to-speech, leveraging the strengths of audio codec models and neural transducers. Transducers, renowned for their superior quality and robustness in speech recognition, are employed to learn monotonic alignments and allow for avoiding using explicit duration predictors. Neural audio codecs efficiently compress audio into discrete codes, revealing the possibility of applying text modeling approaches to speech generation. However, the complexity of predicting multiple tokens per frame from several codebooks, as necessitated by audio codec models with residual quantizers, poses a significant challenge. The proposed system first uses a transducer architecture to learn monotonic alignments between tokenized text and speech codec tokens for the first codebook. Next, a non-autoregressive Transformer predicts the remaining codes using the alignment extracted from transducer loss. The proposed system is trained end-to-end. We show that TTS-Transducer is a competitive and robust alternative to contemporary TTS systems.",
      "authors": [
        "Vladimir Bataev",
        "Subhankar Ghosh",
        "Vitaly Lavrukhin",
        "Jason Li"
      ],
      "published": "2025-01-10T19:50:32Z",
      "updated": "2025-01-10T19:50:32Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.06320v1",
      "landing_url": "https://arxiv.org/abs/2501.06320v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890256"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper centers on neural audio codec discrete codes and describes how a transducer learns monotonic alignments to those codec tokens and uses a Transformer to predict remaining codes, satisfying the inclusion criteria focused on discrete audio token generation and modeling, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "The paper centers on neural audio codec discrete codes and describes how a transducer learns monotonic alignments to those codec tokens and uses a Transformer to predict remaining codes, satisfying the inclusion criteria focused on discrete audio token generation and modeling, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model",
    "abstract": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance in tasks involving audio perception and understanding, such as speech recognition and audio captioning. However, their reasoning capabilities - critical for solving complex real-world problems - remain underexplored. In this work, we conduct the first exploration into integrating Chain-of-Thought (CoT) reasoning into LALMs to enhance their reasoning ability across auditory modalities. We evaluate representative CoT methods, analyzing their performance in both information extraction and reasoning tasks across sound, music, and speech domains. Our findings reveal that CoT methods significantly improve performance on easy and medium tasks but encounter challenges with hard tasks, where reasoning chains can confuse the model rather than improve accuracy. Additionally, we identify a positive correlation between reasoning path length and accuracy, demonstrating the potential of scaling inference for advanced instruction-following and reasoning. This study not only highlights the promise of CoT in enhancing LALM reasoning capabilities but also identifies key limitations and provides actionable directions for future research.",
    "metadata": {
      "arxiv_id": "2501.07246",
      "title": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language Model",
      "summary": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance in tasks involving audio perception and understanding, such as speech recognition and audio captioning. However, their reasoning capabilities - critical for solving complex real-world problems - remain underexplored. In this work, we conduct the first exploration into integrating Chain-of-Thought (CoT) reasoning into LALMs to enhance their reasoning ability across auditory modalities. We evaluate representative CoT methods, analyzing their performance in both information extraction and reasoning tasks across sound, music, and speech domains. Our findings reveal that CoT methods significantly improve performance on easy and medium tasks but encounter challenges with hard tasks, where reasoning chains can confuse the model rather than improve accuracy. Additionally, we identify a positive correlation between reasoning path length and accuracy, demonstrating the potential of scaling inference for advanced instruction-following and reasoning. This study not only highlights the promise of CoT in enhancing LALM reasoning capabilities but also identifies key limitations and provides actionable directions for future research.",
      "authors": [
        "Ziyang Ma",
        "Zhuo Chen",
        "Yuping Wang",
        "Eng Siong Chng",
        "Xie Chen"
      ],
      "published": "2025-01-13T11:54:40Z",
      "updated": "2025-01-13T11:54:40Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.07246v1",
      "landing_url": "https://arxiv.org/abs/2501.07246v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.07246"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on integrating Chain-of-Thought reasoning into Large Audio-Language Models and does not discuss discrete audio-token generation/quantization or codec/semantic tokenization, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on integrating Chain-of-Thought reasoning into Large Audio-Language Models and does not discuss discrete audio-token generation/quantization or codec/semantic tokenization, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset",
    "abstract": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.",
    "metadata": {
      "arxiv_id": "2501.08238",
      "title": "CodecFake+: A Large-Scale Neural Audio Codec-Based Deepfake Speech Dataset",
      "summary": "With the rapid advancement of neural audio codecs, codec-based speech generation (CoSG) systems have become highly powerful. Unfortunately, CoSG also enables the creation of highly realistic deepfake speech, making it easier to mimic an individual's voice and spread misinformation. We refer to this emerging deepfake speech generated by CoSG systems as CodecFake. Detecting such CodecFake is an urgent challenge, yet most existing systems primarily focus on detecting fake speech generated by traditional speech synthesis models. In this paper, we introduce CodecFake+, a large-scale dataset designed to advance CodecFake detection. To our knowledge, CodecFake+ is the largest dataset encompassing the most diverse range of codec architectures. The training set is generated through re-synthesis using 31 publicly available open-source codec models, while the evaluation set includes web-sourced data from 17 advanced CoSG models. We also propose a comprehensive taxonomy that categorizes codecs by their root components: vector quantizer, auxiliary objectives, and decoder types. Our proposed dataset and taxonomy enable detailed analysis at multiple levels to discern the key factors for successful CodecFake detection. At the individual codec level, we validate the effectiveness of using codec re-synthesized speech (CoRS) as training data for large-scale CodecFake detection. At the taxonomy level, we show that detection performance is strongest when the re-synthesis model incorporates disentanglement auxiliary objectives or a frequency-domain decoder. Furthermore, from the perspective of using all the CoRS training data, we show that our proposed taxonomy can be used to select better training data for improving detection performance. Overall, we envision that CodecFake+ will be a valuable resource for both general and fine-grained exploration to develop better anti-spoofing models against CodecFake.",
      "authors": [
        "Xuanjun Chen",
        "Jiawei Du",
        "Haibin Wu",
        "Lin Zhang",
        "I-Ming Lin",
        "I-Hsiang Chiu",
        "Wenze Ren",
        "Yuan Tseng",
        "Yu Tsao",
        "Jyh-Shing Roger Jang",
        "Hung-yi Lee"
      ],
      "published": "2025-01-14T16:26:14Z",
      "updated": "2025-03-17T22:22:05Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.08238v2",
      "landing_url": "https://arxiv.org/abs/2501.08238v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.08238"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Dataset focuses on discrete neural audio codecs and their quantized codes for generating/reattributing speech, matching the discrete token taxonomy plus evaluation of multiple codec configurations, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "Dataset focuses on discrete neural audio codecs and their quantized codes for generating/reattributing speech, matching the discrete token taxonomy plus evaluation of multiple codec configurations, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Geometry-Preserving Encoder/Decoder in Latent Generative Models",
    "abstract": "Generative modeling aims to generate new data samples that resemble a given dataset, with diffusion models recently becoming the most popular generative model. One of the main challenges of diffusion models is solving the problem in the input space, which tends to be very high-dimensional. Recently, solving diffusion models in the latent space through an encoder that maps from the data space to a lower-dimensional latent space has been considered to make the training process more efficient and has shown state-of-the-art results. The variational autoencoder (VAE) is the most commonly used encoder/decoder framework in this domain, known for its ability to learn latent representations and generate data samples. In this paper, we introduce a novel encoder/decoder framework with theoretical properties distinct from those of the VAE, specifically designed to preserve the geometric structure of the data distribution. We demonstrate the significant advantages of this geometry-preserving encoder in the training process of both the encoder and decoder. Additionally, we provide theoretical results proving convergence of the training process, including convergence guarantees for encoder training, and results showing faster convergence of decoder training when using the geometry-preserving encoder.",
    "metadata": {
      "arxiv_id": "2501.09876",
      "title": "Geometry-Preserving Encoder/Decoder in Latent Generative Models",
      "summary": "Generative modeling aims to generate new data samples that resemble a given dataset, with diffusion models recently becoming the most popular generative model. One of the main challenges of diffusion models is solving the problem in the input space, which tends to be very high-dimensional. Recently, solving diffusion models in the latent space through an encoder that maps from the data space to a lower-dimensional latent space has been considered to make the training process more efficient and has shown state-of-the-art results. The variational autoencoder (VAE) is the most commonly used encoder/decoder framework in this domain, known for its ability to learn latent representations and generate data samples. In this paper, we introduce a novel encoder/decoder framework with theoretical properties distinct from those of the VAE, specifically designed to preserve the geometric structure of the data distribution. We demonstrate the significant advantages of this geometry-preserving encoder in the training process of both the encoder and decoder. Additionally, we provide theoretical results proving convergence of the training process, including convergence guarantees for encoder training, and results showing faster convergence of decoder training when using the geometry-preserving encoder.",
      "authors": [
        "Wonjun Lee",
        "Riley C. W. O'Neill",
        "Dongmian Zou",
        "Jeff Calder",
        "Gilad Lerman"
      ],
      "published": "2025-01-16T23:14:34Z",
      "updated": "2025-10-07T05:09:44Z",
      "categories": [
        "math.NA",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.09876v2",
      "landing_url": "https://arxiv.org/abs/2501.09876v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.09876"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on latent-image diffusion models and geometry-preserving encoder/decoder theory without any mention of audio signal tokenization, discrete vocabularies, or codec/semantic unit design, so it fails the audio discrete-token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on latent-image diffusion models and geometry-preserving encoder/decoder theory without any mention of audio signal tokenization, discrete vocabularies, or codec/semantic unit design, so it fails the audio discrete-token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions",
    "abstract": "Recent studies have raised significant concerns regarding the reliability of current mathematics benchmarks, highlighting issues such as simplistic design and potential data contamination. Consequently, developing a reliable benchmark that effectively evaluates large language models' (LLMs) genuine capabilities in mathematical reasoning remains a critical challenge. To address these concerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking LLMs with Random Variables in mathematical reasoning. Specifically, we build question-generating functions to produce random variable questions (RVQs), whose background content mirrors original benchmark problems, but with randomized variable combinations, rendering them \"unseen\" to LLMs. Models must completely understand the inherent question pattern to correctly answer RVQs with diverse variable combinations. Thus, an LLM's genuine reasoning capability is reflected through its accuracy and robustness on RV-Bench. We conducted extensive experiments on over 30 representative LLMs across more than 1,000 RVQs. Our findings propose that LLMs exhibit a proficiency imbalance between encountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals that proficiency generalization across similar mathematical reasoning tasks is limited, but we verified it can still be effectively elicited through test-time scaling.",
    "metadata": {
      "arxiv_id": "2501.11790",
      "title": "Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions",
      "summary": "Recent studies have raised significant concerns regarding the reliability of current mathematics benchmarks, highlighting issues such as simplistic design and potential data contamination. Consequently, developing a reliable benchmark that effectively evaluates large language models' (LLMs) genuine capabilities in mathematical reasoning remains a critical challenge. To address these concerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking LLMs with Random Variables in mathematical reasoning. Specifically, we build question-generating functions to produce random variable questions (RVQs), whose background content mirrors original benchmark problems, but with randomized variable combinations, rendering them \"unseen\" to LLMs. Models must completely understand the inherent question pattern to correctly answer RVQs with diverse variable combinations. Thus, an LLM's genuine reasoning capability is reflected through its accuracy and robustness on RV-Bench. We conducted extensive experiments on over 30 representative LLMs across more than 1,000 RVQs. Our findings propose that LLMs exhibit a proficiency imbalance between encountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals that proficiency generalization across similar mathematical reasoning tasks is limited, but we verified it can still be effectively elicited through test-time scaling.",
      "authors": [
        "Zijin Hong",
        "Hao Wu",
        "Su Dong",
        "Junnan Dong",
        "Yilin Xiao",
        "Yujing Zhang",
        "Zhu Wang",
        "Feiran Huang",
        "Linyi Li",
        "Hongxia Yang",
        "Xiao Huang"
      ],
      "published": "2025-01-20T23:41:22Z",
      "updated": "2025-08-13T13:29:49Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.11790v4",
      "landing_url": "https://arxiv.org/abs/2501.11790v4",
      "doi": "https://doi.org/10.48550/arXiv.2501.11790"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on LLM mathematical reasoning benchmarks with random variable questions and does not address discrete audio token generation, quantization, or codec-related evaluation required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on LLM mathematical reasoning benchmarks with random variable questions and does not address discrete audio token generation, quantization, or codec-related evaluation required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models",
    "abstract": "Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant safety problems, as models can be exploited to generate harmful or inappropriate content through jailbreak attacks. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce Jailbreak-AudioBench, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.",
    "metadata": {
      "arxiv_id": "2501.13772",
      "title": "Jailbreak-AudioBench: In-Depth Evaluation and Analysis of Jailbreak Threats for Large Audio Language Models",
      "summary": "Large Language Models (LLMs) demonstrate impressive zero-shot performance across a wide range of natural language processing tasks. Integrating various modality encoders further expands their capabilities, giving rise to Multimodal Large Language Models (MLLMs) that process not only text but also visual and auditory modality inputs. However, these advanced capabilities may also pose significant safety problems, as models can be exploited to generate harmful or inappropriate content through jailbreak attacks. While prior work has extensively explored how manipulating textual or visual modality inputs can circumvent safeguards in LLMs and MLLMs, the vulnerability of audio-specific jailbreak on Large Audio-Language Models (LALMs) remains largely underexplored. To address this gap, we introduce Jailbreak-AudioBench, which consists of the Toolbox, curated Dataset, and comprehensive Benchmark. The Toolbox supports not only text-to-audio conversion but also various editing techniques for injecting audio hidden semantics. The curated Dataset provides diverse explicit and implicit jailbreak audio examples in both original and edited forms. Utilizing this dataset, we evaluate multiple state-of-the-art LALMs and establish the most comprehensive Jailbreak benchmark to date for audio modality. Finally, Jailbreak-AudioBench establishes a foundation for advancing future research on LALMs safety alignment by enabling the in-depth exposure of more powerful jailbreak threats, such as query-based audio editing, and by facilitating the development of effective defense mechanisms.",
      "authors": [
        "Hao Cheng",
        "Erjia Xiao",
        "Jing Shao",
        "Yichi Wang",
        "Le Yang",
        "Chao Shen",
        "Philip Torr",
        "Jindong Gu",
        "Renjing Xu"
      ],
      "published": "2025-01-23T15:51:38Z",
      "updated": "2026-01-12T08:12:00Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.13772v4",
      "landing_url": "https://arxiv.org/abs/2501.13772v4",
      "doi": "https://doi.org/10.48550/arXiv.2501.13772"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper surveys jailbreak threats for audio LLMs without addressing discrete audio-token/tokenizer design or evaluation, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "This paper surveys jailbreak threats for audio LLMs without addressing discrete audio-token/tokenizer design or evaluation, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio-Language Models for Audio-Centric Tasks: A survey",
    "abstract": "Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds. Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings. ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks. These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension. Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies. Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments. In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs. Specifically, we cover: (1) the background of computer audition and audio-language models; (2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods; (3) foundational pre-training and audio-language pre-training approaches; (4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications; (5) datasets and benchmarks; and (6) current challenges and future directions. Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.",
    "metadata": {
      "arxiv_id": "2501.15177",
      "title": "Audio-Language Models for Audio-Centric Tasks: A survey",
      "summary": "Audio-Language Models (ALMs), which are trained on audio-text data, focus on the processing, understanding, and reasoning of sounds. Unlike traditional supervised learning approaches learning from predefined labels, ALMs utilize natural language as a supervision signal, which is more suitable for describing complex real-world audio recordings. ALMs demonstrate strong zero-shot capabilities and can be flexibly adapted to diverse downstream tasks. These strengths not only enhance the accuracy and generalization of audio processing tasks but also promote the development of models that more closely resemble human auditory perception and comprehension. Recent advances in ALMs have positioned them at the forefront of computer audition research, inspiring a surge of efforts to advance ALM technologies. Despite rapid progress in the field of ALMs, there is still a notable lack of systematic surveys that comprehensively organize and analyze developments. In this paper, we present a comprehensive review of ALMs with a focus on general audio tasks, aiming to fill this gap by providing a structured and holistic overview of ALMs. Specifically, we cover: (1) the background of computer audition and audio-language models; (2) the foundational aspects of ALMs, including prevalent network architectures, training objectives, and evaluation methods; (3) foundational pre-training and audio-language pre-training approaches; (4) task-specific fine-tuning, multi-task tuning and agent systems for downstream applications; (5) datasets and benchmarks; and (6) current challenges and future directions. Our review provides a clear technical roadmap for researchers to understand the development and future trends of existing technologies, offering valuable references for implementation in real-world scenarios.",
      "authors": [
        "Yi Su",
        "Jisheng Bai",
        "Qisheng Xu",
        "Kele Xu",
        "Yong Dou"
      ],
      "published": "2025-01-25T11:15:06Z",
      "updated": "2025-01-25T11:15:06Z",
      "categories": [
        "cs.SD",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15177v1",
      "landing_url": "https://arxiv.org/abs/2501.15177v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.15177"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract surveys audio-language models broadly without focusing on discrete audio tokenization, quantized codecs, tokenizer definitions, or token-level evaluation, so it fails the inclusion focus on discrete audio token research."
    },
    "round-A_JuniorNano_reasoning": "Abstract surveys audio-language models broadly without focusing on discrete audio tokenization, quantized codecs, tokenizer definitions, or token-level evaluation, so it fails the inclusion focus on discrete audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
    "abstract": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
    "metadata": {
      "arxiv_id": "2501.15907",
      "title": "Emilia: A Large-Scale, Extensive, Multilingual, and Diverse Dataset for Speech Generation",
      "summary": "Recent advancements in speech generation have been driven by large-scale training datasets. However, current models struggle to capture the spontaneity and variability inherent in real-world human speech, as they are primarily trained on audio-book datasets limited to formal, read-aloud speaking styles. To address this limitation, we introduce Emilia-Pipe, an open-source preprocessing pipeline designed to extract high-quality training data from valuable yet under-explored in-the-wild sources that capture spontaneous human speech in real-world contexts. Using Emilia-Pipe, we construct Emilia, which comprises over 101k hours of speech across six languages: English, Chinese, German, French, Japanese, and Korean. Furthermore, we expand Emilia to Emilia-Large, a dataset exceeding 216k hours, making it one of the largest open-source speech generation resources available. Extensive experiments show that Emilia-trained models produce markedly more spontaneous, human-like speech than those trained on traditional audio-book datasets, while matching their intelligibility. These models better capture diverse speaker timbres and the full spectrum of real-world conversational styles. Our work also highlights the importance of scaling dataset size for advancing speech generation performance and validates the effectiveness of Emilia for both multilingual and crosslingual speech generation tasks.",
      "authors": [
        "Haorui He",
        "Zengqiang Shang",
        "Chaoren Wang",
        "Xuyuan Li",
        "Yicheng Gu",
        "Hua Hua",
        "Liwei Liu",
        "Chen Yang",
        "Jiaqi Li",
        "Peiyang Shi",
        "Yuancheng Wang",
        "Kai Chen",
        "Pengyuan Zhang",
        "Zhizheng Wu"
      ],
      "published": "2025-01-27T09:59:20Z",
      "updated": "2025-10-08T06:46:48Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.15907v2",
      "landing_url": "https://arxiv.org/abs/2501.15907v2",
      "doi": "https://doi.org/10.1109/TASLPRO.2025.3612835"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Emilia describes building a large spontaneous speech dataset and training models on audio-book style vs in-the-wild speech, but it never discusses discrete tokenization/quantization or vocabularies, so it fails the inclusion criteria focused on discrete audio tokens and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Emilia describes building a large spontaneous speech dataset and training models on audio-book style vs in-the-wild speech, but it never discusses discrete tokenization/quantization or vocabularies, so it fails the inclusion criteria focused on discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
    "abstract": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases. We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches. When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.",
    "metadata": {
      "arxiv_id": "2501.16273",
      "title": "Return of the Encoder: Maximizing Parameter Efficiency for SLMs",
      "summary": "The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.\n  We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.\n  When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.",
      "authors": [
        "Mohamed Elfeki",
        "Rui Liu",
        "Chad Voegele"
      ],
      "published": "2025-01-27T18:06:36Z",
      "updated": "2025-01-30T16:44:45Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.16273v2",
      "landing_url": "https://arxiv.org/abs/2501.16273v2",
      "doi": "https://doi.org/10.48550/arXiv.2501.16273"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on encoder-decoder vs decoder-only architectures for small language models (SLMs) without discussing discrete audio tokenization, quantized vocabularies, or audio codec/tokenizer evaluation, so it fails to meet the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on encoder-decoder vs decoder-only architectures for small language models (SLMs) without discussing discrete audio tokenization, quantized vocabularies, or audio codec/tokenizer evaluation, so it fails to meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DINT Transformer",
    "abstract": "DIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.",
    "metadata": {
      "arxiv_id": "2501.17486",
      "title": "DINT Transformer",
      "summary": "DIFF Transformer addresses the issue of irrelevant context interference by introducing a differential attention mechanism that enhances the robustness of local attention. However, it has two critical limitations: the lack of global context modeling, which is essential for identifying globally significant tokens, and numerical instability due to the absence of strict row normalization in the attention matrix. To overcome these challenges, we propose DINT Transformer, which extends DIFF Transformer by incorporating a differential-integral mechanism. By computing global importance scores and integrating them into the attention matrix, DINT Transformer improves its ability to capture global dependencies. Moreover, the unified parameter design enforces row-normalized attention matrices, improving numerical stability. Experimental results demonstrate that DINT Transformer excels in accuracy and robustness across various practical applications, such as long-context language modeling and key information retrieval. These results position DINT Transformer as a highly effective and promising architecture.",
      "authors": [
        "Yueyang Cang",
        "Yuhang Liu",
        "Xiaoteng Zhang",
        "Erlu Zhao",
        "Li Shi"
      ],
      "published": "2025-01-29T08:53:29Z",
      "updated": "2025-01-29T08:53:29Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.17486v1",
      "landing_url": "https://arxiv.org/abs/2501.17486v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.17486"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on transformer attention mechanisms and robustness in general contexts (likely text) with no mention of discrete audio tokenization, codec/token definitions, or audio-specific evaluations, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on transformer attention mechanisms and robustness in general contexts (likely text) with no mention of discrete audio tokenization, codec/token definitions, or audio-specific evaluations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models",
    "abstract": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",
    "metadata": {
      "arxiv_id": "2501.18119",
      "title": "Self-supervised Quantized Representation for Seamlessly Integrating Knowledge Graphs with Large Language Models",
      "summary": "Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.",
      "authors": [
        "Qika Lin",
        "Tianzhe Zhao",
        "Kai He",
        "Zhen Peng",
        "Fangzhi Xu",
        "Ling Huang",
        "Jingying Ma",
        "Mengling Feng"
      ],
      "published": "2025-01-30T03:40:20Z",
      "updated": "2025-01-30T03:40:20Z",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.18119v1",
      "landing_url": "https://arxiv.org/abs/2501.18119v1",
      "doi": "https://doi.org/10.48550/arXiv.2501.18119"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on KG-to-LLM integration with entity quantization, not discrete audio-token/tokenizer design, so it fails the audio-specific inclusion and should be excluded (score: 1)."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on KG-to-LLM integration with entity quantization, not discrete audio-token/tokenizer design, so it fails the audio-specific inclusion and should be excluded (score: 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs",
    "abstract": "Current Multimodal Large Language Model (MLLM) architectures face a critical tradeoff between performance and efficiency: decoder-only architectures achieve higher performance but lower efficiency, while cross-attention-based architectures offer greater efficiency but lower performance. The key distinction lies in how visual tokens are processed. Decoder-only architectures apply self-attention and FFN operations on visual tokens, while cross-attention architectures skip these computations. To investigate whether redundancy exists in this computationally expensive process, we propose a training-free framework for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and Hollow Attention, which enable adjustable reductions in computations for visual tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these reductions. Extensive experiments demonstrate substantial, structured, and clustered redundancy unique to decoder-only MLLMs, offering valuable insights for future MLLM architecture design. Furthermore, by leveraging our reduction framework as a training-free inference acceleration approach, we achieve performance comparable to or better than state-of-the-art methods while remaining compatible with them. Code will be publicly available at https://github.com/L-Hugh/RedundancyLens.",
    "metadata": {
      "arxiv_id": "2501.19036",
      "title": "RedundancyLens: Revealing and Exploiting Visual Token Processing Redundancy for Efficient Decoder-Only MLLMs",
      "summary": "Current Multimodal Large Language Model (MLLM) architectures face a critical tradeoff between performance and efficiency: decoder-only architectures achieve higher performance but lower efficiency, while cross-attention-based architectures offer greater efficiency but lower performance. The key distinction lies in how visual tokens are processed. Decoder-only architectures apply self-attention and FFN operations on visual tokens, while cross-attention architectures skip these computations. To investigate whether redundancy exists in this computationally expensive process, we propose a training-free framework for analyzing trained MLLMs. It consists of Probe-Activated Dynamic FFN and Hollow Attention, which enable adjustable reductions in computations for visual tokens, as well as a Layer Ranking Algorithm that prioritizes layers for these reductions. Extensive experiments demonstrate substantial, structured, and clustered redundancy unique to decoder-only MLLMs, offering valuable insights for future MLLM architecture design. Furthermore, by leveraging our reduction framework as a training-free inference acceleration approach, we achieve performance comparable to or better than state-of-the-art methods while remaining compatible with them. Code will be publicly available at https://github.com/L-Hugh/RedundancyLens.",
      "authors": [
        "Hongliang Li",
        "Jiaxin Zhang",
        "Wenhui Liao",
        "Dezhi Peng",
        "Kai Ding",
        "Lianwen Jin"
      ],
      "published": "2025-01-31T11:09:16Z",
      "updated": "2025-05-30T12:26:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2501.19036v3",
      "landing_url": "https://arxiv.org/abs/2501.19036v3",
      "doi": "https://doi.org/10.48550/arXiv.2501.19036"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Does not meet any inclusion criteria (focuses on visual tokens in multimodal LLMs rather than discrete audio tokens) and violates the exclusion rule about non-audio token research, so it should definitely be excluded."
    },
    "round-A_JuniorNano_reasoning": "Does not meet any inclusion criteria (focuses on visual tokens in multimodal LLMs rather than discrete audio tokens) and violates the exclusion rule about non-audio token research, so it should definitely be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
    "abstract": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
    "metadata": {
      "arxiv_id": "2502.00250",
      "title": "Transformer-Based Vector Font Classification Using Different Font Formats: TrueType versus PostScript",
      "summary": "Modern fonts adopt vector-based formats, which ensure scalability without loss of quality. While many deep learning studies on fonts focus on bitmap formats, deep learning for vector fonts remains underexplored. In studies involving deep learning for vector fonts, the choice of font representation has often been made conventionally. However, the font representation format is one of the factors that can influence the computational performance of machine learning models in font-related tasks. Here we show that font representations based on PostScript outlines outperform those based on TrueType outlines in Transformer-based vector font classification. TrueType outlines represent character shapes as sequences of points and their associated flags, whereas PostScript outlines represent them as sequences of commands. In previous research, PostScript outlines have been predominantly used when fonts are treated as part of vector graphics, while TrueType outlines are mainly employed when focusing on fonts alone. Whether to use PostScript or TrueType outlines has been mainly determined by file format specifications and precedent settings in previous studies, rather than performance considerations. To date, few studies have compared which outline format provides better embedding representations. Our findings suggest that information aggregation is crucial in Transformer-based deep learning for vector graphics, as in tokenization in language models and patch division in bitmap-based image recognition models. This insight provides valuable guidance for selecting outline formats in future research on vector graphics.",
      "authors": [
        "Takumu Fujioka",
        "Gouhei Tanaka"
      ],
      "published": "2025-02-01T01:16:27Z",
      "updated": "2025-02-01T01:16:27Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00250v1",
      "landing_url": "https://arxiv.org/abs/2502.00250v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00250"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on vector font format comparison rather than discrete audio token generation, representation, or evaluation (outside inclusion scope), so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Focuses on vector font format comparison rather than discrete audio token generation, representation, or evaluation (outside inclusion scope), so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Survey of Quantized Graph Representation Learning: Connecting Graph Structures with Large Language Models",
    "abstract": "Recent years have witnessed rapid advances in graph representation learning, with the continuous embedding approach emerging as the dominant paradigm. However, such methods encounter issues regarding parameter efficiency, interpretability, and robustness. Thus, Quantized Graph Representation (QGR) learning has recently gained increasing interest, which represents the graph structure with discrete codes instead of conventional continuous embeddings. Given its analogous representation form to natural language, QGR also possesses the capability to seamlessly integrate graph structures with large language models (LLMs). As this emerging paradigm is still in its infancy yet holds significant promise, we undertake this thorough survey to promote its rapid future prosperity. We first present the background of the general quantization methods and their merits. Moreover, we provide an in-depth demonstration of current QGR studies from the perspectives of quantized strategies, training objectives, distinctive designs, knowledge graph quantization, and applications. We further explore the strategies for code dependence learning and integration with LLMs. At last, we give discussions and conclude future directions, aiming to provide a comprehensive picture of QGR and inspire future research.",
    "metadata": {
      "arxiv_id": "2502.00681",
      "title": "A Survey of Quantized Graph Representation Learning: Connecting Graph Structures with Large Language Models",
      "summary": "Recent years have witnessed rapid advances in graph representation learning, with the continuous embedding approach emerging as the dominant paradigm. However, such methods encounter issues regarding parameter efficiency, interpretability, and robustness. Thus, Quantized Graph Representation (QGR) learning has recently gained increasing interest, which represents the graph structure with discrete codes instead of conventional continuous embeddings. Given its analogous representation form to natural language, QGR also possesses the capability to seamlessly integrate graph structures with large language models (LLMs). As this emerging paradigm is still in its infancy yet holds significant promise, we undertake this thorough survey to promote its rapid future prosperity. We first present the background of the general quantization methods and their merits. Moreover, we provide an in-depth demonstration of current QGR studies from the perspectives of quantized strategies, training objectives, distinctive designs, knowledge graph quantization, and applications. We further explore the strategies for code dependence learning and integration with LLMs. At last, we give discussions and conclude future directions, aiming to provide a comprehensive picture of QGR and inspire future research.",
      "authors": [
        "Qika Lin",
        "Zhen Peng",
        "Kaize Shi",
        "Kai He",
        "Yiming Xu",
        "Erik Cambria",
        "Mengling Feng"
      ],
      "published": "2025-02-02T05:57:34Z",
      "updated": "2025-02-02T05:57:34Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00681v1",
      "landing_url": "https://arxiv.org/abs/2502.00681v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.00681"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on quantized graph representation learning and integration with LLMs, which has no connection to the discrete audio-token generation and evaluation scope defined in the inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on quantized graph representation learning and integration with LLMs, which has no connection to the discrete audio-token generation and evaluation scope defined in the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
    "abstract": "The rise of multimodal large language models has introduced innovative human-machine interaction paradigms but also significant challenges in machine learning safety. Audio-Language Models (ALMs) are especially relevant due to the intuitive nature of spoken communication, yet little is known about their failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on their ability to bypass alignment mechanisms. We construct adversarial perturbations that generalize across prompts, tasks, and even base audio samples, demonstrating the first universal jailbreaks in the audio modality, and show that these remain effective in simulated real-world conditions. Beyond demonstrating attack feasibility, we analyze how ALMs interpret these audio adversarial examples and reveal them to encode imperceptible first-person toxic speech - suggesting that the most effective perturbations for eliciting toxic outputs specifically embed linguistic features within the audio signal. These results have important implications for understanding the interactions between different modalities in multimodal models, and offer actionable insights for enhancing defenses against adversarial audio attacks.",
    "metadata": {
      "arxiv_id": "2502.00718",
      "title": "\"I am bad\": Interpreting Stealthy, Universal and Robust Audio Jailbreaks in Audio-Language Models",
      "summary": "The rise of multimodal large language models has introduced innovative human-machine interaction paradigms but also significant challenges in machine learning safety. Audio-Language Models (ALMs) are especially relevant due to the intuitive nature of spoken communication, yet little is known about their failure modes. This paper explores audio jailbreaks targeting ALMs, focusing on their ability to bypass alignment mechanisms. We construct adversarial perturbations that generalize across prompts, tasks, and even base audio samples, demonstrating the first universal jailbreaks in the audio modality, and show that these remain effective in simulated real-world conditions. Beyond demonstrating attack feasibility, we analyze how ALMs interpret these audio adversarial examples and reveal them to encode imperceptible first-person toxic speech - suggesting that the most effective perturbations for eliciting toxic outputs specifically embed linguistic features within the audio signal. These results have important implications for understanding the interactions between different modalities in multimodal models, and offer actionable insights for enhancing defenses against adversarial audio attacks.",
      "authors": [
        "Isha Gupta",
        "David Khachaturov",
        "Robert Mullins"
      ],
      "published": "2025-02-02T08:36:23Z",
      "updated": "2025-07-10T14:44:44Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.00718v2",
      "landing_url": "https://arxiv.org/abs/2502.00718v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.00718"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies audio jailbreaks for ALMs via adversarial perturbations and does not address discrete audio token generation, quantization, or token-level evaluation, so it fails the inclusion focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper studies audio jailbreaks for ALMs via adversarial perturbations and does not address discrete audio token generation, quantization, or token-level evaluation, so it fails the inclusion focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis",
    "abstract": "We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3\\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.",
    "metadata": {
      "arxiv_id": "2502.01084",
      "title": "Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis",
      "summary": "We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3\\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at https://tinyurl.com/gmm-lm-tts.",
      "authors": [
        "Weiwei Lin",
        "Chenghan He"
      ],
      "published": "2025-02-03T05:53:59Z",
      "updated": "2025-02-13T09:25:03Z",
      "categories": [
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.01084v2",
      "landing_url": "https://arxiv.org/abs/2502.01084v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.01084"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Core method relies on continuous VAE latent space and Gaussian mixture autoregressor without describing a discrete tokenization/quantization pipeline, so it fails to meet the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Core method relies on continuous VAE latent space and Gaussian mixture autoregressor without describing a discrete tokenization/quantization pipeline, so it fails to meet the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
    "abstract": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
    "metadata": {
      "arxiv_id": "2502.02019",
      "title": "ComplexDec: A Domain-robust High-fidelity Neural Audio Codec with Complex Spectrum Modeling",
      "summary": "Neural audio codecs have been widely adopted in audio-generative tasks because their compact and discrete representations are suitable for both large-language-model-style and regression-based generative models. However, most neural codecs struggle to model out-of-domain audio, resulting in error propagations to downstream generative tasks. In this paper, we first argue that information loss from codec compression degrades out-of-domain robustness. Then, we propose full-band 48~kHz ComplexDec with complex spectral input and output to ease the information loss while adopting the same 24~kbps bitrate as the baseline AuidoDec and ScoreDec. Objective and subjective evaluations demonstrate the out-of-domain robustness of ComplexDec trained using only the 30-hour VCTK corpus.",
      "authors": [
        "Yi-Chiao Wu",
        "Dejan Marković",
        "Steven Krenn",
        "Israel D. Gebru",
        "Alexander Richard"
      ],
      "published": "2025-02-04T05:16:15Z",
      "updated": "2025-02-04T05:16:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02019v1",
      "landing_url": "https://arxiv.org/abs/2502.02019v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02019"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "ComplexDec proposes a high-fidelity neural audio codec with discrete quantized representation and bitrate-focused evaluation, matching the discrete audio token/theme and providing codec-level comparison for out-of-domain robustness, so it meets inclusion fully."
    },
    "round-A_JuniorNano_reasoning": "ComplexDec proposes a high-fidelity neural audio codec with discrete quantized representation and bitrate-focused evaluation, matching the discrete audio token/theme and providing codec-level comparison for out-of-domain robustness, so it meets inclusion fully.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
    "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
    "metadata": {
      "arxiv_id": "2502.02942",
      "title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling",
      "summary": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called \\textit{GenSE}. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability.",
      "authors": [
        "Jixun Yao",
        "Hexin Liu",
        "Chen Chen",
        "Yuchen Hu",
        "EngSiong Chng",
        "Lei Xie"
      ],
      "published": "2025-02-05T07:14:39Z",
      "updated": "2025-02-05T07:14:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.02942v1",
      "landing_url": "https://arxiv.org/abs/2502.02942v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.02942"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "GenSE explicitly discretizes speech into semantic tokens via an SSL model and acoustic tokens via a codec before conditioning a language model on them, so it matches the discrete audio token/codec token requirements and provides modeling/evaluation detail, making it fully includable."
    },
    "round-A_JuniorNano_reasoning": "GenSE explicitly discretizes speech into semantic tokens via an SSL model and acoustic tokens via a codec before conditioning a language model on them, so it matches the discrete audio token/codec token requirements and provides modeling/evaluation detail, making it fully includable.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
    "abstract": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
    "metadata": {
      "arxiv_id": "2502.03128",
      "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
      "summary": "We introduce Metis, a foundation model for unified speech generation. Unlike previous task-specific or multi-task models, Metis follows a pre-training and fine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data using masked generative modeling and then fine-tuned to adapt to diverse speech generation tasks. Specifically, 1) Metis utilizes two discrete speech representations: SSL tokens derived from speech self-supervised learning (SSL) features, and acoustic tokens directly quantized from waveforms. 2) Metis performs masked generative pre-training on SSL tokens, utilizing 300K hours of diverse speech data, without any additional condition. 3) Through fine-tuning with task-specific conditions, Metis achieves efficient adaptation to various speech generation tasks while supporting multimodal input, even when using limited data and trainable parameters. Experiments demonstrate that Metis can serve as a foundation model for unified speech generation: Metis outperforms state-of-the-art task-specific or multi-task systems across five speech generation tasks, including zero-shot text-to-speech, voice conversion, target speaker extraction, speech enhancement, and lip-to-speech, even with fewer than 20M trainable parameters or 300 times less training data. Audio samples are are available at https://metis-demo.github.io/.",
      "authors": [
        "Yuancheng Wang",
        "Jiachen Zheng",
        "Junan Zhang",
        "Xueyao Zhang",
        "Huan Liao",
        "Zhizheng Wu"
      ],
      "published": "2025-02-05T12:36:21Z",
      "updated": "2025-02-05T12:36:21Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03128v1",
      "landing_url": "https://arxiv.org/abs/2502.03128v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03128"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Metis clearly defines discrete SSL-derived and quantized acoustic token representations and evaluates their use in unified speech generation, satisfying inclusion criteria with no exclusions, so it should be included (score=5)."
    },
    "round-A_JuniorNano_reasoning": "Metis clearly defines discrete SSL-derived and quantized acoustic token representations and evaluates their use in unified speech generation, satisfying inclusion criteria with no exclusions, so it should be included (score=5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LED there be DoS: Exploiting variable bitrate IP cameras for network DoS",
    "abstract": "Variable-bitrate video streaming is ubiquitous in video surveillance and CCTV, enabling high-quality video streaming while conserving network bandwidth. However, as the name suggests, variable-bitrate IP cameras can generate sharp traffic spikes depending on the dynamics of the visual input. In this paper, we show that the effectiveness of video compression can be reduced by up to 6X using a simple laser LED pointing at a variable-bitrate IP camera, forcing the camera to generate excessive network traffic. Experiments with IP cameras connected to wired and wireless networks indicate that a laser attack on a single camera can cause significant packet loss in systems sharing the network with the camera and reduce the available bandwidth of a shared network link by 90%. This attack represents a new class of cyber-physical attacks that manipulate variable bitrate devices through changes in the physical environment without a digital presence on the device or the network. We also analyze the broader view of multidimensional cyberattacks that involve both the physical and digital realms and present a taxonomy that categorizes attacks based on their direction of influence (physical-to-digital or digital-to-physical) and their method of operation (environment-driven or device-driven), highlighting multiple areas for future research.",
    "metadata": {
      "arxiv_id": "2502.03177",
      "title": "LED there be DoS: Exploiting variable bitrate IP cameras for network DoS",
      "summary": "Variable-bitrate video streaming is ubiquitous in video surveillance and CCTV, enabling high-quality video streaming while conserving network bandwidth. However, as the name suggests, variable-bitrate IP cameras can generate sharp traffic spikes depending on the dynamics of the visual input. In this paper, we show that the effectiveness of video compression can be reduced by up to 6X using a simple laser LED pointing at a variable-bitrate IP camera, forcing the camera to generate excessive network traffic. Experiments with IP cameras connected to wired and wireless networks indicate that a laser attack on a single camera can cause significant packet loss in systems sharing the network with the camera and reduce the available bandwidth of a shared network link by 90%. This attack represents a new class of cyber-physical attacks that manipulate variable bitrate devices through changes in the physical environment without a digital presence on the device or the network. We also analyze the broader view of multidimensional cyberattacks that involve both the physical and digital realms and present a taxonomy that categorizes attacks based on their direction of influence (physical-to-digital or digital-to-physical) and their method of operation (environment-driven or device-driven), highlighting multiple areas for future research.",
      "authors": [
        "Emmanuel Goldberg",
        "Oleg Brodt",
        "Aviad Elyashar",
        "Rami Puzis"
      ],
      "published": "2025-02-05T13:53:59Z",
      "updated": "2025-02-05T13:53:59Z",
      "categories": [
        "cs.CR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03177v1",
      "landing_url": "https://arxiv.org/abs/2502.03177v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03177"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper describes a cyber-physical denial-of-service attack on IP cameras and has nothing to do with discrete audio tokens or their tokenization/quantization, so it fails the inclusion criteria completely."
    },
    "round-A_JuniorNano_reasoning": "The paper describes a cyber-physical denial-of-service attack on IP cameras and has nothing to do with discrete audio tokens or their tokenization/quantization, so it fails the inclusion criteria completely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
    "abstract": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
    "metadata": {
      "arxiv_id": "2502.03605",
      "title": "Accelerating OTA Circuit Design: Transistor Sizing Based on a Transformer Model and Precomputed Lookup Tables",
      "summary": "Device sizing is crucial for meeting performance specifications in operational transconductance amplifiers (OTAs), and this work proposes an automated sizing framework based on a transformer model. The approach first leverages the driving-point signal flow graph (DP-SFG) to map an OTA circuit and its specifications into transformer-friendly sequential data. A specialized tokenization approach is applied to the sequential data to expedite the training of the transformer on a diverse range of OTA topologies, under multiple specifications. Under specific performance constraints, the trained transformer model is used to accurately predict DP-SFG parameters in the inference phase. The predicted DP-SFG parameters are then translated to transistor sizes using a precomputed look-up table-based approach inspired by the gm/Id methodology. In contrast to previous conventional or machine-learning-based methods, the proposed framework achieves significant improvements in both speed and computational efficiency by reducing the need for expensive SPICE simulations within the optimization loop; instead, almost all SPICE simulations are confined to the one-time training phase. The method is validated on a variety of unseen specifications, and the sizing solution demonstrates over 90% success in meeting specifications with just one SPICE simulation for validation, and 100% success with 3-5 additional SPICE simulations.",
      "authors": [
        "Subhadip Ghosh",
        "Endalk Y. Gebru",
        "Chandramouli V. Kashyap",
        "Ramesh Harjani",
        "Sachin S. Sapatnekar"
      ],
      "published": "2025-02-05T20:48:27Z",
      "updated": "2025-02-05T20:48:27Z",
      "categories": [
        "cs.AR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03605v1",
      "landing_url": "https://arxiv.org/abs/2502.03605v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.03605"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on transformer-based OTA circuit sizing with SPICE simulations rather than any discrete audio tokenization, so it fails to meet the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on transformer-based OTA circuit sizing with SPICE simulations rather than any discrete audio tokenization, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
    "abstract": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
    "metadata": {
      "arxiv_id": "2502.03930",
      "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation",
      "summary": "Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.",
      "authors": [
        "Dongya Jia",
        "Zhuo Chen",
        "Jiawei Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Jian Cong",
        "Xiaobin Zhuang",
        "Chumin Li",
        "Zhen Wei",
        "Yuping Wang",
        "Yuxuan Wang"
      ],
      "published": "2025-02-06T10:09:49Z",
      "updated": "2025-12-08T08:11:20Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.03930v4",
      "landing_url": "https://arxiv.org/abs/2502.03930v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.03930"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study explicitly generates continuous speech representations without discrete tokens, so it doesn’t fit the discrete audio token focus and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study explicitly generates continuous speech representations without discrete tokens, so it doesn’t fit the discrete audio token focus and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Decoder-Only LLMs are Better Controllers for Diffusion Models",
    "abstract": "Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.",
    "metadata": {
      "arxiv_id": "2502.04412",
      "title": "Decoder-Only LLMs are Better Controllers for Diffusion Models",
      "summary": "Groundbreaking advancements in text-to-image generation have recently been achieved with the emergence of diffusion models. These models exhibit a remarkable ability to generate highly artistic and intricately detailed images based on textual prompts. However, obtaining desired generation outcomes often necessitates repetitive trials of manipulating text prompts just like casting spells on a magic mirror, and the reason behind that is the limited capability of semantic understanding inherent in current image generation models. Specifically, existing diffusion models encode the text prompt input with a pre-trained encoder structure, which is usually trained on a limited number of image-caption pairs. The state-of-the-art large language models (LLMs) based on the decoder-only structure have shown a powerful semantic understanding capability as their architectures are more suitable for training on very large-scale unlabeled data. In this work, we propose to enhance text-to-image diffusion models by borrowing the strength of semantic understanding from large language models, and devise a simple yet effective adapter to allow the diffusion models to be compatible with the decoder-only structure. Meanwhile, we also provide a supporting theoretical analysis with various architectures (e.g., encoder-only, encoder-decoder, and decoder-only), and conduct extensive empirical evaluations to verify its effectiveness. The experimental results show that the enhanced models with our adapter module are superior to the stat-of-the-art models in terms of text-to-image generation quality and reliability.",
      "authors": [
        "Ziyi Dong",
        "Yao Xiao",
        "Pengxu Wei",
        "Liang Lin"
      ],
      "published": "2025-02-06T12:17:35Z",
      "updated": "2025-02-06T12:17:35Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04412v1",
      "landing_url": "https://arxiv.org/abs/2502.04412v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.04412"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on text-to-image diffusion models and large language model adapters, with no discrete audio token/tokenizer content or token specification relevant to audio, so it fails the inclusion criteria and meets exclusion criteria for non-audio sequences."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on text-to-image diffusion models and large language model adapters, with no discrete audio token/tokenizer content or token specification relevant to audio, so it fails the inclusion criteria and meets exclusion criteria for non-audio sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "abstract": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.",
    "metadata": {
      "arxiv_id": "2502.04465",
      "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
      "summary": "Large language models have revolutionized natural language processing through self-supervised pretraining on massive datasets. Inspired by this success, researchers have explored adapting these methods to speech by discretizing continuous audio into tokens using neural audio codecs. However, existing approaches face limitations, including high bitrates, the loss of either semantic or acoustic information, and the reliance on multi-codebook designs when trying to capture both, which increases architectural complexity for downstream tasks. To address these challenges, we introduce FocalCodec, an efficient low-bitrate codec based on focal modulation that utilizes a single binary codebook to compress speech between 0.16 and 0.65 kbps. FocalCodec delivers competitive performance in speech resynthesis and voice conversion at lower bitrates than the current state-of-the-art, while effectively handling multilingual speech and noisy environments. Evaluation on downstream tasks shows that FocalCodec successfully preserves sufficient semantic and acoustic information, while also being well-suited for generative modeling. Demo samples and code are available at https://lucadellalib.github.io/focalcodec-web/.",
      "authors": [
        "Luca Della Libera",
        "Francesco Paissan",
        "Cem Subakan",
        "Mirco Ravanelli"
      ],
      "published": "2025-02-06T19:24:50Z",
      "updated": "2025-10-24T19:00:25Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.04465v2",
      "landing_url": "https://arxiv.org/abs/2502.04465v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.04465"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study introduces discrete neural codec tokens (single binary codebook) to compress speech with quality/usage evaluations, satisfying the inclusion criteria while avoiding exclusion."
    },
    "round-A_JuniorNano_reasoning": "Study introduces discrete neural codec tokens (single binary codebook) to compress speech with quality/usage evaluations, satisfying the inclusion criteria while avoiding exclusion.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "L2GNet: Optimal Local-to-Global Representation of Anatomical Structures for Generalized Medical Image Segmentation",
    "abstract": "Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multi-organ segmentation and cardiac datasets demonstrate L2GNet's superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering an novel approach to enhance deep learning models' performance in medical image analysis.",
    "metadata": {
      "arxiv_id": "2502.05229",
      "title": "L2GNet: Optimal Local-to-Global Representation of Anatomical Structures for Generalized Medical Image Segmentation",
      "summary": "Continuous Latent Space (CLS) and Discrete Latent Space (DLS) models, like AttnUNet and VQUNet, have excelled in medical image segmentation. In contrast, Synergistic Continuous and Discrete Latent Space (CDLS) models show promise in handling fine and coarse-grained information. However, they struggle with modeling long-range dependencies. CLS or CDLS-based models, such as TransUNet or SynergyNet are adept at capturing long-range dependencies. Since they rely heavily on feature pooling or aggregation using self-attention, they may capture dependencies among redundant regions. This hinders comprehension of anatomical structure content, poses challenges in modeling intra-class and inter-class dependencies, increases false negatives and compromises generalization. Addressing these issues, we propose L2GNet, which learns global dependencies by relating discrete codes obtained from DLS using optimal transport and aligning codes on a trainable reference. L2GNet achieves discriminative on-the-fly representation learning without an additional weight matrix in self-attention models, making it computationally efficient for medical applications. Extensive experiments on multi-organ segmentation and cardiac datasets demonstrate L2GNet's superiority over state-of-the-art methods, including the CDLS method SynergyNet, offering an novel approach to enhance deep learning models' performance in medical image analysis.",
      "authors": [
        "Vandan Gorade",
        "Sparsh Mittal",
        "Neethi Dasu",
        "Rekha Singhal",
        "KC Santosh",
        "Debesh Jha"
      ],
      "published": "2025-02-06T14:14:44Z",
      "updated": "2025-02-06T14:14:44Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05229v1",
      "landing_url": "https://arxiv.org/abs/2502.05229v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.05229"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This is about medical image segmentation with discrete/continuous latent spaces, unrelated to discrete audio tokens or tokenizers, so it fails all inclusion criteria and hits the exclusion for non-audio research."
    },
    "round-A_JuniorNano_reasoning": "This is about medical image segmentation with discrete/continuous latent spaces, unrelated to discrete audio tokens or tokenizers, so it fails all inclusion criteria and hits the exclusion for non-audio research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
    "abstract": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
    "metadata": {
      "arxiv_id": "2502.05236",
      "title": "Koel-TTS: Enhancing LLM based Speech Generation with Preference Alignment and Classifier Free Guidance",
      "summary": "While autoregressive speech token generation models produce speech with remarkable variety and naturalness, their inherent lack of controllability often results in issues such as hallucinations and undesired vocalizations that do not conform to conditioning inputs. We introduce Koel-TTS, a suite of enhanced encoder-decoder Transformer TTS models that address these challenges by incorporating preference alignment techniques guided by automatic speech recognition and speaker verification models. Additionally, we incorporate classifier-free guidance to further improve synthesis adherence to the transcript and reference speaker audio. Our experiments demonstrate that these optimizations significantly enhance target speaker similarity, intelligibility, and naturalness of synthesized speech. Notably, Koel-TTS directly maps text and context audio to acoustic tokens, and on the aforementioned metrics, outperforms state-of-the-art TTS models, despite being trained on a significantly smaller dataset. Audio samples and demos are available on our website.",
      "authors": [
        "Shehzeen Hussain",
        "Paarth Neekhara",
        "Xuesong Yang",
        "Edresson Casanova",
        "Subhankar Ghosh",
        "Mikyas T. Desta",
        "Roy Fejgin",
        "Rafael Valle",
        "Jason Li"
      ],
      "published": "2025-02-07T06:47:11Z",
      "updated": "2025-07-22T21:32:13Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.05236v2",
      "landing_url": "https://arxiv.org/abs/2502.05236v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.05236"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper describes improvements in Transformer TTS to map text/context audio to acoustic tokens, but the abstract lacks any detail on tokenization/quantization design, vocabularies, or measurable discrete-token evaluations, so it likely does not satisfy the discrete audio-token focus required for inclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper describes improvements in Transformer TTS to map text/context audio to acoustic tokens, but the abstract lacks any detail on tokenization/quantization design, vocabularies, or measurable discrete-token evaluations, so it likely does not satisfy the discrete audio-token focus required for inclusion.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
    "abstract": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
    "metadata": {
      "arxiv_id": "2502.08939",
      "title": "TokenSynth: A Token-based Neural Synthesizer for Instrument Cloning and Text-to-Instrument",
      "summary": "Recent advancements in neural audio codecs have enabled the use of tokenized audio representations in various audio generation tasks, such as text-to-speech, text-to-audio, and text-to-music generation. Leveraging this approach, we propose TokenSynth, a novel neural synthesizer that utilizes a decoder-only transformer to generate desired audio tokens from MIDI tokens and CLAP (Contrastive Language-Audio Pretraining) embedding, which has timbre-related information. Our model is capable of performing instrument cloning, text-to-instrument synthesis, and text-guided timbre manipulation without any fine-tuning. This flexibility enables diverse sound design and intuitive timbre control. We evaluated the quality of the synthesized audio, the timbral similarity between synthesized and target audio/text, and synthesis accuracy (i.e., how accurately it follows the input MIDI) using objective measures. TokenSynth demonstrates the potential of leveraging advanced neural audio codecs and transformers to create powerful and versatile neural synthesizers. The source code, model weights, and audio demos are available at: https://github.com/KyungsuKim42/tokensynth",
      "authors": [
        "Kyungsu Kim",
        "Junghyun Koo",
        "Sungho Lee",
        "Haesun Joung",
        "Kyogu Lee"
      ],
      "published": "2025-02-13T03:40:30Z",
      "updated": "2025-02-13T03:40:30Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.08939v1",
      "landing_url": "https://arxiv.org/abs/2502.08939v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.08939"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "TokenSynth directly works with discrete neural audio codec tokens for instrument cloning/text-to-instrument, includes codec/token modeling and evaluation, and thus meets all inclusion criteria with none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "TokenSynth directly works with discrete neural audio codec tokens for instrument cloning/text-to-instrument, includes codec/token modeling and evaluation, and thus meets all inclusion criteria with none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
    "abstract": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
    "metadata": {
      "arxiv_id": "2502.09282",
      "title": "MsEdF: A Multi-stream Encoder-decoder Framework for Remote Sensing Image Captioning",
      "summary": "Remote sensing images contain complex spatial patterns and semantic structures, which makes the captioning model difficult to accurately describe. Encoder-decoder architectures have become the widely used approach for RSIC by translating visual content into descriptive text. However, many existing methods rely on a single-stream architecture, which weakens the model to accurately describe the image. Such single-stream architectures typically struggle to extract diverse spatial features or capture complex semantic relationships, limiting their effectiveness in scenes with high intraclass similarity or contextual ambiguity. In this work, we propose a novel Multi-stream Encoder-decoder Framework (MsEdF) which improves the performance of RSIC by optimizing both the spatial representation and language generation of encoder-decoder architecture. The encoder fuses information from two complementary image encoders, thereby promoting feature diversity through the integration of multiscale and structurally distinct cues. To improve the capture of context-aware descriptions, we refine the input sequence's semantic modeling on the decoder side using a stacked GRU architecture with an element-wise aggregation scheme. Experiments on three benchmark RSIC datasets show that MsEdF outperforms several baseline models.",
      "authors": [
        "Swadhin Das",
        "Raksha Sharma"
      ],
      "published": "2025-02-13T12:54:13Z",
      "updated": "2025-10-28T04:40:41Z",
      "categories": [
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09282v4",
      "landing_url": "https://arxiv.org/abs/2502.09282v4",
      "doi": "https://doi.org/10.48550/arXiv.2502.09282"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on remote sensing image captioning with multi-stream encoders/decoders and does not address discrete audio tokens or codec/tokenization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on remote sensing image captioning with multi-stream encoders/decoders and does not address discrete audio tokens or codec/tokenization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
    "abstract": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
    "metadata": {
      "arxiv_id": "2502.09520",
      "title": "SQ-GAN: Semantic Image Communications Using Masked Vector Quantization",
      "summary": "This work introduces Semantically Masked Vector Quantized Generative Adversarial Network (SQ-GAN), a novel approach integrating semantically driven image coding and vector quantization to optimize image compression for semantic/task-oriented communications. The method only acts on source coding and is fully compliant with legacy systems. The semantics is extracted from the image computing its semantic segmentation map using off-the-shelf software. A new specifically developed semantic-conditioned adaptive mask module (SAMM) selectively encodes semantically relevant features of the image. The relevance of the different semantic classes is task-specific, and it is incorporated in the training phase by introducing appropriate weights in the loss function. SQ-GAN outperforms state-of-the-art image compression schemes such as JPEG2000, BPG, and deep-learning based methods across multiple metrics, including perceptual quality and semantic segmentation accuracy on the reconstructed image, at extremely low compression rates.",
      "authors": [
        "Francesco Pezone",
        "Sergio Barbarossa",
        "Giuseppe Caire"
      ],
      "published": "2025-02-13T17:35:57Z",
      "updated": "2025-10-10T10:21:13Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.09520v2",
      "landing_url": "https://arxiv.org/abs/2502.09520v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.09520"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on semantic image compression via SQ-GAN and has no relation to discrete audio tokens or tokenizers, so it fails the inclusion topic entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on semantic image compression via SQ-GAN and has no relation to discrete audio tokens or tokenizers, so it fails the inclusion topic entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
    "abstract": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
    "metadata": {
      "arxiv_id": "2502.11897",
      "title": "DLFR-VAE: Dynamic Latent Frame Rate VAE for Video Generation",
      "summary": "In this paper, we propose the Dynamic Latent Frame Rate VAE (DLFR-VAE), a training-free paradigm that can make use of adaptive temporal compression in latent space. While existing video generative models apply fixed compression rates via pretrained VAE, we observe that real-world video content exhibits substantial temporal non-uniformity, with high-motion segments containing more information than static scenes. Based on this insight, DLFR-VAE dynamically adjusts the latent frame rate according to the content complexity. Specifically, DLFR-VAE comprises two core innovations: (1) A Dynamic Latent Frame Rate Scheduler that partitions videos into temporal chunks and adaptively determines optimal frame rates based on information-theoretic content complexity, and (2) A training-free adaptation mechanism that transforms pretrained VAE architectures into a dynamic VAE that can process features with variable frame rates. Our simple but effective DLFR-VAE can function as a plug-and-play module, seamlessly integrating with existing video generation models and accelerating the video generation process.",
      "authors": [
        "Zhihang Yuan",
        "Siyuan Wang",
        "Rui Xie",
        "Hanling Zhang",
        "Tongcheng Fang",
        "Yuzhang Shang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-02-17T15:22:31Z",
      "updated": "2025-04-02T13:25:35Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.11897v2",
      "landing_url": "https://arxiv.org/abs/2502.11897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.11897"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work is about video latent frame rate modeling with continuous VAE features and doesn’t involve discrete audio token generation/tokenizers, so it fails the inclusion focus and falls under exclusion."
    },
    "round-A_JuniorNano_reasoning": "This work is about video latent frame rate modeling with continuous VAE features and doesn’t involve discrete audio token generation/tokenizers, so it fails the inclusion focus and falls under exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "High-Fidelity Music Vocoder using Neural Audio Codecs",
    "abstract": "While neural vocoders have made significant progress in high-fidelity speech synthesis, their application on polyphonic music has remained underexplored. In this work, we propose DisCoder, a neural vocoder that leverages a generative adversarial encoder-decoder architecture informed by a neural audio codec to reconstruct high-fidelity 44.1 kHz audio from mel spectrograms. Our approach first transforms the mel spectrogram into a lower-dimensional representation aligned with the Descript Audio Codec (DAC) latent space before reconstructing it to an audio signal using a fine-tuned DAC decoder. DisCoder achieves state-of-the-art performance in music synthesis on several objective metrics and in a MUSHRA listening study. Our approach also shows competitive performance in speech synthesis, highlighting its potential as a universal vocoder.",
    "metadata": {
      "arxiv_id": "2502.12759",
      "title": "High-Fidelity Music Vocoder using Neural Audio Codecs",
      "summary": "While neural vocoders have made significant progress in high-fidelity speech synthesis, their application on polyphonic music has remained underexplored. In this work, we propose DisCoder, a neural vocoder that leverages a generative adversarial encoder-decoder architecture informed by a neural audio codec to reconstruct high-fidelity 44.1 kHz audio from mel spectrograms. Our approach first transforms the mel spectrogram into a lower-dimensional representation aligned with the Descript Audio Codec (DAC) latent space before reconstructing it to an audio signal using a fine-tuned DAC decoder. DisCoder achieves state-of-the-art performance in music synthesis on several objective metrics and in a MUSHRA listening study. Our approach also shows competitive performance in speech synthesis, highlighting its potential as a universal vocoder.",
      "authors": [
        "Luca A. Lanzendörfer",
        "Florian Grötschla",
        "Michael Ungersböck",
        "Roger Wattenhofer"
      ],
      "published": "2025-02-18T11:25:46Z",
      "updated": "2025-02-18T11:25:46Z",
      "categories": [
        "cs.SD",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.12759v1",
      "landing_url": "https://arxiv.org/abs/2502.12759v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.12759"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Method focuses on mel-spectrogram-to-DAC latent reconstruction without explicitly defining discrete tokens or a vocabulary, so it appears centered on continuous features and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Method focuses on mel-spectrogram-to-DAC latent reconstruction without explicitly defining discrete tokens or a vocabulary, so it appears centered on continuous features and should be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
    "abstract": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba",
    "metadata": {
      "arxiv_id": "2502.13145",
      "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via Quadratic to Linear Distillation",
      "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable performance but face deployment challenges due to their quadratic computational complexity, growing Key-Value cache requirements, and reliance on separate vision encoders. We propose mmMamba, a framework for developing linear-complexity native multimodal state space models through progressive distillation from existing MLLMs using moderate academic computational resources. Our approach enables the direct conversion of trained decoder-only MLLMs to linear-complexity architectures without requiring pre-trained RNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba from trained Transformer and a three-stage distillation recipe, which can effectively transfer the knowledge from Transformer to Mamba while preserving multimodal capabilities. Our method also supports flexible hybrid architectures that combine Transformer and Mamba layers for customizable efficiency-performance trade-offs. Distilled from the Transformer-based decoder-only HoVLE, mmMamba-linear achieves competitive performance against existing linear and quadratic-complexity VLMs, while mmMamba-hybrid further improves performance significantly, approaching HoVLE's capabilities. At 103K tokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory reduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup and 60.2% memory savings. Code and models are released at https://github.com/hustvl/mmMamba",
      "authors": [
        "Bencheng Liao",
        "Hongyuan Tao",
        "Qian Zhang",
        "Tianheng Cheng",
        "Yingyue Li",
        "Haoran Yin",
        "Wenyu Liu",
        "Xinggang Wang"
      ],
      "published": "2025-02-18T18:59:57Z",
      "updated": "2025-03-18T07:02:33Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.13145v2",
      "landing_url": "https://arxiv.org/abs/2502.13145v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.13145"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on multimodal large language model distillation and efficiency, not on generating or evaluating discrete audio tokens or quantized vocabularies, so it fails the audio token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on multimodal large language model distillation and efficiency, not on generating or evaluating discrete audio tokens or quantized vocabularies, so it fails the audio token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MATS: An Audio Language Model under Text-only Supervision",
    "abstract": "Large audio-language models (LALMs), built upon powerful Large Language Models (LLMs), have exhibited remarkable audio comprehension and reasoning capabilities. However, the training of LALMs demands a large corpus of audio-language pairs, which requires substantial costs in both data collection and training resources. In this paper, we propose \\textbf{MATS}, an audio-language multimodal LLM designed to handle \\textbf{M}ultiple \\textbf{A}udio task using solely \\textbf{T}ext-only \\textbf{S}upervision. By leveraging pre-trained audio-language alignment models such as CLAP, we develop a text-only training strategy that projects the shared audio-language latent space into LLM latent space, endowing the LLM with audio comprehension capabilities without relying on audio data during training. To further bridge the modality gap between audio and language embeddings within CLAP, we propose the \\textbf{S}trongly-rel\\textbf{a}ted \\textbf{n}oisy \\textbf{t}ext with \\textbf{a}udio (\\textbf{Santa}) mechanism. Santa maps audio embeddings into CLAP language embedding space while preserving essential information from the audio input. Extensive experiments demonstrate that MATS, despite being trained exclusively on text data, achieves competitive performance compared to recent LALMs trained on large-scale audio-language pairs. The code is publicly available in \\href{https://github.com/wangwen-banban/MATS}{https://github.com/wangwen-banban/MATS}.",
    "metadata": {
      "arxiv_id": "2502.13433",
      "title": "MATS: An Audio Language Model under Text-only Supervision",
      "summary": "Large audio-language models (LALMs), built upon powerful Large Language Models (LLMs), have exhibited remarkable audio comprehension and reasoning capabilities. However, the training of LALMs demands a large corpus of audio-language pairs, which requires substantial costs in both data collection and training resources. In this paper, we propose \\textbf{MATS}, an audio-language multimodal LLM designed to handle \\textbf{M}ultiple \\textbf{A}udio task using solely \\textbf{T}ext-only \\textbf{S}upervision. By leveraging pre-trained audio-language alignment models such as CLAP, we develop a text-only training strategy that projects the shared audio-language latent space into LLM latent space, endowing the LLM with audio comprehension capabilities without relying on audio data during training. To further bridge the modality gap between audio and language embeddings within CLAP, we propose the \\textbf{S}trongly-rel\\textbf{a}ted \\textbf{n}oisy \\textbf{t}ext with \\textbf{a}udio (\\textbf{Santa}) mechanism. Santa maps audio embeddings into CLAP language embedding space while preserving essential information from the audio input. Extensive experiments demonstrate that MATS, despite being trained exclusively on text data, achieves competitive performance compared to recent LALMs trained on large-scale audio-language pairs. The code is publicly available in \\href{https://github.com/wangwen-banban/MATS}{https://github.com/wangwen-banban/MATS}.",
      "authors": [
        "Wen Wang",
        "Ruibing Hou",
        "Hong Chang",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "published": "2025-02-19T05:07:56Z",
      "updated": "2026-01-14T07:42:41Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.13433v3",
      "landing_url": "https://arxiv.org/abs/2502.13433v3",
      "doi": "https://doi.org/10.48550/arXiv.2502.13433"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "The paper focuses on training an audio-language model via text-only supervision without introducing or evaluating any discrete audio token/tokenizer components or quantization design, so it fails to meet the discrete token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on training an audio-language model via text-only supervision without introducing or evaluating any discrete audio token/tokenizer components or quantization design, so it fails to meet the discrete token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "NeSt-VR: An Adaptive Bitrate Algorithm for Virtual Reality Streaming over Wi-Fi",
    "abstract": "Real-time interactive Virtual Reality (VR) streaming is a significantly challenging use case for Wi-Fi given its high throughput and low latency requirements, especially considering the constraints imposed by the possible presence of other users and the variability of the available bandwidth. Adaptive BitRate (ABR) algorithms dynamically adjust the encoded bitrate in response to varying network conditions to maintain smooth video playback. In this paper, we present the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR), a configurable algorithm implemented in Air Light VR (ALVR), an open-source VR streaming solution. NeSt-VR effectively adjusts video bitrate based on real-time network metrics, such as frame delivery rate, network latency, and estimated available bandwidth, to guarantee user satisfaction. These metrics are part of a comprehensive set we integrated into ALVR to characterize network performance and support the decision-making process of any ABR algorithm, validated through extensive emulated experiments. NeSt-VR is evaluated in both single- and multi-user scenarios, including tests with network capacity fluctuations, user mobility, and co-channel interference. Our results demonstrate that NeSt-VR successfully manages Wi-Fi capacity fluctuations and enhances interactive VR streaming performance in both controlled experiments at UPF's lab and professional tests at CREW's facilities.",
    "metadata": {
      "arxiv_id": "2502.14947",
      "title": "NeSt-VR: An Adaptive Bitrate Algorithm for Virtual Reality Streaming over Wi-Fi",
      "summary": "Real-time interactive Virtual Reality (VR) streaming is a significantly challenging use case for Wi-Fi given its high throughput and low latency requirements, especially considering the constraints imposed by the possible presence of other users and the variability of the available bandwidth. Adaptive BitRate (ABR) algorithms dynamically adjust the encoded bitrate in response to varying network conditions to maintain smooth video playback. In this paper, we present the Network-aware Step-wise ABR algorithm for VR streaming (NeSt-VR), a configurable algorithm implemented in Air Light VR (ALVR), an open-source VR streaming solution. NeSt-VR effectively adjusts video bitrate based on real-time network metrics, such as frame delivery rate, network latency, and estimated available bandwidth, to guarantee user satisfaction. These metrics are part of a comprehensive set we integrated into ALVR to characterize network performance and support the decision-making process of any ABR algorithm, validated through extensive emulated experiments. NeSt-VR is evaluated in both single- and multi-user scenarios, including tests with network capacity fluctuations, user mobility, and co-channel interference. Our results demonstrate that NeSt-VR successfully manages Wi-Fi capacity fluctuations and enhances interactive VR streaming performance in both controlled experiments at UPF's lab and professional tests at CREW's facilities.",
      "authors": [
        "Miguel Casasnovas",
        "Ferran Maura",
        "Isjtar Vandebroeck",
        "Haryo Sukmawanto",
        "Eric Joris",
        "Boris Bellalta"
      ],
      "published": "2025-02-20T18:12:02Z",
      "updated": "2025-02-20T18:12:02Z",
      "categories": [
        "cs.NI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.14947v1",
      "landing_url": "https://arxiv.org/abs/2502.14947v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.14947"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on VR streaming adaptation algorithms over Wi-Fi and does not study discrete audio token generation/quantization or related evaluations, so it clearly fails the inclusion criteria and matches exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on VR streaming adaptation algorithms over Wi-Fi and does not study discrete audio token generation/quantization or related evaluations, so it clearly fails the inclusion criteria and matches exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs",
    "abstract": "Scaling large language models (LLMs) has shown great potential for improving retrieval model performance; however, previous studies have mainly focused on dense retrieval trained with contrastive loss (CL), neglecting the scaling behavior of other retrieval paradigms and optimization techniques, such as sparse retrieval and knowledge distillation (KD). In this work, we conduct a systematic comparative study on how different retrieval paradigms (sparse vs. dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect retrieval performance across different model scales. Using MSMARCO passages as the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a fixed compute budget, we evaluate various training configurations on both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where larger models achieve significant performance gains, whereas KD-trained models show minimal improvement, performing similarly across the 1B, 3B, and 8B scales. (2) Sparse retrieval models consistently outperform dense retrieval across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks, and they demonstrate greater robustness to imperfect supervised signals. (3) We successfully scale sparse retrieval models with the combination of CL and KD losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation sets.",
    "metadata": {
      "arxiv_id": "2502.15526",
      "title": "Scaling Sparse and Dense Retrieval in Decoder-Only LLMs",
      "summary": "Scaling large language models (LLMs) has shown great potential for improving retrieval model performance; however, previous studies have mainly focused on dense retrieval trained with contrastive loss (CL), neglecting the scaling behavior of other retrieval paradigms and optimization techniques, such as sparse retrieval and knowledge distillation (KD). In this work, we conduct a systematic comparative study on how different retrieval paradigms (sparse vs. dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect retrieval performance across different model scales. Using MSMARCO passages as the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a fixed compute budget, we evaluate various training configurations on both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where larger models achieve significant performance gains, whereas KD-trained models show minimal improvement, performing similarly across the 1B, 3B, and 8B scales. (2) Sparse retrieval models consistently outperform dense retrieval across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks, and they demonstrate greater robustness to imperfect supervised signals. (3) We successfully scale sparse retrieval models with the combination of CL and KD losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation sets.",
      "authors": [
        "Hansi Zeng",
        "Julian Killingback",
        "Hamed Zamani"
      ],
      "published": "2025-02-21T15:28:26Z",
      "updated": "2025-02-21T15:28:26Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.15526v1",
      "landing_url": "https://arxiv.org/abs/2502.15526v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.15526"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on scaling sparse/dense retrieval for decoder-only LLMs with no mention of discrete audio tokenization, codecs, or audio-specific quantization, so it fails the inclusion topic requirements."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on scaling sparse/dense retrieval for decoder-only LLMs with no mention of discrete audio tokenization, codecs, or audio-specific quantization, so it fails the inclusion topic requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
    "abstract": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission. Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
    "metadata": {
      "arxiv_id": "2502.16240",
      "title": "Speech Enhancement Using Continuous Embeddings of Neural Audio Codec",
      "summary": "Recent advancements in Neural Audio Codec (NAC) models have inspired their use in various speech processing tasks, including speech enhancement (SE). In this work, we propose a novel, efficient SE approach by leveraging the pre-quantization output of a pretrained NAC encoder. Unlike prior NAC-based SE methods, which process discrete speech tokens using Language Models (LMs), we perform SE within the continuous embedding space of the pretrained NAC, which is highly compressed along the time dimension for efficient representation. Our lightweight SE model, optimized through an embedding-level loss, delivers results comparable to SE baselines trained on larger datasets, with a significantly lower real-time factor of 0.005. Additionally, our method achieves a low GMAC of 3.94, reducing complexity 18-fold compared to Sepformer in a simulated cloud-based audio transmission environment. This work highlights a new, efficient NAC-based SE solution, particularly suitable for cloud applications where NAC is used to compress audio before transmission.\n  Copyright 20XX IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.",
      "authors": [
        "Haoyang Li",
        "Jia Qi Yip",
        "Tianyu Fan",
        "Eng Siong Chng"
      ],
      "published": "2025-02-22T14:25:55Z",
      "updated": "2025-02-22T14:25:55Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16240v1",
      "landing_url": "https://arxiv.org/abs/2502.16240v1",
      "doi": "https://doi.org/10.1109/ICASSP49660.2025.10890379"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The method works entirely in continuous NAC embeddings and never treats quantized discrete tokens as a primary object, so it fails to meet the discrete-token inclusion requirements and should be excluded (score 1)."
    },
    "round-A_JuniorNano_reasoning": "The method works entirely in continuous NAC embeddings and never treats quantized discrete tokens as a primary object, so it fails to meet the discrete-token inclusion requirements and should be excluded (score 1).",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
    "abstract": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
    "metadata": {
      "arxiv_id": "2502.16474",
      "title": "Unified Semantic and ID Representation Learning for Deep Recommenders",
      "summary": "Effective recommendation is crucial for large-scale online platforms. Traditional recommendation systems primarily rely on ID tokens to uniquely identify items, which can effectively capture specific item relationships but suffer from issues such as redundancy and poor performance in cold-start scenarios. Recent approaches have explored using semantic tokens as an alternative, yet they face challenges, including item duplication and inconsistent performance gains, leaving the potential advantages of semantic tokens inadequately examined. To address these limitations, we propose a Unified Semantic and ID Representation Learning framework that leverages the complementary strengths of both token types. In our framework, ID tokens capture unique item attributes, while semantic tokens represent shared, transferable characteristics. Additionally, we analyze the role of cosine similarity and Euclidean distance in embedding search, revealing that cosine similarity is more effective in decoupling accumulated embeddings, while Euclidean distance excels in distinguishing unique items. Our framework integrates cosine similarity in earlier layers and Euclidean distance in the final layer to optimize representation learning. Experiments on three benchmark datasets show that our method significantly outperforms state-of-the-art baselines, with improvements ranging from 6\\% to 17\\% and a reduction in token size by over 80%. These results demonstrate the effectiveness of combining ID and semantic tokenization to enhance the generalization ability of recommender systems.",
      "authors": [
        "Guanyu Lin",
        "Zhigang Hua",
        "Tao Feng",
        "Shuang Yang",
        "Bo Long",
        "Jiaxuan You"
      ],
      "published": "2025-02-23T07:17:28Z",
      "updated": "2025-02-23T07:17:28Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16474v1",
      "landing_url": "https://arxiv.org/abs/2502.16474v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.16474"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on text-based recommender tokens (IDs/semantics) for recommendation tasks, not discrete audio token generation/quantization, so it fails all inclusion criteria and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on text-based recommender tokens (IDs/semantics) for recommendation tasks, not discrete audio token generation/quantization, so it fails all inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
    "abstract": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
    "metadata": {
      "arxiv_id": "2502.16897",
      "title": "Balancing Speech Understanding and Generation Using Continual Pre-training for Codec-based Speech LLM",
      "summary": "Recent advances in speech language models (LLMs) have extended textual LLMs to the speech domain, but balancing speech understanding and generation remains challenging, especially with codec-based representations. We propose a continual pre-training (CPT) framework that adapts a textual LLM to handle codec-discretized speech, mitigating modality mismatch and preserving linguistic reasoning. Our unified model supports both understanding and generation, achieving strong results across ASR, TTS, S2T-Trans, and S2S-Trans. Notably, we present the first end-to-end, single-pass S2S-Trans system using only neural codec tokens, without intermediate transcriptions, translations, or semantic tokens. CPT proves essential for cross-modal alignment and task generalization, making it a powerful tool for building robust, unified speech LLMs.",
      "authors": [
        "Jiatong Shi",
        "Chunlei Zhang",
        "Jinchuan Tian",
        "Junrui Ni",
        "Hao Zhang",
        "Shinji Watanabe",
        "Dong Yu"
      ],
      "published": "2025-02-24T06:50:40Z",
      "updated": "2025-11-27T18:46:39Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.16897v2",
      "landing_url": "https://arxiv.org/abs/2502.16897v2",
      "doi": "https://doi.org/10.48550/arXiv.2502.16897"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The abstract centers on adapting a textual LLM to codec-discretized speech and emphasizes using neural codec tokens across ASR, TTS, and single-pass S2S-Trans tasks, so it explicitly treats discrete audio tokens and their evaluation, meeting all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The abstract centers on adapting a textual LLM to codec-discretized speech and emphasizes using neural codec tokens across ASR, TTS, and single-pass S2S-Trans tasks, so it explicitly treats discrete audio tokens and their evaluation, meeting all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook",
    "abstract": "The emergence of audio language models is empowered by neural audio codecs, which establish critical mappings between continuous waveforms and discrete tokens compatible with language model paradigms. The evolutionary trends from multi-layer residual vector quantizer to single-layer quantizer are beneficial for language-autoregressive decoding. However, the capability to handle multi-domain audio signals through a single codebook remains constrained by inter-domain distribution discrepancies. In this work, we introduce UniCodec, a unified audio codec with a single codebook to support multi-domain audio data, including speech, music, and sound. To achieve this, we propose a partitioned domain-adaptive codebook method and domain Mixture-of-Experts strategy to capture the distinct characteristics of each audio domain. Furthermore, to enrich the semantic density of the codec without auxiliary modules, we propose a self-supervised mask prediction modeling approach. Comprehensive objective and subjective evaluations demonstrate that UniCodec achieves excellent audio reconstruction performance across the three audio domains, outperforming existing unified neural codecs with a single codebook, and even surpasses state-of-the-art domain-specific codecs on both acoustic and semantic representation capabilities.",
    "metadata": {
      "arxiv_id": "2502.20067",
      "title": "UniCodec: Unified Audio Codec with Single Domain-Adaptive Codebook",
      "summary": "The emergence of audio language models is empowered by neural audio codecs, which establish critical mappings between continuous waveforms and discrete tokens compatible with language model paradigms. The evolutionary trends from multi-layer residual vector quantizer to single-layer quantizer are beneficial for language-autoregressive decoding. However, the capability to handle multi-domain audio signals through a single codebook remains constrained by inter-domain distribution discrepancies. In this work, we introduce UniCodec, a unified audio codec with a single codebook to support multi-domain audio data, including speech, music, and sound. To achieve this, we propose a partitioned domain-adaptive codebook method and domain Mixture-of-Experts strategy to capture the distinct characteristics of each audio domain. Furthermore, to enrich the semantic density of the codec without auxiliary modules, we propose a self-supervised mask prediction modeling approach. Comprehensive objective and subjective evaluations demonstrate that UniCodec achieves excellent audio reconstruction performance across the three audio domains, outperforming existing unified neural codecs with a single codebook, and even surpasses state-of-the-art domain-specific codecs on both acoustic and semantic representation capabilities.",
      "authors": [
        "Yidi Jiang",
        "Qian Chen",
        "Shengpeng Ji",
        "Yu Xi",
        "Wen Wang",
        "Chong Zhang",
        "Xianghu Yue",
        "ShiLiang Zhang",
        "Haizhou Li"
      ],
      "published": "2025-02-27T13:16:41Z",
      "updated": "2025-02-27T13:16:41Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2502.20067v1",
      "landing_url": "https://arxiv.org/abs/2502.20067v1",
      "doi": "https://doi.org/10.48550/arXiv.2502.20067"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "UniCodec proposes a single codebook neural audio codec that explicitly quantizes multi-domain audio into discrete tokens with evaluation on reconstruction and semantics, matching the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "UniCodec proposes a single codebook neural audio codec that explicitly quantizes multi-domain audio into discrete tokens with evaluation on reconstruction and semantics, matching the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hidden Convexity of Fair PCA and Fast Solver via Eigenvalue Optimization",
    "abstract": "Principal Component Analysis (PCA) is a foundational technique in machine learning for dimensionality reduction of high-dimensional datasets. However, PCA could lead to biased outcomes that disadvantage certain subgroups of the underlying datasets. To address the bias issue, a Fair PCA (FPCA) model was introduced by Samadi et al. (2018) for equalizing the reconstruction loss between subgroups. The semidefinite relaxation (SDR) based approach proposed by Samadi et al. (2018) is computationally expensive even for suboptimal solutions. To improve efficiency, several alternative variants of the FPCA model have been developed. These variants often shift the focus away from equalizing the reconstruction loss. In this paper, we identify a hidden convexity in the FPCA model and introduce an algorithm for convex optimization via eigenvalue optimization. Our approach achieves the desired fairness in reconstruction loss without sacrificing performance. As demonstrated in real-world datasets, the proposed FPCA algorithm runs $8\\times$ faster than the SDR-based algorithm, and only at most 85% slower than the standard PCA.",
    "metadata": {
      "arxiv_id": "2503.00299",
      "title": "Hidden Convexity of Fair PCA and Fast Solver via Eigenvalue Optimization",
      "summary": "Principal Component Analysis (PCA) is a foundational technique in machine learning for dimensionality reduction of high-dimensional datasets. However, PCA could lead to biased outcomes that disadvantage certain subgroups of the underlying datasets. To address the bias issue, a Fair PCA (FPCA) model was introduced by Samadi et al. (2018) for equalizing the reconstruction loss between subgroups. The semidefinite relaxation (SDR) based approach proposed by Samadi et al. (2018) is computationally expensive even for suboptimal solutions. To improve efficiency, several alternative variants of the FPCA model have been developed. These variants often shift the focus away from equalizing the reconstruction loss. In this paper, we identify a hidden convexity in the FPCA model and introduce an algorithm for convex optimization via eigenvalue optimization. Our approach achieves the desired fairness in reconstruction loss without sacrificing performance. As demonstrated in real-world datasets, the proposed FPCA algorithm runs $8\\times$ faster than the SDR-based algorithm, and only at most 85% slower than the standard PCA.",
      "authors": [
        "Junhui Shen",
        "Aaron J. Davis",
        "Ding Lu",
        "Zhaojun Bai"
      ],
      "published": "2025-03-01T02:13:20Z",
      "updated": "2025-03-01T02:13:20Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "math.OC",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00299v1",
      "landing_url": "https://arxiv.org/abs/2503.00299v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.00299"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper about fair PCA has nothing to do with discrete audio tokens, so it fails the inclusion focus on tokenization/quantization for audio."
    },
    "round-A_JuniorNano_reasoning": "Paper about fair PCA has nothing to do with discrete audio tokens, so it fails the inclusion focus on tokenization/quantization for audio.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Extremely low-bitrate Image Compression Semantically Disentangled by LMMs from a Human Perception Perspective",
    "abstract": "It remains a significant challenge to compress images at extremely low bitrate while achieving both semantic consistency and high perceptual quality. Inspired by human progressive perception mechanism, we propose a Semantically Disentangled Image Compression framework (SEDIC) in this paper. Initially, an extremely compressed reference image is obtained through a learned image encoder. Then we leverage LMMs to extract essential semantic components, including overall descriptions, object detailed description, and semantic segmentation masks. We propose a training-free Object Restoration model with Attention Guidance (ORAG) built on pre-trained ControlNet to restore object details conditioned by object-level text descriptions and semantic masks. Based on the proposed ORAG, we design a multistage semantic image decoder to progressively restore the details object by object, starting from the extremely compressed reference image, ultimately generating high-quality and high-fidelity reconstructions. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at extremely low-bitrates ($\\le$ 0.05 bpp).",
    "metadata": {
      "arxiv_id": "2503.00399",
      "title": "Extremely low-bitrate Image Compression Semantically Disentangled by LMMs from a Human Perception Perspective",
      "summary": "It remains a significant challenge to compress images at extremely low bitrate while achieving both semantic consistency and high perceptual quality. Inspired by human progressive perception mechanism, we propose a Semantically Disentangled Image Compression framework (SEDIC) in this paper. Initially, an extremely compressed reference image is obtained through a learned image encoder. Then we leverage LMMs to extract essential semantic components, including overall descriptions, object detailed description, and semantic segmentation masks. We propose a training-free Object Restoration model with Attention Guidance (ORAG) built on pre-trained ControlNet to restore object details conditioned by object-level text descriptions and semantic masks. Based on the proposed ORAG, we design a multistage semantic image decoder to progressively restore the details object by object, starting from the extremely compressed reference image, ultimately generating high-quality and high-fidelity reconstructions. Experimental results demonstrate that SEDIC significantly outperforms state-of-the-art approaches, achieving superior perceptual quality and semantic consistency at extremely low-bitrates ($\\le$ 0.05 bpp).",
      "authors": [
        "Juan Song",
        "Lijie Yang",
        "Mingtao Feng"
      ],
      "published": "2025-03-01T08:27:11Z",
      "updated": "2025-10-14T07:36:33Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.00399v4",
      "landing_url": "https://arxiv.org/abs/2503.00399v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.00399"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on extremely low-bitrate image compression and semantic restoration via LMMs rather than any discrete audio token codec/tokenization topic, so it fails the inclusion criteria and meets the exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on extremely low-bitrate image compression and semantic restoration via LMMs rather than any discrete audio token codec/tokenization topic, so it fails the inclusion criteria and meets the exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
    "abstract": "Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse multi-task audio dataset with simple annotations. Then, we leverage closed-source models to conduct secondary labeling, QA generation, along with structured COT process. These datasets together form a high-quality reasoning dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to achieve great logical capabilities in audio reasoning. Experiments show state-of-the-art performance across key benchmarks, including MMAU-mini (+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our findings stress the core of structured CoT training in advancing audio reasoning.",
    "metadata": {
      "arxiv_id": "2503.02318",
      "title": "Audio-Reasoner: Improving Reasoning Capability in Large Audio Language Models",
      "summary": "Recent advancements in multimodal reasoning have largely overlooked the audio modality. We introduce Audio-Reasoner, a large-scale audio language model for deep reasoning in audio tasks. We meticulously curated a large-scale and diverse multi-task audio dataset with simple annotations. Then, we leverage closed-source models to conduct secondary labeling, QA generation, along with structured COT process. These datasets together form a high-quality reasoning dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to achieve great logical capabilities in audio reasoning. Experiments show state-of-the-art performance across key benchmarks, including MMAU-mini (+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our findings stress the core of structured CoT training in advancing audio reasoning.",
      "authors": [
        "Zhifei Xie",
        "Mingbao Lin",
        "Zihang Liu",
        "Pengcheng Wu",
        "Shuicheng Yan",
        "Chunyan Miao"
      ],
      "published": "2025-03-04T06:18:34Z",
      "updated": "2025-09-20T06:37:34Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.02318v2",
      "landing_url": "https://arxiv.org/abs/2503.02318v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.02318"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on training large audio LM for reasoning with QA datasets rather than any quantization/tokenization of audio signals, so it fails to meet the discrete audio token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on training large audio LM for reasoning with QA datasets rather than any quantization/tokenization of audio signals, so it fails to meet the discrete audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
    "abstract": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
    "metadata": {
      "arxiv_id": "2503.03474",
      "title": "Enhancing Spoken Discourse Modeling in Language Models Using Gestural Cues",
      "summary": "Research in linguistics shows that non-verbal cues, such as gestures, play a crucial role in spoken discourse. For example, speakers perform hand gestures to indicate topic shifts, helping listeners identify transitions in discourse. In this work, we investigate whether the joint modeling of gestures using human motion sequences and language can improve spoken discourse modeling in language models. To integrate gestures into language models, we first encode 3D human motion sequences into discrete gesture tokens using a VQ-VAE. These gesture token embeddings are then aligned with text embeddings through feature alignment, mapping them into the text embedding space. To evaluate the gesture-aligned language model on spoken discourse, we construct text infilling tasks targeting three key discourse cues grounded in linguistic research: discourse connectives, stance markers, and quantifiers. Results show that incorporating gestures enhances marker prediction accuracy across the three tasks, highlighting the complementary information that gestures can offer in modeling spoken discourse. We view this work as an initial step toward leveraging non-verbal cues to advance spoken language modeling in language models.",
      "authors": [
        "Varsha Suresh",
        "M. Hamza Mughal",
        "Christian Theobalt",
        "Vera Demberg"
      ],
      "published": "2025-03-05T13:10:07Z",
      "updated": "2025-03-05T13:10:07Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.03474v1",
      "landing_url": "https://arxiv.org/abs/2503.03474v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.03474"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discrete audio token production or quantization appears in a gesture-aligned language modeling study, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "No discrete audio token production or quantization appears in a gesture-aligned language modeling study, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
    "abstract": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
    "metadata": {
      "arxiv_id": "2503.04606",
      "title": "The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation",
      "summary": "Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.",
      "authors": [
        "Aoxiong Yin",
        "Kai Shen",
        "Yichong Leng",
        "Xu Tan",
        "Xinyu Zhou",
        "Juncheng Li",
        "Siliang Tang"
      ],
      "published": "2025-03-06T16:53:14Z",
      "updated": "2025-04-29T10:34:28Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04606v3",
      "landing_url": "https://arxiv.org/abs/2503.04606v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.04606"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on text-to-video generation with language and diffusion models, without addressing discrete audio tokenization or codec/quantization methods for audio representations, so it fails the inclusion topic and meets the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on text-to-video generation with language and diffusion models, without addressing discrete audio tokenization or codec/quantization methods for audio representations, so it fails the inclusion topic and meets the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
    "abstract": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
    "metadata": {
      "arxiv_id": "2503.04713",
      "title": "Scaling Rich Style-Prompted Text-to-Speech Datasets",
      "summary": "We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale dataset that annotates speech utterances with rich style captions. While rich abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale human-annotated datasets, existing large-scale datasets only cover basic tags (e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech embedders, classifiers and an audio language model to automatically scale rich tag annotations for the first time. ParaSpeechCaps covers a total of 59 style tags, including both speaker-level intrinsic tags and utterance-level situational tags. It consists of 342 hours of human-labelled data (PSC-Base) and 2427 hours of automatically annotated data (PSC-Scaled). We finetune Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and achieve improved style consistency (+7.9% Consistency MOS) and speech quality (+15.5% Naturalness MOS) over the best performing baseline that combines existing rich style tag datasets. We ablate several of our dataset design choices to lay the foundation for future work in this space. Our dataset, models and code are released at https://github.com/ajd12342/paraspeechcaps .",
      "authors": [
        "Anuj Diwan",
        "Zhisheng Zheng",
        "David Harwath",
        "Eunsol Choi"
      ],
      "published": "2025-03-06T18:57:40Z",
      "updated": "2025-09-24T19:42:38Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.04713v2",
      "landing_url": "https://arxiv.org/abs/2503.04713v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.04713"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on dataset/finetuning for style-prompted TTS without addressing discrete audio tokens, tokenizers, vocabularies, or quantization schemes, so it fails the inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on dataset/finetuning for style-prompted TTS without addressing discrete audio tokens, tokenizers, vocabularies, or quantization schemes, so it fails the inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Task Vector Quantization for Memory-Efficient Model Merging",
    "abstract": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
    "metadata": {
      "arxiv_id": "2503.06921",
      "title": "Task Vector Quantization for Memory-Efficient Model Merging",
      "summary": "Model merging enables efficient multi-task models by combining task-specific fine-tuned checkpoints. However, storing multiple task-specific checkpoints requires significant memory, limiting scalability and restricting model merging to larger models and diverse tasks. In this paper, we propose quantizing task vectors (i.e., the difference between pre-trained and fine-tuned checkpoints) instead of quantizing fine-tuned checkpoints. We observe that task vectors exhibit a narrow weight range, enabling low precision quantization (e.g., 4 bit) within existing task vector merging frameworks. To further mitigate quantization errors within ultra-low bit precision (e.g., 2 bit), we introduce Residual Task Vector Quantization, which decomposes the task vector into a base vector and offset component. We allocate bits based on quantization sensitivity, ensuring precision while minimizing error within a memory budget. Experiments on image classification and dense prediction show our method maintains or improves model merging performance while using only 8% of the memory required for full-precision checkpoints.",
      "authors": [
        "Youngeun Kim",
        "Seunghwan Lee",
        "Aecheon Jung",
        "Bogon Ryu",
        "Sungeun Hong"
      ],
      "published": "2025-03-10T05:00:24Z",
      "updated": "2025-08-07T10:57:05Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.06921v2",
      "landing_url": "https://arxiv.org/abs/2503.06921v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.06921"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on quantizing task vectors for merging vision models and lacks any treatment of audio-discrete-token generation/quantization or codec/semantic token modeling, so it fails the audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on quantizing task vectors for merging vision models and lacks any treatment of audio-discrete-token generation/quantization or codec/semantic token modeling, so it fails the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Mellow: a small audio language model for reasoning",
    "abstract": "Multimodal Audio-Language Models (ALMs) can understand and reason over both audio and text. Typically, reasoning performance correlates with model size, with the best results achieved by models exceeding 8 billion parameters. However, no prior work has explored enabling small audio-language models to perform reasoning tasks, despite the potential applications for edge devices. To address this gap, we introduce Mellow, a small Audio-Language Model specifically designed for reasoning. Mellow achieves state-of-the-art performance among existing small audio-language models and surpasses several larger models in reasoning capabilities. For instance, Mellow scores 52.11 on MMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times fewer parameters and being trained on 60 times less data (audio hrs). To train Mellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded reasoning in models. It consists of a mixture of existing datasets (30% of the data) and synthetically generated data (70%). The synthetic dataset is derived from audio captioning datasets, where Large Language Models (LLMs) generate detailed and multiple-choice questions focusing on audio events, objects, acoustic scenes, signal properties, semantics, and listener emotions. To evaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks, assessing on both in-distribution and out-of-distribution data, including audio understanding, deductive reasoning, and comparative reasoning. Finally, we conduct extensive ablation studies to explore the impact of projection layer choices, synthetic data generation methods, and language model pretraining on reasoning performance. Our training dataset, findings, and baseline pave the way for developing small ALMs capable of reasoning.",
    "metadata": {
      "arxiv_id": "2503.08540",
      "title": "Mellow: a small audio language model for reasoning",
      "summary": "Multimodal Audio-Language Models (ALMs) can understand and reason over both audio and text. Typically, reasoning performance correlates with model size, with the best results achieved by models exceeding 8 billion parameters. However, no prior work has explored enabling small audio-language models to perform reasoning tasks, despite the potential applications for edge devices. To address this gap, we introduce Mellow, a small Audio-Language Model specifically designed for reasoning. Mellow achieves state-of-the-art performance among existing small audio-language models and surpasses several larger models in reasoning capabilities. For instance, Mellow scores 52.11 on MMAU, comparable to SoTA Qwen2 Audio (which scores 52.5) while using 50 times fewer parameters and being trained on 60 times less data (audio hrs). To train Mellow, we introduce ReasonAQA, a dataset designed to enhance audio-grounded reasoning in models. It consists of a mixture of existing datasets (30% of the data) and synthetically generated data (70%). The synthetic dataset is derived from audio captioning datasets, where Large Language Models (LLMs) generate detailed and multiple-choice questions focusing on audio events, objects, acoustic scenes, signal properties, semantics, and listener emotions. To evaluate Mellow's reasoning ability, we benchmark it on a diverse set of tasks, assessing on both in-distribution and out-of-distribution data, including audio understanding, deductive reasoning, and comparative reasoning. Finally, we conduct extensive ablation studies to explore the impact of projection layer choices, synthetic data generation methods, and language model pretraining on reasoning performance. Our training dataset, findings, and baseline pave the way for developing small ALMs capable of reasoning.",
      "authors": [
        "Soham Deshmukh",
        "Satvik Dixit",
        "Rita Singh",
        "Bhiksha Raj"
      ],
      "published": "2025-03-11T15:29:00Z",
      "updated": "2025-03-11T15:29:00Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.08540v1",
      "landing_url": "https://arxiv.org/abs/2503.08540v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.08540"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Mellow focuses on reasoning capabilities of a small ALM using existing datasets and synthetic QA pairs without proposing, analyzing, or evaluating any discrete-token/tokenizer/quantization design, so it does not fit the discrete audio token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Mellow focuses on reasoning capabilities of a small ALM using existing datasets and synthetic QA pairs without proposing, analyzing, or evaluating any discrete-token/tokenizer/quantization design, so it does not fit the discrete audio token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
    "abstract": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
    "metadata": {
      "arxiv_id": "2503.09509",
      "title": "ViM-VQ: Efficient Post-Training Vector Quantization for Visual Mamba",
      "summary": "Visual Mamba networks (ViMs) extend the selective state space model (Mamba) to various vision tasks and demonstrate significant potential. As a promising compression technique, vector quantization (VQ) decomposes network weights into codebooks and assignments, significantly reducing memory usage and computational latency, thereby enabling the deployment of ViMs on edge devices. Although existing VQ methods have achieved extremely low-bit quantization (e.g., 3-bit, 2-bit, and 1-bit) in convolutional neural networks and Transformer-based networks, directly applying these methods to ViMs results in unsatisfactory accuracy. We identify several key challenges: 1) The weights of Mamba-based blocks in ViMs contain numerous outliers, significantly amplifying quantization errors. 2) When applied to ViMs, the latest VQ methods suffer from excessive memory consumption, lengthy calibration procedures, and suboptimal performance in the search for optimal codewords. In this paper, we propose ViM-VQ, an efficient post-training vector quantization method tailored for ViMs. ViM-VQ consists of two innovative components: 1) a fast convex combination optimization algorithm that efficiently updates both the convex combinations and the convex hulls to search for optimal codewords, and 2) an incremental vector quantization strategy that incrementally confirms optimal codewords to mitigate truncation errors. Experimental results demonstrate that ViM-VQ achieves state-of-the-art performance in low-bit quantization across various visual tasks.",
      "authors": [
        "Juncan Deng",
        "Shuaiting Li",
        "Zeyu Wang",
        "Kedong Xu",
        "Hong Gu",
        "Kejie Huang"
      ],
      "published": "2025-03-12T16:18:45Z",
      "updated": "2025-07-30T16:58:48Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09509v2",
      "landing_url": "https://arxiv.org/abs/2503.09509v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.09509"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This vision-focused paper on Vector Quantization for ViMs is unrelated to discrete audio tokens and thus fails the inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "This vision-focused paper on Vector Quantization for ViMs is unrelated to discrete audio tokens and thus fails the inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
    "abstract": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
    "metadata": {
      "arxiv_id": "2503.09787",
      "title": "Bidirectional Learned Facial Animation Codec for Low Bitrate Talking Head Videos",
      "summary": "Existing deep facial animation coding techniques efficiently compress talking head videos by applying deep generative models. Instead of compressing the entire video sequence, these methods focus on compressing only the keyframe and the keypoints of non-keyframes (target frames). The target frames are then reconstructed by utilizing a single keyframe, and the keypoints of the target frame. Although these unidirectional methods can reduce the bitrate, they rely on a single keyframe and often struggle to capture large head movements accurately, resulting in distortions in the facial region. In this paper, we propose a novel bidirectional learned animation codec that generates natural facial videos using past and future keyframes. First, in the Bidirectional Reference-Guided Auxiliary Stream Enhancement (BRG-ASE) process, we introduce a compact auxiliary stream for non-keyframes, which is enhanced by adaptively selecting one of two keyframes (past and future). This stream improves video quality with a slight increase in bitrate. Then, in the Bidirectional Reference-Guided Video Reconstruction (BRG-VRec) process, we animate the adaptively selected keyframe and reconstruct the target frame using both the animated keyframe and the auxiliary frame. Extensive experiments demonstrate a 55% bitrate reduction compared to the latest animation based video codec, and a 35% bitrate reduction compared to the latest video coding standard, Versatile Video Coding (VVC) on a talking head video dataset. It showcases the efficiency of our approach in improving video quality while simultaneously decreasing bitrate.",
      "authors": [
        "Riku Takahashi",
        "Ryugo Morita",
        "Fuma Kimishima",
        "Kosuke Iwama",
        "Jinjia Zhou"
      ],
      "published": "2025-03-12T19:39:09Z",
      "updated": "2025-03-12T19:39:09Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.09787v1",
      "landing_url": "https://arxiv.org/abs/2503.09787v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.09787"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on bidirectional facial animation video compression and does not describe discrete audio tokenization, quantization, or any audio-centric codec/token design, so it fails all inclusion requirements and matches the exclusion of non-audio research."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on bidirectional facial animation video compression and does not describe discrete audio tokenization, quantization, or any audio-centric codec/token design, so it fails all inclusion requirements and matches the exclusion of non-audio research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-View Node Pruning for Accurate Graph Representation",
    "abstract": "Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling framework to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge.",
    "metadata": {
      "arxiv_id": "2503.11737",
      "title": "Multi-View Node Pruning for Accurate Graph Representation",
      "summary": "Graph pooling, which compresses a whole graph into a smaller coarsened graph, is an essential component of graph representation learning. To efficiently compress a given graph, graph pooling methods often drop their nodes with attention-based scoring with the task loss. However, this often results in simply removing nodes with lower degrees without consideration of their feature-level relevance to the given task. To fix this problem, we propose a Multi-View Pruning(MVP), a graph pruning method based on a multi-view framework and reconstruction loss. Given a graph, MVP first constructs multiple graphs for different views either by utilizing the predefined modalities or by randomly partitioning the input features, to consider the importance of each node in diverse perspectives. Then, it learns the score for each node by considering both the reconstruction and the task loss. MVP can be incorporated with any hierarchical pooling framework to score the nodes. We validate MVP on multiple benchmark datasets by coupling it with two graph pooling methods, and show that it significantly improves the performance of the base graph pooling method, outperforming all baselines. Further analysis shows that both the encoding of multiple views and the consideration of reconstruction loss are the key to the success of MVP, and that it indeed identifies nodes that are less important according to domain knowledge.",
      "authors": [
        "Hanjin Kim",
        "Jiseong Park",
        "Seojin Kim",
        "Jueun Choi",
        "Doheon Lee",
        "Sung Ju Hwang"
      ],
      "published": "2025-03-14T14:44:54Z",
      "updated": "2025-07-17T01:33:12Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11737v4",
      "landing_url": "https://arxiv.org/abs/2503.11737v4",
      "doi": "https://doi.org/10.48550/arXiv.2503.11737"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This paper focuses on graph pooling methods without any discrete audio token/codec discussion, so it fails the inclusion criteria tied to audio token generation and is excluded."
    },
    "round-A_JuniorNano_reasoning": "This paper focuses on graph pooling methods without any discrete audio token/codec discussion, so it fails the inclusion criteria tied to audio token generation and is excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification",
    "abstract": "In the emerging field of goal-oriented communications, the focus has shifted from reconstructing data to directly performing specific learning tasks, such as classification, segmentation, or pattern recognition, on the received coded data. In the commonly studied scenario of classification from compressed images, a key objective is to enable learning directly on entropy-coded data, thereby bypassing the computationally intensive step of data reconstruction. Conventional entropy-coding methods, such as Huffman and Arithmetic coding, are effective for compression but disrupt the data structure, making them less suitable for direct learning without decoding. This paper investigates the use of low-density parity-check (LDPC) codes -- originally designed for channel coding -- as an alternative entropy-coding approach. It is hypothesized that the structured nature of LDPC codes can be leveraged more effectively by deep learning models for tasks like classification. At the receiver side, gated recurrent unit (GRU) models are trained to perform image classification directly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST, and CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in classification tasks, while requiring significantly smaller learning models. Furthermore, the paper analyzes why LDPC codes preserve data structure more effectively than traditional entropy-coding techniques and explores the impact of key code parameters on classification performance. These results suggest that LDPC-based entropy coding offers an optimal balance between learning efficiency and model complexity, eliminating the need for prior decoding.",
    "metadata": {
      "arxiv_id": "2503.11954",
      "title": "Goal-Oriented Source Coding using LDPC Codes for Compressed-Domain Image Classification",
      "summary": "In the emerging field of goal-oriented communications, the focus has shifted from reconstructing data to directly performing specific learning tasks, such as classification, segmentation, or pattern recognition, on the received coded data. In the commonly studied scenario of classification from compressed images, a key objective is to enable learning directly on entropy-coded data, thereby bypassing the computationally intensive step of data reconstruction. Conventional entropy-coding methods, such as Huffman and Arithmetic coding, are effective for compression but disrupt the data structure, making them less suitable for direct learning without decoding. This paper investigates the use of low-density parity-check (LDPC) codes -- originally designed for channel coding -- as an alternative entropy-coding approach. It is hypothesized that the structured nature of LDPC codes can be leveraged more effectively by deep learning models for tasks like classification. At the receiver side, gated recurrent unit (GRU) models are trained to perform image classification directly on LDPC-coded data. Experiments on datasets like MNIST, Fashion-MNIST, and CIFAR show that LDPC codes outperform Huffman and Arithmetic coding in classification tasks, while requiring significantly smaller learning models. Furthermore, the paper analyzes why LDPC codes preserve data structure more effectively than traditional entropy-coding techniques and explores the impact of key code parameters on classification performance. These results suggest that LDPC-based entropy coding offers an optimal balance between learning efficiency and model complexity, eliminating the need for prior decoding.",
      "authors": [
        "Ahcen Aliouat",
        "Elsa Dupraz"
      ],
      "published": "2025-03-15T01:52:09Z",
      "updated": "2025-03-15T01:52:09Z",
      "categories": [
        "eess.IV",
        "cs.AI",
        "cs.CV",
        "cs.IT",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.11954v1",
      "landing_url": "https://arxiv.org/abs/2503.11954v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.11954"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on LDPC-coded image classification and does not address discrete audio tokens, so it fails to meet the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on LDPC-coded image classification and does not address discrete audio tokens, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
    "abstract": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
    "metadata": {
      "arxiv_id": "2503.12115",
      "title": "Universal Speech Token Learning via Low-Bitrate Neural Codec and Pretrained Representations",
      "summary": "Current large speech language models are mainly based on semantic tokens from discretization of self-supervised learned representations and acoustic tokens from a neural codec, following a semantic-modeling and acoustic-synthesis paradigm. However, semantic tokens discard paralinguistic attributes of speakers that is important for natural spoken communication, while prompt-based acoustic synthesis from semantic tokens has limits in recovering paralinguistic details and suffers from robustness issues, especially when there are domain gaps between the prompt and the target. This paper unifies two types of tokens and proposes the UniCodec, a universal speech token learning that encapsulates all semantics of speech, including linguistic and paralinguistic information, into a compact and semantically-disentangled unified token. Such a unified token can not only benefit speech language models in understanding with paralinguistic hints but also help speech generation with high-quality output. A low-bitrate neural codec is leveraged to learn such disentangled discrete representations at global and local scales, with knowledge distilled from self-supervised learned features. Extensive evaluations on multilingual datasets demonstrate its effectiveness in generating natural, expressive and long-term consistent output quality with paralinguistic attributes well preserved in several speech processing tasks.",
      "authors": [
        "Xue Jiang",
        "Xiulian Peng",
        "Yuan Zhang",
        "Yan Lu"
      ],
      "published": "2025-03-15T12:50:43Z",
      "updated": "2025-10-15T06:52:30Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12115v2",
      "landing_url": "https://arxiv.org/abs/2503.12115v2",
      "doi": "https://doi.org/10.1109/JSTSP.2024.3488557"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Paper clearly proposes unified discrete tokens learned via neural codec and SSL representations with evaluations, so it meets all discrete-audio-token inclusion criteria and none of the exclusions; Score: 5"
    },
    "round-A_JuniorNano_reasoning": "Paper clearly proposes unified discrete tokens learned via neural codec and SSL representations with evaluations, so it meets all discrete-audio-token inclusion criteria and none of the exclusions; Score: 5",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Versatile Physics-based Character Control with Hybrid Latent Representation",
    "abstract": "We present a versatile latent representation that enables physically simulated character to efficiently utilize motion priors. To build a powerful motion embedding that is shared across multiple tasks, the physics controller should employ rich latent space that is easily explored and capable of generating high-quality motion. We propose integrating continuous and discrete latent representations to build a versatile motion prior that can be adapted to a wide range of challenging control tasks. Specifically, we build a discrete latent model to capture distinctive posterior distribution without collapse, and simultaneously augment the sampled vector with the continuous residuals to generate high-quality, smooth motion without jittering. We further incorporate Residual Vector Quantization, which not only maximizes the capacity of the discrete motion prior, but also efficiently abstracts the action space during the task learning phase. We demonstrate that our agent can produce diverse yet smooth motions simply by traversing the learned motion prior through unconditional motion generation. Furthermore, our model robustly satisfies sparse goal conditions with highly expressive natural motions, including head-mounted device tracking and motion in-betweening at irregular intervals, which could not be achieved with existing latent representations.",
    "metadata": {
      "arxiv_id": "2503.12814",
      "title": "Versatile Physics-based Character Control with Hybrid Latent Representation",
      "summary": "We present a versatile latent representation that enables physically simulated character to efficiently utilize motion priors. To build a powerful motion embedding that is shared across multiple tasks, the physics controller should employ rich latent space that is easily explored and capable of generating high-quality motion. We propose integrating continuous and discrete latent representations to build a versatile motion prior that can be adapted to a wide range of challenging control tasks. Specifically, we build a discrete latent model to capture distinctive posterior distribution without collapse, and simultaneously augment the sampled vector with the continuous residuals to generate high-quality, smooth motion without jittering. We further incorporate Residual Vector Quantization, which not only maximizes the capacity of the discrete motion prior, but also efficiently abstracts the action space during the task learning phase. We demonstrate that our agent can produce diverse yet smooth motions simply by traversing the learned motion prior through unconditional motion generation. Furthermore, our model robustly satisfies sparse goal conditions with highly expressive natural motions, including head-mounted device tracking and motion in-betweening at irregular intervals, which could not be achieved with existing latent representations.",
      "authors": [
        "Jinseok Bae",
        "Jungdam Won",
        "Donggeun Lim",
        "Inwoo Hwang",
        "Young Min Kim"
      ],
      "published": "2025-03-17T04:45:51Z",
      "updated": "2025-03-17T04:45:51Z",
      "categories": [
        "cs.GR",
        "cs.AI",
        "cs.RO"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.12814v1",
      "landing_url": "https://arxiv.org/abs/2503.12814v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.12814"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on physics-based character control with hybrid latent representations for motion priors rather than discrete audio tokens or any codec/quantization for audio, so it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on physics-based character control with hybrid latent representations for motion priors rather than discrete audio tokens or any codec/quantization for audio, so it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
    "abstract": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
    "metadata": {
      "arxiv_id": "2503.14928",
      "title": "Shushing! Let's Imagine an Authentic Speech from the Silent Video",
      "summary": "Vision-guided speech generation aims to produce authentic speech from facial appearance or lip motions without relying on auditory signals, offering significant potential for applications such as dubbing in filmmaking and assisting individuals with aphonia. Despite recent progress, existing methods struggle to achieve unified cross-modal alignment across semantics, timbre, and emotional prosody from visual cues, prompting us to propose Consistent Video-to-Speech (CV2S) as an extended task to enhance cross-modal consistency. To tackle emerging challenges, we introduce ImaginTalk, a novel cross-modal diffusion framework that generates faithful speech using only visual input, operating within a discrete space. Specifically, we propose a discrete lip aligner that predicts discrete speech tokens from lip videos to capture semantic information, while an error detector identifies misaligned tokens, which are subsequently refined through masked language modeling with BERT. To further enhance the expressiveness of the generated speech, we develop a style diffusion transformer equipped with a face-style adapter that adaptively customizes identity and prosody dynamics across both the channel and temporal dimensions while ensuring synchronization with lip-aware semantic features. Extensive experiments demonstrate that ImaginTalk can generate high-fidelity speech with more accurate semantic details and greater expressiveness in timbre and emotion compared to state-of-the-art baselines. Demos are shown at our project page: https://imagintalk.github.io.",
      "authors": [
        "Jiaxin Ye",
        "Hongming Shan"
      ],
      "published": "2025-03-19T06:28:17Z",
      "updated": "2025-03-19T06:28:17Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.14928v1",
      "landing_url": "https://arxiv.org/abs/2503.14928v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.14928"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract describe generating speech via discrete speech tokens predicted from lip videos and refined through quantized modeling, satisfying the discrete audio token focus outlined in the inclusion criteria with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe generating speech via discrete speech tokens predicted from lip videos and refined through quantized modeling, satisfying the discrete audio token focus outlined in the inclusion criteria with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "The Model Hears You: Audio Language Model Deployments Should Consider the Principle of Least Privilege",
    "abstract": "The latest Audio Language Models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this paper, we urge a closer examination of how these models are built and deployed. Our experiments show that end-to-end modeling, compared with cascaded pipelines, creates socio-technical safety risks such as identity inference, biased decision-making, and emotion detection. This raises concerns about whether Audio LMs store voiceprints and function in ways that create uncertainty under existing legal regimes. We then argue that the Principle of Least Privilege should be considered to guide the development and deployment of these models. Specifically, evaluations should assess (1) the privacy and safety risks associated with end-to-end modeling; and (2) the appropriate scope of information access. Finally, we highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs.",
    "metadata": {
      "arxiv_id": "2503.16833",
      "title": "The Model Hears You: Audio Language Model Deployments Should Consider the Principle of Least Privilege",
      "summary": "The latest Audio Language Models (Audio LMs) process speech directly instead of relying on a separate transcription step. This shift preserves detailed information, such as intonation or the presence of multiple speakers, that would otherwise be lost in transcription. However, it also introduces new safety risks, including the potential misuse of speaker identity cues and other sensitive vocal attributes, which could have legal implications. In this paper, we urge a closer examination of how these models are built and deployed. Our experiments show that end-to-end modeling, compared with cascaded pipelines, creates socio-technical safety risks such as identity inference, biased decision-making, and emotion detection. This raises concerns about whether Audio LMs store voiceprints and function in ways that create uncertainty under existing legal regimes. We then argue that the Principle of Least Privilege should be considered to guide the development and deployment of these models. Specifically, evaluations should assess (1) the privacy and safety risks associated with end-to-end modeling; and (2) the appropriate scope of information access. Finally, we highlight related gaps in current audio LM benchmarks and identify key open research questions, both technical and policy-related, that must be addressed to enable the responsible deployment of end-to-end Audio LMs.",
      "authors": [
        "Luxi He",
        "Xiangyu Qi",
        "Michel Liao",
        "Inyoung Cheong",
        "Prateek Mittal",
        "Danqi Chen",
        "Peter Henderson"
      ],
      "published": "2025-03-21T04:03:59Z",
      "updated": "2025-09-09T00:51:12Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.16833v2",
      "landing_url": "https://arxiv.org/abs/2503.16833v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.16833"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on safety/deployment concerns for end-to-end audio LMs without discussing discrete audio token generation, vocabularies, quantization, or token-level evaluations, so it does not meet inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on safety/deployment concerns for end-to-end audio LMs without discussing discrete audio token generation, vocabularies, quantization, or token-level evaluations, so it does not meet inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Beyond the Encoder: Joint Encoder-Decoder Contrastive Pre-Training Improves Dense Prediction",
    "abstract": "Contrastive learning methods in self-supervised settings have primarily focused on pre-training encoders, while decoders are typically introduced and trained separately for downstream dense prediction tasks. However, this conventional approach overlooks the potential benefits of jointly pre-training both encoder and decoder. In this paper, we propose DeCon, an efficient encoder-decoder self-supervised learning (SSL) framework that supports joint contrastive pre-training. We first extend existing SSL architectures to accommodate diverse decoders and their corresponding contrastive losses. Then, we introduce a weighted encoder-decoder contrastive loss with non-competing objectives to enable the joint pre-training of encoder-decoder architectures. By adapting an established contrastive SSL framework for dense prediction tasks, DeCon achieves new state-of-the-art results: on COCO object detection and instance segmentation when pre-trained on COCO dataset; across almost all dense downstream benchmark tasks when pre-trained on COCO+ and ImageNet-1K. Our results demonstrate that joint pre-training enhances the representation power of the encoder and improves performance in dense prediction tasks. This gain persists across heterogeneous decoder architectures, various encoder architectures, and in out-of-domain limited-data scenarios.",
    "metadata": {
      "arxiv_id": "2503.17526",
      "title": "Beyond the Encoder: Joint Encoder-Decoder Contrastive Pre-Training Improves Dense Prediction",
      "summary": "Contrastive learning methods in self-supervised settings have primarily focused on pre-training encoders, while decoders are typically introduced and trained separately for downstream dense prediction tasks. However, this conventional approach overlooks the potential benefits of jointly pre-training both encoder and decoder. In this paper, we propose DeCon, an efficient encoder-decoder self-supervised learning (SSL) framework that supports joint contrastive pre-training. We first extend existing SSL architectures to accommodate diverse decoders and their corresponding contrastive losses. Then, we introduce a weighted encoder-decoder contrastive loss with non-competing objectives to enable the joint pre-training of encoder-decoder architectures. By adapting an established contrastive SSL framework for dense prediction tasks, DeCon achieves new state-of-the-art results: on COCO object detection and instance segmentation when pre-trained on COCO dataset; across almost all dense downstream benchmark tasks when pre-trained on COCO+ and ImageNet-1K. Our results demonstrate that joint pre-training enhances the representation power of the encoder and improves performance in dense prediction tasks. This gain persists across heterogeneous decoder architectures, various encoder architectures, and in out-of-domain limited-data scenarios.",
      "authors": [
        "Sébastien Quetin",
        "Tapotosh Ghosh",
        "Farhad Maleki"
      ],
      "published": "2025-03-21T20:19:13Z",
      "updated": "2025-07-31T16:37:43Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.17526v2",
      "landing_url": "https://arxiv.org/abs/2503.17526v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.17526"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on joint encoder-decoder contrastive pre-training for visual dense prediction without any discrete audio tokenization or codec/token quantization discussion, so it fails the inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on joint encoder-decoder contrastive pre-training for visual dense prediction without any discrete audio tokenization or codec/token quantization discussion, so it fails the inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations",
    "abstract": "Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on the quantization and entropy coding schemes employed. In this paper, we introduce SINR, an innovative compression algorithm that leverages the patterns in the vector spaces formed by weights of INRs. We compress these vector spaces using a high-dimensional sparse code within a dictionary. Further analysis reveals that the atoms of the dictionary used to generate the sparse code do not need to be learned or transmitted to successfully recover the INR weights. We demonstrate that the proposed approach can be integrated with any existing INR-based signal compression technique. Our results indicate that SINR achieves substantial reductions in storage requirements for INRs across various configurations, outperforming conventional INR-based compression baselines. Furthermore, SINR maintains high-quality decoding across diverse data modalities, including images, occupancy fields, and Neural Radiance Fields.",
    "metadata": {
      "arxiv_id": "2503.19576",
      "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations",
      "summary": "Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on the quantization and entropy coding schemes employed. In this paper, we introduce SINR, an innovative compression algorithm that leverages the patterns in the vector spaces formed by weights of INRs. We compress these vector spaces using a high-dimensional sparse code within a dictionary. Further analysis reveals that the atoms of the dictionary used to generate the sparse code do not need to be learned or transmitted to successfully recover the INR weights. We demonstrate that the proposed approach can be integrated with any existing INR-based signal compression technique. Our results indicate that SINR achieves substantial reductions in storage requirements for INRs across various configurations, outperforming conventional INR-based compression baselines. Furthermore, SINR maintains high-quality decoding across diverse data modalities, including images, occupancy fields, and Neural Radiance Fields.",
      "authors": [
        "Dhananjaya Jayasundara",
        "Sudarshan Rajagopalan",
        "Yasiru Ranasinghe",
        "Trac D. Tran",
        "Vishal M. Patel"
      ],
      "published": "2025-03-25T11:53:51Z",
      "updated": "2025-03-25T11:53:51Z",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19576v1",
      "landing_url": "https://arxiv.org/abs/2503.19576v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.19576"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This INR compression work focuses on sparsity-driven weight-space coding for implicit neural signals, not on discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This INR compression work focuses on sparsity-driven weight-space coding for implicit neural signals, not on discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment",
    "abstract": "Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype violations (e.g., a man claiming to regularly get manicures) and biological knowledge violations (e.g., a man claiming to be pregnant). Results revealed that Qwen2-Audio exhibited increased surprisal for speaker-incongruent content and its surprisal values significantly predicted human N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker characteristics. Importantly, neither model replicated the human-like processing distinction between social violations (eliciting N400 effects) and biological violations (eliciting P600 effects). These findings reveal both the potential and limitations of current LALMs in processing speaker-contextualized language, and suggest differences in social-linguistic processing mechanisms between humans and LALMs.",
    "metadata": {
      "arxiv_id": "2503.19586",
      "title": "Distinct social-linguistic processing between humans and large audio-language models: Evidence from model-brain alignment",
      "summary": "Voice-based AI development faces unique challenges in processing both linguistic and paralinguistic information. This study compares how large audio-language models (LALMs) and humans integrate speaker characteristics during speech comprehension, asking whether LALMs process speaker-contextualized language in ways that parallel human cognitive mechanisms. We compared two LALMs' (Qwen2-Audio and Ultravox 0.5) processing patterns with human EEG responses. Using surprisal and entropy metrics from the models, we analyzed their sensitivity to speaker-content incongruency across social stereotype violations (e.g., a man claiming to regularly get manicures) and biological knowledge violations (e.g., a man claiming to be pregnant). Results revealed that Qwen2-Audio exhibited increased surprisal for speaker-incongruent content and its surprisal values significantly predicted human N400 responses, while Ultravox 0.5 showed limited sensitivity to speaker characteristics. Importantly, neither model replicated the human-like processing distinction between social violations (eliciting N400 effects) and biological violations (eliciting P600 effects). These findings reveal both the potential and limitations of current LALMs in processing speaker-contextualized language, and suggest differences in social-linguistic processing mechanisms between humans and LALMs.",
      "authors": [
        "Hanlin Wu",
        "Xufeng Duan",
        "Zhenguang Cai"
      ],
      "published": "2025-03-25T12:10:47Z",
      "updated": "2025-10-25T10:06:37Z",
      "categories": [
        "cs.CL",
        "q-bio.NC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.19586v2",
      "landing_url": "https://arxiv.org/abs/2503.19586v2",
      "doi": "https://doi.org/10.18653/v1/2025.cmcl-1.18"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus on social-linguistic processing of audio-language models and EEG alignment rather than discrete audio token design, quantization, or tokenizer evaluation, so it fails to meet any inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus on social-linguistic processing of audio-language models and EEG alignment rather than discrete audio token design, quantization, or tokenizer evaluation, so it fails to meet any inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
    "abstract": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
    "metadata": {
      "arxiv_id": "2503.20499",
      "title": "FireRedTTS-1S: An Upgraded Streamable Foundation Text-to-Speech System",
      "summary": "In this work, we upgrade FireRedTTS to a new version, FireRedTTS-1S, a high-quality streaming foundation text-to-speech system. FireRedTTS-1S achieves streaming speech generation via two steps: text-to-semantic decoding and semantic-to-acoustic decoding. In text-to-semantic decoding, a semantic-aware speech tokenizer converts the speech signal into semantic tokens, which can be synthesized from the text via a language model in an auto-regressive manner. Meanwhile, the semantic-to-acoustic decoding module simultaneously translates generated semantic tokens into the speech signal in a streaming way. We implement two approaches to achieve this module: 1) a chunk-wise streamable flow-matching approach, and 2) a multi-stream language model-based approach. They both present high-quality and streamable speech generation but differ in real-time factor (RTF) and latency. Specifically, flow-matching decoding can generate speech by chunks, presenting a lower RTF of 0.1 but a higher latency of 300ms. Instead, the multi-stream language model generates speech by frames in an autoregressive manner, presenting a higher RTF of 0.3 but a low latency of 150ms. In experiments on zero-shot voice cloning, the objective results validate FireRedTTS-1S as a high-quality foundation model with comparable intelligibility and speaker similarity over industrial baseline systems. Furthermore, the subjective score of FireRedTTS-1S highlights its impressive synthesis performance, achieving comparable quality to the ground-truth recordings. These results validate FireRedTTS-1S as a high-quality streaming foundation TTS system.",
      "authors": [
        "Hao-Han Guo",
        "Yao Hu",
        "Fei-Yu Shen",
        "Xu Tang",
        "Yi-Chen Wu",
        "Feng-Long Xie",
        "Kun Xie"
      ],
      "published": "2025-03-26T12:39:06Z",
      "updated": "2025-05-26T11:34:20Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.20499v3",
      "landing_url": "https://arxiv.org/abs/2503.20499v3",
      "doi": "https://doi.org/10.48550/arXiv.2503.20499"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "FireRedTTS-1S explicitly builds and evaluates a semantic-aware speech tokenizer producing discrete tokens and their streaming decoding, therefore satisfying the discrete audio-token focus with assessments of reconstruction quality and latency trade-offs."
    },
    "round-A_JuniorNano_reasoning": "FireRedTTS-1S explicitly builds and evaluates a semantic-aware speech tokenizer producing discrete tokens and their streaming decoding, therefore satisfying the discrete audio-token focus with assessments of reconstruction quality and latency trade-offs.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Molecular Quantum Transformer",
    "abstract": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
    "metadata": {
      "arxiv_id": "2503.21686",
      "title": "Molecular Quantum Transformer",
      "summary": "The Transformer model, renowned for its powerful attention mechanism, has achieved state-of-the-art performance in various artificial intelligence tasks but faces challenges such as high computational cost and memory usage. Researchers are exploring quantum computing to enhance the Transformer's design, though it still shows limited success with classical data. With a growing focus on leveraging quantum machine learning for quantum data, particularly in quantum chemistry, we propose the Molecular Quantum Transformer (MQT) for modeling interactions in molecular quantum systems. By utilizing quantum circuits to implement the attention mechanism on the molecular configurations, MQT can efficiently calculate ground-state energies for all configurations. Numerical demonstrations show that in calculating ground-state energies for H2, LiH, BeH2, and H4, MQT outperforms the classical Transformer, highlighting the promise of quantum effects in Transformer structures. Furthermore, its pretraining capability on diverse molecular data facilitates the efficient learning of new molecules, extending its applicability to complex molecular systems with minimal additional effort. Our method offers an alternative to existing quantum algorithms for estimating ground-state energies, opening new avenues in quantum chemistry and materials science.",
      "authors": [
        "Yuichi Kamata",
        "Quoc Hoan Tran",
        "Yasuhiro Endo",
        "Hirotaka Oshima"
      ],
      "published": "2025-03-27T16:54:15Z",
      "updated": "2025-05-16T02:38:13Z",
      "categories": [
        "quant-ph",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.21686v2",
      "landing_url": "https://arxiv.org/abs/2503.21686v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.21686"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on quantum transformers for molecular ground-state energies and does not address discrete audio token/tokenizer research, so it fails the inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on quantum transformers for molecular ground-state energies and does not address discrete audio token/tokenizer research, so it fails the inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "New universal operator approximation theorem for encoder-decoder architectures (Preprint)",
    "abstract": "Motivated by the rapidly growing field of mathematics for operator approximation with neural networks, we present a novel universal operator approximation theorem for a broad class of encoder-decoder architectures. In this study, we focus on approximating continuous operators in $\\mathcal{C}(\\mathcal{X}, \\mathcal{Y})$, where $\\mathcal{X}$ and $\\mathcal{Y}$ are infinite-dimensional normed or metric spaces, and we consider uniform convergence on compact subsets of $\\mathcal{X}$. Unlike standard results in the operator learning literature, we investigate the case where the approximating operator sequence can be chosen independently of the compact sets. Taking a topological perspective, we analyze different types of operator approximation and show that compact-set-independent approximation is a strictly stronger property in most relevant operator learning frameworks. To establish our results, we introduce a new approximation property tailored to encoder-decoder architectures, which enables us to prove a universal operator approximation theorem ensuring uniform convergence on every compact subset. This result unifies and extends existing universal operator approximation theorems for various encoder-decoder architectures, including classical DeepONets, BasisONets, special cases of MIONets, architectures based on frames and other related approaches.",
    "metadata": {
      "arxiv_id": "2503.24092",
      "title": "New universal operator approximation theorem for encoder-decoder architectures (Preprint)",
      "summary": "Motivated by the rapidly growing field of mathematics for operator approximation with neural networks, we present a novel universal operator approximation theorem for a broad class of encoder-decoder architectures. In this study, we focus on approximating continuous operators in $\\mathcal{C}(\\mathcal{X}, \\mathcal{Y})$, where $\\mathcal{X}$ and $\\mathcal{Y}$ are infinite-dimensional normed or metric spaces, and we consider uniform convergence on compact subsets of $\\mathcal{X}$. Unlike standard results in the operator learning literature, we investigate the case where the approximating operator sequence can be chosen independently of the compact sets. Taking a topological perspective, we analyze different types of operator approximation and show that compact-set-independent approximation is a strictly stronger property in most relevant operator learning frameworks. To establish our results, we introduce a new approximation property tailored to encoder-decoder architectures, which enables us to prove a universal operator approximation theorem ensuring uniform convergence on every compact subset. This result unifies and extends existing universal operator approximation theorems for various encoder-decoder architectures, including classical DeepONets, BasisONets, special cases of MIONets, architectures based on frames and other related approaches.",
      "authors": [
        "Janek Gödeke",
        "Pascal Fernsel"
      ],
      "published": "2025-03-31T13:43:21Z",
      "updated": "2025-03-31T13:43:21Z",
      "categories": [
        "math.FA",
        "cs.LG",
        "math.GN"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.24092v1",
      "landing_url": "https://arxiv.org/abs/2503.24092v1",
      "doi": "https://doi.org/10.48550/arXiv.2503.24092"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on abstract operator approximation in encoder-decoder neural architectures rather than discrete audio tokenization/quantization or codec token modeling, so it fails to meet the inclusion theme and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on abstract operator approximation in encoder-decoder neural architectures rather than discrete audio tokenization/quantization or codec token modeling, so it fails to meet the inclusion theme and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
    "abstract": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
    "metadata": {
      "arxiv_id": "2503.24164",
      "title": "SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation",
      "summary": "Large vision and language models show strong performance in tasks like image captioning, visual question answering, and retrieval. However, challenges remain in integrating speech, text, and vision into a unified model, especially for spoken tasks. Speech generation methods vary (some produce speech directly), others through text (but their impact on quality is unclear). Evaluation often relies on automatic speech recognition, which may introduce bias. We propose SVLA, a unified speech vision language model based on a transformer architecture that handles multimodal inputs and outputs. We train it on 38.2 million speech text image examples, including 64.1 hours of synthetic speech. We also introduce Speech VQA Accuracy, a new metric for evaluating spoken responses. SVLA improves multimodal understanding and generation by better combining speech, vision, and language.",
      "authors": [
        "Ngoc Dung Huynh",
        "Mohamed Reda Bouadjenek",
        "Imran Razzak",
        "Hakim Hacid",
        "Sunil Aryal"
      ],
      "published": "2025-03-31T14:46:34Z",
      "updated": "2025-07-07T14:41:48Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2503.24164v2",
      "landing_url": "https://arxiv.org/abs/2503.24164v2",
      "doi": "https://doi.org/10.48550/arXiv.2503.24164"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes a unified speech-vision-language model but never details any discrete audio tokenization, quantized vocabularies, or codec/token evaluations, so it fails the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes a unified speech-vision-language model but never details any discrete audio tokenization, quantized vocabularies, or codec/token evaluations, so it fails the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.",
    "metadata": {
      "arxiv_id": "2504.03059",
      "title": "Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization",
      "summary": "3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.",
      "authors": [
        "Haishan Wang",
        "Mohammad Hassan Vali",
        "Arno Solin"
      ],
      "published": "2025-04-03T22:19:34Z",
      "updated": "2025-04-08T22:40:23Z",
      "categories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.03059v2",
      "landing_url": "https://arxiv.org/abs/2504.03059v2",
      "doi": "https://doi.org/10.1007/978-3-031-95911-0_24"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on compressing 3D Gaussian Splatting parameters rather than any discrete audio-token pipeline, so it fails to meet the discrete audio-token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on compressing 3D Gaussian Splatting parameters rather than any discrete audio-token pipeline, so it fails to meet the discrete audio-token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "3DM-WeConvene: Learned Image Compression with 3D Multi-Level Wavelet-Domain Convolution and Entropy Model",
    "abstract": "Learned image compression (LIC) has recently made significant progress, surpassing traditional methods. However, most LIC approaches operate mainly in the spatial domain and lack mechanisms for reducing frequency-domain correlations. To address this, we propose a novel framework that integrates low-complexity 3D multi-level Discrete Wavelet Transform (DWT) into convolutional layers and entropy coding, reducing both spatial and channel correlations to improve frequency selectivity and rate-distortion (R-D) performance. Our proposed 3D multi-level wavelet-domain convolution (3DM-WeConv) layer first applies 3D multi-level DWT (e.g., 5/3 and 9/7 wavelets from JPEG 2000) to transform data into the wavelet domain. Then, different-sized convolutions are applied to different frequency subbands, followed by inverse 3D DWT to restore the spatial domain. The 3DM-WeConv layer can be flexibly used within existing CNN-based LIC models. We also introduce a 3D wavelet-domain channel-wise autoregressive entropy model (3DWeChARM), which performs slice-based entropy coding in the 3D DWT domain. Low-frequency (LF) slices are encoded first to provide priors for high-frequency (HF) slices. A two-step training strategy is adopted: first balancing LF and HF rates, then fine-tuning with separate weights. Extensive experiments demonstrate that our framework consistently outperforms state-of-the-art CNN-based LIC methods in R-D performance and computational complexity, with larger gains for high-resolution images. On the Kodak, Tecnick 100, and CLIC test sets, our method achieves BD-Rate reductions of -12.24%, -15.51%, and -12.97%, respectively, compared to H.266/VVC.",
    "metadata": {
      "arxiv_id": "2504.04658",
      "title": "3DM-WeConvene: Learned Image Compression with 3D Multi-Level Wavelet-Domain Convolution and Entropy Model",
      "summary": "Learned image compression (LIC) has recently made significant progress, surpassing traditional methods. However, most LIC approaches operate mainly in the spatial domain and lack mechanisms for reducing frequency-domain correlations. To address this, we propose a novel framework that integrates low-complexity 3D multi-level Discrete Wavelet Transform (DWT) into convolutional layers and entropy coding, reducing both spatial and channel correlations to improve frequency selectivity and rate-distortion (R-D) performance.\n  Our proposed 3D multi-level wavelet-domain convolution (3DM-WeConv) layer first applies 3D multi-level DWT (e.g., 5/3 and 9/7 wavelets from JPEG 2000) to transform data into the wavelet domain. Then, different-sized convolutions are applied to different frequency subbands, followed by inverse 3D DWT to restore the spatial domain. The 3DM-WeConv layer can be flexibly used within existing CNN-based LIC models.\n  We also introduce a 3D wavelet-domain channel-wise autoregressive entropy model (3DWeChARM), which performs slice-based entropy coding in the 3D DWT domain. Low-frequency (LF) slices are encoded first to provide priors for high-frequency (HF) slices.\n  A two-step training strategy is adopted: first balancing LF and HF rates, then fine-tuning with separate weights.\n  Extensive experiments demonstrate that our framework consistently outperforms state-of-the-art CNN-based LIC methods in R-D performance and computational complexity, with larger gains for high-resolution images. On the Kodak, Tecnick 100, and CLIC test sets, our method achieves BD-Rate reductions of -12.24%, -15.51%, and -12.97%, respectively, compared to H.266/VVC.",
      "authors": [
        "Haisheng Fu",
        "Jie Liang",
        "Feng Liang",
        "Zhenman Fang",
        "Guohe Zhang",
        "Jingning Han"
      ],
      "published": "2025-04-07T01:11:50Z",
      "updated": "2025-04-07T01:11:50Z",
      "categories": [
        "cs.CV",
        "stat.AP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04658v1",
      "landing_url": "https://arxiv.org/abs/2504.04658v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.04658"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe learned image compression with wavelet-domain convolutions and entropy models, which do not involve discrete audio tokens or codec-based tokenization, so it fails the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe learned image compression with wavelet-domain convolutions and entropy models, which do not involve discrete audio tokens or codec-based tokenization, so it fails the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "L3AC: Towards a Lightweight and Lossless Audio Codec",
    "abstract": "Neural audio codecs have recently gained traction for their ability to compress high-fidelity audio and provide discrete tokens for generative modeling. However, leading approaches often rely on resource-intensive models and complex multi-quantizer architectures, limiting their practicality in real-world applications. In this work, we introduce L3AC, a lightweight neural audio codec that addresses these challenges by leveraging a single quantizer and a highly efficient architecture. To enhance reconstruction fidelity while minimizing model complexity, L3AC explores streamlined convolutional networks and local Transformer modules, alongside TConv--a novel structure designed to capture acoustic variations across multiple temporal scales. Despite its compact design, extensive experiments across diverse datasets demonstrate that L3AC matches or exceeds the reconstruction quality of leading codecs while reducing computational overhead by an order of magnitude. The single-quantizer design further enhances its adaptability for downstream tasks. The source code is publicly available at https://github.com/zhai-lw/L3AC.",
    "metadata": {
      "arxiv_id": "2504.04949",
      "title": "L3AC: Towards a Lightweight and Lossless Audio Codec",
      "summary": "Neural audio codecs have recently gained traction for their ability to compress high-fidelity audio and provide discrete tokens for generative modeling. However, leading approaches often rely on resource-intensive models and complex multi-quantizer architectures, limiting their practicality in real-world applications. In this work, we introduce L3AC, a lightweight neural audio codec that addresses these challenges by leveraging a single quantizer and a highly efficient architecture. To enhance reconstruction fidelity while minimizing model complexity, L3AC explores streamlined convolutional networks and local Transformer modules, alongside TConv--a novel structure designed to capture acoustic variations across multiple temporal scales. Despite its compact design, extensive experiments across diverse datasets demonstrate that L3AC matches or exceeds the reconstruction quality of leading codecs while reducing computational overhead by an order of magnitude. The single-quantizer design further enhances its adaptability for downstream tasks. The source code is publicly available at https://github.com/zhai-lw/L3AC.",
      "authors": [
        "Linwei Zhai",
        "Han Ding",
        "Cui Zhao",
        "fei wang",
        "Ge Wang",
        "Wang Zhi",
        "Wei Xi"
      ],
      "published": "2025-04-07T11:34:39Z",
      "updated": "2025-08-15T12:56:31Z",
      "categories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.04949v2",
      "landing_url": "https://arxiv.org/abs/2504.04949v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.04949"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "L3AC introduces a lightweight neural codec with explicit single-quantizer discrete tokens and evaluation of reconstruction quality, so it clearly satisfies the discrete audio token inclusion requirements and no exclusions apply."
    },
    "round-A_JuniorNano_reasoning": "L3AC introduces a lightweight neural codec with explicit single-quantizer discrete tokens and evaluation of reconstruction quality, so it clearly satisfies the discrete audio token inclusion requirements and no exclusions apply.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
    "abstract": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
    "metadata": {
      "arxiv_id": "2504.05197",
      "title": "P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation",
      "summary": "Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.",
      "authors": [
        "Yong Ren",
        "Jiangyan Yi",
        "Tao Wang",
        "Jianhua Tao",
        "Zheng Lian",
        "Zhengqi Wen",
        "Chenxing Li",
        "Ruibo Fu",
        "Ye Bai",
        "Xiaohui Zhang"
      ],
      "published": "2025-04-07T15:47:09Z",
      "updated": "2025-05-05T16:34:37Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.05197v2",
      "landing_url": "https://arxiv.org/abs/2504.05197v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.05197"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on watermarking NSG model parameters with adapters and gradient tricks rather than producing or evaluating discrete audio tokens/codecs, so it fails to meet the discrete-token inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on watermarking NSG model parameters with adapters and gradient tricks rather than producing or evaluating discrete audio tokens/codecs, so it fails to meet the discrete-token inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
    "abstract": "While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.",
    "metadata": {
      "arxiv_id": "2504.06225",
      "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation",
      "summary": "While decoder-only large language models (LLMs) have shown impressive results, encoder-decoder models are still widely adopted in real-world applications for their inference efficiency and richer encoder representation. In this paper, we study a novel problem: adapting pretrained decoder-only LLMs to encoder-decoder, with the goal of leveraging the strengths of both approaches to achieve a more favorable quality-efficiency trade-off. We argue that adaptation not only enables inheriting the capability of decoder-only LLMs but also reduces the demand for computation compared to pretraining from scratch. We rigorously explore different pretraining objectives and parameter initialization/optimization techniques. Through extensive experiments based on Gemma 2 (2B and 9B) and a suite of newly pretrained mT5-sized models (up to 1.6B), we demonstrate the effectiveness of adaptation and the advantage of encoder-decoder LLMs. Under similar inference budget, encoder-decoder LLMs achieve comparable (often better) pretraining performance but substantially better finetuning performance than their decoder-only counterpart. For example, Gemma 2B-2B outperforms Gemma 2B by $\\sim$7\\% after instruction tuning. Encoder-decoder adaptation also allows for flexible combination of different-sized models, where Gemma 9B-2B significantly surpasses Gemma 2B-2B by $>$3\\%. The adapted encoder representation also yields better results on SuperGLUE. We will release our checkpoints to facilitate future research.",
      "authors": [
        "Biao Zhang",
        "Fedor Moiseev",
        "Joshua Ainslie",
        "Paul Suganthan",
        "Min Ma",
        "Surya Bhupatiraju",
        "Fede Lebron",
        "Orhan Firat",
        "Armand Joulin",
        "Zhe Dong"
      ],
      "published": "2025-04-08T17:13:41Z",
      "updated": "2025-04-08T17:13:41Z",
      "categories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06225v1",
      "landing_url": "https://arxiv.org/abs/2504.06225v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.06225"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract focus entirely on adapting decoder-only LLMs to encoder–decoder LLMs for general text (no discrete audio tokenization, codec, or token-level quantization discussion), so it fails the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract focus entirely on adapting decoder-only LLMs to encoder–decoder LLMs for general text (no discrete audio tokenization, codec, or token-level quantization discussion), so it fails the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
    "abstract": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
    "metadata": {
      "arxiv_id": "2504.06561",
      "title": "A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication",
      "summary": "This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.",
      "authors": [
        "Xiao-Hang Jiang",
        "Yang Ai",
        "Rui-Chen Zheng",
        "Zhen-Hua Ling"
      ],
      "published": "2025-04-09T03:49:00Z",
      "updated": "2025-04-09T03:49:00Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06561v1",
      "landing_url": "https://arxiv.org/abs/2504.06561v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.06561"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "StreamCodec introduces an explicit residual scalar-vector quantizer and codebooks to convert audio into discrete codes with codec-focused evaluation (ViSQOL, latency/throughput) for high-fidelity real-time communication, satisfying all inclusion criteria and none of the exclusions."
    },
    "round-A_JuniorNano_reasoning": "StreamCodec introduces an explicit residual scalar-vector quantizer and codebooks to convert audio into discrete codes with codec-focused evaluation (ViSQOL, latency/throughput) for high-fidelity real-time communication, satisfying all inclusion criteria and none of the exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture",
    "abstract": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction.",
    "metadata": {
      "arxiv_id": "2504.06738",
      "title": "EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture",
      "summary": "In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction.",
      "authors": [
        "Wenfeng Feng",
        "Hongxiang Wang",
        "Jianlong Wang",
        "Xin Zhang",
        "Jingjing Zhao",
        "Yueyue Liang",
        "Xiang Chen",
        "Duokui Han"
      ],
      "published": "2025-04-09T09:51:41Z",
      "updated": "2025-10-16T12:43:03Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06738v2",
      "landing_url": "https://arxiv.org/abs/2504.06738v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.06738"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract describe vision transformer architecture for images with no mention of discrete audio token generation, quantization, vocabularies, or codec-related evaluation, so it fails the audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract describe vision transformer architecture for images with no mention of discrete audio token generation, quantization, vocabularies, or codec-related evaluation, so it fails the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CHIME: A Compressive Framework for Holistic Interest Modeling",
    "abstract": "Modeling holistic user interests is important for improving recommendation systems but is challenged by high computational cost and difficulty in handling diverse information with full behavior context. Existing search-based methods might lose critical signals during behavior selection. To overcome these limitations, we propose CHIME: A Compressive Framework for Holistic Interest Modeling. It uses adapted large language models to encode complete user behaviors with heterogeneous inputs. We introduce multi-granular contrastive learning objectives to capture both persistent and transient interest patterns and apply residual vector quantization to generate compact embeddings. CHIME demonstrates superior ranking performance across diverse datasets, establishing a robust solution for scalable holistic interest modeling in recommendation systems.",
    "metadata": {
      "arxiv_id": "2504.06780",
      "title": "CHIME: A Compressive Framework for Holistic Interest Modeling",
      "summary": "Modeling holistic user interests is important for improving recommendation systems but is challenged by high computational cost and difficulty in handling diverse information with full behavior context. Existing search-based methods might lose critical signals during behavior selection. To overcome these limitations, we propose CHIME: A Compressive Framework for Holistic Interest Modeling. It uses adapted large language models to encode complete user behaviors with heterogeneous inputs. We introduce multi-granular contrastive learning objectives to capture both persistent and transient interest patterns and apply residual vector quantization to generate compact embeddings. CHIME demonstrates superior ranking performance across diverse datasets, establishing a robust solution for scalable holistic interest modeling in recommendation systems.",
      "authors": [
        "Yong Bai",
        "Rui Xiang",
        "Kaiyuan Li",
        "Yongxiang Tang",
        "Yanhua Cheng",
        "Xialong Liu",
        "Peng Jiang",
        "Kun Gai"
      ],
      "published": "2025-04-09T11:08:49Z",
      "updated": "2025-04-09T11:08:49Z",
      "categories": [
        "cs.IR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.06780v1",
      "landing_url": "https://arxiv.org/abs/2504.06780v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.06780"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study addresses recommendation interest modeling via LLMs and vector quantization for user behavior, with no focus on discrete audio token generation, codec quantization, or related evaluations, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study addresses recommendation interest modeling via LLMs and vector quantization for user behavior, with no focus on discrete audio token generation, codec quantization, or related evaluations, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "metadata": {
      "arxiv_id": "2504.07053",
      "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
      "summary": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
      "authors": [
        "Liang-Hsuan Tseng",
        "Yi-Chang Chen",
        "Kuan-Yi Lee",
        "Da-Shan Shiu",
        "Hung-yi Lee"
      ],
      "published": "2025-04-09T17:14:33Z",
      "updated": "2025-05-22T14:49:03Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07053v2",
      "landing_url": "https://arxiv.org/abs/2504.07053v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.07053"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Title/abstract describe discrete speech tokenization aligned to text with reconstruction and evaluations, satisfying the discrete-token inclusion criteria without hitting any exclusion, so include."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe discrete speech tokenization aligned to text with reconstruction and evaluations, satisfying the discrete-token inclusion criteria without hitting any exclusion, so include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models",
    "abstract": "We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction built upon the LauraGPT backbone. LauraTSE employs a small-scale auto-regressive decoder-only language model that generates the initial layers of the target speech's discrete codec representations from the continuous embeddings of both the mixture and reference speech. These outputs serve as coarse-grained predictions. To refine them, a one-step encoder-only language model reconstructs the full codec representation by integrating information from both the mixture and the reference speech, adding fine-grained details. Experimental results show that our approach can achieve promising performance. Additionally, we conduct ablation studies to investigate the data scalability and the contribution of the encoder-only model.",
    "metadata": {
      "arxiv_id": "2504.07402",
      "title": "LauraTSE: Target Speaker Extraction using Auto-Regressive Decoder-Only Language Models",
      "summary": "We propose LauraTSE, an Auto-Regressive Decoder-Only Language Model for Target Speaker Extraction built upon the LauraGPT backbone. LauraTSE employs a small-scale auto-regressive decoder-only language model that generates the initial layers of the target speech's discrete codec representations from the continuous embeddings of both the mixture and reference speech. These outputs serve as coarse-grained predictions. To refine them, a one-step encoder-only language model reconstructs the full codec representation by integrating information from both the mixture and the reference speech, adding fine-grained details. Experimental results show that our approach can achieve promising performance. Additionally, we conduct ablation studies to investigate the data scalability and the contribution of the encoder-only model.",
      "authors": [
        "Beilong Tang",
        "Bang Zeng",
        "Ming Li"
      ],
      "published": "2025-04-10T02:55:22Z",
      "updated": "2025-08-16T21:00:02Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.07402v3",
      "landing_url": "https://arxiv.org/abs/2504.07402v3",
      "doi": "https://doi.org/10.48550/arXiv.2504.07402"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Generates discrete codec representations from continuous inputs and evaluates refinement for target speaker extraction, so it directly targets discrete audio tokens and their modeling even if focused on speaker extraction."
    },
    "round-A_JuniorNano_reasoning": "Generates discrete codec representations from continuous inputs and evaluates refinement for target speaker extraction, so it directly targets discrete audio tokens and their modeling even if focused on speaker extraction.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
    "abstract": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
    "metadata": {
      "arxiv_id": "2504.08274",
      "title": "Generalized Multilingual Text-to-Speech Generation with Language-Aware Style Adaptation",
      "summary": "Text-to-Speech (TTS) models can generate natural, human-like speech across multiple languages by transforming phonemes into waveforms. However, multilingual TTS remains challenging due to discrepancies in phoneme vocabularies and variations in prosody and speaking style across languages. Existing approaches either train separate models for each language, which achieve high performance at the cost of increased computational resources, or use a unified model for multiple languages that struggles to capture fine-grained, language-specific style variations. In this work, we propose LanStyleTTS, a non-autoregressive, language-aware style adaptive TTS framework that standardizes phoneme representations and enables fine-grained, phoneme-level style control across languages. This design supports a unified multilingual TTS model capable of producing accurate and high-quality speech without the need to train language-specific models. We evaluate LanStyleTTS by integrating it with several state-of-the-art non-autoregressive TTS architectures. Results show consistent performance improvements across different model backbones. Furthermore, we investigate a range of acoustic feature representations, including mel-spectrograms and autoencoder-derived latent features. Our experiments demonstrate that latent encodings can significantly reduce model size and computational cost while preserving high-quality speech generation.",
      "authors": [
        "Haowei Lou",
        "Hye-young Paik",
        "Sheng Li",
        "Wen Hu",
        "Lina Yao"
      ],
      "published": "2025-04-11T06:12:57Z",
      "updated": "2025-04-11T06:12:57Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08274v1",
      "landing_url": "https://arxiv.org/abs/2504.08274v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08274"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on multilingual TTS, phoneme representations, and continuous acoustic features (mel-spectrograms or latent encodings) without introducing or evaluating discrete audio tokens, so it fails the inclusion requirements and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on multilingual TTS, phoneme representations, and continuous acoustic features (mel-spectrograms or latent encodings) without introducing or evaluating discrete audio tokens, so it fails the inclusion requirements and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey",
    "abstract": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.",
    "metadata": {
      "arxiv_id": "2504.08528",
      "title": "On The Landscape of Spoken Language Models: A Comprehensive Survey",
      "summary": "The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both \"pure\" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.",
      "authors": [
        "Siddhant Arora",
        "Kai-Wei Chang",
        "Chung-Ming Chien",
        "Yifan Peng",
        "Haibin Wu",
        "Yossi Adi",
        "Emmanuel Dupoux",
        "Hung-Yi Lee",
        "Karen Livescu",
        "Shinji Watanabe"
      ],
      "published": "2025-04-11T13:40:53Z",
      "updated": "2025-04-11T13:40:53Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.08528v1",
      "landing_url": "https://arxiv.org/abs/2504.08528v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.08528"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "Abstract surveys spoken language models broadly without discussing discrete audio tokenizers, vocabularies, or quantization schemes, so it does not meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract surveys spoken language models broadly without discussing discrete audio tokenizers, vocabularies, or quantization schemes, so it does not meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling",
    "abstract": "Recent advancements in audio language models have underscored the pivotal role of audio tokenization, which converts audio signals into discrete tokens, thereby facilitating the application of language model architectures to the audio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and semantically rich audio codec tokenizer for audio language models. Prior methods, such as Encodec, typically encode individual audio frames into discrete tokens without considering the use of context information across frames. Unlike these methods, we introduce a novel query-based compression strategy to capture holistic information with a set of learnable query tokens by explicitly modeling the context information across frames. This design not only enables the codec model to capture more semantic information but also encodes the audio signal with fewer token sequences. Additionally, to enhance the semantic information in audio codec models, we introduce the following: (1) A masked autoencoder (MAE) loss, (2) Vector quantization based on semantic priors, and (3) An autoregressive (AR) prediction loss. As a result, ALMTokenizer achieves competitive reconstruction performance relative to state-of-the-art approaches while operating at a lower bitrate. Within the same audio language model framework, ALMTokenizer outperforms previous tokenizers in audio understanding and generation tasks.",
    "metadata": {
      "arxiv_id": "2504.10344",
      "title": "ALMTokenizer: A Low-bitrate and Semantic-rich Audio Codec Tokenizer for Audio Language Modeling",
      "summary": "Recent advancements in audio language models have underscored the pivotal role of audio tokenization, which converts audio signals into discrete tokens, thereby facilitating the application of language model architectures to the audio domain. In this study, we introduce ALMTokenizer, a novel low-bitrate and semantically rich audio codec tokenizer for audio language models. Prior methods, such as Encodec, typically encode individual audio frames into discrete tokens without considering the use of context information across frames. Unlike these methods, we introduce a novel query-based compression strategy to capture holistic information with a set of learnable query tokens by explicitly modeling the context information across frames. This design not only enables the codec model to capture more semantic information but also encodes the audio signal with fewer token sequences. Additionally, to enhance the semantic information in audio codec models, we introduce the following: (1) A masked autoencoder (MAE) loss, (2) Vector quantization based on semantic priors, and (3) An autoregressive (AR) prediction loss. As a result, ALMTokenizer achieves competitive reconstruction performance relative to state-of-the-art approaches while operating at a lower bitrate. Within the same audio language model framework, ALMTokenizer outperforms previous tokenizers in audio understanding and generation tasks.",
      "authors": [
        "Dongchao Yang",
        "Songxiang Liu",
        "Haohan Guo",
        "Jiankun Zhao",
        "Yuanyuan Wang",
        "Helin Wang",
        "Zeqian Ju",
        "Xubo Liu",
        "Xueyuan Chen",
        "Xu Tan",
        "Xixin Wu",
        "Helen Meng"
      ],
      "published": "2025-04-14T15:51:56Z",
      "updated": "2025-04-14T15:51:56Z",
      "categories": [
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.10344v1",
      "landing_url": "https://arxiv.org/abs/2504.10344v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.10344"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Work explicitly introduces a codec tokenizer that converts audio signals into discrete, semantic tokens with bitrate and reconstruction evaluations, matching the inclusion criteria with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Work explicitly introduces a codec tokenizer that converts audio signals into discrete, semantic tokens with bitrate and reconstruction evaluations, matching the inclusion criteria with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning",
    "abstract": "This paper addresses the joint transceiver design, including pilot transmission, channel feature extraction and feedback, as well as precoding, for low-overhead downlink massive multiple-input multiple-output (MIMO) communication in frequency-division duplex (FDD) systems. Although deep learning (DL) has shown great potential in tackling this problem, existing methods often suffer from poor scalability in practical systems, as the solution obtained in the training phase merely works for a fixed feedback capacity and a fixed number of users in the deployment phase. To address this limitation, we propose a novel DL-based framework comprised of choreographed neural networks, which can utilize one training phase to generate all the transceiver solutions used in the deployment phase with varying sizes of feedback codebooks and numbers of users. The proposed framework includes a residual vector-quantized variational autoencoder (RVQ-VAE) for efficient channel feedback and an edge graph attention network (EGAT) for robust multiuser precoding. It can adapt to different feedback capacities by flexibly adjusting the RVQ codebook sizes using the hierarchical codebook structure, and scale with the number of users through a feedback module sharing scheme and the inherent scalability of EGAT. Moreover, a progressive training strategy is proposed to further enhance data transmission performance and generalization capability. Numerical results on a real-world dataset demonstrate the superior scalability and performance of our approach over existing methods.",
    "metadata": {
      "arxiv_id": "2504.11162",
      "title": "Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning",
      "summary": "This paper addresses the joint transceiver design, including pilot transmission, channel feature extraction and feedback, as well as precoding, for low-overhead downlink massive multiple-input multiple-output (MIMO) communication in frequency-division duplex (FDD) systems. Although deep learning (DL) has shown great potential in tackling this problem, existing methods often suffer from poor scalability in practical systems, as the solution obtained in the training phase merely works for a fixed feedback capacity and a fixed number of users in the deployment phase. To address this limitation, we propose a novel DL-based framework comprised of choreographed neural networks, which can utilize one training phase to generate all the transceiver solutions used in the deployment phase with varying sizes of feedback codebooks and numbers of users. The proposed framework includes a residual vector-quantized variational autoencoder (RVQ-VAE) for efficient channel feedback and an edge graph attention network (EGAT) for robust multiuser precoding. It can adapt to different feedback capacities by flexibly adjusting the RVQ codebook sizes using the hierarchical codebook structure, and scale with the number of users through a feedback module sharing scheme and the inherent scalability of EGAT. Moreover, a progressive training strategy is proposed to further enhance data transmission performance and generalization capability. Numerical results on a real-world dataset demonstrate the superior scalability and performance of our approach over existing methods.",
      "authors": [
        "Lin Zhu",
        "Weifeng Zhu",
        "Shuowen Zhang",
        "Shuguang Cui",
        "Liang Liu"
      ],
      "published": "2025-04-15T13:11:26Z",
      "updated": "2025-04-15T13:11:26Z",
      "categories": [
        "eess.SP",
        "cs.IT"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.11162v1",
      "landing_url": "https://arxiv.org/abs/2504.11162v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.11162"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe scalable DL-based transceiver design for FDD massive MIMO system (communications, channel feedback, precoding) with no mention of discrete audio tokens or codec/tokenizer design, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe scalable DL-based transceiver design for FDD massive MIMO system (communications, channel feedback, precoding) with no mention of discrete audio tokens or codec/tokenizer design, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction",
    "abstract": "Moiré patterns, resulting from aliasing between object light signals and camera sampling frequencies, often degrade image quality during capture. Traditional demoiréing methods have generally treated images as a whole for processing and training, neglecting the unique signal characteristics of different color channels. Moreover, the randomness and variability of moiré pattern generation pose challenges to the robustness of existing methods when applied to real-world data. To address these issues, this paper presents SIDME (Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction), a novel model designed to generate high-quality visual images by effectively processing moiré patterns. SIDME combines a masked encoder-decoder architecture with self-supervised learning, allowing the model to reconstruct images using the inherent properties of camera sampling frequencies. A key innovation is the random masked image reconstructor, which utilizes an encoder-decoder structure to handle the reconstruction task. Furthermore, since the green channel in camera sampling has a higher sampling frequency compared to red and blue channels, a specialized self-supervised loss function is designed to improve the training efficiency and effectiveness. To ensure the generalization ability of the model, a self-supervised moiré image generation method has been developed to produce a dataset that closely mimics real-world conditions. Extensive experiments demonstrate that SIDME outperforms existing methods in processing real moiré pattern data, showing its superior generalization performance and robustness.",
    "metadata": {
      "arxiv_id": "2504.12245",
      "title": "SIDME: Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction",
      "summary": "Moiré patterns, resulting from aliasing between object light signals and camera sampling frequencies, often degrade image quality during capture. Traditional demoiréing methods have generally treated images as a whole for processing and training, neglecting the unique signal characteristics of different color channels. Moreover, the randomness and variability of moiré pattern generation pose challenges to the robustness of existing methods when applied to real-world data. To address these issues, this paper presents SIDME (Self-supervised Image Demoiréing via Masked Encoder-Decoder Reconstruction), a novel model designed to generate high-quality visual images by effectively processing moiré patterns. SIDME combines a masked encoder-decoder architecture with self-supervised learning, allowing the model to reconstruct images using the inherent properties of camera sampling frequencies. A key innovation is the random masked image reconstructor, which utilizes an encoder-decoder structure to handle the reconstruction task. Furthermore, since the green channel in camera sampling has a higher sampling frequency compared to red and blue channels, a specialized self-supervised loss function is designed to improve the training efficiency and effectiveness. To ensure the generalization ability of the model, a self-supervised moiré image generation method has been developed to produce a dataset that closely mimics real-world conditions. Extensive experiments demonstrate that SIDME outperforms existing methods in processing real moiré pattern data, showing its superior generalization performance and robustness.",
      "authors": [
        "Xia Wang",
        "Haiyang Sun",
        "Tiantian Cao",
        "Yueying Sun",
        "Min Feng"
      ],
      "published": "2025-04-16T16:50:41Z",
      "updated": "2025-04-16T16:50:41Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12245v1",
      "landing_url": "https://arxiv.org/abs/2504.12245v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12245"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on self-supervised image demoiréing with masked encoder-decoder reconstruction and does not address discrete audio tokens, tokenization, or quantized audio representations, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on self-supervised image demoiréing with masked encoder-decoder reconstruction and does not address discrete audio tokens, tokenization, or quantized audio representations, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate",
    "abstract": "Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.",
    "metadata": {
      "arxiv_id": "2504.12259",
      "title": "VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate",
      "summary": "Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.",
      "authors": [
        "Zhihang Yuan",
        "Rui Xie",
        "Yuzhang Shang",
        "Hanling Zhang",
        "Siyuan Wang",
        "Shengen Yan",
        "Guohao Dai",
        "Yu Wang"
      ],
      "published": "2025-04-16T17:09:13Z",
      "updated": "2025-04-16T17:09:13Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12259v1",
      "landing_url": "https://arxiv.org/abs/2504.12259v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.12259"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on diffusion-based video generation and does not address any discrete audio tokenization, codec, or quantization topics required by the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on diffusion-based video generation and does not address any discrete audio tokenization, codec, or quantization topics required by the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
    "abstract": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
    "metadata": {
      "arxiv_id": "2504.12339",
      "title": "GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM",
      "summary": "While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.",
      "authors": [
        "Yaodong Song",
        "Hongjie Chen",
        "Jie Lian",
        "Yuxin Zhang",
        "Guangmin Xia",
        "Zehan Li",
        "Genliang Zhao",
        "Jian Kang",
        "Jie Li",
        "Yongxiang Li",
        "Xuelong Li"
      ],
      "published": "2025-04-15T01:44:56Z",
      "updated": "2025-05-28T14:24:12Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12339v2",
      "landing_url": "https://arxiv.org/abs/2504.12339v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.12339"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "GOAT-TTS clearly operates on discrete speech tokens (modality-alignment branch and speech-generation branch predict speech tokens and account for quantization), so it fits the discrete audio token theme even if details are sparse; include it (better to include)."
    },
    "round-A_JuniorNano_reasoning": "GOAT-TTS clearly operates on discrete speech tokens (modality-alignment branch and speech-generation branch predict speech tokens and account for quantization), so it fits the discrete audio token theme even if details are sparse; include it (better to include).",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
    "abstract": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
    "metadata": {
      "arxiv_id": "2504.12715",
      "title": "Hierarchical Vector Quantized Graph Autoencoder with Annealing-Based Code Selection",
      "summary": "Graph self-supervised learning has gained significant attention recently. However, many existing approaches heavily depend on perturbations, and inappropriate perturbations may corrupt the graph's inherent information. The Vector Quantized Variational Autoencoder (VQ-VAE) is a powerful autoencoder extensively used in fields such as computer vision; however, its application to graph data remains underexplored. In this paper, we provide an empirical analysis of vector quantization in the context of graph autoencoders, demonstrating its significant enhancement of the model's capacity to capture graph topology. Furthermore, we identify two key challenges associated with vector quantization when applying in graph data: codebook underutilization and codebook space sparsity. For the first challenge, we propose an annealing-based encoding strategy that promotes broad code utilization in the early stages of training, gradually shifting focus toward the most effective codes as training progresses. For the second challenge, we introduce a hierarchical two-layer codebook that captures relationships between embeddings through clustering. The second layer codebook links similar codes, encouraging the model to learn closer embeddings for nodes with similar features and structural topology in the graph. Our proposed model outperforms 16 representative baseline methods in self-supervised link prediction and node classification tasks across multiple datasets.",
      "authors": [
        "Long Zeng",
        "Jianxiang Yu",
        "Jiapeng Zhu",
        "Qingsong Zhong",
        "Xiang Li"
      ],
      "published": "2025-04-17T07:43:52Z",
      "updated": "2025-04-17T07:43:52Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.12715v1",
      "landing_url": "https://arxiv.org/abs/2504.12715v1",
      "doi": "https://doi.org/10.1145/3696410.3714656"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on graph autoencoders and vector quantization for graph topology rather than discrete audio token generation/quantization, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on graph autoencoders and vector quantization for graph topology rather than discrete audio token generation/quantization, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
    "abstract": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
    "metadata": {
      "arxiv_id": "2504.14075",
      "title": "Towards Scale-Aware Low-Light Enhancement via Structure-Guided Transformer Design",
      "summary": "Current Low-light Image Enhancement (LLIE) techniques predominantly rely on either direct Low-Light (LL) to Normal-Light (NL) mappings or guidance from semantic features or illumination maps. Nonetheless, the intrinsic ill-posedness of LLIE and the difficulty in retrieving robust semantics from heavily corrupted images hinder their effectiveness in extremely low-light environments. To tackle this challenge, we present SG-LLIE, a new multi-scale CNN-Transformer hybrid framework guided by structure priors. Different from employing pre-trained models for the extraction of semantics or illumination maps, we choose to extract robust structure priors based on illumination-invariant edge detectors. Moreover, we develop a CNN-Transformer Hybrid Structure-Guided Feature Extractor (HSGFE) module at each scale with in the UNet encoder-decoder architecture. Besides the CNN blocks which excels in multi-scale feature extraction and fusion, we introduce a Structure-Guided Transformer Block (SGTB) in each HSGFE that incorporates structural priors to modulate the enhancement process. Extensive experiments show that our method achieves state-of-the-art performance on several LLIE benchmarks in both quantitative metrics and visual quality. Our solution ranks second in the NTIRE 2025 Low-Light Enhancement Challenge. Code is released at https://github.com/minyan8/imagine.",
      "authors": [
        "Wei Dong",
        "Yan Min",
        "Han Zhou",
        "Jun Chen"
      ],
      "published": "2025-04-18T20:57:16Z",
      "updated": "2025-04-18T20:57:16Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14075v1",
      "landing_url": "https://arxiv.org/abs/2504.14075v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14075"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on image enhancement and contains no discussion of discrete audio tokens, tokenizers, or quantized vocabularies, so it fails to meet the audio-token inclusion criteria and must be excluded."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on image enhancement and contains no discussion of discrete audio tokens, tokenizers, or quantized vocabularies, so it fails to meet the audio-token inclusion criteria and must be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
    "abstract": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
    "metadata": {
      "arxiv_id": "2504.14092",
      "title": "Retinex-guided Histogram Transformer for Mask-free Shadow Removal",
      "summary": "While deep learning methods have achieved notable progress in shadow removal, many existing approaches rely on shadow masks that are difficult to obtain, limiting their generalization to real-world scenes. In this work, we propose ReHiT, an efficient mask-free shadow removal framework based on a hybrid CNN-Transformer architecture guided by Retinex theory. We first introduce a dual-branch pipeline to separately model reflectance and illumination components, and each is restored by our developed Illumination-Guided Hybrid CNN-Transformer (IG-HCT) module. Second, besides the CNN-based blocks that are capable of learning residual dense features and performing multi-scale semantic fusion, multi-scale semantic fusion, we develop the Illumination-Guided Histogram Transformer Block (IGHB) to effectively handle non-uniform illumination and spatially complex shadows. Extensive experiments on several benchmark datasets validate the effectiveness of our approach over existing mask-free methods. Trained solely on the NTIRE 2025 Shadow Removal Challenge dataset, our solution delivers competitive results with one of the smallest parameter sizes and fastest inference speeds among top-ranked entries, highlighting its applicability for real-world applications with limited computational resources. The code is available at https://github.com/dongw22/oath.",
      "authors": [
        "Wei Dong",
        "Han Zhou",
        "Seyed Amirreza Mousavi",
        "Jun Chen"
      ],
      "published": "2025-04-18T22:19:40Z",
      "updated": "2025-04-18T22:19:40Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14092v1",
      "landing_url": "https://arxiv.org/abs/2504.14092v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14092"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about shadow removal in images, not discrete audio tokens or tokenizers, so it fails the topic inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "Paper is about shadow removal in images, not discrete audio tokens or tokenizers, so it fails the topic inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Lightweight Road Environment Segmentation using Vector Quantization",
    "abstract": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
    "metadata": {
      "arxiv_id": "2504.14113",
      "title": "Lightweight Road Environment Segmentation using Vector Quantization",
      "summary": "Road environment segmentation plays a significant role in autonomous driving. Numerous works based on Fully Convolutional Networks (FCNs) and Transformer architectures have been proposed to leverage local and global contextual learning for efficient and accurate semantic segmentation. In both architectures, the encoder often relies heavily on extracting continuous representations from the image, which limits the ability to represent meaningful discrete information. To address this limitation, we propose segmentation of the autonomous driving environment using vector quantization. Vector quantization offers three primary advantages for road environment segmentation. (1) Each continuous feature from the encoder is mapped to a discrete vector from the codebook, helping the model discover distinct features more easily than with complex continuous features. (2) Since a discrete feature acts as compressed versions of the encoder's continuous features, they also compress noise or outliers, enhancing the image segmentation task. (3) Vector quantization encourages the latent space to form coarse clusters of continuous features, forcing the model to group similar features, making the learned representations more structured for the decoding process. In this work, we combined vector quantization with the lightweight image segmentation model MobileUNETR and used it as a baseline model for comparison to demonstrate its efficiency. Through experiments, we achieved 77.0 % mIoU on Cityscapes, outperforming the baseline by 2.9 % without increasing the model's initial size or complexity.",
      "authors": [
        "Jiyong Kwag",
        "Alper Yilmaz",
        "Charles Toth"
      ],
      "published": "2025-04-19T00:13:21Z",
      "updated": "2025-04-19T00:13:21Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.14113v1",
      "landing_url": "https://arxiv.org/abs/2504.14113v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.14113"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses vector quantization for image segmentation in autonomous driving with no link to audio discrete tokens or codec/tokenizer design, so it fails the inclusion criteria and matches the exclusion criteria for non-audio discrete representations."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses vector quantization for image segmentation in autonomous driving with no link to audio discrete tokens or codec/tokenizer design, so it fails the inclusion criteria and matches the exclusion criteria for non-audio discrete representations.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Switchable Priors for Neural Image Compression",
    "abstract": "Neural image compression (NIC) usually adopts a predefined family of probabilistic distributions as the prior of the latent variables, and meanwhile relies on entropy models to estimate the parameters for the probabilistic family. More complex probabilistic distributions may fit the latent variables more accurately, but also incur higher complexity of the entropy models, limiting their practical value. To address this dilemma, we propose a solution to decouple the entropy model complexity from the prior distributions. We use a finite set of trainable priors that correspond to samples of the parametric probabilistic distributions. We train the entropy model to predict the index of the appropriate prior within the set, rather than the specific parameters. Switching between the trained priors further enables us to embrace a skip mode into the prior set, which simply omits a latent variable during the entropy coding. To demonstrate the practical value of our solution, we present a lightweight NIC model, namely FastNIC, together with the learning of switchable priors. FastNIC obtains a better trade-off between compression efficiency and computational complexity for neural image compression. We also implanted the switchable priors into state-of-the-art NIC models and observed improved compression efficiency with a significant reduction of entropy coding complexity.",
    "metadata": {
      "arxiv_id": "2504.16586",
      "title": "Learning Switchable Priors for Neural Image Compression",
      "summary": "Neural image compression (NIC) usually adopts a predefined family of probabilistic distributions as the prior of the latent variables, and meanwhile relies on entropy models to estimate the parameters for the probabilistic family. More complex probabilistic distributions may fit the latent variables more accurately, but also incur higher complexity of the entropy models, limiting their practical value. To address this dilemma, we propose a solution to decouple the entropy model complexity from the prior distributions. We use a finite set of trainable priors that correspond to samples of the parametric probabilistic distributions. We train the entropy model to predict the index of the appropriate prior within the set, rather than the specific parameters. Switching between the trained priors further enables us to embrace a skip mode into the prior set, which simply omits a latent variable during the entropy coding. To demonstrate the practical value of our solution, we present a lightweight NIC model, namely FastNIC, together with the learning of switchable priors. FastNIC obtains a better trade-off between compression efficiency and computational complexity for neural image compression. We also implanted the switchable priors into state-of-the-art NIC models and observed improved compression efficiency with a significant reduction of entropy coding complexity.",
      "authors": [
        "Haotian Zhang",
        "Yuqi Li",
        "Li Li",
        "Dong Liu"
      ],
      "published": "2025-04-23T10:06:58Z",
      "updated": "2025-04-23T10:06:58Z",
      "categories": [
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.16586v1",
      "landing_url": "https://arxiv.org/abs/2504.16586v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.16586"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on neural image compression priors for latent variables in image codecs, so it does not study discrete audio tokens or tokenizers for audio signals, therefore it fails the inclusion criteria entirely."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on neural image compression priors for latent variables in image codecs, so it does not study discrete audio tokens or tokenizers for audio signals, therefore it fails the inclusion criteria entirely.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech",
    "abstract": "Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication.",
    "metadata": {
      "arxiv_id": "2504.20447",
      "title": "APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech",
      "summary": "Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication.",
      "authors": [
        "Zhicheng Lian",
        "Lizhi Wang",
        "Hua Huang"
      ],
      "published": "2025-04-29T05:45:09Z",
      "updated": "2025-04-29T05:45:09Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.20447v1",
      "landing_url": "https://arxiv.org/abs/2504.20447v1",
      "doi": "https://doi.org/10.48550/arXiv.2504.20447"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "APG-MOS focuses on auditory-inspired MOS prediction via perceptual modeling and semantic distortion, without proposing or analyzing any discrete audio tokenization/quantization pipeline or token representations, so it fails the discrete-token inclusion criteria and should likely be excluded."
    },
    "round-A_JuniorNano_reasoning": "APG-MOS focuses on auditory-inspired MOS prediction via perceptual modeling and semantic distortion, without proposing or analyzing any discrete audio tokenization/quantization pipeline or token representations, so it fails the discrete-token inclusion criteria and should likely be excluded.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
    "abstract": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
    "metadata": {
      "arxiv_id": "2504.20629",
      "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation",
      "summary": "In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT.",
      "authors": [
        "Jeongsoo Choi",
        "Ji-Hoon Kim",
        "Kim Sung-Bin",
        "Tae-Hyun Oh",
        "Joon Son Chung"
      ],
      "published": "2025-04-29T10:56:24Z",
      "updated": "2025-10-03T10:54:34Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.MM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2504.20629v2",
      "landing_url": "https://arxiv.org/abs/2504.20629v2",
      "doi": "https://doi.org/10.48550/arXiv.2504.20629"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "AlignDiT focuses on diffusion-based multimodal speech synthesis without any mention of discrete audio tokenization, codecs, or quantized vocabularies, so it fails the core token-centric inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "AlignDiT focuses on diffusion-based multimodal speech synthesis without any mention of discrete audio tokenization, codecs, or quantized vocabularies, so it fails the core token-centric inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation",
    "abstract": "While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.",
    "metadata": {
      "arxiv_id": "2505.03273",
      "title": "SepALM: Audio Language Models Are Error Correctors for Robust Speech Separation",
      "summary": "While contemporary speech separation technologies adeptly process lengthy mixed audio waveforms, they are frequently challenged by the intricacies of real-world environments, including noisy and reverberant settings, which can result in artifacts or distortions in the separated speech. To overcome these limitations, we introduce SepALM, a pioneering approach that employs audio language models (ALMs) to rectify and re-synthesize speech within the text domain following preliminary separation. SepALM comprises four core components: a separator, a corrector, a synthesizer, and an aligner. By integrating an ALM-based end-to-end error correction mechanism, we mitigate the risk of error accumulation and circumvent the optimization hurdles typically encountered in conventional methods that amalgamate automatic speech recognition (ASR) with large language models (LLMs). Additionally, we have developed Chain-of-Thought (CoT) prompting and knowledge distillation techniques to facilitate the reasoning and training processes of the ALM. Our experiments substantiate that SepALM not only elevates the precision of speech separation but also markedly bolsters adaptability in novel acoustic environments.",
      "authors": [
        "Zhaoxi Mu",
        "Xinyu Yang",
        "Gang Wang"
      ],
      "published": "2025-05-06T08:04:37Z",
      "updated": "2025-05-26T07:01:19Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.03273v2",
      "landing_url": "https://arxiv.org/abs/2505.03273v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.03273"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The work focuses on speech separation correction via audio language models and CoT prompting without discussing discrete audio token generation, quantization, vocabularies, or token-level modeling, so it fails the inclusion requirements and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The work focuses on speech separation correction via audio language models and CoT prompting without discussing discrete audio token generation, quantization, vocabularies, or token-level modeling, so it fails the inclusion requirements and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
    "abstract": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
    "metadata": {
      "arxiv_id": "2505.03682",
      "title": "Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays",
      "summary": "Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.",
      "authors": [
        "Christopher Flöter",
        "Sergej Geringer",
        "Guido Reina",
        "Daniel Weiskopf",
        "Timo Ropinski"
      ],
      "published": "2025-05-06T16:27:47Z",
      "updated": "2025-05-06T16:27:47Z",
      "categories": [
        "cs.HC",
        "cs.GR"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.03682v1",
      "landing_url": "https://arxiv.org/abs/2505.03682v1",
      "doi": "https://doi.org/10.1145/3715669.3725870"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This VR foveated frame-rate study has nothing to do with encoding audio into discrete tokens or tokenizer/quantization methods, so it fails to meet the topic inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This VR foveated frame-rate study has nothing to do with encoding audio into discrete tokens or tokenizer/quantization methods, so it fails to meet the topic inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
    "abstract": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
    "metadata": {
      "arxiv_id": "2505.05159",
      "title": "FlexSpeech: Towards Stable, Controllable and Expressive Text-to-Speech",
      "summary": "Current speech generation research can be categorized into two primary classes: non-autoregressive and autoregressive. The fundamental distinction between these approaches lies in the duration prediction strategy employed for predictable-length sequences. The NAR methods ensure stability in speech generation by explicitly and independently modeling the duration of each phonetic unit. Conversely, AR methods employ an autoregressive paradigm to predict the compressed speech token by implicitly modeling duration with Markov properties. Although this approach improves prosody, it does not provide the structural guarantees necessary for stability. To simultaneously address the issues of stability and naturalness in speech generation, we propose FlexSpeech, a stable, controllable, and expressive TTS model. The motivation behind FlexSpeech is to incorporate Markov dependencies and preference optimization directly on the duration predictor to boost its naturalness while maintaining explicit modeling of the phonetic units to ensure stability. Specifically, we decompose the speech generation task into two components: an AR duration predictor and a NAR acoustic model. The acoustic model is trained on a substantial amount of data to learn to render audio more stably, given reference audio prosody and phone durations. The duration predictor is optimized in a lightweight manner for different stylistic variations, thereby enabling rapid style transfer while maintaining a decoupled relationship with the specified speaker timbre. Experimental results demonstrate that our approach achieves SOTA stability and naturalness in zero-shot TTS. More importantly, when transferring to a specific stylistic domain, we can accomplish lightweight optimization of the duration module solely with about 100 data samples, without the need to adjust the acoustic model, thereby enabling rapid and stable style transfer.",
      "authors": [
        "Linhan Ma",
        "Dake Guo",
        "He Wang",
        "Jin Xu",
        "Lei Xie"
      ],
      "published": "2025-05-08T11:55:19Z",
      "updated": "2025-05-15T08:28:37Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05159v3",
      "landing_url": "https://arxiv.org/abs/2505.05159v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.05159"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "FlexSpeech focuses on duration prediction and stability/control in TTS but lacks any mention of discrete audio tokenization, quantization, vocabularies, or token-level modeling, so it does not satisfy the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "FlexSpeech focuses on duration prediction and stability/control in TTS but lacks any mention of discrete audio tokenization, quantization, vocabularies, or token-level modeling, so it does not satisfy the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization",
    "abstract": "Recent advances in Protein Structure Prediction Models (PPMs), such as AlphaFold2 and ESMFold, have revolutionized computational biology by achieving unprecedented accuracy in predicting three-dimensional protein folding structures. However, these models face significant scalability challenges, particularly when processing proteins with long amino acid sequences (e.g., sequence length > 1,000). The primary bottleneck that arises from the exponential growth in activation sizes is driven by the unique data structure in PPM, which introduces an additional dimension that leads to substantial memory and computational demands. These limitations have hindered the effective scaling of PPM for real-world applications, such as analyzing large proteins or complex multimers with critical biological and pharmaceutical relevance. In this paper, we present LightNobel, the first hardware-software co-designed accelerator developed to overcome scalability limitations on the sequence length in PPM. At the software level, we propose Token-wise Adaptive Activation Quantization (AAQ), which leverages unique token-wise characteristics, such as distogram patterns in PPM activations, to enable fine-grained quantization techniques without compromising accuracy. At the hardware level, LightNobel integrates the multi-precision reconfigurable matrix processing unit (RMPU) and versatile vector processing unit (VVPU) to enable the efficient execution of AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100 GPUs, respectively, while maintaining negligible accuracy loss. It also reduces the peak memory requirement up to 120.05x in PPM, enabling scalable processing for proteins with long sequences.",
    "metadata": {
      "arxiv_id": "2505.05893",
      "title": "LightNobel: Improving Sequence Length Limitation in Protein Structure Prediction Model via Adaptive Activation Quantization",
      "summary": "Recent advances in Protein Structure Prediction Models (PPMs), such as AlphaFold2 and ESMFold, have revolutionized computational biology by achieving unprecedented accuracy in predicting three-dimensional protein folding structures. However, these models face significant scalability challenges, particularly when processing proteins with long amino acid sequences (e.g., sequence length > 1,000). The primary bottleneck that arises from the exponential growth in activation sizes is driven by the unique data structure in PPM, which introduces an additional dimension that leads to substantial memory and computational demands. These limitations have hindered the effective scaling of PPM for real-world applications, such as analyzing large proteins or complex multimers with critical biological and pharmaceutical relevance.\n  In this paper, we present LightNobel, the first hardware-software co-designed accelerator developed to overcome scalability limitations on the sequence length in PPM. At the software level, we propose Token-wise Adaptive Activation Quantization (AAQ), which leverages unique token-wise characteristics, such as distogram patterns in PPM activations, to enable fine-grained quantization techniques without compromising accuracy. At the hardware level, LightNobel integrates the multi-precision reconfigurable matrix processing unit (RMPU) and versatile vector processing unit (VVPU) to enable the efficient execution of AAQ. Through these innovations, LightNobel achieves up to 8.44x, 8.41x speedup and 37.29x, 43.35x higher power efficiency over the latest NVIDIA A100 and H100 GPUs, respectively, while maintaining negligible accuracy loss. It also reduces the peak memory requirement up to 120.05x in PPM, enabling scalable processing for proteins with long sequences.",
      "authors": [
        "Seunghee Han",
        "Soongyu Choi",
        "Joo-Young Kim"
      ],
      "published": "2025-05-09T09:01:10Z",
      "updated": "2025-05-09T09:01:10Z",
      "categories": [
        "cs.AR",
        "cs.AI",
        "cs.ET",
        "cs.LG",
        "q-bio.BM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.05893v1",
      "landing_url": "https://arxiv.org/abs/2505.05893v1",
      "doi": "https://doi.org/10.1145/3695053.3731006"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on protein structure prediction hardware/quantization rather than discrete audio token generation or evaluation, so it fails all inclusion criteria and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on protein structure prediction hardware/quantization rather than discrete audio token generation or evaluation, so it fails all inclusion criteria and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding",
    "abstract": "Achieving high-fidelity audio compression while preserving perceptual quality across diverse content remains a key challenge in Neural Audio Coding (NAC). We introduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC) framework that leverages psychoacoustically guided multi-band frequency reconstruction. At its core is a Multi-Band Spectral Residual Vector Quantization (MBS-RVQ) module that allocates bitrate across frequency bands based on perceptual salience. This design enables efficient compression while disentangling speaker identity from content using distinct codebooks. MUFFIN incorporates a transformer-inspired convolutional backbone and a modified snake activation to enhance resolution in fine-grained spectral regions. Experimental results on multiple benchmarks demonstrate that MUFFIN consistently outperforms existing approaches in reconstruction quality. A high-compression variant achieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves effective in downstream generative tasks, highlighting its promise as a token representation for integration with language models. Audio samples and code are available.",
    "metadata": {
      "arxiv_id": "2505.07235",
      "title": "Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding",
      "summary": "Achieving high-fidelity audio compression while preserving perceptual quality across diverse content remains a key challenge in Neural Audio Coding (NAC). We introduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC) framework that leverages psychoacoustically guided multi-band frequency reconstruction. At its core is a Multi-Band Spectral Residual Vector Quantization (MBS-RVQ) module that allocates bitrate across frequency bands based on perceptual salience. This design enables efficient compression while disentangling speaker identity from content using distinct codebooks. MUFFIN incorporates a transformer-inspired convolutional backbone and a modified snake activation to enhance resolution in fine-grained spectral regions. Experimental results on multiple benchmarks demonstrate that MUFFIN consistently outperforms existing approaches in reconstruction quality. A high-compression variant achieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves effective in downstream generative tasks, highlighting its promise as a token representation for integration with language models. Audio samples and code are available.",
      "authors": [
        "Dianwen Ng",
        "Kun Zhou",
        "Yi-Wen Chao",
        "Zhiwei Xiong",
        "Bin Ma",
        "Eng Siong Chng"
      ],
      "published": "2025-05-12T05:20:43Z",
      "updated": "2025-05-12T05:20:43Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.07235v1",
      "landing_url": "https://arxiv.org/abs/2505.07235v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.07235"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Abstract describes MUFFIN using multi-band spectral residual VQ with discrete codebooks for neural audio coding, including evaluation and token-based compression decisions, so it clearly satisfies inclusion while meeting no exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes MUFFIN using multi-band spectral residual VQ with discrete codebooks for neural audio coding, including evaluation and token-based compression decisions, so it clearly satisfies inclusion while meeting no exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
    "abstract": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
    "metadata": {
      "arxiv_id": "2505.09407",
      "title": "Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits",
      "summary": "Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.",
      "authors": [
        "Subrit Dikshit",
        "Ritu Tiwari",
        "Priyank Jain"
      ],
      "published": "2025-05-14T14:04:44Z",
      "updated": "2025-05-14T14:04:44Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.09407v1",
      "landing_url": "https://arxiv.org/abs/2505.09407v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.09407"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on quantum machine translation for multilingual text and makes no mention of discrete audio-tokenization, codec/quantization, or any audio signal processing, so it fails every inclusion item and hits the exclusion criteria for not targeting audio tokens."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on quantum machine translation for multilingual text and makes no mention of discrete audio-tokenization, codec/quantization, or any audio signal processing, so it fails every inclusion item and hits the exclusion criteria for not targeting audio tokens.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
    "abstract": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
    "metadata": {
      "arxiv_id": "2505.11391",
      "title": "LipDiffuser: Lip-to-Speech Generation with Conditional Diffusion Models",
      "summary": "We present LipDiffuser, a conditional diffusion model for lip-to-speech generation synthesizing natural and intelligible speech directly from silent video recordings. Our approach leverages the magnitude-preserving ablated diffusion model (MP-ADM) architecture as a denoiser model. To effectively condition the model, we incorporate visual features using magnitude-preserving feature-wise linear modulation (MP-FiLM) alongside speaker embeddings. A neural vocoder then reconstructs the speech waveform from the generated mel-spectrograms. Evaluations on LRS3 demonstrate that LipDiffuser outperforms existing lip-to-speech baselines in perceptual speech quality and speaker similarity, while remaining competitive in downstream automatic speech recognition. These findings are also supported by a formal listening experiment.",
      "authors": [
        "Julius Richter",
        "Danilo de Oliveira",
        "Tal Peer",
        "Timo Gerkmann"
      ],
      "published": "2025-05-16T15:56:07Z",
      "updated": "2025-10-24T13:26:17Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11391v3",
      "landing_url": "https://arxiv.org/abs/2505.11391v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.11391"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "LipDiffuser focuses on continuous mel-spectrogram diffusion and vocoder-based lip-to-speech without describing any discrete audio token/tokenizer design or codebook quantization, so it fails to meet the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "LipDiffuser focuses on continuous mel-spectrogram diffusion and vocoder-based lip-to-speech without describing any discrete audio token/tokenizer design or codebook quantization, so it fails to meet the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Flash Invariant Point Attention",
    "abstract": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.",
    "metadata": {
      "arxiv_id": "2505.11580",
      "title": "Flash Invariant Point Attention",
      "summary": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa.",
      "authors": [
        "Andrew Liu",
        "Axel Elaldi",
        "Nicholas T Franklin",
        "Nathan Russell",
        "Gurinder S Atwal",
        "Yih-En A Ban",
        "Olivia Viessmann"
      ],
      "published": "2025-05-16T16:19:05Z",
      "updated": "2025-05-16T16:19:05Z",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-bio.BM"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11580v1",
      "landing_url": "https://arxiv.org/abs/2505.11580v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.11580"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is about geometry-aware protein modeling and FlashIPA rather than discrete audio token generation or quantization, so it fails to meet any of the inclusion criteria centered on discrete audio tokens and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper is about geometry-aware protein modeling and FlashIPA rather than discrete audio token generation or quantization, so it fails to meet any of the inclusion criteria centered on discrete audio tokens and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
    "abstract": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
    "metadata": {
      "arxiv_id": "2505.11918",
      "title": "Transformers as Unsupervised Learning Algorithms: A study on Gaussian Mixtures",
      "summary": "The transformer architecture has demonstrated remarkable capabilities in modern artificial intelligence, among which the capability of implicitly learning an internal model during inference time is widely believed to play a key role in the under standing of pre-trained large language models. However, most recent works have been focusing on studying supervised learning topics such as in-context learning, leaving the field of unsupervised learning largely unexplored. This paper investigates the capabilities of transformers in solving Gaussian Mixture Models (GMMs), a fundamental unsupervised learning problem through the lens of statistical estimation. We propose a transformer-based learning framework called TGMM that simultaneously learns to solve multiple GMM tasks using a shared transformer backbone. The learned models are empirically demonstrated to effectively mitigate the limitations of classical methods such as Expectation-Maximization (EM) or spectral algorithms, at the same time exhibit reasonable robustness to distribution shifts. Theoretically, we prove that transformers can approximate both the EM algorithm and a core component of spectral methods (cubic tensor power iterations). These results bridge the gap between practical success and theoretical understanding, positioning transformers as versatile tools for unsupervised learning.",
      "authors": [
        "Zhiheng Chen",
        "Ruofan Wu",
        "Guanhua Fang"
      ],
      "published": "2025-05-17T09:02:18Z",
      "updated": "2025-05-17T09:02:18Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.11918v1",
      "landing_url": "https://arxiv.org/abs/2505.11918v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.11918"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on transformers solving Gaussian Mixture Models in general machine learning without any reference to discrete audio tokenization, so it fails to meet the inclusion criteria for audio token research."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on transformers solving Gaussian Mixture Models in general machine learning without any reference to discrete audio tokenization, so it fails to meet the inclusion criteria for audio token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
    "abstract": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
    "metadata": {
      "arxiv_id": "2505.12053",
      "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
      "summary": "Modern video generation frameworks based on Latent Diffusion Models suffer from inefficiencies in tokenization due to the Frame-Proportional Information Assumption. Existing tokenizers provide fixed temporal compression rates, causing the computational cost of the diffusion model to scale linearly with the frame rate. The paper proposes the Duration-Proportional Information Assumption: the upper bound on the information capacity of a video is proportional to the duration rather than the number of frames. Based on this insight, the paper introduces VFRTok, a Transformer-based video tokenizer, that enables variable frame rate encoding and decoding through asymmetric frame rate training between the encoder and decoder. Furthermore, the paper proposes Partial Rotary Position Embeddings (RoPE) to decouple position and content modeling, which groups correlated patches into unified tokens. The Partial RoPE effectively improves content-awareness, enhancing the video generation capability. Benefiting from the compact and continuous spatio-temporal representation, VFRTok achieves competitive reconstruction quality and state-of-the-art generation fidelity while using only 1/8 tokens compared to existing tokenizers. The code and weights are released at: https://github.com/KwaiVGI/VFRTok.",
      "authors": [
        "Tianxiong Zhong",
        "Xingye Tian",
        "Boyuan Jiang",
        "Xuebo Wang",
        "Xin Tao",
        "Pengfei Wan",
        "Zhiwei Zhang"
      ],
      "published": "2025-05-17T15:32:54Z",
      "updated": "2025-09-28T09:51:18Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12053v2",
      "landing_url": "https://arxiv.org/abs/2505.12053v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.12053"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on a video tokenizer for variable frame rate video modeling without addressing discrete audio token generation/quantization, so it fails the inclusion criterion on audio-discrete-token research."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on a video tokenizer for variable frame rate video modeling without addressing discrete audio token generation/quantization, so it fails the inclusion criterion on audio-discrete-token research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
    "abstract": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
    "metadata": {
      "arxiv_id": "2505.12994",
      "title": "Codec-Based Deepfake Source Tracing via Neural Audio Codec Taxonomy",
      "summary": "Recent advances in neural audio codec-based speech generation (CoSG) models have produced remarkably realistic audio deepfakes. We refer to deepfake speech generated by CoSG systems as codec-based deepfake, or CodecFake. Although existing anti-spoofing research on CodecFake predominantly focuses on verifying the authenticity of audio samples, almost no attention was given to tracing the CoSG used in generating these deepfakes. In CodecFake generation, processes such as speech-to-unit encoding, discrete unit modeling, and unit-to-speech decoding are fundamentally based on neural audio codecs. Motivated by this, we introduce source tracing for CodecFake via neural audio codec taxonomy, which dissects neural audio codecs to trace CoSG. Our experimental results on the CodecFake+ dataset provide promising initial evidence for the feasibility of CodecFake source tracing while also highlighting several challenges that warrant further investigation.",
      "authors": [
        "Xuanjun Chen",
        "I-Ming Lin",
        "Lin Zhang",
        "Jiawei Du",
        "Haibin Wu",
        "Hung-yi Lee",
        "Jyh-Shing Roger Jang"
      ],
      "published": "2025-05-19T11:31:32Z",
      "updated": "2025-08-03T07:58:47Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.12994v3",
      "landing_url": "https://arxiv.org/abs/2505.12994v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.12994"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The work analyzes neural audio codec-based discrete units for tracing CoSG deepfake generation, aligning with the discrete audio token focus while providing codec taxonomy insights rather than just continuous features."
    },
    "round-A_JuniorNano_reasoning": "The work analyzes neural audio codec-based discrete units for tracing CoSG deepfake generation, aligning with the discrete audio token focus while providing codec taxonomy insights rather than just continuous features.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
    "abstract": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
    "metadata": {
      "arxiv_id": "2505.13000",
      "title": "DualCodec: A Low-Frame-Rate, Semantically-Enhanced Neural Audio Codec for Speech Generation",
      "summary": "Neural audio codecs form the foundational building blocks for language model (LM)-based speech generation. Typically, there is a trade-off between frame rate and audio quality. This study introduces a low-frame-rate, semantically enhanced codec model. Existing approaches distill semantically rich self-supervised (SSL) representations into the first-layer codec tokens. This work proposes DualCodec, a dual-stream encoding approach that integrates SSL and waveform representations within an end-to-end codec framework. In this setting, DualCodec enhances the semantic information in the first-layer codec and enables the codec system to maintain high audio quality while operating at a low frame rate. Note that a low-frame-rate codec improves the efficiency of speech generation. Experimental results on audio codec and speech generation tasks confirm the effectiveness of the proposed DualCodec compared to state-of-the-art codec systems, such as Mimi Codec, SpeechTokenizer, DAC, and Encodec. Demos are available at: https://dualcodec.github.io, code is available at: https://github.com/jiaqili3/DualCodec",
      "authors": [
        "Jiaqi Li",
        "Xiaolong Lin",
        "Zhekai Li",
        "Shixi Huang",
        "Yuancheng Wang",
        "Chaoren Wang",
        "Zhenpeng Zhan",
        "Zhizheng Wu"
      ],
      "published": "2025-05-19T11:41:08Z",
      "updated": "2025-10-01T15:01:57Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13000v2",
      "landing_url": "https://arxiv.org/abs/2505.13000v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13000"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Meets inclusion criteria by proposing a discrete neural audio codec (DualCodec) that integrates SSL and waveform representations for semantic tokens, characterizes codec design trade-offs, and evaluates quality/efficiency, so include."
    },
    "round-A_JuniorNano_reasoning": "Meets inclusion criteria by proposing a discrete neural audio codec (DualCodec) that integrates SSL and waveform representations for semantic tokens, characterizes codec design trade-offs, and evaluates quality/efficiency, so include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space",
    "abstract": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.",
    "metadata": {
      "arxiv_id": "2505.13181",
      "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space",
      "summary": "We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.",
      "authors": [
        "Zhengrui Ma",
        "Yang Feng",
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou",
        "Min Zhang"
      ],
      "published": "2025-05-19T14:38:59Z",
      "updated": "2025-10-24T11:35:47Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13181v2",
      "landing_url": "https://arxiv.org/abs/2505.13181v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13181"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "SLED models continuous latent sequences without tokenization or quantization, so it fails the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "SLED models continuous latent sequences without tokenization or quantization, so it fails the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information",
    "abstract": "Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.",
    "metadata": {
      "arxiv_id": "2505.13237",
      "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information",
      "summary": "Large audio-language models (LALMs) extend the large language models with multimodal understanding in speech, audio, etc. While their performances on speech and audio-processing tasks are extensively studied, their reasoning abilities remain underexplored. Particularly, their multi-hop reasoning, the ability to recall and integrate multiple facts, lacks systematic evaluation. Existing benchmarks focus on general speech and audio-processing tasks, conversational abilities, and fairness but overlook this aspect. To bridge this gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning based on speech and audio information. Results show that LALMs struggle to integrate speech/audio representations for multi-hop reasoning, even when they extract the relevant information correctly, highlighting a fundamental challenge in multimodal reasoning. Our findings expose a critical limitation in LALMs, offering insights and resources for future research.",
      "authors": [
        "Chih-Kai Yang",
        "Neo Ho",
        "Yen-Ting Piao",
        "Hung-yi Lee"
      ],
      "published": "2025-05-19T15:20:32Z",
      "updated": "2025-08-24T16:09:17Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13237v3",
      "landing_url": "https://arxiv.org/abs/2505.13237v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.13237"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on evaluating multi-hop reasoning in large audio-language models rather than studying the generation, quantization, or evaluation of discrete audio tokens, so it fails to meet the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on evaluating multi-hop reasoning in large audio-language models rather than studying the generation, quantization, or evaluation of discrete audio tokens, so it fails to meet the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
    "abstract": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
    "metadata": {
      "arxiv_id": "2505.13830",
      "title": "Improving Noise Robustness of LLM-based Zero-shot TTS via Discrete Acoustic Token Denoising",
      "summary": "Large language model (LLM) based zero-shot text-to-speech (TTS) methods tend to preserve the acoustic environment of the audio prompt, leading to degradation in synthesized speech quality when the audio prompt contains noise. In this paper, we propose a novel neural codec-based speech denoiser and integrate it with the advanced LLM-based TTS model, LauraTTS, to achieve noise-robust zero-shot TTS. The proposed codec denoiser consists of an audio codec, a token denoiser, and an embedding refiner. The token denoiser predicts the first two groups of clean acoustic tokens from the noisy ones, which can serve as the acoustic prompt for LauraTTS to synthesize high-quality personalized speech or be converted to clean speech waveforms through the embedding refiner and codec decoder. Experimental results show that our proposed codec denoiser outperforms state-of-the-art speech enhancement (SE) methods, and the proposed noise-robust LauraTTS surpasses the approach using additional SE models.",
      "authors": [
        "Ye-Xin Lu",
        "Hui-Peng Du",
        "Fei Liu",
        "Yang Ai",
        "Zhen-Hua Ling"
      ],
      "published": "2025-05-20T02:18:45Z",
      "updated": "2025-05-22T04:41:35Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13830v2",
      "landing_url": "https://arxiv.org/abs/2505.13830v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.13830"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The paper explicitly models discrete acoustic tokens via a neural codec-based denoiser before feeding them to an LLM TTS system, which clearly matches the inclusion focus on discrete audio token generation/use while avoiding any exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper explicitly models discrete acoustic tokens via a neural codec-based denoiser before feeding them to an LLM TTS system, which clearly matches the inclusion focus on discrete audio token generation/use while avoiding any exclusion criteria.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding",
    "abstract": "The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (https://github.com/Honee-W/U-SAM/).",
    "metadata": {
      "arxiv_id": "2505.13880",
      "title": "U-SAM: An audio language Model for Unified Speech, Audio, and Music Understanding",
      "summary": "The text generation paradigm for audio tasks has opened new possibilities for unified audio understanding. However, existing models face significant challenges in achieving a comprehensive understanding across diverse audio types, such as speech, general audio events, and music. Furthermore, their exclusive reliance on cross-entropy loss for alignment often falls short, as it treats all tokens equally and fails to account for redundant audio features, leading to weaker cross-modal alignment. To deal with the above challenges, this paper introduces U-SAM, an advanced audio language model that integrates specialized encoders for speech, audio, and music with a pre-trained large language model (LLM). U-SAM employs a Mixture of Experts (MoE) projector for task-aware feature fusion, dynamically routing and integrating the domain-specific encoder outputs. Additionally, U-SAM incorporates a Semantic-Aware Contrastive Loss Module, which explicitly identifies redundant audio features under language supervision and rectifies their semantic and spectral representations to enhance cross-modal alignment. Extensive experiments demonstrate that U-SAM consistently outperforms both specialized models and existing audio language models across multiple benchmarks. Moreover, it exhibits emergent capabilities on unseen tasks, showcasing its generalization potential. Code is available (https://github.com/Honee-W/U-SAM/).",
      "authors": [
        "Ziqian Wang",
        "Xianjun Xia",
        "Xinfa Zhu",
        "Lei Xie"
      ],
      "published": "2025-05-20T03:34:53Z",
      "updated": "2025-05-27T09:36:03Z",
      "categories": [
        "eess.AS",
        "cs.SD",
        "eess.SP"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.13880v3",
      "landing_url": "https://arxiv.org/abs/2505.13880v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.13880"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes large audio language model with specialized encoders, Mixture-of-Experts fusion, and contrastive loss, but it makes no mention of encoding audio into a finite vocabulary of discrete tokens or any tokenizer/quantization details, so it fails the discrete-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes large audio language model with specialized encoders, Mixture-of-Experts fusion, and contrastive loss, but it makes no mention of encoding audio into a finite vocabulary of discrete tokens or any tokenizer/quantization details, so it fails the discrete-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
    "abstract": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.",
    "metadata": {
      "arxiv_id": "2505.14103",
      "title": "AudioJailbreak: Jailbreak Attacks against End-to-End Large Audio-Language Models",
      "summary": "Jailbreak attacks to Large audio-language models (LALMs) are studied recently, but they achieve suboptimal effectiveness, applicability, and practicability, particularly, assuming that the adversary can fully manipulate user prompts. In this work, we first conduct an extensive experiment showing that advanced text jailbreak attacks cannot be easily ported to end-to-end LALMs via text-to speech (TTS) techniques. We then propose AudioJailbreak, a novel audio jailbreak attack, featuring (1) asynchrony: the jailbreak audio does not need to align with user prompts in the time axis by crafting suffixal jailbreak audios; (2) universality: a single jailbreak perturbation is effective for different prompts by incorporating multiple prompts into perturbation generation; (3) stealthiness: the malicious intent of jailbreak audios will not raise the awareness of victims by proposing various intent concealment strategies; and (4) over-the-air robustness: the jailbreak audios remain effective when being played over the air by incorporating the reverberation distortion effect with room impulse response into the generation of the perturbations. In contrast, all prior audio jailbreak attacks cannot offer asynchrony, universality, stealthiness, or over-the-air robustness. Moreover, AudioJailbreak is also applicable to the adversary who cannot fully manipulate user prompts, thus has a much broader attack scenario. Extensive experiments with thus far the most LALMs demonstrate the high effectiveness of AudioJailbreak. We highlight that our work peeks into the security implications of audio jailbreak attacks against LALMs, and realistically fosters improving their security robustness. The implementation and audio samples are available at our website https://audiojailbreak.github.io/AudioJailbreak.",
      "authors": [
        "Guangke Chen",
        "Fu Song",
        "Zhe Zhao",
        "Xiaojun Jia",
        "Yang Liu",
        "Yanchen Qiao",
        "Weizhe Zhang"
      ],
      "published": "2025-05-20T09:10:45Z",
      "updated": "2025-05-21T03:36:20Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14103v2",
      "landing_url": "https://arxiv.org/abs/2505.14103v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14103"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title and abstract focus on audio jailbreak attacks and adversarial robustness, with no discussion of discrete audio tokenization, codecs, or quantization methods, so it fails the inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Title and abstract focus on audio jailbreak attacks and adversarial robustness, with no discussion of discrete audio tokenization, codecs, or quantization methods, so it fails the inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AudSemThinker: Enhancing Audio-Language Models through Reasoning over Semantics of Sound",
    "abstract": "Audio-language models have shown promising results in various sound understanding tasks, yet they remain limited in their ability to reason over the fine-grained semantics of sound. In this paper, we present AudSemThinker, a model whose reasoning is structured around a framework of auditory semantics inspired by human cognition. To support this, we introduce AudSem, a novel dataset specifically curated for semantic descriptor reasoning in audio-language models. AudSem addresses the persistent challenge of data contamination in zero-shot evaluations by providing a carefully filtered collection of audio samples paired with captions generated through a robust multi-stage pipeline. Our experiments demonstrate that AudSemThinker outperforms state-of-the-art models across multiple training settings, highlighting its strength in semantic audio reasoning. Both AudSemThinker and the AudSem dataset are released publicly.",
    "metadata": {
      "arxiv_id": "2505.14142",
      "title": "AudSemThinker: Enhancing Audio-Language Models through Reasoning over Semantics of Sound",
      "summary": "Audio-language models have shown promising results in various sound understanding tasks, yet they remain limited in their ability to reason over the fine-grained semantics of sound. In this paper, we present AudSemThinker, a model whose reasoning is structured around a framework of auditory semantics inspired by human cognition. To support this, we introduce AudSem, a novel dataset specifically curated for semantic descriptor reasoning in audio-language models. AudSem addresses the persistent challenge of data contamination in zero-shot evaluations by providing a carefully filtered collection of audio samples paired with captions generated through a robust multi-stage pipeline. Our experiments demonstrate that AudSemThinker outperforms state-of-the-art models across multiple training settings, highlighting its strength in semantic audio reasoning. Both AudSemThinker and the AudSem dataset are released publicly.",
      "authors": [
        "Gijs Wijngaard",
        "Elia Formisano",
        "Michele Esposito",
        "Michel Dumontier"
      ],
      "published": "2025-05-20T09:46:29Z",
      "updated": "2025-09-30T09:19:46Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14142v2",
      "landing_url": "https://arxiv.org/abs/2505.14142v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.14142"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on reasoning for audio-language models and dataset construction without addressing discrete audio-token generation/quantization/codec methods required by inclusion criteria, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on reasoning for audio-language models and dataset construction without addressing discrete audio-token generation/quantization/codec methods required by inclusion criteria, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Discrete Audio Representations for Automated Audio Captioning",
    "abstract": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
    "metadata": {
      "arxiv_id": "2505.14989",
      "title": "Discrete Audio Representations for Automated Audio Captioning",
      "summary": "Discrete audio representations, termed audio tokens, are broadly categorized into semantic and acoustic tokens, typically generated through unsupervised tokenization of continuous audio representations. However, their applicability to automated audio captioning (AAC) remains underexplored. This paper systematically investigates the viability of audio token-driven models for AAC through comparative analyses of various tokenization methods. Our findings reveal that audio tokenization leads to performance degradation in AAC models compared to those that directly utilize continuous audio representations. To address this issue, we introduce a supervised audio tokenizer trained with an audio tagging objective. Unlike unsupervised tokenizers, which lack explicit semantic understanding, the proposed tokenizer effectively captures audio event information. Experiments conducted on the Clotho dataset demonstrate that the proposed audio tokens outperform conventional audio tokens in the AAC task.",
      "authors": [
        "Jingguang Tian",
        "Haoqin Sun",
        "Xinhui Hu",
        "Xinkang Xu"
      ],
      "published": "2025-05-21T00:27:38Z",
      "updated": "2025-05-21T00:27:38Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.14989v1",
      "landing_url": "https://arxiv.org/abs/2505.14989v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.14989"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "Study clearly centers on discrete audio tokens and tokenizers, provides comparative analyses and evaluations in the AAC downstream task, and defines tokenization methods, so it satisfies the inclusion criteria with no exclusion triggers."
    },
    "round-A_JuniorNano_reasoning": "Study clearly centers on discrete audio tokens and tokenizers, provides comparative analyses and evaluations in the AAC downstream task, and defines tokenization methods, so it satisfies the inclusion criteria with no exclusion triggers.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
    "abstract": "The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.",
    "metadata": {
      "arxiv_id": "2505.15406",
      "title": "Audio Jailbreak: An Open Comprehensive Benchmark for Jailbreaking Large Audio-Language Models",
      "summary": "The rise of Large Audio Language Models (LAMs) brings both potential and risks, as their audio outputs may contain harmful or unethical content. However, current research lacks a systematic, quantitative evaluation of LAM safety especially against jailbreak attacks, which are challenging due to the temporal and semantic nature of speech. To bridge this gap, we introduce AJailBench, the first benchmark specifically designed to evaluate jailbreak vulnerabilities in LAMs. We begin by constructing AJailBench-Base, a dataset of 1,495 adversarial audio prompts spanning 10 policy-violating categories, converted from textual jailbreak attacks using realistic text to speech synthesis. Using this dataset, we evaluate several state-of-the-art LAMs and reveal that none exhibit consistent robustness across attacks. To further strengthen jailbreak testing and simulate more realistic attack conditions, we propose a method to generate dynamic adversarial variants. Our Audio Perturbation Toolkit (APT) applies targeted distortions across time, frequency, and amplitude domains. To preserve the original jailbreak intent, we enforce a semantic consistency constraint and employ Bayesian optimization to efficiently search for perturbations that are both subtle and highly effective. This results in AJailBench-APT, an extended dataset of optimized adversarial audio samples. Our findings demonstrate that even small, semantically preserved perturbations can significantly reduce the safety performance of leading LAMs, underscoring the need for more robust and semantically aware defense mechanisms.",
      "authors": [
        "Zirui Song",
        "Qian Jiang",
        "Mingxuan Cui",
        "Mingzhe Li",
        "Lang Gao",
        "Zeyu Zhang",
        "Zixiang Xu",
        "Yanbo Wang",
        "Chenxi Wang",
        "Guangxian Ouyang",
        "Zhenhao Chen",
        "Xiuying Chen"
      ],
      "published": "2025-05-21T11:47:47Z",
      "updated": "2025-05-21T11:47:47Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.15406v1",
      "landing_url": "https://arxiv.org/abs/2505.15406v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.15406"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on benchmarking jailbreak attacks on large audio-language models and adversarial perturbations, without addressing discrete audio tokenization, codec quantization, or vocabularies, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on benchmarking jailbreak attacks on large audio-language models and adversarial perturbations, without addressing discrete audio tokenization, codec quantization, or vocabularies, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "abstract": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.",
    "metadata": {
      "arxiv_id": "2505.15957",
      "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
      "summary": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.",
      "authors": [
        "Chih-Kai Yang",
        "Neo S. Ho",
        "Hung-yi Lee"
      ],
      "published": "2025-05-21T19:17:29Z",
      "updated": "2025-10-01T16:02:37Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.CL",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.15957v3",
      "landing_url": "https://arxiv.org/abs/2505.15957v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.15957"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper is a general survey on evaluating large audio-language models, without any focus on discrete audio token/codec quantization methods or token-level modeling that the inclusion criteria require, so it should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper is a general survey on evaluating large audio-language models, without any focus on discrete audio token/codec quantization methods or token-level modeling that the inclusion criteria require, so it should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
    "abstract": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
    "metadata": {
      "arxiv_id": "2505.16177",
      "title": "Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression",
      "summary": "Most existing approaches for image and video compression perform transform coding in the pixel space to reduce redundancy. However, due to the misalignment between the pixel-space distortion and human perception, such schemes often face the difficulties in achieving both high-realism and high-fidelity at ultra-low bitrate. To solve this problem, we propose \\textbf{G}enerative \\textbf{L}atent \\textbf{C}oding (\\textbf{GLC}) models for image and video compression, termed GLC-image and GLC-Video. The transform coding of GLC is conducted in the latent space of a generative vector-quantized variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent space offers greater sparsity, richer semantics and better alignment with human perception, and show its advantages in achieving high-realism and high-fidelity compression. To further enhance performance, we improve the hyper prior by introducing a spatial categorical hyper module in GLC-image and a spatio-temporal categorical hyper module in GLC-video. Additionally, the code-prediction-based loss function is proposed to enhance the semantic consistency. Experiments demonstrate that our scheme shows high visual quality at ultra-low bitrate for both image and video compression. For image compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp, achieving the same FID as previous SOTA model MS-ILLM while using $45\\%$ fewer bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves 65.3\\% bitrate saving over PLVC in terms of DISTS.",
      "authors": [
        "Linfeng Qi",
        "Zhaoyang Jia",
        "Jiahao Li",
        "Bin Li",
        "Houqiang Li",
        "Yan Lu"
      ],
      "published": "2025-05-22T03:31:33Z",
      "updated": "2025-05-22T03:31:33Z",
      "categories": [
        "eess.IV",
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16177v1",
      "landing_url": "https://arxiv.org/abs/2505.16177v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16177"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on latent-space image/video compression without any discrete audio tokenization, so it fails the audio-token inclusion criteria and falls under the exclusion for non-audio discrete sequences."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on latent-space image/video compression without any discrete audio tokenization, so it fails the audio-token inclusion criteria and falls under the exclusion for non-audio discrete sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
    "abstract": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
    "metadata": {
      "arxiv_id": "2505.16845",
      "title": "Unlocking Temporal Flexibility: Neural Speech Codec with Variable Frame Rate",
      "summary": "Most neural speech codecs achieve bitrate adjustment through intra-frame mechanisms, such as codebook dropout, at a Constant Frame Rate (CFR). However, speech segments inherently have time-varying information density (e.g., silent intervals versus voiced regions). This property makes CFR not optimal in terms of bitrate and token sequence length, hindering efficiency in real-time applications. In this work, we propose a Temporally Flexible Coding (TFC) technique, introducing variable frame rate (VFR) into neural speech codecs for the first time. TFC enables seamlessly tunable average frame rates and dynamically allocates frame rates based on temporal entropy. Experimental results show that a codec with TFC achieves optimal reconstruction quality with high flexibility, and maintains competitive performance even at lower frame rates. Our approach is promising for the integration with other efforts to develop low-frame-rate neural speech codecs for more efficient downstream tasks.",
      "authors": [
        "Hanglei Zhang",
        "Yiwei Guo",
        "Zhihan Li",
        "Xiang Hao",
        "Xie Chen",
        "Kai Yu"
      ],
      "published": "2025-05-22T16:10:01Z",
      "updated": "2025-05-22T16:10:01Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.16845v1",
      "landing_url": "https://arxiv.org/abs/2505.16845v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.16845"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Neural speech codec work proposing temporally flexible discrete coding with quantized frames and measurable reconstruction quality clearly targets discrete audio token generation and evaluation, so include."
    },
    "round-A_JuniorNano_reasoning": "Neural speech codec work proposing temporally flexible discrete coding with quantized frames and measurable reconstruction quality clearly targets discrete audio token generation and evaluation, so include.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
    "abstract": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
    "metadata": {
      "arxiv_id": "2505.17076",
      "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English",
      "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.",
      "authors": [
        "Haoyang Zhang",
        "Hexin Liu",
        "Xiangyu Zhang",
        "Qiquan Zhang",
        "Yuchen Hu",
        "Junqi Zhao",
        "Fei Tian",
        "Xuerui Yang",
        "Leibny Paola Garcia",
        "Eng Siong Chng"
      ],
      "published": "2025-05-20T06:01:19Z",
      "updated": "2025-06-13T17:21:25Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17076v3",
      "landing_url": "https://arxiv.org/abs/2505.17076v3",
      "doi": "https://doi.org/10.48550/arXiv.2505.17076"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper investigates discrete speech tokenization by analyzing how frame rates affect generated semantic tokens for Mandarin and English, matching the inclusion focus on tokenizers and their downstream evaluation, so better to include."
    },
    "round-A_JuniorNano_reasoning": "The paper investigates discrete speech tokenization by analyzing how frame rates affect generated semantic tokens for Mandarin and English, matching the inclusion focus on tokenizers and their downstream evaluation, so better to include.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
    "abstract": "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.",
    "metadata": {
      "arxiv_id": "2505.17496",
      "title": "Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models",
      "summary": "End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.",
      "authors": [
        "Chi-Yuan Hsiao",
        "Ke-Han Lu",
        "Kai-Wei Chang",
        "Chih-Kai Yang",
        "Wei-Chih Chen",
        "Hung-yi Lee"
      ],
      "published": "2025-05-23T05:50:14Z",
      "updated": "2025-05-23T05:50:14Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17496v1",
      "landing_url": "https://arxiv.org/abs/2505.17496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17496"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper discusses mitigation of catastrophic forgetting in end-to-end spoken language model training without any focus on discrete audio token generation, quantization, or vocabulary design, so it fails to meet the inclusion criteria and falls squarely under the exclusion conditions."
    },
    "round-A_JuniorNano_reasoning": "The paper discusses mitigation of catastrophic forgetting in end-to-end spoken language model training without any focus on discrete audio token generation, quantization, or vocabulary design, so it fails to meet the inclusion criteria and falls squarely under the exclusion conditions.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models",
    "abstract": "Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, a comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 11,316 text samples and 245,355 audio samples with over 1,000 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and architecture. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.",
    "metadata": {
      "arxiv_id": "2505.17568",
      "title": "JALMBench: Benchmarking Jailbreak Vulnerabilities in Audio Language Models",
      "summary": "Audio Language Models (ALMs) have made significant progress recently. These models integrate the audio modality directly into the model, rather than converting speech into text and inputting text to Large Language Models (LLMs). While jailbreak attacks on LLMs have been extensively studied, the security of ALMs with audio modalities remains largely unexplored. Currently, there is a lack of an adversarial audio dataset and a unified framework specifically designed to evaluate and compare attacks and ALMs. In this paper, we present JALMBench, a comprehensive benchmark to assess the safety of ALMs against jailbreak attacks. JALMBench includes a dataset containing 11,316 text samples and 245,355 audio samples with over 1,000 hours. It supports 12 mainstream ALMs, 4 text-transferred and 4 audio-originated attack methods, and 5 defense methods. Using JALMBench, we provide an in-depth analysis of attack efficiency, topic sensitivity, voice diversity, and architecture. Additionally, we explore mitigation strategies for the attacks at both the prompt level and the response level.",
      "authors": [
        "Zifan Peng",
        "Yule Liu",
        "Zhen Sun",
        "Mingchen Li",
        "Zeren Luo",
        "Jingyi Zheng",
        "Wenhan Dong",
        "Xinlei He",
        "Xuechao Wang",
        "Yingjie Xue",
        "Shengmin Xu",
        "Xinyi Huang"
      ],
      "published": "2025-05-23T07:29:55Z",
      "updated": "2025-10-03T04:14:47Z",
      "categories": [
        "cs.CR",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17568v2",
      "landing_url": "https://arxiv.org/abs/2505.17568v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17568"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract focuses on benchmarking jailbreak attacks on audio language models rather than studying discrete audio tokenization, so it fails to meet the required inclusion focus on token/codec/quantization research."
    },
    "round-A_JuniorNano_reasoning": "Abstract focuses on benchmarking jailbreak attacks on audio language models rather than studying discrete audio tokenization, so it fails to meet the required inclusion focus on token/codec/quantization research.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
    "abstract": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
    "metadata": {
      "arxiv_id": "2505.17604",
      "title": "Adaptive Semantic Token Communication for Transformer-based Edge Inference",
      "summary": "This paper presents an adaptive framework for edge inference based on a dynamically configurable transformer-powered deep joint source channel coding (DJSCC) architecture. Motivated by a practical scenario where a resource constrained edge device engages in goal oriented semantic communication, such as selectively transmitting essential features for object detection to an edge server, our approach enables efficient task aware data transmission under varying bandwidth and channel conditions. To achieve this, input data is tokenized into compact high level semantic representations, refined by a transformer, and transmitted over noisy wireless channels. As part of the DJSCC pipeline, we employ a semantic token selection mechanism that adaptively compresses informative features into a user specified number of tokens per sample. These tokens are then further compressed through the JSCC module, enabling a flexible token communication strategy that adjusts both the number of transmitted tokens and their embedding dimensions. We incorporate a resource allocation algorithm based on Lyapunov stochastic optimization to enhance robustness under dynamic network conditions, effectively balancing compression efficiency and task performance. Experimental results demonstrate that our system consistently outperforms existing baselines, highlighting its potential as a strong foundation for AI native semantic communication in edge intelligence applications.",
      "authors": [
        "Alessio Devoto",
        "Jary Pomponi",
        "Mattia Merluzzi",
        "Paolo Di Lorenzo",
        "Simone Scardapane"
      ],
      "published": "2025-05-23T08:15:05Z",
      "updated": "2025-05-23T08:15:05Z",
      "categories": [
        "cs.LG",
        "cs.ET"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17604v1",
      "landing_url": "https://arxiv.org/abs/2505.17604v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.17604"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on semantic tokens for transformer edge inference in object detection over wireless channels, not on discrete audio tokens/codec design, so it fails the audio-token inclusion scope."
    },
    "round-A_JuniorNano_reasoning": "Focuses on semantic tokens for transformer edge inference in object detection over wireless channels, not on discrete audio tokens/codec design, so it fails the audio-token inclusion scope.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
    "abstract": "Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.",
    "metadata": {
      "arxiv_id": "2505.17726",
      "title": "Slot-MLLM: Object-Centric Visual Tokenization for Multimodal LLM",
      "summary": "Recently, multimodal large language models (MLLMs) have emerged as a key approach in achieving artificial general intelligence. In particular, vision-language MLLMs have been developed to generate not only text but also visual outputs from multimodal inputs. This advancement requires efficient image tokens that LLMs can process effectively both in input and output. However, existing image tokenization methods for MLLMs typically capture only global abstract concepts or uniformly segmented image patches, restricting MLLMs' capability to effectively understand or generate detailed visual content, particularly at the object level. To address this limitation, we propose an object-centric visual tokenizer based on Slot Attention specifically for MLLMs. In particular, based on the Q-Former encoder, diffusion decoder, and residual vector quantization, our proposed discretized slot tokens can encode local visual details while maintaining high-level semantics, and also align with textual data to be integrated seamlessly within a unified next-token prediction framework of LLMs. The resulting Slot-MLLM demonstrates significant performance improvements over baselines with previous visual tokenizers across various vision-language tasks that entail local detailed comprehension and generation. Notably, this work is the first demonstration of the feasibility of object-centric slot attention performed with MLLMs and in-the-wild natural images.",
      "authors": [
        "Donghwan Chi",
        "Hyomin Kim",
        "Yoonjin Oh",
        "Yongjin Kim",
        "Donghoon Lee",
        "Daejin Jo",
        "Jongmin Kim",
        "Junyeob Baek",
        "Sungjin Ahn",
        "Sungwoong Kim"
      ],
      "published": "2025-05-23T10:43:45Z",
      "updated": "2025-05-26T05:10:50Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.17726v2",
      "landing_url": "https://arxiv.org/abs/2505.17726v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.17726"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The study focuses on image/object-centric visual tokenization for multimodal LLMs rather than discrete audio token generation/quantization, so it clearly fails to meet the audio token inclusion criteria and falls under the exclusion for non-audio discrete sequences."
    },
    "round-A_JuniorNano_reasoning": "The study focuses on image/object-centric visual tokenization for multimodal LLMs rather than discrete audio token generation/quantization, so it clearly fails to meet the audio token inclusion criteria and falls under the exclusion for non-audio discrete sequences.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
    "metadata": {
      "arxiv_id": "2505.18231",
      "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
      "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
      "authors": [
        "Donghyun Son",
        "Euntae Choi",
        "Sungjoo Yoo"
      ],
      "published": "2025-05-23T12:40:07Z",
      "updated": "2025-12-14T08:17:35Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18231v2",
      "landing_url": "https://arxiv.org/abs/2505.18231v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.18231"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on LLM KV cache quantization, not on audio-derived discrete token/codebook evaluation, so it fails the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Focuses on LLM KV cache quantization, not on audio-derived discrete token/codebook evaluation, so it fails the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding",
    "abstract": "The ever-growing size of neural networks poses serious challenges on resource-constrained devices, such as embedded sensors. Compression algorithms that reduce their size can mitigate these problems, provided that model performance stays close to the original. We propose a novel post-training compression framework that combines rate-aware quantization with entropy coding by (1) extending the well-known layer-wise loss by a quadratic rate estimation, and (2) providing locally exact solutions to this modified objective following the Optimal Brain Surgeon (OBS) method. Our method allows for very fast decoding and is compatible with arbitrary quantization grids. We verify our results empirically by testing on various computer-vision networks, achieving a 20-40\\% decrease in bit rate at the same performance as the popular compression algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.",
    "metadata": {
      "arxiv_id": "2505.18758",
      "title": "Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding",
      "summary": "The ever-growing size of neural networks poses serious challenges on resource-constrained devices, such as embedded sensors. Compression algorithms that reduce their size can mitigate these problems, provided that model performance stays close to the original. We propose a novel post-training compression framework that combines rate-aware quantization with entropy coding by (1) extending the well-known layer-wise loss by a quadratic rate estimation, and (2) providing locally exact solutions to this modified objective following the Optimal Brain Surgeon (OBS) method. Our method allows for very fast decoding and is compatible with arbitrary quantization grids. We verify our results empirically by testing on various computer-vision networks, achieving a 20-40\\% decrease in bit rate at the same performance as the popular compression algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.",
      "authors": [
        "Alexander Conzelmann",
        "Robert Bamler"
      ],
      "published": "2025-05-24T15:52:49Z",
      "updated": "2025-05-24T15:52:49Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.18758v1",
      "landing_url": "https://arxiv.org/abs/2505.18758v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.18758"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper is about rate-constrained quantization and entropy coding for image-model compression, not discrete audio tokens or audio-tokenizers, so it fails the inclusion scope despite being about quantization."
    },
    "round-A_JuniorNano_reasoning": "Paper is about rate-constrained quantization and entropy coding for image-model compression, not discrete audio tokens or audio-tokenizers, so it fails the inclusion scope despite being about quantization.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Towards Reliable Large Audio Language Model",
    "abstract": "Recent advancements in large audio language models (LALMs) have demonstrated impressive results and promising prospects in universal understanding and reasoning across speech, music, and general sound. However, these models still lack the ability to recognize their knowledge boundaries and refuse to answer questions they don't know proactively. While there have been successful attempts to enhance the reliability of LLMs, reliable LALMs remain largely unexplored. In this paper, we systematically investigate various approaches towards reliable LALMs, including training-free methods such as multi-modal chain-of-thought (MCoT), and training-based methods such as supervised fine-tuning (SFT). Besides, we identify the limitations of previous evaluation metrics and propose a new metric, the Reliability Gain Index (RGI), to assess the effectiveness of different reliable methods. Our findings suggest that both training-free and training-based methods enhance the reliability of LALMs to different extents. Moreover, we find that awareness of reliability is a \"meta ability\", which can be transferred across different audio modalities, although significant structural and content differences exist among sound, music, and speech.",
    "metadata": {
      "arxiv_id": "2505.19294",
      "title": "Towards Reliable Large Audio Language Model",
      "summary": "Recent advancements in large audio language models (LALMs) have demonstrated impressive results and promising prospects in universal understanding and reasoning across speech, music, and general sound. However, these models still lack the ability to recognize their knowledge boundaries and refuse to answer questions they don't know proactively. While there have been successful attempts to enhance the reliability of LLMs, reliable LALMs remain largely unexplored. In this paper, we systematically investigate various approaches towards reliable LALMs, including training-free methods such as multi-modal chain-of-thought (MCoT), and training-based methods such as supervised fine-tuning (SFT). Besides, we identify the limitations of previous evaluation metrics and propose a new metric, the Reliability Gain Index (RGI), to assess the effectiveness of different reliable methods. Our findings suggest that both training-free and training-based methods enhance the reliability of LALMs to different extents. Moreover, we find that awareness of reliability is a \"meta ability\", which can be transferred across different audio modalities, although significant structural and content differences exist among sound, music, and speech.",
      "authors": [
        "Ziyang Ma",
        "Xiquan Li",
        "Yakun Song",
        "Wenxi Chen",
        "Chenpeng Du",
        "Jian Wu",
        "Yuanzhe Chen",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "published": "2025-05-25T20:00:31Z",
      "updated": "2025-05-25T20:00:31Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "cs.HC",
        "cs.MM",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19294v1",
      "landing_url": "https://arxiv.org/abs/2505.19294v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.19294"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on reliability techniques for large audio language models without any mention of discrete audio token generation, quantization, or vocabularies, so it fails the core inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on reliability techniques for large audio language models without any mention of discrete audio token generation, quantization, or vocabularies, so it fails the core inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
    "abstract": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
    "metadata": {
      "arxiv_id": "2505.19595",
      "title": "Accelerating Diffusion-based Text-to-Speech Model Training with Dual Modality Alignment",
      "summary": "The goal of this paper is to optimize the training process of diffusion-based text-to-speech models. While recent studies have achieved remarkable advancements, their training demands substantial time and computational costs, largely due to the implicit guidance of diffusion models in learning complex intermediate representations. To address this, we propose A-DMA, an effective strategy for Accelerating training with Dual Modality Alignment. Our method introduces a novel alignment pipeline leveraging both text and speech modalities: text-guided alignment, which incorporates contextual representations, and speech-guided alignment, which refines semantic representations. By aligning hidden states with discriminative features, our training scheme reduces the reliance on diffusion models for learning complex representations. Extensive experiments demonstrate that A-DMA doubles the convergence speed while achieving superior performance over baselines. Code and demo samples are available at: https://github.com/ZhikangNiu/A-DMA",
      "authors": [
        "Jeongsoo Choi",
        "Zhikang Niu",
        "Ji-Hoon Kim",
        "Chunhui Wang",
        "Joon Son Chung",
        "Xie Chen"
      ],
      "published": "2025-05-26T07:07:16Z",
      "updated": "2025-05-30T16:52:09Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19595v2",
      "landing_url": "https://arxiv.org/abs/2505.19595v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19595"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Focuses on accelerating diffusion-based TTS training with continuous representations and does not discuss discrete audio token quantization/codec/tokenizer details, so it fails the inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "Focuses on accelerating diffusion-based TTS training with continuous representations and does not discuss discrete audio token quantization/codec/tokenizer details, so it fails the inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
    "abstract": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios: Audio Interference Attack, Instruction Following Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index, their vulnerabilities and resilience were quantitatively assessed. Experimental results reveal significant performance disparities among models; no single model consistently outperforms others across all attack types. The position of malicious content critically influences attack effectiveness, particularly when placed at the beginning of sequences. A negative correlation between instruction-following capability and robustness suggests models adhering strictly to instructions may be more susceptible, contrasting with greater resistance by safety-aligned models. Additionally, system prompts show mixed effectiveness, indicating the need for tailored strategies. This work introduces a benchmark framework and highlights the importance of integrating robustness into training pipelines. Findings emphasize developing multi-modal defenses and architectural designs that decouple capability from susceptibility for secure LALMs deployment.",
    "metadata": {
      "arxiv_id": "2505.19598",
      "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study",
      "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios: Audio Interference Attack, Instruction Following Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index, their vulnerabilities and resilience were quantitatively assessed. Experimental results reveal significant performance disparities among models; no single model consistently outperforms others across all attack types. The position of malicious content critically influences attack effectiveness, particularly when placed at the beginning of sequences. A negative correlation between instruction-following capability and robustness suggests models adhering strictly to instructions may be more susceptible, contrasting with greater resistance by safety-aligned models. Additionally, system prompts show mixed effectiveness, indicating the need for tailored strategies. This work introduces a benchmark framework and highlights the importance of integrating robustness into training pipelines. Findings emphasize developing multi-modal defenses and architectural designs that decouple capability from susceptibility for secure LALMs deployment.",
      "authors": [
        "Guanyu Hou",
        "Jiaming He",
        "Yinhang Zhou",
        "Ji Guo",
        "Yitong Qiao",
        "Rui Zhang",
        "Wenbo Jiang"
      ],
      "published": "2025-05-26T07:08:38Z",
      "updated": "2025-07-10T15:58:00Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19598v2",
      "landing_url": "https://arxiv.org/abs/2505.19598v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19598"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: The paper studies robustness of LALMs to malicious audio injections without describing any discrete audio-token generation, quantization, or tokenizer design, so it fails the inclusion criteria focused on discrete token methods."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: The paper studies robustness of LALMs to malicious audio injections without describing any discrete audio-token generation, quantization, or tokenizer design, so it fails the inclusion criteria focused on discrete token methods.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
    "abstract": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
    "metadata": {
      "arxiv_id": "2505.19669",
      "title": "Zero-Shot Streaming Text to Speech Synthesis with Transducer and Auto-Regressive Modeling",
      "summary": "Zero-shot streaming text-to-speech is an important research topic in human-computer interaction. Existing methods primarily use a lookahead mechanism, relying on future text to achieve natural streaming speech synthesis, which introduces high processing latency. To address this issue, we propose SMLLE, a streaming framework for generating high-quality speech frame-by-frame. SMLLE employs a Transducer to convert text into semantic tokens in real time while simultaneously obtaining duration alignment information. The combined outputs are then fed into a fully autoregressive (AR) streaming model to reconstruct mel-spectrograms. To further stabilize the generation process, we design a Delete < Bos > Mechanism that allows the AR model to access future text introducing as minimal delay as possible. Experimental results suggest that the SMLLE outperforms current streaming TTS methods and achieves comparable performance over sentence-level TTS systems. Samples are available on shy-98.github.io/SMLLE_demo_page/.",
      "authors": [
        "Haiyang Sun",
        "Shujie Hu",
        "Shujie Liu",
        "Lingwei Meng",
        "Hui Wang",
        "Bing Han",
        "Yifan Yang",
        "Yanqing Liu",
        "Sheng Zhao",
        "Yan Lu",
        "Yanmin Qian"
      ],
      "published": "2025-05-26T08:25:01Z",
      "updated": "2025-06-02T10:03:25Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19669v2",
      "landing_url": "https://arxiv.org/abs/2505.19669v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.19669"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The abstract focuses on streaming TTS through a Transducer+AR pipeline without describing discrete audio token/quantization vocabularies, so it fails to meet the discrete-token inclusion criterion and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The abstract focuses on streaming TTS through a Transducer+AR pipeline without describing discrete audio token/quantization vocabularies, so it fails to meet the discrete-token inclusion criterion and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models",
    "abstract": "Large Audio Language Models (LALMs) have extended the capabilities of Large Language Models (LLMs) by enabling audio-based human interactions. However, recent research has revealed that LALMs remain vulnerable to harmful queries due to insufficient safety-alignment. Despite advances in defence measures for text and vision LLMs, effective safety-alignment strategies and audio-safety dataset specifically targeting LALMs are notably absent. Meanwhile defence measures based on Supervised Fine-tuning (SFT) struggle to address safety improvement while avoiding over-rejection issues, significantly compromising helpfulness. In this work, we propose an unsupervised safety-fine-tuning strategy as remedy that reshapes model's representation space to enhance existing LALMs safety-alignment while balancing the risk of over-rejection. Our experiments, conducted across three generations of Qwen LALMs, demonstrate that our approach significantly improves LALMs safety under three modality input conditions (audio-text, text-only, and audio-only) while increasing over-rejection rate by only 0.88% on average. Warning: this paper contains harmful examples.",
    "metadata": {
      "arxiv_id": "2505.19670",
      "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models",
      "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large Language Models (LLMs) by enabling audio-based human interactions. However, recent research has revealed that LALMs remain vulnerable to harmful queries due to insufficient safety-alignment. Despite advances in defence measures for text and vision LLMs, effective safety-alignment strategies and audio-safety dataset specifically targeting LALMs are notably absent. Meanwhile defence measures based on Supervised Fine-tuning (SFT) struggle to address safety improvement while avoiding over-rejection issues, significantly compromising helpfulness. In this work, we propose an unsupervised safety-fine-tuning strategy as remedy that reshapes model's representation space to enhance existing LALMs safety-alignment while balancing the risk of over-rejection. Our experiments, conducted across three generations of Qwen LALMs, demonstrate that our approach significantly improves LALMs safety under three modality input conditions (audio-text, text-only, and audio-only) while increasing over-rejection rate by only 0.88% on average. Warning: this paper contains harmful examples.",
      "authors": [
        "Hao Yang",
        "Lizhen Qu",
        "Ehsan Shareghi",
        "Gholamreza Haffari"
      ],
      "published": "2025-05-26T08:25:25Z",
      "updated": "2025-05-26T08:25:25Z",
      "categories": [
        "cs.CL",
        "cs.MM",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.19670v1",
      "landing_url": "https://arxiv.org/abs/2505.19670v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.19670"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Paper focuses on safety fine-tuning of large audio language models rather than research on discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion scope of discrete token work and falls under the exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Paper focuses on safety fine-tuning of large audio language models rather than research on discrete audio tokenization/quantization or token-level modeling, so it fails the inclusion scope of discrete token work and falls under the exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Learning Hierarchical Sparse Transform Coding of 3DGS",
    "abstract": "3D Gaussian Splatting (3DGS) supports fast, high quality, novel view synthesis but has a heavy memory footprint, making the compression of its model crucial. Current state-of-the-art (SOTA) 3DGS compression methods adopt an anchor-based architecture that pairs the Scaffold-GS representation with conditional entropy coding. However, these methods forego the analysis-synthesis transform, a vital mechanism in visual data compression. As a result, redundancy remains intact in the signal and its removal is left to the entropy coder, which computationally overburdens the entropy coding module, increasing coding latency. Even with added complexity thorough redundancy removal is a task unsuited to an entropy coder. To fix this critical omission, we introduce a Sparsity-guided Hierarchical Transform Coding (SHTC) method, the first study on the end-to-end learned neural transform coding of 3DGS. SHTC applies KLT to decorrelate intra-anchor attributes, followed by quantization and entropy coding, and then compresses KLT residuals with a low-complexity, scene-adaptive neural transform. Aided by the sparsity prior and deep unfolding technique, the learned transform uses only a few trainable parameters, reducing the memory usage. Overall, SHTC achieves an appreciably improved R-D performance and at the same time higher decoding speed over SOTA. Its prior-guided, parameter-efficient design may also inspire low-complexity neural image and video codecs. Our code will be released at https://github.com/hxu160/SHTC_for_3DGS_compression.",
    "metadata": {
      "arxiv_id": "2505.22908",
      "title": "Learning Hierarchical Sparse Transform Coding of 3DGS",
      "summary": "3D Gaussian Splatting (3DGS) supports fast, high quality, novel view synthesis but has a heavy memory footprint, making the compression of its model crucial. Current state-of-the-art (SOTA) 3DGS compression methods adopt an anchor-based architecture that pairs the Scaffold-GS representation with conditional entropy coding. However, these methods forego the analysis-synthesis transform, a vital mechanism in visual data compression. As a result, redundancy remains intact in the signal and its removal is left to the entropy coder, which computationally overburdens the entropy coding module, increasing coding latency. Even with added complexity thorough redundancy removal is a task unsuited to an entropy coder. To fix this critical omission, we introduce a Sparsity-guided Hierarchical Transform Coding (SHTC) method, the first study on the end-to-end learned neural transform coding of 3DGS. SHTC applies KLT to decorrelate intra-anchor attributes, followed by quantization and entropy coding, and then compresses KLT residuals with a low-complexity, scene-adaptive neural transform. Aided by the sparsity prior and deep unfolding technique, the learned transform uses only a few trainable parameters, reducing the memory usage. Overall, SHTC achieves an appreciably improved R-D performance and at the same time higher decoding speed over SOTA. Its prior-guided, parameter-efficient design may also inspire low-complexity neural image and video codecs. Our code will be released at https://github.com/hxu160/SHTC_for_3DGS_compression.",
      "authors": [
        "Hao Xu",
        "Xiaolin Wu",
        "Xi Zhang"
      ],
      "published": "2025-05-28T22:17:24Z",
      "updated": "2025-11-25T02:01:41Z",
      "categories": [
        "cs.CV",
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.22908v2",
      "landing_url": "https://arxiv.org/abs/2505.22908v2",
      "doi": "https://doi.org/10.48550/arXiv.2505.22908"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work focuses on 3D Gaussian splatting compression for graphics, not on discrete audio tokenization or codec-related discrete representations, so it fails the topic inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This work focuses on 3D Gaussian splatting compression for graphics, not on discrete audio tokenization or codec-related discrete representations, so it fails the topic inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units",
    "abstract": "Spoken language models (SLMs) operate on acoustic units obtained by discretizing self-supervised speech representations. Although the characteristics of these units directly affect performance, the interaction between codebook size and unit coarseness (i.e., duration) remains unexplored. We investigate SLM performance as we vary codebook size and unit coarseness using the simple duration-penalized dynamic programming (DPDP) method. New analyses are performed across different linguistic levels. At the phone and word levels, coarseness provides little benefit, as long as the codebook size is chosen appropriately. However, when producing whole sentences in a resynthesis task, SLMs perform better with coarser units. In lexical and syntactic language modeling tasks, coarser units also give higher accuracies at lower bitrates. We therefore show that coarser units aren't always better, but that DPDP is a simple and efficient way to obtain coarser units for the tasks where they are beneficial.",
    "metadata": {
      "arxiv_id": "2505.23494",
      "title": "Spoken Language Modeling with Duration-Penalized Self-Supervised Units",
      "summary": "Spoken language models (SLMs) operate on acoustic units obtained by discretizing self-supervised speech representations. Although the characteristics of these units directly affect performance, the interaction between codebook size and unit coarseness (i.e., duration) remains unexplored. We investigate SLM performance as we vary codebook size and unit coarseness using the simple duration-penalized dynamic programming (DPDP) method. New analyses are performed across different linguistic levels. At the phone and word levels, coarseness provides little benefit, as long as the codebook size is chosen appropriately. However, when producing whole sentences in a resynthesis task, SLMs perform better with coarser units. In lexical and syntactic language modeling tasks, coarser units also give higher accuracies at lower bitrates. We therefore show that coarser units aren't always better, but that DPDP is a simple and efficient way to obtain coarser units for the tasks where they are beneficial.",
      "authors": [
        "Nicol Visser",
        "Herman Kamper"
      ],
      "published": "2025-05-29T14:43:48Z",
      "updated": "2025-05-29T14:43:48Z",
      "categories": [
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.23494v1",
      "landing_url": "https://arxiv.org/abs/2505.23494v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.23494"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study explicitly explores discretized SSL speech units/codebooks and evaluates downstream modeling trade-offs, matching inclusion criteria for discrete audio tokens, so it should be included (score 5)."
    },
    "round-A_JuniorNano_reasoning": "Study explicitly explores discretized SSL speech units/codebooks and evaluates downstream modeling trade-offs, matching inclusion criteria for discrete audio tokens, so it should be included (score 5).",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
    "abstract": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
    "metadata": {
      "arxiv_id": "2505.24496",
      "title": "Speech Token Prediction via Compressed-to-fine Language Modeling for Speech Generation",
      "summary": "Neural audio codecs, used as speech tokenizers, have demonstrated remarkable potential in the field of speech generation. However, to ensure high-fidelity audio reconstruction, neural audio codecs typically encode audio into long sequences of speech tokens, posing a significant challenge for downstream language models in long-context modeling. We observe that speech token sequences exhibit short-range dependency: due to the monotonic alignment between text and speech in text-to-speech (TTS) tasks, the prediction of the current token primarily relies on its local context, while long-range tokens contribute less to the current token prediction and often contain redundant information. Inspired by this observation, we propose a \\textbf{compressed-to-fine language modeling} approach to address the challenge of long sequence speech tokens within neural codec language models: (1) \\textbf{Fine-grained Initial and Short-range Information}: Our approach retains the prompt and local tokens during prediction to ensure text alignment and the integrity of paralinguistic information; (2) \\textbf{Compressed Long-range Context}: Our approach compresses long-range token spans into compact representations to reduce redundant information while preserving essential semantics. Extensive experiments on various neural audio codecs and downstream language models validate the effectiveness and generalizability of the proposed approach, highlighting the importance of token compression in improving speech generation within neural codec language models. The demo of audio samples will be available at https://anonymous.4open.science/r/SpeechTokenPredictionViaCompressedToFinedLM.",
      "authors": [
        "Wenrui Liu",
        "Qian Chen",
        "Wen Wang",
        "Yafeng Chen",
        "Jin Xu",
        "Zhifang Guo",
        "Guanrou Yang",
        "Weiqin Li",
        "Xiaoda Yang",
        "Tao Jin",
        "Minghui Fang",
        "Jialong Zuo",
        "Bai Jionghao",
        "Zemin Liu"
      ],
      "published": "2025-05-30T11:47:29Z",
      "updated": "2025-05-30T11:47:29Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24496v1",
      "landing_url": "https://arxiv.org/abs/2505.24496v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24496"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Study directly addresses discrete neural audio codec tokens and proposes a compressed-to-fine modeling method for long sequences, matching the inclusion requirements with clear token and codec discussion and no exclusion criteria met."
    },
    "round-A_JuniorNano_reasoning": "Study directly addresses discrete neural audio codec tokens and proposes a compressed-to-fine modeling method for long sequences, matching the inclusion requirements with clear token and codec discussion and no exclusion criteria met.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
    "abstract": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
    "metadata": {
      "arxiv_id": "2505.24717",
      "title": "PDE-Transformer: Efficient and Versatile Transformers for Physics Simulations",
      "summary": "We introduce PDE-Transformer, an improved transformer-based architecture for surrogate modeling of physics simulations on regular grids. We combine recent architectural improvements of diffusion transformers with adjustments specific for large-scale simulations to yield a more scalable and versatile general-purpose transformer architecture, which can be used as the backbone for building large-scale foundation models in physical sciences. We demonstrate that our proposed architecture outperforms state-of-the-art transformer architectures for computer vision on a large dataset of 16 different types of PDEs. We propose to embed different physical channels individually as spatio-temporal tokens, which interact via channel-wise self-attention. This helps to maintain a consistent information density of tokens when learning multiple types of PDEs simultaneously. We demonstrate that our pre-trained models achieve improved performance on several challenging downstream tasks compared to training from scratch and also beat other foundation model architectures for physics simulations.",
      "authors": [
        "Benjamin Holzschuh",
        "Qiang Liu",
        "Georg Kohl",
        "Nils Thuerey"
      ],
      "published": "2025-05-30T15:39:54Z",
      "updated": "2025-05-30T15:39:54Z",
      "categories": [
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2505.24717v1",
      "landing_url": "https://arxiv.org/abs/2505.24717v1",
      "doi": "https://doi.org/10.48550/arXiv.2505.24717"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Physics-focused PDE-transformer model on grid simulations has nothing to do with discrete audio token generation or codec/semantic token research, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Physics-focused PDE-transformer model on grid simulations has nothing to do with discrete audio token generation or codec/semantic token research, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation",
    "abstract": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.",
    "metadata": {
      "arxiv_id": "2506.00385",
      "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity Reconstruction and Generation",
      "summary": "Neural audio codecs have made significant strides in efficiently mapping raw audio waveforms into discrete token representations, which are foundational for contemporary audio generative models. However, most existing codecs are optimized primarily for reconstruction quality, often at the expense of the downstream modelability of the encoded tokens. Motivated by the need to overcome this bottleneck, we introduce $\\textbf{MagiCodec}$, a novel single-layer, streaming Transformer-based audio codec. MagiCodec is designed with a multistage training pipeline that incorporates Gaussian noise injection and latent regularization, explicitly targeting the enhancement of semantic expressiveness in the generated codes while preserving high reconstruction fidelity. We analytically derive the effect of noise injection in the frequency domain, demonstrating its efficacy in attenuating high-frequency components and fostering robust tokenization. Extensive experimental evaluations show that MagiCodec surpasses state-of-the-art codecs in both reconstruction quality and downstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like distributions, as observed in natural languages, thereby improving compatibility with language-model-based generative architectures. The code and pre-trained models are available at https://github.com/Ereboas/MagiCodec.",
      "authors": [
        "Yakun Song",
        "Jiawei Chen",
        "Xiaobin Zhuang",
        "Chenpeng Du",
        "Ziyang Ma",
        "Jian Wu",
        "Jian Cong",
        "Dongya Jia",
        "Zhuo Chen",
        "Yuping Wang",
        "Yuxuan Wang",
        "Xie Chen"
      ],
      "published": "2025-05-31T04:31:02Z",
      "updated": "2025-05-31T04:31:02Z",
      "categories": [
        "cs.SD",
        "cs.AI",
        "cs.LG",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00385v1",
      "landing_url": "https://arxiv.org/abs/2506.00385v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00385"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Describes a codec that generates discrete audio tokens with quantization-like training, evaluates reconstruction quality and downstream modelability, and so meets the inclusion criteria without invoking any exclusion condition."
    },
    "round-A_JuniorNano_reasoning": "Describes a codec that generates discrete audio tokens with quantization-like training, evaluates reconstruction quality and downstream modelability, and so meets the inclusion criteria without invoking any exclusion condition.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Your Demands Deserve More Bits: Referring Semantic Image Compression at Ultra-low Bitrate",
    "abstract": "With the help of powerful generative models, Semantic Image Compression (SIC) has achieved impressive performance at ultra-low bitrate. However, due to coarse-grained visual-semantic alignment and inherent randomness, the reliability of SIC is seriously concerned for reconstructing completely different object instances, even they are semantically consistent with original images. To tackle this issue, we propose a novel Referring Semantic Image Compression (RSIC) framework to improve the fidelity of user-specified content while retaining extreme compression ratios. Specifically, RSIC consists of three modules: Global Description Encoding (GDE), Referring Guidance Encoding (RGE), and Guided Generative Decoding (GGD). GDE and RGE encode global semantic information and local features, respectively, while GGD handles the non-uniformly guided generative process based on the encoded information. In this way, our RSIC achieves flexible customized compression according to user demands, which better balance the local fidelity, global realism, semantic alignment, and bit overhead. Extensive experiments on three datasets verify the compression efficiency and flexibility of the proposed method.",
    "metadata": {
      "arxiv_id": "2506.00526",
      "title": "Your Demands Deserve More Bits: Referring Semantic Image Compression at Ultra-low Bitrate",
      "summary": "With the help of powerful generative models, Semantic Image Compression (SIC) has achieved impressive performance at ultra-low bitrate. However, due to coarse-grained visual-semantic alignment and inherent randomness, the reliability of SIC is seriously concerned for reconstructing completely different object instances, even they are semantically consistent with original images. To tackle this issue, we propose a novel Referring Semantic Image Compression (RSIC) framework to improve the fidelity of user-specified content while retaining extreme compression ratios. Specifically, RSIC consists of three modules: Global Description Encoding (GDE), Referring Guidance Encoding (RGE), and Guided Generative Decoding (GGD). GDE and RGE encode global semantic information and local features, respectively, while GGD handles the non-uniformly guided generative process based on the encoded information. In this way, our RSIC achieves flexible customized compression according to user demands, which better balance the local fidelity, global realism, semantic alignment, and bit overhead. Extensive experiments on three datasets verify the compression efficiency and flexibility of the proposed method.",
      "authors": [
        "Chenhao Wu",
        "Qingbo Wu",
        "Haoran Wei",
        "Shuai Chen",
        "Mingzhou He",
        "King Ngi Ngan",
        "Fanman Meng",
        "Hongliang Li"
      ],
      "published": "2025-05-31T12:15:03Z",
      "updated": "2025-05-31T12:15:03Z",
      "categories": [
        "eess.IV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00526v1",
      "landing_url": "https://arxiv.org/abs/2506.00526v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00526"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on semantic image compression and does not address discrete audio tokens, tokenizers, or quantization for audio signals, so it fails to meet the audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on semantic image compression and does not address discrete audio tokens, tokenizers, or quantization for audio signals, so it fails to meet the audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Slow Feature Analysis as Variational Inference Objective",
    "abstract": "This work presents a novel probabilistic interpretation of Slow Feature Analysis (SFA) through the lens of variational inference. Unlike prior formulations that recover linear SFA from Gaussian state-space models with linear emissions, this approach relaxes the key constraint of linearity. While it does not lead to full equivalence to non-linear SFA, it recasts the classical slowness objective in a variational framework. Specifically, it allows the slowness objective to be interpreted as a regularizer to a reconstruction loss. Furthermore, we provide arguments, why -- from the perspective of slowness optimization -- the reconstruction loss takes on the role of the constraints that ensure informativeness in SFA. We conclude with a discussion of potential new research directions.",
    "metadata": {
      "arxiv_id": "2506.00580",
      "title": "Slow Feature Analysis as Variational Inference Objective",
      "summary": "This work presents a novel probabilistic interpretation of Slow Feature Analysis (SFA) through the lens of variational inference. Unlike prior formulations that recover linear SFA from Gaussian state-space models with linear emissions, this approach relaxes the key constraint of linearity. While it does not lead to full equivalence to non-linear SFA, it recasts the classical slowness objective in a variational framework. Specifically, it allows the slowness objective to be interpreted as a regularizer to a reconstruction loss. Furthermore, we provide arguments, why -- from the perspective of slowness optimization -- the reconstruction loss takes on the role of the constraints that ensure informativeness in SFA. We conclude with a discussion of potential new research directions.",
      "authors": [
        "Merlin Schüler",
        "Laurenz Wiskott"
      ],
      "published": "2025-05-31T14:29:02Z",
      "updated": "2025-05-31T14:29:02Z",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00580v1",
      "landing_url": "https://arxiv.org/abs/2506.00580v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00580"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper reinterprets Slow Feature Analysis as variational inference without addressing discrete audio token generation, quantization, or codec tokenization, so it fails the topic inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper reinterprets Slow Feature Analysis as variational inference without addressing discrete audio token generation, quantization, or codec tokenization, so it fails the topic inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
    "abstract": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
    "metadata": {
      "arxiv_id": "2506.00809",
      "title": "FUSE: Universal Speech Enhancement using Multi-Stage Fusion of Sparse Compression and Token Generation Models for the URGENT 2025 Challenge",
      "summary": "We propose a multi-stage framework for universal speech enhancement, designed for the Interspeech 2025 URGENT Challenge. Our system first employs a Sparse Compression Network to robustly separate sources and extract an initial clean speech estimate from noisy inputs. This is followed by an efficient generative model that refines speech quality by leveraging self-supervised features and optimizing a masked language modeling objective on acoustic tokens derived from a neural audio codec. In the final stage, a fusion network integrates the outputs of the first two stages with the original noisy signal, achieving a balanced improvement in both signal fidelity and perceptual quality. Additionally, a shift trick that aggregates multiple time-shifted predictions, along with output blending, further boosts performance. Experimental results on challenging multilingual datasets with variable sampling rates and diverse distortion types validate the effectiveness of our approach.",
      "authors": [
        "Nabarun Goswami",
        "Tatsuya Harada"
      ],
      "published": "2025-06-01T03:23:27Z",
      "updated": "2025-06-01T03:23:27Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00809v1",
      "landing_url": "https://arxiv.org/abs/2506.00809v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00809"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "The system explicitly refines speech quality via acoustic tokens derived from a neural audio codec and optimizes a masked language modeling objective on those discrete units, so it clearly centers on discrete audio tokens with codec-based quantization and token-level modeling, matching the inclusion criteria without violating any exclusions."
    },
    "round-A_JuniorNano_reasoning": "The system explicitly refines speech quality via acoustic tokens derived from a neural audio codec and optimizes a masked language modeling objective on those discrete units, so it clearly centers on discrete audio tokens with codec-based quantization and token-level modeling, matching the inclusion criteria without violating any exclusions.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
    "abstract": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
    "metadata": {
      "arxiv_id": "2506.00843",
      "title": "HASRD: Hierarchical Acoustic and Semantic Representation Disentanglement",
      "summary": "Effective speech representations for spoken language models must balance semantic relevance with acoustic fidelity for high-quality reconstruction. However, existing approaches struggle to achieve both simultaneously. To address this, we introduce Hierarchical Acoustic and Semantic Representation Disentanglement (HASRD, pronounced `hazard'), a framework that factorizes self-supervised learning representations into discrete semantic and acoustic tokens. HASRD assigns the semantic representation to the first codebook, while encoding acoustic residuals in subsequent codebooks. This preserves ASR performance while achieving high-quality reconstruction. Additionally, we enhance HASRD's encoder efficiency, improving ASR performance without compromising reconstruction quality. Compared to SpeechTokenizer, HASRD achieves a 44% relative WER improvement, superior reconstruction quality, and 2x lower bitrate, demonstrating its effectiveness in disentangling acoustic and semantic information.",
      "authors": [
        "Amir Hussein",
        "Sameer Khurana",
        "Gordon Wichern",
        "Francois G. Germain",
        "Jonathan Le Roux"
      ],
      "published": "2025-06-01T05:38:39Z",
      "updated": "2025-06-01T05:38:39Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.00843v1",
      "landing_url": "https://arxiv.org/abs/2506.00843v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.00843"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Introduces HASRD which explicitly disentangles discrete semantic and acoustic token codebooks with quantization-based representations, improving ASR and reconstruction metrics, so it meets inclusion criteria fully."
    },
    "round-A_JuniorNano_reasoning": "Introduces HASRD which explicitly disentangles discrete semantic and acoustic token codebooks with quantization-based representations, improving ASR and reconstruction metrics, so it meets inclusion criteria fully.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism",
    "abstract": "In this study, we focus on heart murmur classification (HMC) and hypothesize that combining neural audio codec representations (NACRs) such as EnCodec with spectral features (SFs), such as MFCC, will yield superior performance. We believe such fusion will trigger their complementary behavior as NACRs excel at capturing fine-grained acoustic patterns such as rhythm changes, spectral features focus on frequency-domain properties such as harmonic structure, spectral energy distribution crucial for analyzing the complex of heart sounds. To this end, we propose, BAOMI, a novel framework banking on novel bandit-based cross-attention mechanism for effective fusion. Here, a agent provides more weightage to most important heads in multi-head cross-attention mechanism and helps in mitigating the noise. With BAOMI, we report the topmost performance in comparison to individual NACRs, SFs, and baseline fusion techniques and setting new state-of-the-art.",
    "metadata": {
      "arxiv_id": "2506.01148",
      "title": "Towards Fusion of Neural Audio Codec-based Representations with Spectral for Heart Murmur Classification via Bandit-based Cross-Attention Mechanism",
      "summary": "In this study, we focus on heart murmur classification (HMC) and hypothesize that combining neural audio codec representations (NACRs) such as EnCodec with spectral features (SFs), such as MFCC, will yield superior performance. We believe such fusion will trigger their complementary behavior as NACRs excel at capturing fine-grained acoustic patterns such as rhythm changes, spectral features focus on frequency-domain properties such as harmonic structure, spectral energy distribution crucial for analyzing the complex of heart sounds. To this end, we propose, BAOMI, a novel framework banking on novel bandit-based cross-attention mechanism for effective fusion. Here, a agent provides more weightage to most important heads in multi-head cross-attention mechanism and helps in mitigating the noise. With BAOMI, we report the topmost performance in comparison to individual NACRs, SFs, and baseline fusion techniques and setting new state-of-the-art.",
      "authors": [
        "Orchid Chetia Phukan",
        "Girish",
        "Mohd Mujtaba Akhtar",
        "Swarup Ranjan Behera",
        "Priyabrata Mallick",
        "Santanu Roy",
        "Arun Balaji Buduru",
        "Rajesh Sharma"
      ],
      "published": "2025-06-01T20:01:15Z",
      "updated": "2025-06-01T20:01:15Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01148v1",
      "landing_url": "https://arxiv.org/abs/2506.01148v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01148"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Abstract describes fusing EnCodec representations with spectral features for murmur classification but never treats the discrete codec tokens themselves, their vocabularies, quantization details, or any token-level evaluation, so it lacks the discrete-token focus required for inclusion."
    },
    "round-A_JuniorNano_reasoning": "Abstract describes fusing EnCodec representations with spectral features for murmur classification but never treats the discrete codec tokens themselves, their vocabularies, quantization details, or any token-level evaluation, so it lacks the discrete-token focus required for inclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Zero-Shot Text-to-Speech for Vietnamese",
    "abstract": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
    "metadata": {
      "arxiv_id": "2506.01322",
      "title": "Zero-Shot Text-to-Speech for Vietnamese",
      "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941 hours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook, we conduct experiments on three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook consistently enhances model performance across various metrics. Moreover, VALL-E and VoiceCraft exhibit superior performance in synthesizing short sentences, highlighting their robustness in handling diverse linguistic contexts. We publicly release PhoAudiobook to facilitate further research and development in Vietnamese text-to-speech.",
      "authors": [
        "Thi Vu",
        "Linh The Nguyen",
        "Dat Quoc Nguyen"
      ],
      "published": "2025-06-02T05:07:06Z",
      "updated": "2025-06-02T05:07:06Z",
      "categories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01322v1",
      "landing_url": "https://arxiv.org/abs/2506.01322v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01322"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper only introduces a Vietnamese TTS dataset and benchmarks zero-shot models without discussing any discrete audio tokenization/quantization processes or vocabularies, so it fails the inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper only introduces a Vietnamese TTS dataset and benchmarks zero-shot models without discussing any discrete audio tokenization/quantization processes or vocabularies, so it fails the inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion",
    "abstract": "Recent advancements in diffusion models have revolutionized video generation, enabling the creation of high-quality, temporally consistent videos. However, generating high frame-rate (FPS) videos remains a significant challenge due to issues such as flickering and degradation in long sequences, particularly in fast-motion scenarios. Existing methods often suffer from computational inefficiencies and limitations in maintaining video quality over extended frames. In this paper, we present a novel, training-free approach for high FPS video generation using pre-trained diffusion models. Our method, DiffuseSlide, introduces a new pipeline that leverages key frames from low FPS videos and applies innovative techniques, including noise re-injection and sliding window latent denoising, to achieve smooth, consistent video outputs without the need for additional fine-tuning. Through extensive experiments, we demonstrate that our approach significantly improves video quality, offering enhanced temporal coherence and spatial fidelity. The proposed method is not only computationally efficient but also adaptable to various video generation tasks, making it ideal for applications such as virtual reality, video games, and high-quality content creation.",
    "metadata": {
      "arxiv_id": "2506.01454",
      "title": "DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion",
      "summary": "Recent advancements in diffusion models have revolutionized video generation, enabling the creation of high-quality, temporally consistent videos. However, generating high frame-rate (FPS) videos remains a significant challenge due to issues such as flickering and degradation in long sequences, particularly in fast-motion scenarios. Existing methods often suffer from computational inefficiencies and limitations in maintaining video quality over extended frames. In this paper, we present a novel, training-free approach for high FPS video generation using pre-trained diffusion models. Our method, DiffuseSlide, introduces a new pipeline that leverages key frames from low FPS videos and applies innovative techniques, including noise re-injection and sliding window latent denoising, to achieve smooth, consistent video outputs without the need for additional fine-tuning. Through extensive experiments, we demonstrate that our approach significantly improves video quality, offering enhanced temporal coherence and spatial fidelity. The proposed method is not only computationally efficient but also adaptable to various video generation tasks, making it ideal for applications such as virtual reality, video games, and high-quality content creation.",
      "authors": [
        "Geunmin Hwang",
        "Hyun-kyu Ko",
        "Younghyun Kim",
        "Seungryong Lee",
        "Eunbyung Park"
      ],
      "published": "2025-06-02T09:12:41Z",
      "updated": "2025-06-02T09:12:41Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.01454v1",
      "landing_url": "https://arxiv.org/abs/2506.01454v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.01454"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on training-free high-FPS video generation using diffusion on image frames and does not involve discrete audio tokenization, so it fails all inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on training-free high-FPS video generation using diffusion on image frames and does not involve discrete audio tokenization, so it fails all inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs",
    "abstract": "In recent years, there has been a growing focus on fairness and inclusivity within speech technology, particularly in areas such as automatic speech recognition and speech sentiment analysis. When audio is transcoded prior to processing, as is the case in streaming or real-time applications, any inherent bias in the coding mechanism may result in disparities. This not only affects user experience but can also have broader societal implications by perpetuating stereotypes and exclusion. Thus, it is important that audio coding mechanisms are unbiased. In this work, we contribute towards the scarce research with respect to language and gender biases of audio codecs. By analyzing the speech quality of over 2 million multilingual audio files after transcoding through a representative subset of codecs (PSTN, VoIP and neural), our results indicate that PSTN codecs are strongly biased in terms of gender and that neural codecs introduce language biases.",
    "metadata": {
      "arxiv_id": "2506.02545",
      "title": "On the Language and Gender Biases in PSTN, VoIP and Neural Audio Codecs",
      "summary": "In recent years, there has been a growing focus on fairness and inclusivity within speech technology, particularly in areas such as automatic speech recognition and speech sentiment analysis. When audio is transcoded prior to processing, as is the case in streaming or real-time applications, any inherent bias in the coding mechanism may result in disparities. This not only affects user experience but can also have broader societal implications by perpetuating stereotypes and exclusion. Thus, it is important that audio coding mechanisms are unbiased. In this work, we contribute towards the scarce research with respect to language and gender biases of audio codecs. By analyzing the speech quality of over 2 million multilingual audio files after transcoding through a representative subset of codecs (PSTN, VoIP and neural), our results indicate that PSTN codecs are strongly biased in terms of gender and that neural codecs introduce language biases.",
      "authors": [
        "Kemal Altwlkany",
        "Amar Kuric",
        "Emanuel Lacic"
      ],
      "published": "2025-06-03T07:32:42Z",
      "updated": "2025-09-25T12:11:52Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02545v2",
      "landing_url": "https://arxiv.org/abs/2506.02545v2",
      "doi": "https://doi.org/10.21437/Interspeech.2025-481"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper analyzes gender/language bias in PSTN/VoIP/neural codecs without focusing on tokenization, discrete vocabularies, or codec token design/evaluation, so it fails the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper analyzes gender/language bias in PSTN/VoIP/neural codecs without focusing on tokenization, discrete vocabularies, or codec token design/evaluation, so it fails the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
    "abstract": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
    "metadata": {
      "arxiv_id": "2506.02863",
      "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech",
      "summary": "Recent advancements in generative artificial intelligence have significantly transformed the field of style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications remains challenging due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS. To address these gaps, we introduce CapSpeech, a new benchmark designed for a series of CapTTS-related tasks, including style-captioned text-to-speech synthesis with sound events (CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS (EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated audio-caption pairs. In addition, we introduce two new datasets collected and recorded by a professional voice actor and experienced audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside the datasets, we conduct comprehensive experiments using both autoregressive and non-autoregressive models on CapSpeech. Our results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. To the best of our knowledge, CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.",
      "authors": [
        "Helin Wang",
        "Jiarui Hai",
        "Dading Chong",
        "Karan Thakkar",
        "Tiantian Feng",
        "Dongchao Yang",
        "Junhyeok Lee",
        "Thomas Thebaud",
        "Laureano Moro Velazquez",
        "Jesus Villalba",
        "Zengyi Qin",
        "Shrikanth Narayanan",
        "Mounya Elhiali",
        "Najim Dehak"
      ],
      "published": "2025-06-03T13:28:55Z",
      "updated": "2025-09-26T13:07:51Z",
      "categories": [
        "eess.AS",
        "cs.AI",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02863v2",
      "landing_url": "https://arxiv.org/abs/2506.02863v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.02863"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: CapSpeech describes datasets and CapTTS benchmarks without discussing discrete codec/token generation, quantization, or vocabularies, so it fails the inclusion focus; Score 1."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: CapSpeech describes datasets and CapTTS benchmarks without discussing discrete codec/token generation, quantization, or vocabularies, so it fails the inclusion focus; Score 1.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing",
    "abstract": "Neural speech editing enables seamless partial edits to speech utterances, allowing modifications to selected content while preserving the rest of the audio unchanged. This useful technique, however, also poses new risks of deepfakes. To encourage research on detecting such partially edited deepfake speech, we introduce PartialEdit, a deepfake speech dataset curated using advanced neural editing techniques. We explore both detection and localization tasks on PartialEdit. Our experiments reveal that models trained on the existing PartialSpoof dataset fail to detect partially edited speech generated by neural speech editing models. As recent speech editing models almost all involve neural audio codecs, we also provide insights into the artifacts the model learned on detecting these deepfakes. Further information about the PartialEdit dataset and audio samples can be found on the project page: https://yzyouzhang.com/PartialEdit/index.html.",
    "metadata": {
      "arxiv_id": "2506.02958",
      "title": "PartialEdit: Identifying Partial Deepfakes in the Era of Neural Speech Editing",
      "summary": "Neural speech editing enables seamless partial edits to speech utterances, allowing modifications to selected content while preserving the rest of the audio unchanged. This useful technique, however, also poses new risks of deepfakes. To encourage research on detecting such partially edited deepfake speech, we introduce PartialEdit, a deepfake speech dataset curated using advanced neural editing techniques. We explore both detection and localization tasks on PartialEdit. Our experiments reveal that models trained on the existing PartialSpoof dataset fail to detect partially edited speech generated by neural speech editing models. As recent speech editing models almost all involve neural audio codecs, we also provide insights into the artifacts the model learned on detecting these deepfakes. Further information about the PartialEdit dataset and audio samples can be found on the project page: https://yzyouzhang.com/PartialEdit/index.html.",
      "authors": [
        "You Zhang",
        "Baotong Tian",
        "Lin Zhang",
        "Zhiyao Duan"
      ],
      "published": "2025-06-03T14:52:16Z",
      "updated": "2025-06-03T14:52:16Z",
      "categories": [
        "eess.AS",
        "cs.SD"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.02958v1",
      "landing_url": "https://arxiv.org/abs/2506.02958v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.02958"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on detecting partially edited deepfake speech and dataset curation without proposing or analyzing any discrete token/codebook/quantization scheme, so it fails the criterion requiring explicit discrete audio token work."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on detecting partially edited deepfake speech and dataset curation without proposing or analyzing any discrete token/codebook/quantization scheme, so it fails the criterion requiring explicit discrete audio token work.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
    "abstract": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
    "metadata": {
      "arxiv_id": "2506.03077",
      "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
      "summary": "Training language models on long sequence data is a demanding requirement for enhancing the model's capability on complex tasks, e.g., long-chain reasoning. However, as the sequence length scales up, the memory cost for storing activation values becomes huge during the Backpropagation (BP) process, even with the application of gradient checkpointing technique. To tackle this challenge, we propose a memory-efficient and exact BP method called StreamBP, which performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner, significantly reducing the memory cost of activation values and logits. The proposed method is applicable to common objectives such as SFT, GRPO, and DPO. From an implementation perspective, StreamBP achieves less computational FLOPs and faster BP speed by leveraging the causal structure of the language model. Compared to gradient checkpointing, StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger, while using comparable or even less BP time. Note that StreamBP's sequence length scaling ability can be directly transferred to batch size scaling for accelerating training. We further develop a communication-efficient distributed StreamBP to effectively support multi-GPU training and broaden its applicability. Our code can be easily integrated into the training pipeline of any transformer models and is available at https://github.com/Ledzy/StreamBP.",
      "authors": [
        "Qijun Luo",
        "Mengqi Li",
        "Lei Zhao",
        "Xiao Li"
      ],
      "published": "2025-06-03T16:54:15Z",
      "updated": "2025-06-03T16:54:15Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.03077v1",
      "landing_url": "https://arxiv.org/abs/2506.03077v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.03077"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on efficient exact backpropagation for long-context LLM training and does not discuss discrete audio tokenization, tokenizers, quantization, or any audio-based vocabularies, so it fails all inclusion criteria and matches exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on efficient exact backpropagation for long-context LLM training and does not discuss discrete audio tokenization, tokenizers, quantization, or any audio-based vocabularies, so it fails all inclusion criteria and matches exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition",
    "abstract": "Compression-based representations (CBRs) from neural audio codecs such as EnCodec capture intricate acoustic features like pitch and timbre, while representation-learning-based representations (RLRs) from pre-trained models trained for speech representation learning such as WavLM encode high-level semantic and prosodic information. Previous research on Speech Emotion Recognition (SER) has explored both, however, fusion of CBRs and RLRs haven't been explored yet. In this study, we solve this gap and investigate the fusion of RLRs and CBRs and hypothesize they will be more effective by providing complementary information. To this end, we propose, HYFuse, a novel framework that fuses the representations by transforming them to hyperbolic space. With HYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the top performance in comparison to individual representations as well as the homogeneous fusion of RLRs and CBRs and report SOTA.",
    "metadata": {
      "arxiv_id": "2506.03403",
      "title": "HYFuse: Aligning Heterogeneous Speech Pre-Trained Representations in Hyperbolic Space for Speech Emotion Recognition",
      "summary": "Compression-based representations (CBRs) from neural audio codecs such as EnCodec capture intricate acoustic features like pitch and timbre, while representation-learning-based representations (RLRs) from pre-trained models trained for speech representation learning such as WavLM encode high-level semantic and prosodic information. Previous research on Speech Emotion Recognition (SER) has explored both, however, fusion of CBRs and RLRs haven't been explored yet. In this study, we solve this gap and investigate the fusion of RLRs and CBRs and hypothesize they will be more effective by providing complementary information. To this end, we propose, HYFuse, a novel framework that fuses the representations by transforming them to hyperbolic space. With HYFuse, through fusion of x-vector (RLR) and Soundstream (CBR), we achieve the top performance in comparison to individual representations as well as the homogeneous fusion of RLRs and CBRs and report SOTA.",
      "authors": [
        "Orchid Chetia Phukan",
        "Girish",
        "Mohd Mujtaba Akhtar",
        "Swarup Ranjan Behera",
        "Pailla Balakrishna Reddy",
        "Arun Balaji Buduru",
        "Rajesh Sharma"
      ],
      "published": "2025-06-03T21:26:24Z",
      "updated": "2025-06-03T21:26:24Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.03403v1",
      "landing_url": "https://arxiv.org/abs/2506.03403v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.03403"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Study focuses on fusing codec and SSL representations for SER rather than on defining or evaluating discrete token vocabularies/quantizers, so it fails to hit the discrete-token research criteria."
    },
    "round-A_JuniorNano_reasoning": "Study focuses on fusing codec and SSL representations for SER rather than on defining or evaluating discrete token vocabularies/quantizers, so it fails to hit the discrete-token research criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
    "abstract": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
    "metadata": {
      "arxiv_id": "2506.04134",
      "title": "UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation",
      "summary": "Cued Speech (CS) enhances lipreading via hand coding, offering visual phonemic cues that support precise speech perception for the hearing-impaired. The task of CS Video-to-Speech generation (CSV2S) aims to convert CS videos into intelligible speech signals. Most existing research focuses on CS Recognition (CSR), which transcribes video content into text. Consequently, a common solution for CSV2S is to integrate CSR with a text-to-speech (TTS) system. However, this pipeline relies on text as an intermediate medium, which may lead to error propagation and temporal misalignment between speech and CS video dynamics. In contrast, directly generating audio speech from CS video (direct CSV2S) often suffers from the inherent multimodal complexity and the limited availability of CS data. To address these challenges, we propose UniCUE, the first unified framework for CSV2S that directly generates speech from CS videos without relying on intermediate text. The core innovation of UniCUE lies in integrating an understanding task (CSR) that provides fine-grained CS visual-semantic cues to guide speech generation. Specifically, UniCUE incorporates a pose-aware visual processor, a semantic alignment pool that enables precise visual-semantic mapping, and a VisioPhonetic adapter to bridge the understanding and generation tasks within a unified architecture. To support this framework, we construct UniCUE-HI, a large-scale Mandarin CS dataset containing 11282 videos from 14 cuers, including both hearing-impaired and normal-hearing individuals. Extensive experiments on this dataset demonstrate that UniCUE achieves state-of-the-art performance across multiple evaluation metrics.",
      "authors": [
        "Jinting Wang",
        "Shan Yang",
        "Chenxing Li",
        "Dong Yu",
        "Li Liu"
      ],
      "published": "2025-06-04T16:26:49Z",
      "updated": "2025-11-11T03:49:45Z",
      "categories": [
        "cs.CV",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04134v4",
      "landing_url": "https://arxiv.org/abs/2506.04134v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.04134"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "UniCUE focuses on direct video-to-speech generation without describing any discrete token/codebook quantization or tokenizer design, so it fails the discrete audio token inclusion requirements."
    },
    "round-A_JuniorNano_reasoning": "UniCUE focuses on direct video-to-speech generation without describing any discrete token/codebook quantization or tokenizer design, so it fails the discrete audio token inclusion requirements.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bringing Interpretability to Neural Audio Codecs",
    "abstract": "The advent of neural audio codecs has increased in popularity due to their potential for efficiently modeling audio with transformers. Such advanced codecs represent audio from a highly continuous waveform to low-sampled discrete units. In contrast to semantic units, acoustic units may lack interpretability because their training objectives primarily focus on reconstruction performance. This paper proposes a two-step approach to explore the encoding of speech information within the codec tokens. The primary goal of the analysis stage is to gain deeper insight into how speech attributes such as content, identity, and pitch are encoded. The synthesis stage then trains an AnCoGen network for post-hoc explanation of codecs to extract speech attributes from the respective tokens directly.",
    "metadata": {
      "arxiv_id": "2506.04492",
      "title": "Bringing Interpretability to Neural Audio Codecs",
      "summary": "The advent of neural audio codecs has increased in popularity due to their potential for efficiently modeling audio with transformers. Such advanced codecs represent audio from a highly continuous waveform to low-sampled discrete units. In contrast to semantic units, acoustic units may lack interpretability because their training objectives primarily focus on reconstruction performance. This paper proposes a two-step approach to explore the encoding of speech information within the codec tokens. The primary goal of the analysis stage is to gain deeper insight into how speech attributes such as content, identity, and pitch are encoded. The synthesis stage then trains an AnCoGen network for post-hoc explanation of codecs to extract speech attributes from the respective tokens directly.",
      "authors": [
        "Samir Sadok",
        "Julien Hauret",
        "Éric Bavu"
      ],
      "published": "2025-06-04T22:17:16Z",
      "updated": "2025-06-04T22:17:16Z",
      "categories": [
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04492v1",
      "landing_url": "https://arxiv.org/abs/2506.04492v1",
      "doi": "https://doi.org/10.21437/Interspeech.2025-115"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The work analyzes neural audio codec discrete codes, including interpretability of speech attributes and a post-hoc token attribute extractor, so it matches the discrete audio token focus; Score:4."
    },
    "round-A_JuniorNano_reasoning": "The work analyzes neural audio codec discrete codes, including interpretability of speech attributes and a post-hoc token attribute extractor, so it matches the discrete audio token focus; Score:4.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Kernel $k$-Medoids as General Vector Quantization",
    "abstract": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
    "metadata": {
      "arxiv_id": "2506.04786",
      "title": "Kernel $k$-Medoids as General Vector Quantization",
      "summary": "Vector Quantization (VQ) is a widely used technique in machine learning and data compression, valued for its simplicity and interpretability. Among hard VQ methods, $k$-medoids clustering and Kernel Density Estimation (KDE) approaches represent two prominent yet seemingly unrelated paradigms -- one distance-based, the other rooted in probability density matching. In this paper, we investigate their connection through the lens of Quadratic Unconstrained Binary Optimization (QUBO). We compare a heuristic QUBO formulation for $k$-medoids, which balances centrality and diversity, with a principled QUBO derived from minimizing Maximum Mean Discrepancy in KDE-based VQ. Surprisingly, we show that the KDE-QUBO is a special case of the $k$-medoids-QUBO under mild assumptions on the kernel's feature map. This reveals a deeper structural relationship between these two approaches and provides new insight into the geometric interpretation of the weighting parameters used in QUBO formulations for VQ.",
      "authors": [
        "Thore Gerlach",
        "Sascha Mücke",
        "Christian Bauckhage"
      ],
      "published": "2025-06-05T09:14:25Z",
      "updated": "2025-09-04T20:11:34Z",
      "categories": [
        "cs.LG",
        "quant-ph"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.04786v2",
      "landing_url": "https://arxiv.org/abs/2506.04786v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.04786"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on general kernel k-medoids/vector quantization without any discrete audio tokenization, codec, or semantic token context, so it fails the discrete audio token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on general kernel k-medoids/vector quantization without any discrete audio tokenization, codec, or semantic token context, so it fails the discrete audio token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models",
    "abstract": "Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.",
    "metadata": {
      "arxiv_id": "2506.05140",
      "title": "AudioLens: A Closer Look at Auditory Attribute Perception of Large Audio-Language Models",
      "summary": "Understanding the internal mechanisms of large audio-language models (LALMs) is crucial for interpreting their behavior and improving performance. This work presents the first in-depth analysis of how LALMs internally perceive and recognize auditory attributes. By applying vocabulary projection on three state-of-the-art LALMs, we track how attribute information evolves across layers and token positions. We find that attribute information generally decreases with layer depth when recognition fails, and that resolving attributes at earlier layers correlates with better accuracy. Moreover, LALMs heavily rely on querying auditory inputs for predicting attributes instead of aggregating necessary information in hidden states at attribute-mentioning positions. Based on our findings, we demonstrate a method to enhance LALMs. Our results offer insights into auditory attribute processing, paving the way for future improvements.",
      "authors": [
        "Chih-Kai Yang",
        "Neo Ho",
        "Yi-Jyun Lee",
        "Hung-yi Lee"
      ],
      "published": "2025-06-05T15:22:47Z",
      "updated": "2025-08-24T16:16:18Z",
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05140v2",
      "landing_url": "https://arxiv.org/abs/2506.05140v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05140"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies how large audio-language models perceive attributes rather than proposing or evaluating discrete audio tokens/tokenizers, so it fails the requirement that the core research object be discrete token sequences and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "The paper studies how large audio-language models perceive attributes rather than proposing or evaluating discrete audio tokens/tokenizers, so it fails the requirement that the core research object be discrete token sequences and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
    "abstract": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
    "metadata": {
      "arxiv_id": "2506.05432",
      "title": "PCDVQ: Enhancing Vector Quantization for Large Language Models via Polar Coordinate Decoupling",
      "summary": "Large Language Models (LLMs) face significant challenges in edge deployment due to their massive parameter scale. Vector Quantization (VQ), a clustering-based quantization method, serves as a prevalent solution to this issue for its extremely low-bit (even at 2-bit) and considerable accuracy. Since a vector is a quantity in mathematics and physics that has both direction and magnitude, existing VQ works typically quantize them in a coupled manner. However, we find that direction exhibits significantly greater sensitivity to quantization compared to the magnitude. For instance, when separately clustering the directions and magnitudes of weight vectors in LLaMA-2-7B, the accuracy drop of zero-shot tasks are 46.5\\% and 2.3\\%, respectively. This gap even increases with the reduction of clustering centers. Further, Euclidean distance, a common metric to access vector similarities in current VQ works, places greater emphasis on reducing the magnitude error. This property is contrary to the above finding, unavoidably leading to larger quantization errors. To these ends, this paper proposes Polar Coordinate Decoupled Vector Quantization (PCDVQ), an effective and efficient VQ framework consisting of two key modules: 1) Polar Coordinate Decoupling (PCD), which transforms vectors into their polar coordinate representations and perform independent quantization of the direction and magnitude parameters.2) Distribution Aligned Codebook Construction (DACC), which optimizes the direction and magnitude codebooks in accordance with the source distribution. Experimental results show that PCDVQ outperforms baseline methods at 2-bit level by at least 1.5\\% zero-shot accuracy, establishing a novel paradigm for accurate and highly compressed LLMs.",
      "authors": [
        "Yuxuan Yue",
        "Zukang Xu",
        "Zhihang Yuan",
        "Dawei Yang",
        "Jianlong Wu",
        "Liqiang Nie"
      ],
      "published": "2025-06-05T08:58:58Z",
      "updated": "2025-06-26T06:17:49Z",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05432v2",
      "landing_url": "https://arxiv.org/abs/2506.05432v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.05432"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "No discrete audio tokens or tokenizer design—just LLM weight quantization—so the work fails the audio token inclusion criteria and should be excluded."
    },
    "round-A_JuniorNano_reasoning": "No discrete audio tokens or tokenizer design—just LLM weight quantization—so the work fails the audio token inclusion criteria and should be excluded.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation",
    "abstract": "Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.",
    "metadata": {
      "arxiv_id": "2506.05952",
      "title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation",
      "summary": "Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.",
      "authors": [
        "Dongjie Fu",
        "Tengjiao Sun",
        "Pengcheng Fang",
        "Xiaohao Cai",
        "Hansung Kim"
      ],
      "published": "2025-06-06T10:26:54Z",
      "updated": "2026-01-13T11:39:54Z",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.05952v3",
      "landing_url": "https://arxiv.org/abs/2506.05952v3",
      "doi": "https://doi.org/10.48550/arXiv.2506.05952"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "This work targets 3D human motion generation without describing any discrete audio tokenization, codec, or quantization mechanisms, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "This work targets 3D human motion generation without describing any discrete audio tokenization, codec, or quantization mechanisms, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?",
    "abstract": "Quantization is essential for deploying large audio language models (LALMs) efficiently in resource-constrained environments. However, its impact on complex tasks, such as zero-shot audio spoofing detection, remains underexplored. This study evaluates the zero-shot capabilities of five LALMs, GAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct datasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their robustness to quantization (FP32, FP16, INT8). Despite high initial spoof detection accuracy, our analysis demonstrates severe predictive biases toward spoof classification across all models, rendering their practical performance equivalent to random classification. Interestingly, quantization to FP16 precision resulted in negligible performance degradation compared to FP32, effectively halving memory and computational requirements without materially impacting accuracy. However, INT8 quantization intensified model biases, significantly degrading balanced accuracy. These findings highlight critical architectural limitations and emphasize FP16 quantization as an optimal trade-off, providing guidelines for practical deployment and future model refinement.",
    "metadata": {
      "arxiv_id": "2506.06756",
      "title": "Can Quantized Audio Language Models Perform Zero-Shot Spoofing Detection?",
      "summary": "Quantization is essential for deploying large audio language models (LALMs) efficiently in resource-constrained environments. However, its impact on complex tasks, such as zero-shot audio spoofing detection, remains underexplored. This study evaluates the zero-shot capabilities of five LALMs, GAMA, LTU-AS, MERaLiON, Qwen-Audio, and SALMONN, across three distinct datasets: ASVspoof2019, In-the-Wild, and WaveFake, and investigates their robustness to quantization (FP32, FP16, INT8). Despite high initial spoof detection accuracy, our analysis demonstrates severe predictive biases toward spoof classification across all models, rendering their practical performance equivalent to random classification. Interestingly, quantization to FP16 precision resulted in negligible performance degradation compared to FP32, effectively halving memory and computational requirements without materially impacting accuracy. However, INT8 quantization intensified model biases, significantly degrading balanced accuracy. These findings highlight critical architectural limitations and emphasize FP16 quantization as an optimal trade-off, providing guidelines for practical deployment and future model refinement.",
      "authors": [
        "Bikash Dutta",
        "Rishabh Ranjan",
        "Shyam Sathvik",
        "Mayank Vatsa",
        "Richa Singh"
      ],
      "published": "2025-06-07T10:56:33Z",
      "updated": "2025-06-07T10:56:33Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06756v1",
      "landing_url": "https://arxiv.org/abs/2506.06756v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.06756"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper simply evaluates existing quantized audio language models for zero-shot spoofing detection and their bias/performance trade-offs, without proposing or analyzing discrete audio-token/tokenizer specifications or vocabularies, so it fails to meet the discrete-token-focused inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper simply evaluates existing quantized audio language models for zero-shot spoofing detection and their bias/performance trade-offs, without proposing or analyzing discrete audio-token/tokenizer specifications or vocabularies, so it fails to meet the discrete-token-focused inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "DONUT: A Decoder-Only Model for Trajectory Prediction",
    "abstract": "Predicting the motion of other agents in a scene is highly relevant for autonomous driving, as it allows a self-driving car to anticipate. Inspired by the success of decoder-only models for language modeling, we propose DONUT, a Decoder-Only Network for Unrolling Trajectories. Unlike existing encoder-decoder forecasting models, we encode historical trajectories and predict future trajectories with a single autoregressive model. This allows the model to make iterative predictions in a consistent manner, and ensures that the model is always provided with up-to-date information, thereby enhancing performance. Furthermore, inspired by multi-token prediction for language modeling, we introduce an 'overprediction' strategy that gives the model the auxiliary task of predicting trajectories at longer temporal horizons. This allows the model to better anticipate the future and further improves performance. Through experiments, we demonstrate that our decoder-only approach outperforms the encoder-decoder baseline, and achieves new state-of-the-art results on the Argoverse 2 single-agent motion forecasting benchmark.",
    "metadata": {
      "arxiv_id": "2506.06854",
      "title": "DONUT: A Decoder-Only Model for Trajectory Prediction",
      "summary": "Predicting the motion of other agents in a scene is highly relevant for autonomous driving, as it allows a self-driving car to anticipate. Inspired by the success of decoder-only models for language modeling, we propose DONUT, a Decoder-Only Network for Unrolling Trajectories. Unlike existing encoder-decoder forecasting models, we encode historical trajectories and predict future trajectories with a single autoregressive model. This allows the model to make iterative predictions in a consistent manner, and ensures that the model is always provided with up-to-date information, thereby enhancing performance. Furthermore, inspired by multi-token prediction for language modeling, we introduce an 'overprediction' strategy that gives the model the auxiliary task of predicting trajectories at longer temporal horizons. This allows the model to better anticipate the future and further improves performance. Through experiments, we demonstrate that our decoder-only approach outperforms the encoder-decoder baseline, and achieves new state-of-the-art results on the Argoverse 2 single-agent motion forecasting benchmark.",
      "authors": [
        "Markus Knoche",
        "Daan de Geus",
        "Bastian Leibe"
      ],
      "published": "2025-06-07T16:24:29Z",
      "updated": "2025-08-01T14:07:37Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.06854v2",
      "landing_url": "https://arxiv.org/abs/2506.06854v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.06854"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Reasoning: This study centers on trajectory prediction for driving agents and does not address discrete audio-token generation, so it fails to meet the audio-token inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "Reasoning: This study centers on trajectory prediction for driving agents and does not address discrete audio-token generation, so it fails to meet the audio-token inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
    "abstract": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
    "metadata": {
      "arxiv_id": "2506.07081",
      "title": "Streaming Endpointer for Spoken Dialogue using Neural Audio Codecs and Label-Delayed Training",
      "summary": "Accurate, low-latency endpointing is crucial for effective spoken dialogue systems. While traditional endpointers often rely on spectrum-based audio features, this work proposes real-time speech endpointing for multi-turn dialogues using streaming, low-bitrate Neural Audio Codec (NAC) features, building upon recent advancements in neural audio codecs. To further reduce cutoff errors, we introduce a novel label delay training scheme. At a fixed median latency of 160 ms, our combined NAC and label delay approach achieves significant relative cutoff error reductions: 42.7% for a single-stream endpointer and 37.5% for a two-stream configuration, compared to baseline methods. Finally, we demonstrate efficient integration with a codec-based pretrained speech large language model, improving its median response time by 1200 ms and reducing its cutoff error by 35%.",
      "authors": [
        "Sathvik Udupa",
        "Shinji Watanabe",
        "Petr Schwarz",
        "Jan Cernocky"
      ],
      "published": "2025-06-08T10:54:23Z",
      "updated": "2025-06-19T09:40:25Z",
      "categories": [
        "cs.SD",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07081v2",
      "landing_url": "https://arxiv.org/abs/2506.07081v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07081"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 2,
      "reasoning": "While the paper leverages neural audio codec features for endpointing, it focuses on latency/error reduction and lacks any discrete token/tokenizer design, quantization specs, or token-level evaluation, so it better be excluded under the discrete-audio-token criteria."
    },
    "round-A_JuniorNano_reasoning": "While the paper leverages neural audio codec features for endpointing, it focuses on latency/error reduction and lacks any discrete token/tokenizer design, quantization specs, or token-level evaluation, so it better be excluded under the discrete-audio-token criteria.",
    "round-A_JuniorNano_evaluation": 2,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:2)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:2)"
  },
  {
    "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding",
    "abstract": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and answer questions about the audio. While prior LALMs have shown strong performance on standard benchmarks, there has been alarming evidence that LALMs can hallucinate what is presented in the audio. To mitigate the hallucination of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time strategy that uses contrastive decoding to compare the token prediction logits with and without the audio context. By contrastive decoding, AAD promotes the tokens whose probability increases when the audio is present. We conduct our experiment on object hallucination datasets with three LALMs and show that AAD improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We conduct thorough ablation studies to understand the effectiveness of each component in AAD.",
    "metadata": {
      "arxiv_id": "2506.07233",
      "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding",
      "summary": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and answer questions about the audio. While prior LALMs have shown strong performance on standard benchmarks, there has been alarming evidence that LALMs can hallucinate what is presented in the audio. To mitigate the hallucination of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time strategy that uses contrastive decoding to compare the token prediction logits with and without the audio context. By contrastive decoding, AAD promotes the tokens whose probability increases when the audio is present. We conduct our experiment on object hallucination datasets with three LALMs and show that AAD improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We conduct thorough ablation studies to understand the effectiveness of each component in AAD.",
      "authors": [
        "Tzu-wen Hsu",
        "Ke-Han Lu",
        "Cheng-Han Chiang",
        "Hung-yi Lee"
      ],
      "published": "2025-06-08T17:36:50Z",
      "updated": "2025-09-12T18:49:28Z",
      "categories": [
        "eess.AS",
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07233v2",
      "landing_url": "https://arxiv.org/abs/2506.07233v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.07233"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on audio-aware decoding for hallucination reduction in LALMs rather than on discrete audio-token generation/quantization or token-level modeling as required, so it fails the inclusion and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on audio-aware decoding for hallucination reduction in LALMs rather than on discrete audio-token generation/quantization or token-level modeling as required, so it fails the inclusion and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Bit-level BPE: Below the byte boundary",
    "abstract": "Byte-level fallbacks for subword tokenization have become a common practice in large language models. In particular, it has been demonstrated to be incredibly effective as a pragmatic solution for preventing OOV, especially in the context of larger models. However, breaking a character down to individual bytes significantly increases the sequence length for long-tail tokens in languages such as Chinese, Japanese, and Korean (CJK) and other character-diverse contexts such as emoji. The increased sequence length results in longer computation during both training and inference. In this work, we propose a simple compression technique that reduces the sequence length losslessly.",
    "metadata": {
      "arxiv_id": "2506.07541",
      "title": "Bit-level BPE: Below the byte boundary",
      "summary": "Byte-level fallbacks for subword tokenization have become a common practice in large language models. In particular, it has been demonstrated to be incredibly effective as a pragmatic solution for preventing OOV, especially in the context of larger models. However, breaking a character down to individual bytes significantly increases the sequence length for long-tail tokens in languages such as Chinese, Japanese, and Korean (CJK) and other character-diverse contexts such as emoji. The increased sequence length results in longer computation during both training and inference. In this work, we propose a simple compression technique that reduces the sequence length losslessly.",
      "authors": [
        "Sangwhan Moon",
        "Tatsuya Hiraoka",
        "Naoaki Okazaki"
      ],
      "published": "2025-06-09T08:28:16Z",
      "updated": "2025-06-09T08:28:16Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.07541v1",
      "landing_url": "https://arxiv.org/abs/2506.07541v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.07541"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe byte-level BPE for text tokenization and compression, not discrete audio tokens, so does not meet any inclusion criteria and triggers exclusion immediately."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe byte-level BPE for text tokenization and compression, not discrete audio tokens, so does not meet any inclusion criteria and triggers exclusion immediately.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes",
    "abstract": "This study builds on the architecture of the Disentangler of Visual Priors (DVP), a type of autoencoder that learns to interpret scenes by decomposing the perceived objects into independent visual aspects of shape, size, orientation, and color appearance. These aspects are expressed as latent parameters which control a differentiable renderer that performs image reconstruction, so that the model can be trained end-to-end with gradient using reconstruction loss. In this study, we extend the original DVP so that it can handle multiple objects in a scene. We also exploit the interpretability of its latent by using the decoder to sample additional training examples and devising alternative training modes that rely on loss functions defined not only in the image space, but also in the latent space. This significantly facilitates training, which is otherwise challenging due to the presence of extensive plateaus in the image-space reconstruction loss. To examine the performance of this approach, we propose a new benchmark featuring multiple 2D objects, which subsumes the previously proposed Multi-dSprites dataset while being more parameterizable. We compare the DVP extended in these ways with two baselines (MONet and LIVE) and demonstrate its superiority in terms of reconstruction quality and capacity to decompose overlapping objects. We also analyze the gradients induced by the considered loss functions, explain how they impact the efficacy of training, and discuss the limitations of differentiable rendering in autoencoders and the ways in which they can be addressed.",
    "metadata": {
      "arxiv_id": "2506.08191",
      "title": "Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes",
      "summary": "This study builds on the architecture of the Disentangler of Visual Priors (DVP), a type of autoencoder that learns to interpret scenes by decomposing the perceived objects into independent visual aspects of shape, size, orientation, and color appearance. These aspects are expressed as latent parameters which control a differentiable renderer that performs image reconstruction, so that the model can be trained end-to-end with gradient using reconstruction loss. In this study, we extend the original DVP so that it can handle multiple objects in a scene. We also exploit the interpretability of its latent by using the decoder to sample additional training examples and devising alternative training modes that rely on loss functions defined not only in the image space, but also in the latent space. This significantly facilitates training, which is otherwise challenging due to the presence of extensive plateaus in the image-space reconstruction loss. To examine the performance of this approach, we propose a new benchmark featuring multiple 2D objects, which subsumes the previously proposed Multi-dSprites dataset while being more parameterizable. We compare the DVP extended in these ways with two baselines (MONet and LIVE) and demonstrate its superiority in terms of reconstruction quality and capacity to decompose overlapping objects. We also analyze the gradients induced by the considered loss functions, explain how they impact the efficacy of training, and discuss the limitations of differentiable rendering in autoencoders and the ways in which they can be addressed.",
      "authors": [
        "Antoni Nowinowski",
        "Krzysztof Krawiec"
      ],
      "published": "2025-06-09T20:06:42Z",
      "updated": "2025-06-09T20:06:42Z",
      "categories": [
        "cs.CV"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.08191v1",
      "landing_url": "https://arxiv.org/abs/2506.08191v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.08191"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper studies differentiable visual object decomposition in autoencoders and has no connection to discrete audio token generation, quantization, or codec/tokenizer design, so it fails to meet the inclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper studies differentiable visual object decomposition in autoencoders and has no connection to discrete audio token generation, quantization, or codec/tokenizer design, so it fails to meet the inclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
    "abstract": "Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.",
    "metadata": {
      "arxiv_id": "2506.08210",
      "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
      "summary": "Both text-to-image generation and large language models (LLMs) have made significant advancements. However, many text-to-image models still employ the somewhat outdated T5 and CLIP as their text encoders. In this work, we investigate the effectiveness of using modern decoder-only LLMs as text encoders for text-to-image diffusion models. We build a standardized training and evaluation pipeline that allows us to isolate and evaluate the effect of different text embeddings. We train a total of 27 text-to-image models with 12 different text encoders to analyze the critical aspects of LLMs that could impact text-to-image generation, including the approaches to extract embeddings, different LLMs variants, and model sizes. Our experiments reveal that the de facto way of using last-layer embeddings as conditioning leads to inferior performance. Instead, we explore embeddings from various layers and find that using layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs with this conditioning outperform the baseline T5 model, showing enhanced performance in advanced visio-linguistic reasoning skills.",
      "authors": [
        "Andrew Z. Wang",
        "Songwei Ge",
        "Tero Karras",
        "Ming-Yu Liu",
        "Yogesh Balaji"
      ],
      "published": "2025-06-09T20:29:53Z",
      "updated": "2025-06-09T20:29:53Z",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.08210v1",
      "landing_url": "https://arxiv.org/abs/2506.08210v1",
      "doi": "https://doi.org/10.48550/arXiv.2506.08210"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "The paper focuses on decoder-only LLMs for text-to-image generation and does not involve discrete audio tokens, tokenizers, or quantization, so it fails the inclusion and meets exclusion criteria."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on decoder-only LLMs for text-to-image generation and does not involve discrete audio tokens, tokenizers, or quantization, so it fails the inclusion and meets exclusion criteria.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  },
  {
    "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model",
    "abstract": "Large Audio-Language Models (LALMs) have significantly advanced intelligent human-computer interaction, yet their reliance on text-based outputs limits their ability to generate natural speech responses directly, hindering seamless audio interactions. To address this, we introduce Step-Audio-AQAA, a fully end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model integrates a dual-codebook audio tokenizer for linguistic and semantic feature extraction, a 130-billion-parameter backbone LLM and a neural vocoder for high-fidelity speech synthesis. Our post-training approach employs interleaved token-output of text and audio to enhance semantic coherence and combines Direct Preference Optimization (DPO) with model merge to improve performance. Evaluations on the StepEval-Audio-360 benchmark demonstrate that Step-Audio-AQAA excels especially in speech control, outperforming the state-of-art LALMs in key areas. This work contributes a promising solution for end-to-end LALMs and highlights the critical role of token-based vocoder in enhancing overall performance for AQAA tasks.",
    "metadata": {
      "arxiv_id": "2506.08967",
      "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model",
      "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent human-computer interaction, yet their reliance on text-based outputs limits their ability to generate natural speech responses directly, hindering seamless audio interactions. To address this, we introduce Step-Audio-AQAA, a fully end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model integrates a dual-codebook audio tokenizer for linguistic and semantic feature extraction, a 130-billion-parameter backbone LLM and a neural vocoder for high-fidelity speech synthesis. Our post-training approach employs interleaved token-output of text and audio to enhance semantic coherence and combines Direct Preference Optimization (DPO) with model merge to improve performance. Evaluations on the StepEval-Audio-360 benchmark demonstrate that Step-Audio-AQAA excels especially in speech control, outperforming the state-of-art LALMs in key areas. This work contributes a promising solution for end-to-end LALMs and highlights the critical role of token-based vocoder in enhancing overall performance for AQAA tasks.",
      "authors": [
        "Ailin Huang",
        "Bingxin Li",
        "Bruce Wang",
        "Boyong Wu",
        "Chao Yan",
        "Chengli Feng",
        "Heng Wang",
        "Hongyu Zhou",
        "Hongyuan Wang",
        "Jingbei Li",
        "Jianjian Sun",
        "Joanna Wang",
        "Mingrui Chen",
        "Peng Liu",
        "Ruihang Miao",
        "Shilei Jiang",
        "Tian Fei",
        "Wang You",
        "Xi Chen",
        "Xuerui Yang",
        "Yechang Huang",
        "Yuxiang Zhang",
        "Zheng Ge",
        "Zheng Gong",
        "Zhewei Huang",
        "Zixin Zhang",
        "Bin Wang",
        "Bo Li",
        "Buyun Ma",
        "Changxin Miao",
        "Changyi Wan",
        "Chen Xu",
        "Dapeng Shi",
        "Dingyuan Hu",
        "Enle Liu",
        "Guanzhe Huang",
        "Gulin Yan",
        "Hanpeng Hu",
        "Haonan Jia",
        "Jiahao Gong",
        "Jiaoren Wu",
        "Jie Wu",
        "Jie Yang",
        "Junzhe Lin",
        "Kaixiang Li",
        "Lei Xia",
        "Longlong Gu",
        "Ming Li",
        "Nie Hao",
        "Ranchen Ming",
        "Shaoliang Pang",
        "Siqi Liu",
        "Song Yuan",
        "Tiancheng Cao",
        "Wen Li",
        "Wenqing He",
        "Xu Zhao",
        "Xuelin Zhang",
        "Yanbo Yu",
        "Yinmin Zhong",
        "Yu Zhou",
        "Yuanwei Liang",
        "Yuanwei Lu",
        "Yuxiang Yang",
        "Zidong Yang",
        "Zili Zhang",
        "Binxing Jiao",
        "Heung-Yeung Shum",
        "Jiansheng Chen",
        "Jing Li",
        "Xiangyu Zhang",
        "Xinhao Zhang",
        "Yibo Zhu",
        "Daxin Jiang",
        "Shuchang Zhou",
        "Chen Hu"
      ],
      "published": "2025-06-10T16:37:39Z",
      "updated": "2025-06-13T10:07:42Z",
      "categories": [
        "cs.SD",
        "cs.CL",
        "eess.AS"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.08967v2",
      "landing_url": "https://arxiv.org/abs/2506.08967v2",
      "doi": "https://doi.org/10.48550/arXiv.2506.08967"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 5,
      "reasoning": "Step-Audio-AQAA clearly centers on discrete audio tokens via its dual-codebook tokenizer and neural vocoder quantization in an end-to-end AQAA setup, satisfying the inclusion wording about token/codec design and evaluations, so it should be included."
    },
    "round-A_JuniorNano_reasoning": "Step-Audio-AQAA clearly centers on discrete audio tokens via its dual-codebook tokenizer and neural vocoder quantization in an end-to-end AQAA setup, satisfying the inclusion wording about token/codec design and evaluations, so it should be included.",
    "round-A_JuniorNano_evaluation": 5,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:5)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
    "abstract": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
    "metadata": {
      "arxiv_id": "2506.09349",
      "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
      "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
      "authors": [
        "Chao-Hong Tan",
        "Qian Chen",
        "Wen Wang",
        "Chong Deng",
        "Qinglin Zhang",
        "Luyao Cheng",
        "Hai Yu",
        "Xin Zhang",
        "Xiang Lv",
        "Tianyu Zhao",
        "Chong Zhang",
        "Yukun Ma",
        "Yafeng Chen",
        "Hui Wang",
        "Jiaqing Liu",
        "Xiangang Li",
        "Jieping Ye"
      ],
      "published": "2025-06-11T02:57:22Z",
      "updated": "2025-12-23T08:50:59Z",
      "categories": [
        "cs.CL"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09349v4",
      "landing_url": "https://arxiv.org/abs/2506.09349v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09349"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 4,
      "reasoning": "The paper focuses on joint autoregressive modeling of discrete speech tokens via dual-resolution representations for speech-text generation, which matches the discrete audio token research scope and reports benchmark results, so it leans toward inclusion."
    },
    "round-A_JuniorNano_reasoning": "The paper focuses on joint autoregressive modeling of discrete speech tokens via dual-resolution representations for speech-text generation, which matches the discrete audio token research scope and reports benchmark results, so it leans toward inclusion.",
    "round-A_JuniorNano_evaluation": 4,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "include (junior:4)",
    "review_skipped": false,
    "discard_reason": null
  },
  {
    "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements",
    "abstract": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements, identifying their start and stop times, directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases, therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3), are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 308 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s across tasks, within typical rater tolerance for timestamp review, enabling practical fidelity QC. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a privacy-preserving, scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.",
    "metadata": {
      "arxiv_id": "2506.09707",
      "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements",
      "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements, identifying their start and stop times, directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases, therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3), are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 308 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s across tasks, within typical rater tolerance for timestamp review, enabling practical fidelity QC. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a privacy-preserving, scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.",
      "authors": [
        "Suhas BN",
        "Andrew M. Sherrill",
        "Jyoti Alaparthi",
        "Dominik Mattioli",
        "Rosa I. Arriaga",
        "Chris W. Wiese",
        "Saeed Abdullah"
      ],
      "published": "2025-06-11T13:21:06Z",
      "updated": "2025-12-19T07:32:25Z",
      "categories": [
        "eess.AS",
        "cs.CL",
        "cs.HC"
      ],
      "pdf_url": "https://arxiv.org/pdf/2506.09707v4",
      "landing_url": "https://arxiv.org/abs/2506.09707v4",
      "doi": "https://doi.org/10.48550/arXiv.2506.09707"
    },
    "round-A_JuniorNano_output": {
      "evaluation": 1,
      "reasoning": "Title/abstract describe temporal localization of therapy fidelity using LoRA on large audio-language model, not discrete audio token/codebook/tokenizer development, so it fails all inclusion criteria and matches exclusion."
    },
    "round-A_JuniorNano_reasoning": "Title/abstract describe temporal localization of therapy fidelity using LoRA on large audio-language model, not discrete audio token/codebook/tokenizer development, so it fails all inclusion criteria and matches exclusion.",
    "round-A_JuniorNano_evaluation": 1,
    "round-A_JuniorMini_output": null,
    "round-A_JuniorMini_reasoning": null,
    "round-A_JuniorMini_evaluation": null,
    "round-B_SeniorLead_output": null,
    "round-B_SeniorLead_reasoning": null,
    "round-B_SeniorLead_evaluation": null,
    "final_verdict": "exclude (junior:1)",
    "review_skipped": false,
    "discard_reason": "exclude (junior:1)"
  }
]